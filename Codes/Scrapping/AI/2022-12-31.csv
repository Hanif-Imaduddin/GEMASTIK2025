title,authors,abstract,submitted_date,pdf_link
Introducing Political Ecology of Creative-Ai,Andre Holzapfel,"This chapter introduces the perspective of political ecology to the application of artificial intelligence to artistic processes (Creative-Ai). Hence, the environmental and social impact of the development and employment of Creative-Ai are the focus of this text, when we consider them as part of an economic system that transforms artistic creation to a commodity. I first analyse specific Creative-Ai cases, and then conduct a speculation that takes Jacques Attali's writing on the role of music in society as a vantage point, and investigates the environmental and social consequences of an automatic composition network controlled by a large music streaming platform. Whereas the possibilities that emerge from Creative-Ai may be promising from an artistic perspective, its entanglement with corporate interest raises severe concerns. These concerns can only be addressed by a wide cross-sectoral alliance between research and arts that develops a critical perspective on the future directions of Creative-Ai. △ Less","21 December, 2022",https://arxiv.org/pdf/2301.10233
Methodological reflections for AI alignment research using human feedback,Thilo Hagendorff;Sarah Fabi,"The field of artificial intelligence (AI) alignment aims to investigate whether AI technologies align with human interests and values and function in a safe and ethical manner. AI alignment is particularly relevant for large language models (LLMs), which have the potential to exhibit unintended behavior due to their ability to learn and adapt in ways that are difficult to predict. In this paper, we discuss methodological challenges for the alignment problem specifically in the context of LLMs trained to summarize texts. In particular, we focus on methods for collecting reliable human feedback on summaries to train a reward model which in turn improves the summarization model. We conclude by suggesting specific improvements in the experimental design of alignment studies for LLMs' summarization capabilities. △ Less","22 December, 2022",https://arxiv.org/pdf/2301.06859
The problem with AI consciousness: A neurogenetic case against synthetic sentience,Yoshija Walter;Lukas Zbinden,"Ever since the creation of the first artificial intelligence (AI) machinery built on machine learning (ML), public society has entertained the idea that eventually computers could become sentient and develop a consciousness of their own. As these models now get increasingly better and convincingly more anthropomorphic, even some engineers have started to believe that AI might become conscious, which would result in serious social consequences. The present paper argues against the plausibility of sentient AI primarily based on the theory of neurogenetic structuralism, which claims that the physiology of biological neurons and their structural organization into complex brains are necessary prerequisites for true consciousness to emerge. △ Less","7 December, 2022",https://arxiv.org/pdf/2301.05397
Need of 6G for the Metaverse Realization,Bartlomiej Siniarski;Chamitha De Alwis;Gokul Yenduri;Thien Huynh-The;GÜrkan GÜr;Thippa Reddy Gadekallu;Madhusanka Liyanage,"The concept of the Metaverse aims to bring a fully-fledged extended reality environment to provide next generation applications and services. Development of the Metaverse is backed by many technologies, including, 5G, artificial intelligence, edge computing and extended reality. The advent of 6G is envisaged to mark a significant milestone in the development of the Metaverse, facilitating near-zero-latency, a plethora of new services and upgraded real-world infrastructure. This paper establishes the advantages of providing the Metaverse services over 6G along with an overview of the demanded technical requirements. The paper provides an insight to the concepts of the Metaverse and the envisaged technical capabilities of 6G mobile networks. Then, the technical aspects covering 6G for the development of the Metaverse, ranging from validating digital assets, interoperability, and efficient user interaction in the Metaverse to related security and privacy aspects are elaborated. Subsequently, the role of 6G technologies towards enabling the Metaverse, including artificial intelligence, blockchain, open radio access networks, edge computing, cloudification and internet of everything. The paper also presents 6G integration challenges and outlines ongoing projects towards developing the Metaverse technologies to facilitate the Metaverse applications and services. △ Less","28 December, 2022",https://arxiv.org/pdf/2301.03386
Deep leakage from gradients,Yaqiong Mu,"With the development of artificial intelligence technology, Federated Learning (FL) model has been widely used in many industries for its high efficiency and confidentiality. Some researchers have explored its confidentiality and designed some algorithms to attack training data sets, but these algorithms all have their own limitations. Therefore, most people still believe that local machine learning gradient information is safe and reliable. In this paper, an algorithm based on gradient features is designed to attack the federated learning model in order to attract more attention to the security of federated learning systems. In federated learning system, gradient contains little information compared with the original training data set, but this project intends to restore the original training image data through gradient information. Convolutional Neural Network (CNN) has excellent performance in image processing. Therefore, the federated learning model of this project is equipped with Convolutional Neural Network structure, and the model is trained by using image data sets. The algorithm calculates the virtual gradient by generating virtual image labels. Then the virtual gradient is matched with the real gradient to restore the original image. This attack algorithm is written in Python language, uses cat and dog classification Kaggle data sets, and gradually extends from the full connection layer to the convolution layer, thus improving the universality. At present, the average squared error between the data recovered by this algorithm and the original image information is approximately 5, and the vast majority of images can be completely restored according to the gradient information given, indicating that the gradient of federated learning system is not absolutely safe and reliable. △ Less","15 December, 2022",https://arxiv.org/pdf/2301.02621
A Survey on Protein Representation Learning: Retrospect and Prospect,Lirong Wu;Yufei Huang;Haitao Lin;Stan Z. Li,"Proteins are fundamental biological entities that play a key role in life activities. The amino acid sequences of proteins can be folded into stable 3D structures in the real physicochemical world, forming a special kind of sequence-structure data. With the development of Artificial Intelligence (AI) techniques, Protein Representation Learning (PRL) has recently emerged as a promising research topic for extracting informative knowledge from massive protein sequences or structures. To pave the way for AI researchers with little bioinformatics background, we present a timely and comprehensive review of PRL formulations and existing PRL methods from the perspective of model architectures, pretext tasks, and downstream applications. We first briefly introduce the motivations for protein representation learning and formulate it in a general and unified framework. Next, we divide existing PRL methods into three main categories: sequence-based, structure-based, and sequence-structure co-modeling. Finally, we discuss some technical challenges and potential directions for improving protein representation learning. The latest advances in PRL methods are summarized in a GitHub repository https://github.com/LirongWu/awesome-protein-representation-learning. △ Less","30 December, 2022",https://arxiv.org/pdf/2301.00813
Autonomous Driving Simulator based on Neurorobotics Platform,Wei Cao;Liguo Zhou;Yuhong Huang;Alois Knoll,"There are many artificial intelligence algorithms for autonomous driving, but directly installing these algorithms on vehicles is unrealistic and expensive. At the same time, many of these algorithms need an environment to train and optimize. Simulation is a valuable and meaningful solution with training and testing functions, and it can say that simulation is a critical link in the autonomous driving world. There are also many different applications or systems of simulation from companies or academies such as SVL and Carla. These simulators flaunt that they have the closest real-world simulation, but their environment objects, such as pedestrians and other vehicles around the agent-vehicle, are already fixed programmed. They can only move along the pre-setting trajectory, or random numbers determine their movements. What is the situation when all environmental objects are also installed by Artificial Intelligence, or their behaviors are like real people or natural reactions of other drivers? This problem is a blind spot for most of the simulation applications, or these applications cannot be easy to solve this problem. The Neurorobotics Platform from the TUM team of Prof. Alois Knoll has the idea about ""Engines"" and ""Transceiver Functions"" to solve the multi-agents problem. This report will start with a little research on the Neurorobotics Platform and analyze the potential and possibility of developing a new simulator to achieve the true real-world simulation goal. Then based on the NRP-Core Platform, this initial development aims to construct an initial demo experiment. The consist of this report starts with the basic knowledge of NRP-Core and its installation, then focus on the explanation of the necessary components for a simulation experiment, at last, about the details of constructions for the autonomous driving system, which is integrated object detection and autonomous control. △ Less","30 December, 2022",https://arxiv.org/pdf/2301.00089
RFID-Cloud Integration for Smart Management of Public Car Parking Spaces,Umar Yahya;Ndawula Noah;Asingwire Hanifah;Lubega Faham;Abdal Kasule;Hamisi Ramadhan Mubarak,"Effective management of public shared spaces such as car parking space, is one challenging transformational aspect for many cities, especially in the developing World. By leveraging sensing technologies, cloud computing, and Artificial Intelligence, Cities are increasingly being managed smartly. Smart Cities not only bring convenience to City dwellers, but also improve their quality of life as advocated for by United Nations in the 2030 Sustainable Development Goal on Sustainable Cities and Communities. Through integration of Internet of Things and Cloud Computing, this paper presents a successful proof-of-concept implementation of a framework for managing public car parking spaces. Reservation of parking slots is done through a cloud-hosted application, while access to and out of the parking slot is enabled through Radio Frequency Identification (RFID) technology which in real-time, accordingly triggers update of the parking slot availability in the cloud-hosted database. This framework could bring considerable convenience to City dwellers since motorists only have to drive to a parking space when sure of a vacant parking slot, an important stride towards realization of sustainable smart cities and communities. △ Less","24 December, 2022",https://arxiv.org/pdf/2212.14684
ComplAI: Theory of A Unified Framework for Multi-factor Assessment of Black-Box Supervised Machine Learning Models,Arkadipta De;Satya Swaroop Gudipudi;Sourab Panchanan;Maunendra Sankar Desarkar,"The advances in Artificial Intelligence are creating new opportunities to improve lives of people around the world, from business to healthcare, from lifestyle to education. For example, some systems profile the users using their demographic and behavioral characteristics to make certain domain-specific predictions. Often, such predictions impact the life of the user directly or indirectly (e.g., loan disbursement, determining insurance coverage, shortlisting applications, etc.). As a result, the concerns over such AI-enabled systems are also increasing. To address these concerns, such systems are mandated to be responsible i.e., transparent, fair, and explainable to developers and end-users. In this paper, we present ComplAI, a unique framework to enable, observe, analyze and quantify explainability, robustness, performance, fairness, and model behavior in drift scenarios, and to provide a single Trust Factor that evaluates different supervised Machine Learning models not just from their ability to make correct predictions but from overall responsibility perspective. The framework helps users to (a) connect their models and enable explanations, (b) assess and visualize different aspects of the model, such as robustness, drift susceptibility, and fairness, and (c) compare different models (from different model families or obtained through different hyperparameter settings) from an overall perspective thereby facilitating actionable recourse for improvement of the models. It is model agnostic and works with different supervised machine learning scenarios (i.e., Binary Classification, Multi-class Classification, and Regression) and frameworks. It can be seamlessly integrated with any ML life-cycle framework. Thus, this already deployed framework aims to unify critical aspects of Responsible AI systems for regulating the development process of such real systems. △ Less","30 December, 2022",https://arxiv.org/pdf/2212.14599
Investigation and rectification of NIDS datasets and standardized feature set derivation for network attack detection with graph neural networks,Anton Raskovalov;Nikita Gabdullin;Vasily Dolmatov,"Network Intrusion and Detection Systems (NIDS) are essential for malicious traffic and cyberattack detection in modern networks. Artificial intelligence-based NIDS are powerful tools that can learn complex data correlations for accurate attack prediction. Graph Neural Networks (GNNs) provide an opportunity to analyze network topology along with flow features which makes them particularly suitable for NIDS applications. However, successful application of such tool requires large amounts of carefully collected and labeled data for training and testing. In this paper we inspect different versions of ToN-IoT dataset and point out inconsistencies in some versions. We filter the full version of ToN-IoT and present a new version labeled ToN-IoT-R. To ensure generalization we propose a new standardized and compact set of flow features which are derived solely from NetFlowv5-compatible data. We separate numeric data and flags into different categories and propose a new dataset-agnostic normalization approach for numeric features. This allows us to preserve meaning of flow flags and we propose to conduct targeted analysis based on, for instance, network protocols. For flow classification we use E-GraphSage algorithm with modified node initialization technique that allows us to add node degree to node features. We achieve high classification accuracy on ToN-IoT-R and compare it with previously published results for ToN-IoT, NF-ToN-IoT, and NF-ToN-IoT-v2. We highlight the importance of careful data collection and labeling and appropriate data preprocessing choice and conclude that the proposed set of features is more applicable for real NIDS due to being less demanding to traffic monitoring equipment while preserving high flow classification accuracy. △ Less","29 December, 2022",https://arxiv.org/pdf/2212.13994
Architecture Decisions in AI-based Systems Development: An Empirical Study,Beiqi Zhang;Tianyang Liu;Peng Liang;Chong Wang;Mojtaba Shahin;Jiaxin Yu,"Artificial Intelligence (AI) technologies have been developed rapidly, and AI-based systems have been widely used in various application domains with opportunities and challenges. However, little is known about the architecture decisions made in AI-based systems development, which has a substantial impact on the success and sustainability of these systems. To this end, we conducted an empirical study by collecting and analyzing the data from Stack Overflow (SO) and GitHub. More specifically, we searched on SO with six sets of keywords and explored 32 AI-based projects on GitHub, and finally we collected 174 posts and 128 GitHub issues related to architecture decisions. The results show that in AI-based systems development (1) architecture decisions are expressed in six linguistic patterns, among which Solution Proposal and Information Giving are most frequently used, (2) Technology Decision, Component Decision, and Data Decision are the main types of architecture decisions made, (3) Game is the most common application domain among the eighteen application domains identified, (4) the dominant quality attribute considered in architecture decision-making is Performance, and (5) the main limitations and challenges encountered by practitioners in making architecture decisions are Design Issues and Data Issues. Our results suggest that the limitations and challenges when making architecture decisions in AI-based systems development are highly specific to the characteristics of AI-based systems and are mainly of technical nature, which need to be properly confronted. △ Less","28 December, 2022",https://arxiv.org/pdf/2212.13866
StyleID: Identity Disentanglement for Anonymizing Faces,Minh-Ha Le;Niklas Carlsson,"Privacy of machine learning models is one of the remaining challenges that hinder the broad adoption of Artificial Intelligent (AI). This paper considers this problem in the context of image datasets containing faces. Anonymization of such datasets is becoming increasingly important due to their central role in the training of autonomous cars, for example, and the vast amount of data generated by surveillance systems. While most prior work de-identifies facial images by modifying identity features in pixel space, we instead project the image onto the latent space of a Generative Adversarial Network (GAN) model, find the features that provide the biggest identity disentanglement, and then manipulate these features in latent space, pixel space, or both. The main contribution of the paper is the design of a feature-preserving anonymization framework, StyleID, which protects the individuals' identity, while preserving as many characteristics of the original faces in the image dataset as possible. As part of the contribution, we present a novel disentanglement metric, three complementing disentanglement methods, and new insights into identity disentanglement. StyleID provides tunable privacy, has low computational complexity, and is shown to outperform current state-of-the-art solutions. △ Less","28 December, 2022",https://arxiv.org/pdf/2212.13791
A Compositional Approach to Creating Architecture Frameworks with an Application to Distributed AI Systems,Hans-Martin Heyn;Eric Knauss;Patrizio Pelliccione,"Artificial intelligence (AI) in its various forms finds more and more its way into complex distributed systems. For instance, it is used locally, as part of a sensor system, on the edge for low-latency high-performance inference, or in the cloud, e.g. for data mining. Modern complex systems, such as connected vehicles, are often part of an Internet of Things (IoT). To manage complexity, architectures are described with architecture frameworks, which are composed of a number of architectural views connected through correspondence rules. Despite some attempts, the definition of a mathematical foundation for architecture frameworks that are suitable for the development of distributed AI systems still requires investigation and study. In this paper, we propose to extend the state of the art on architecture framework by providing a mathematical model for system architectures, which is scalable and supports co-evolution of different aspects for example of an AI system. Based on Design Science Research, this study starts by identifying the challenges with architectural frameworks. Then, we derive from the identified challenges four rules and we formulate them by exploiting concepts from category theory. We show how compositional thinking can provide rules for the creation and management of architectural frameworks for complex systems, for example distributed systems with AI. The aim of the paper is not to provide viewpoints or architecture models specific to AI systems, but instead to provide guidelines based on a mathematical formulation on how a consistent framework can be built up with existing, or newly created, viewpoints. To put in practice and test the approach, the identified and formulated rules are applied to derive an architectural framework for the EU Horizon 2020 project ``Very efficient deep learning in the IoT"" (VEDLIoT) in the form of a case study. △ Less","27 December, 2022",https://arxiv.org/pdf/2212.13570
Measuring an artificial intelligence agent's trust in humans using machine incentives,Tim Johnson;Nick Obradovich,"Scientists and philosophers have debated whether humans can trust advanced artificial intelligence (AI) agents to respect humanity's best interests. Yet what about the reverse? Will advanced AI agents trust humans? Gauging an AI agent's trust in humans is challenging because--absent costs for dishonesty--such agents might respond falsely about their trust in humans. Here we present a method for incentivizing machine decisions without altering an AI agent's underlying algorithms or goal orientation. In two separate experiments, we then employ this method in hundreds of trust games between an AI agent (a Large Language Model (LLM) from OpenAI) and a human experimenter (author TJ). In our first experiment, we find that the AI agent decides to trust humans at higher rates when facing actual incentives than when making hypothetical decisions. Our second experiment replicates and extends these findings by automating game play and by homogenizing question wording. We again observe higher rates of trust when the AI agent faces real incentives. Across both experiments, the AI agent's trust decisions appear unrelated to the magnitude of stakes. Furthermore, to address the possibility that the AI agent's trust decisions reflect a preference for uncertainty, the experiments include two conditions that present the AI agent with a non-social decision task that provides the opportunity to choose a certain or uncertain option; in those conditions, the AI agent consistently chooses the certain option. Our experiments suggest that one of the most advanced AI language models to date alters its social behavior in response to incentives and displays behavior consistent with trust toward a human interlocutor when incentivized. △ Less","27 December, 2022",https://arxiv.org/pdf/2212.13371
"VQA and Visual Reasoning: An Overview of Recent Datasets, Methods and Challenges",Rufai Yusuf Zakari;Jim Wilson Owusu;Hailin Wang;Ke Qin;Zaharaddeen Karami Lawal;Yuezhou Dong,"Artificial Intelligence (AI) and its applications have sparked extraordinary interest in recent years. This achievement can be ascribed in part to advances in AI subfields including Machine Learning (ML), Computer Vision (CV), and Natural Language Processing (NLP). Deep learning, a sub-field of machine learning that employs artificial neural network concepts, has enabled the most rapid growth in these domains. The integration of vision and language has sparked a lot of attention as a result of this. The tasks have been created in such a way that they properly exemplify the concepts of deep learning. In this review paper, we provide a thorough and an extensive review of the state of the arts approaches, key models design principles and discuss existing datasets, methods, their problem formulation and evaluation measures for VQA and Visual reasoning tasks to understand vision and language representation learning. We also present some potential future paths in this field of research, with the hope that our study may generate new ideas and novel approaches to handle existing difficulties and develop new applications. △ Less","26 December, 2022",https://arxiv.org/pdf/2212.13296
Structure-based drug discovery with deep learning,Rıza Özçelik;Derek van Tilborg;José Jiménez-Luna;Francesca Grisoni,"Artificial intelligence (AI) in the form of deep learning bears promise for drug discovery and chemical biology, \textit{e.g.}, to predict protein structure and molecular bioactivity, plan organic synthesis, and design molecules \textit{de novo}. While most of the deep learning efforts in drug discovery have focused on ligand-based approaches, structure-based drug discovery has the potential to tackle unsolved challenges, such as affinity prediction for unexplored protein targets, binding-mechanism elucidation, and the rationalization of related chemical kinetic properties. Advances in deep learning methodologies and the availability of accurate predictions for protein tertiary structure advocate for a \textit{renaissance} in structure-based approaches for drug discovery guided by AI. This review summarizes the most prominent algorithmic concepts in structure-based deep learning for drug discovery, and forecasts opportunities, applications, and challenges ahead. △ Less","26 December, 2022",https://arxiv.org/pdf/2212.13295
Artificial Intelligence to Enhance Mission Science Output for In-situ Observations: Dealing with the Sparse Data Challenge,M. I. Sitnov;G. K. Stephens;V. G. Merkin;C. -P. Wang;D. Turner;K. Genestreti;M. Argall;T. Y. Chen;A. Y. Ukhorskiy;S. Wing;Y. -H. Liu,"In the Earth's magnetosphere, there are fewer than a dozen dedicated probes beyond low-Earth orbit making in-situ observations at any given time. As a result, we poorly understand its global structure and evolution, the mechanisms of its main activity processes, magnetic storms, and substorms. New Artificial Intelligence (AI) methods, including machine learning, data mining, and data assimilation, as well as new AI-enabled missions will need to be developed to meet this Sparse Data challenge. △ Less","26 December, 2022",https://arxiv.org/pdf/2212.13289
Saliency-Augmented Memory Completion for Continual Learning,Guangji Bai;Chen Ling;Yuyang Gao;Liang Zhao,"Continual Learning is considered a key step toward next-generation Artificial Intelligence. Among various methods, replay-based approaches that maintain and replay a small episodic memory of previous samples are one of the most successful strategies against catastrophic forgetting. However, since forgetting is inevitable given bounded memory and unbounded tasks, how to forget is a problem continual learning must address. Therefore, beyond simply avoiding catastrophic forgetting, an under-explored issue is how to reasonably forget while ensuring the merits of human memory, including 1. storage efficiency, 2. generalizability, and 3. some interpretability. To achieve these simultaneously, our paper proposes a new saliency-augmented memory completion framework for continual learning, inspired by recent discoveries in memory completion separation in cognitive neuroscience. Specifically, we innovatively propose to store the part of the image most important to the tasks in episodic memory by saliency map extraction and memory encoding. When learning new tasks, previous data from memory are inpainted by an adaptive data generation module, which is inspired by how humans complete episodic memory. The module's parameters are shared across all tasks and it can be jointly trained with a continual learning classifier as bilevel optimization. Extensive experiments on several continual learning and image classification benchmarks demonstrate the proposed method's effectiveness and efficiency. △ Less","26 December, 2022",https://arxiv.org/pdf/2212.13242
Biologically Inspired Design Concept Generation Using Generative Pre-Trained Transformers,Qihao Zhu;Xinyu Zhang;Jianxi Luo,"Biological systems in nature have evolved for millions of years to adapt and survive the environment. Many features they developed can be inspirational and beneficial for solving technical problems in modern industries. This leads to a specific form of design-by-analogy called bio-inspired design (BID). Although BID as a design method has been proven beneficial, the gap between biology and engineering continuously hinders designers from effectively applying the method. Therefore, we explore the recent advance of artificial intelligence (AI) for a data-driven approach to bridge the gap. This paper proposes a generative design approach based on the generative pre-trained language model (PLM) to automatically retrieve and map biological analogy and generate BID in the form of natural language. The latest generative pre-trained transformer, namely GPT-3, is used as the base PLM. Three types of design concept generators are identified and fine-tuned from the PLM according to the looseness of the problem space representation. Machine evaluators are also fine-tuned to assess the mapping relevancy between the domains within the generated BID concepts. The approach is evaluated and then employed in a real-world project of designing light-weighted flying cars during its conceptual design phase The results show our approach can generate BID concepts with good performance. △ Less","26 December, 2022",https://arxiv.org/pdf/2212.13196
"Beyond 5G Networks: Integration of Communication, Computing, Caching, and Control",Musbahu Mohammed Adam;Liqiang Zhao;Kezhi Wang;Zhu Han,"In recent years, the exponential proliferation of smart devices with their intelligent applications poses severe challenges on conventional cellular networks. Such challenges can be potentially overcome by integrating communication, computing, caching, and control (i4C) technologies. In this survey, we first give a snapshot of different aspects of the i4C, comprising background, motivation, leading technological enablers, potential applications, and use cases. Next, we describe different models of communication, computing, caching, and control (4C) to lay the foundation of the integration approach. We review current state-of-the-art research efforts related to the i4C, focusing on recent trends of both conventional and artificial intelligence (AI)-based integration approaches. We also highlight the need for intelligence in resources integration. Then, we discuss integration of sensing and communication (ISAC) and classify the integration approaches into various classes. Finally, we propose open challenges and present future research directions for beyond 5G networks, such as 6G. △ Less","26 December, 2022",https://arxiv.org/pdf/2212.13141
"Understanding Ethics, Privacy, and Regulations in Smart Video Surveillance for Public Safety",Babak Rahimi Ardabili;Armin Danesh Pazho;Ghazal Alinezhad Noghre;Christopher Neff;Arun Ravindran;Hamed Tabkhi,"Recently, Smart Video Surveillance (SVS) systems have been receiving more attention among scholars and developers as a substitute for the current passive surveillance systems. These systems are used to make the policing and monitoring systems more efficient and improve public safety. However, the nature of these systems in monitoring the public's daily activities brings different ethical challenges. There are different approaches for addressing privacy issues in implementing the SVS. In this paper, we are focusing on the role of design considering ethical and privacy challenges in SVS. Reviewing four policy protection regulations that generate an overview of best practices for privacy protection, we argue that ethical and privacy concerns could be addressed through four lenses: algorithm, system, model, and data. As an case study, we describe our proposed system and illustrate how our system can create a baseline for designing a privacy perseverance system to deliver safety to society. We used several Artificial Intelligence algorithms, such as object detection, single and multi camera re-identification, action recognition, and anomaly detection, to provide a basic functional system. We also use cloud-native services to implement a smartphone application in order to deliver the outputs to the end users. △ Less","25 December, 2022",https://arxiv.org/pdf/2212.12936
An Exact Mapping From ReLU Networks to Spiking Neural Networks,Ana Stanojevic;Stanisław Woźniak;Guillaume Bellec;Giovanni Cherubini;Angeliki Pantazi;Wulfram Gerstner,"Deep spiking neural networks (SNNs) offer the promise of low-power artificial intelligence. However, training deep SNNs from scratch or converting deep artificial neural networks to SNNs without loss of performance has been a challenge. Here we propose an exact mapping from a network with Rectified Linear Units (ReLUs) to an SNN that fires exactly one spike per neuron. For our constructive proof, we assume that an arbitrary multi-layer ReLU network with or without convolutional layers, batch normalization and max pooling layers was trained to high performance on some training set. Furthermore, we assume that we have access to a representative example of input data used during training and to the exact parameters (weights and biases) of the trained ReLU network. The mapping from deep ReLU networks to SNNs causes zero percent drop in accuracy on CIFAR10, CIFAR100 and the ImageNet-like data sets Places365 and PASS. More generally our work shows that an arbitrary deep ReLU network can be replaced by an energy-efficient single-spike neural network without any loss of performance. △ Less","23 December, 2022",https://arxiv.org/pdf/2212.12522
Ethical Design of Computers: From Semiconductors to IoT and Artificial Intelligence,Sudeep Pasricha;Marilyn Wolf,"Computing systems are tightly integrated today into our professional, social, and private lives. An important consequence of this growing ubiquity of computing is that it can have significant ethical implications of which computing professionals should take account. In most real-world scenarios, it is not immediately obvious how particular technical choices during the design and use of computing systems could be viewed from an ethical perspective. This article provides a perspective on the ethical challenges within semiconductor chip design, IoT applications, and the increasing use of artificial intelligence in the design processes, tools, and hardware-software stacks of these systems. △ Less","23 December, 2022",https://arxiv.org/pdf/2212.12508
Influence of AI in human lives,Meenu Varghese;Satheesh Raj;Vigneshwaran Venkatesh,"Artificial Intelligence is one of the most significant and prominent technological innovations which has reshaped all aspects of human life on the lines of ease from magnitudes like shopping, data collection, driving, everyday life, medical approach and many more. On the contrary, although recent developments in both subjects that are backed by technology, progress on AI alongside CE must have mostly been undertaken in isolation, providing little understanding into how the two areas intersect. Artificial intelligence is now widely used in services, from back-office tasks to front-line interactions with customers. This trend has accelerated in recent years. Artificial intelligence (AI)-based virtual assistants are changing successful engagement away from being dominated by humans and toward being dominated by technologies. As a result, people are expected to solve their own problems before calling customer care representatives, eventually emerging as a crucial component of providing services as value co-creators. AI-powered chats may potentially go awry, which could enrage, perplex, and anger customers. Considering all these, the main objectives of this study will engage the following 1. To identify the alterations in the scope of human searches for information offered by the application of AI? 2. To analyse how AI helps in the way someone drives the car 3. To evaluate how AI has changed the way customer interact with the customers △ Less","15 December, 2022",https://arxiv.org/pdf/2212.12305
Introduction to Machine Learning for Physicians: A Survival Guide for Data Deluge,Ričards Marcinkevičs;Ece Ozkan;Julia E. Vogt,"Many modern research fields increasingly rely on collecting and analysing massive, often unstructured, and unwieldy datasets. Consequently, there is growing interest in machine learning and artificial intelligence applications that can harness this `data deluge'. This broad nontechnical overview provides a gentle introduction to machine learning with a specific focus on medical and biological applications. We explain the common types of machine learning algorithms and typical tasks that can be solved, illustrating the basics with concrete examples from healthcare. Lastly, we provide an outlook on open challenges, limitations, and potential impacts of machine-learning-powered medicine. △ Less","23 December, 2022",https://arxiv.org/pdf/2212.12303
Multi-Lingual DALL-E Storytime,Noga Mudrik;Adam S. Charles,"While recent advancements in artificial intelligence (AI) language models demonstrate cutting-edge performance when working with English texts, equivalent models do not exist in other languages or do not reach the same performance level. This undesired effect of AI advancements increases the gap between access to new technology from different populations across the world. This unsought bias mainly discriminates against individuals whose English skills are less developed, e.g., non-English speakers children. Following significant advancements in AI research in recent years, OpenAI has recently presented DALL-E: a powerful tool for creating images based on English text prompts. While DALL-E is a promising tool for many applications, its decreased performance when given input in a different language, limits its audience and deepens the gap between populations. An additional limitation of the current DALL-E model is that it only allows for the creation of a few images in response to a given input prompt, rather than a series of consecutive coherent frames that tell a story or describe a process that changes over time. Here, we present an easy-to-use automatic DALL-E storytelling framework that leverages the existing DALL-E model to enable fast and coherent visualizations of non-English songs and stories, pushing the limit of the one-step-at-a-time option DALL-E currently offers. We show that our framework is able to effectively visualize stories from non-English texts and portray the changes in the plot over time. It is also able to create a narrative and maintain interpretable changes in the description across frames. Additionally, our framework offers users the ability to specify constraints on the story elements, such as a specific location or context, and to maintain a consistent style throughout the visualization. △ Less","22 December, 2022",https://arxiv.org/pdf/2212.11985
Realizing Molecular Machine Learning through Communications for Biological AI: Future Directions and Challenges,Sasitharan Balasubramaniam;Samitha Somathilaka;Sehee Sun;Adrian Ratwatte;Massimiliano Pierobon,"Artificial Intelligence (AI) and Machine Learning (ML) are weaving their way into the fabric of society, where they are playing a crucial role in numerous facets of our lives. As we witness the increased deployment of AI and ML in various types of devices, we benefit from their use into energy-efficient algorithms for low powered devices. In this paper, we investigate a scale and medium that is far smaller than conventional devices as we move towards molecular systems that can be utilized to perform machine learning functions, i.e., Molecular Machine Learning (MML). Fundamental to the operation of MML is the transport, processing, and interpretation of information propagated by molecules through chemical reactions. We begin by reviewing the current approaches that have been developed for MML, before we move towards potential new directions that rely on gene regulatory networks inside biological organisms as well as their population interactions to create neural networks. We then investigate mechanisms for training machine learning structures in biological cells based on calcium signaling and demonstrate their application to build an Analog to Digital Converter (ADC). Lastly, we look at potential future directions as well as challenges that this area could solve. △ Less","22 December, 2022",https://arxiv.org/pdf/2212.11910
RescureService: A Benchmark Microservice System for the Research of Mobile Edge and Cloud Computing,Xiang He;Teng Wang;Lei Liu;Jianan Li;Zihang Su;Yingming Guo;Zhiying Tu;Hanchuan Xu;Zhongjie Wang,"The dramatic development of cloud and edge computing allows for better Quality of Service (QoS) in many scenarios by deploying services on cloud and edge servers. Microservice technology is also adopted in these scenarios to decompose complex business logic into many small independent services. Meanwhile, as microservice systems continue to grow, providing stable QoS in these systems becomes a challenge, and many different approaches have been proposed for stable QoS. However, the microservice systems used in the experiments of these work have problems such as the low number of services and a single type of service. Therefore, we developed the open-source benchmark microservice system RescureService with 20+ services, including database, front-end, business logic, data processing, and artificial intelligence services in the disaster relief scenario. Measuring tools are provided to measure the service properties to help researchers prepare experimental data, and the service properties pre-measured are also presented. Meanwhile, the fulfillment of benchmark requirements is detailed, and the results show that our RescureService meets the requirements of a benchmark system in research. Moreover, instructions are given to describe adopting our system in service computing as examples. △ Less","27 October, 2022",https://arxiv.org/pdf/2212.11758
Towards Sustainable Artificial Intelligence: An Overview of Environmental Protection Uses and Issues,Arnault Pachot;Céline Patissier,"Artificial Intelligence (AI) is used to create more sustainable production methods and model climate change, making it a valuable tool in the fight against environmental degradation. This paper describes the paradox of an energy-consuming technology serving the ecological challenges of tomorrow. The study provides an overview of the sectors that use AI-based solutions for environmental protection. It draws on numerous examples from AI for Green players to present use cases and concrete examples. In the second part of the study, the negative impacts of AI on the environment and the emerging technological solutions to support Green AI are examined. It is also shown that the research on less energy-consuming AI is motivated more by cost and energy autonomy constraints than by environmental considerations. This leads to a rebound effect that favors an increase in the complexity of models. Finally, the need to integrate environmental indicators into algorithms is discussed. The environmental dimension is part of the broader ethical problem of AI, and addressing it is crucial for ensuring the sustainability of AI in the long term. △ Less","22 December, 2022",https://arxiv.org/pdf/2212.11738
Time to Market Reduction for Hydrogen Fuel Cell Stacks using Generative Adversarial Networks,Nicolas Morizet;Perceval Desforges;Christophe Geissler;Elodie Pahon;Samir Jemeï;Daniel Hissel,"To face the dependency on fossil fuels and limit carbon emissions, fuel cells are a very promising technology and appear to be a key candidate to tackle the increase of the energy demand and promote the energy transition. To meet future needs for both transport and stationary applications, the time to market of fuel cell stacks must be drastically reduced. Here, a new concept to shorten their development time by introducing a disruptive and highefficiency data augmentation approach based on artificial intelligence is presented. Our results allow reducing the testing time before introducing a product on the market from a thousand to a few hours. The innovative concept proposed here can support engineering and research tasks during the fuel cell development process to achieve decreased development costs alongside a reduced time to market. △ Less","22 December, 2022",https://arxiv.org/pdf/2212.11733
Circumventing interpretability: How to defeat mind-readers,Lee Sharkey,"The increasing capabilities of artificial intelligence (AI) systems make it ever more important that we interpret their internals to ensure that their intentions are aligned with human values. Yet there is reason to believe that misaligned artificial intelligence will have a convergent instrumental incentive to make its thoughts difficult for us to interpret. In this article, I discuss many ways that a capable AI might circumvent scalable interpretability methods and suggest a framework for thinking about these potential future risks. △ Less","21 December, 2022",https://arxiv.org/pdf/2212.11415
Sensitivity analysis of biological washout and depth selection for a machine learning based dose verification framework in proton therapy,Shixiong Yu;Yuxiang Liu;Zongsheng Hu;Haozhao Zhang;Pengyu Qi;Hao Peng,"Dose verification based on proton-induced positron emitters is a promising quality assurance tool and may leverage the strength of artificial intelligence. To move a step closer towards practical application, the sensitivity analysis of two factors needs to be performed: biological washout and depth selection. selection. A bi-directional recurrent neural network (RNN) model was developed. The training dataset was generated based upon a CT image-based phantom (abdomen region) and multiple beam energies/pathways, using Monte-Carlo simulation (1 mm spatial resolution, no biological washout). For the modeling of biological washout, a simplified analytical model was applied to change raw activity profiles over a period of 5 minutes, incorporating both physical decay and biological washout. For the study of depth selection (a challenge linked to multi field/angle irradiation), truncations were applied at different window lengths (100, 125, 150 mm) to raw activity profiles. Finally, the performance of a worst-case scenario was examined by combining both factors (depth selection: 125 mm, biological washout: 5 mins). The accuracy was quantitatively evaluated in terms of range uncertainty, mean absolute error (MAE) and mean relative errors (MRE). Our proposed AI framework shows good immunity to the perturbation associated with two factors. The detection of proton-induced positron emitters, combined with machine learning, has great potential to implement online patient-specific verification in proton therapy. △ Less","21 December, 2022",https://arxiv.org/pdf/2212.11352
Annotated History of Modern AI and Deep Learning,Juergen Schmidhuber,"Machine learning is the science of credit assignment: finding patterns in observations that predict the consequences of actions and help to improve future performance. Credit assignment is also required for human understanding of how the world works, not only for individuals navigating daily life, but also for academic professionals like historians who interpret the present in light of past events. Here I focus on the history of modern artificial intelligence (AI) which is dominated by artificial neural networks (NNs) and deep learning, both conceptually closer to the old field of cybernetics than to what's been called AI since 1956 (e.g., expert systems and logic programming). A modern history of AI will emphasize breakthroughs outside of the focus of traditional AI text books, in particular, mathematical foundations of today's NNs such as the chain rule (1676), the first NNs (linear regression, circa 1800), and the first working deep learners (1965-). From the perspective of 2022, I provide a timeline of the -- in hindsight -- most important relevant events in the history of NNs, deep learning, AI, computer science, and mathematics in general, crediting those who laid foundations of the field. The text contains numerous hyperlinks to relevant overview sites from my AI Blog. It supplements my previous deep learning survey (2015) which provides hundreds of additional references. Finally, to round it off, I'll put things in a broader historic context spanning the time since the Big Bang until when the universe will be many times older than it is now. △ Less","29 December, 2022",https://arxiv.org/pdf/2212.11279
One Artist's Personal Reflections on Methods and Ethics of Creating Mixed Media Artificial Intelligence Art,Jane Adams,"I intend to make a scientific contribution of my subjective experience as a single unit of self-described ``artist'' leveraging artificial intelligence as an assistive visual creation tool, in the hopes that it may provide some inspiration or deeper meaning for fellow artists and computer scientists in this medium. First, I will provide some background on my personal history thus far as an artist. Neither artist nor scientist can exist in a vaccuum, so I then will provide some (albeit a non-exhaustive list of) related work that has helped me contextualize my own work and thinking in this area. I often consider my methods in the creative process chronologically, so I have divided that section according to the loose structure of my artistic workflow. These foundations provide a fertile grounding for discussion around topics of subject matter, reception, community, and ethics. I then conclude with some ideas for future work in the realms of theory of authorship, explainability tooling, and research framing. △ Less","30 November, 2022",https://arxiv.org/pdf/2212.11232
Device-Bind Key-Storageless Hardware AI Model IP Protection: A PUF and Permute-Diffusion Encryption-Enabled Approach,Qianqian Pan;Mianxiong Dong;Kaoru Ota;Jun Wu,"Machine learning as a service (MLaaS) framework provides intelligent services or well-trained artificial intelligence (AI) models for local devices. However, in the process of model transmission and deployment, there are security issues, i.e. AI model leakage due to the unreliable transmission environments and illegal abuse at local devices without permission. Although existing works study the intellectual property (IP) protection of AI models, they mainly focus on the watermark-based and encryption-based methods and have the following problems: (i) The watermark-based methods only provide passive verification afterward rather than active protection. (ii) Encryption-based methods are low efficiency in computation and low security in key storage. (iii) The existing methods are not device-bind without the ability to avoid illegal abuse of AI models. To deal with these problems, we propose a device-bind and key-storageless hardware AI model IP protection mechanism. First, a physical unclonable function (PUF) and permute-diffusion encryption-based AI model protection framework is proposed, including the PUF-based secret key generation and the geometric-value transformation-based weights encryption. Second, we design a PUF-based key generation protocol, where delay-based Anderson PUF is adopted to generate the derive-bind secret key. Besides, convolutional coding and convolutional interleaving technologies are combined to improve the stability of PUF-based key generation and reconstruction. Third, a permute and diffusion-based intelligent model weights encryption/decryption method is proposed to achieve effective IP protection, where chaos theory is utilized to convert the PUF-based secret key to encryption/decryption keys. Finally, experimental evaluation demonstrates the effectiveness of the proposed intelligent model IP protection mechanism. △ Less","21 December, 2022",https://arxiv.org/pdf/2212.11133
THMA: Tencent HD Map AI System for Creating HD Map Annotations,Kun Tang;Xu Cao;Zhipeng Cao;Tong Zhou;Erlong Li;Ao Liu;Shengtao Zou;Chang Liu;Shuqi Mei;Elena Sizikova;Chao Zheng,"Nowadays, autonomous vehicle technology is becoming more and more mature. Critical to progress and safety, high-definition (HD) maps, a type of centimeter-level map collected using a laser sensor, provide accurate descriptions of the surrounding environment. The key challenge of HD map production is efficient, high-quality collection and annotation of large-volume datasets. Due to the demand for high quality, HD map production requires significant manual human effort to create annotations, a very time-consuming and costly process for the map industry. In order to reduce manual annotation burdens, many artificial intelligence (AI) algorithms have been developed to pre-label the HD maps. However, there still exists a large gap between AI algorithms and the traditional manual HD map production pipelines in accuracy and robustness. Furthermore, it is also very resource-costly to build large-scale annotated datasets and advanced machine learning algorithms for AI-based HD map automatic labeling systems. In this paper, we introduce the Tencent HD Map AI (THMA) system, an innovative end-to-end, AI-based, active learning HD map labeling system capable of producing and labeling HD maps with a scale of hundreds of thousands of kilometers. In THMA, we train AI models directly from massive HD map datasets via supervised, self-supervised, and weakly supervised learning to achieve high accuracy and efficiency required by downstream users. THMA has been deployed by the Tencent Map team to provide services to downstream companies and users, serving over 1,000 labeling workers and producing more than 30,000 kilometers of HD map data per day at most. More than 90 percent of the HD map data in Tencent Map is labeled automatically by THMA, accelerating the traditional HD map labeling process by more than ten times. △ Less","14 December, 2022",https://arxiv.org/pdf/2212.11123
Diamond Abrasive Electroplated Surface Anomaly Detection using Convolutional Neural Networks for Industrial Quality Inspection,Parviz Ali,"Electroplated diamond abrasive tools require nickel coating on a metal surface for abrasive bonding and part functionality. The electroplated nickel-coated abrasive tool is expected to have a high-quality part performance by having a nickel coating thickness of between 50% to 60% of the abrasive median diameter, uniformity of the nickel layer, abrasive distribution over the electroplated surface, and bright gloss. Electroplating parameters are set accordingly for this purpose. Industrial quality inspection for defects of these abrasive electroplated parts with optical inspection instruments is extremely challenging due to the diamond's light refraction, dispersion nature, and reflective bright nickel surface. The difficulty posed by this challenge requires parts to be quality inspected manually with an eye loupe that is subjective and costly. In this study, we use a Convolutional Neural Network (CNN) model in the production line to detect abrasive electroplated part anomalies allowing us to fix or eliminate those parts or elements that are in bad condition from the production chain and ultimately reduce manual quality inspection cost. We used 744 samples to train our model. Our model successfully identified over 99% of the parts with an anomaly. Keywords: Artificial Intelligence, Anomaly Detection, Industrial Quality Inspection, Electroplating, Diamond Abrasive Tool △ Less","11 December, 2022",https://arxiv.org/pdf/2212.11122
NP4G : Network Programming for Generalization,Shoichiro Hara;Yuji Watanabe,"Automatic programming has been actively studied for a long time by various approaches including genetic programming. In recent years, automatic programming using neural networks such as GPT-3 has been actively studied and is attracting a lot of attention. However, these methods are illogical inference based on experience by enormous learning, and their thinking process is unclear. Even using the method by logical inference with a clear thinking process, the system that automatically generates any programs has not yet been realized. Especially, the inductive inference generalized by logical inference from one example is an important issue that the artificial intelligence can acquire knowledge by itself. In this study, we propose NP4G: Network Programming for Generalization, which can automatically generate programs by inductive inference. Because the proposed method can realize ""sequence"", ""selection"", and ""iteration"" in programming and can satisfy the conditions of the structured program theorem, it is expected that NP4G is a method automatically acquire any programs by inductive inference. As an example, we automatically construct a bitwise NOT operation program from several training data by generalization using NP4G. Although NP4G only randomly selects and connects nodes, by adjusting the number of nodes and the number of phase of ""Phased Learning"", we show the bitwise NOT operation programs are acquired in a comparatively short time and at a rate of about 7 in 10 running. The source code of NP4G is available on GitHub as a public repository. △ Less","8 December, 2022",https://arxiv.org/pdf/2212.11118
Deep set conditioned latent representations for action recognition,Akash Singh;Tom De Schepper;Kevin Mets;Peter Hellinckx;Jose Oramas;Steven Latre,"In recent years multi-label, multi-class video action recognition has gained significant popularity. While reasoning over temporally connected atomic actions is mundane for intelligent species, standard artificial neural networks (ANN) still struggle to classify them. In the real world, atomic actions often temporally connect to form more complex composite actions. The challenge lies in recognising composite action of varying durations while other distinct composite or atomic actions occur in the background. Drawing upon the success of relational networks, we propose methods that learn to reason over the semantic concept of objects and actions. We empirically show how ANNs benefit from pretraining, relational inductive biases and unordered set-based latent representations. In this paper we propose deep set conditioned I3D (SCI3D), a two stream relational network that employs latent representation of state and visual representation for reasoning over events and actions. They learn to reason about temporally connected actions in order to identify all of them in the video. The proposed method achieves an improvement of around 1.49% mAP in atomic action recognition and 17.57% mAP in composite action recognition, over a I3D-NL baseline, on the CATER dataset. △ Less","21 December, 2022",https://arxiv.org/pdf/2212.11030
The Internet of Senses: Building on Semantic Communications and Edge Intelligence,Roghayeh Joda;Medhat Elsayed;Hatem Abou-zeid;Ramy Atawia;Akram Bin Sediq;Gary Boudreau;Melike Erol-Kantarci;Lajos Hanzo,"The Internet of Senses (IoS) holds the promise of flawless telepresence-style communication for all human `receptors' and therefore blurs the difference of virtual and real environments. We commence by highlighting the compelling use cases empowered by the IoS and also the key network requirements. We then elaborate on how the emerging semantic communications and Artificial Intelligence (AI)/Machine Learning (ML) paradigms along with 6G technologies may satisfy the requirements of IoS use cases. On one hand, semantic communications can be applied for extracting meaningful and significant information and hence efficiently exploit the resources and for harnessing a priori information at the receiver to satisfy IoS requirements. On the other hand, AI/ML facilitates frugal network resource management by making use of the enormous amount of data generated in IoS edge nodes and devices, as well as by optimizing the IoS performance via intelligent agents. However, the intelligent agents deployed at the edge are not completely aware of each others' decisions and the environments of each other, hence they operate in a partially rather than fully observable environment. Therefore, we present a case study of Partially Observable Markov Decision Processes (POMDP) for improving the User Equipment (UE) throughput and energy consumption, as they are imperative for IoS use cases, using Reinforcement Learning for astutely activating and deactivating the component carriers in carrier aggregation. Finally, we outline the challenges and open issues of IoS implementations and employing semantic communications, edge intelligence as well as learning under partial observability in the IoS context. △ Less","20 December, 2022",https://arxiv.org/pdf/2212.10748
Requirements Engineering for Artificial Intelligence Systems: A Systematic Mapping Study,Khlood Ahmad;Mohamed Abdelrazek;Chetan Arora;Muneera Bano;John Grundy,"[Context] In traditional software systems, Requirements Engineering (RE) activities are well-established and researched. However, building Artificial Intelligence (AI) based software with limited or no insight into the system's inner workings poses significant new challenges to RE. Existing literature has focused on using AI to manage RE activities, with limited research on RE for AI (RE4AI). [Objective] This paper investigates current approaches for specifying requirements for AI systems, identifies available frameworks, methodologies, tools, and techniques used to model requirements, and finds existing challenges and limitations. [Method] We performed a systematic mapping study to find papers on current RE4AI approaches. We identified 43 primary studies and analysed the existing methodologies, models, tools, and techniques used to specify and model requirements in real-world scenarios. [Results] We found several challenges and limitations of existing RE4AI practices. The findings highlighted that current RE applications were not adequately adaptable for building AI systems and emphasised the need to provide new techniques and tools to support RE4AI. [Conclusion] Our results showed that most of the empirical studies on RE4AI focused on autonomous, self-driving vehicles and managing data requirements, and areas such as ethics, trust, and explainability need further research. △ Less","20 December, 2022",https://arxiv.org/pdf/2212.10693
Local Differential Privacy Image Generation Using Flow-based Deep Generative Models,Hisaichi Shibata;Shouhei Hanaoka;Yang Cao;Masatoshi Yoshikawa;Tomomi Takenaga;Yukihiro Nomura;Naoto Hayashi;Osamu Abe,"Diagnostic radiologists need artificial intelligence (AI) for medical imaging, but access to medical images required for training in AI has become increasingly restrictive. To release and use medical images, we need an algorithm that can simultaneously protect privacy and preserve pathologies in medical images. To develop such an algorithm, here, we propose DP-GLOW, a hybrid of a local differential privacy (LDP) algorithm and one of the flow-based deep generative models (GLOW). By applying a GLOW model, we disentangle the pixelwise correlation of images, which makes it difficult to protect privacy with straightforward LDP algorithms for images. Specifically, we map images onto the latent vector of the GLOW model, each element of which follows an independent normal distribution, and we apply the Laplace mechanism to the latent vector. Moreover, we applied DP-GLOW to chest X-ray images to generate LDP images while preserving pathologies. △ Less","20 December, 2022",https://arxiv.org/pdf/2212.10688
Analysis of Explainable Artificial Intelligence Methods on Medical Image Classification,Vinay Jogani;Joy Purohit;Ishaan Shivhare;Seema C Shrawne,"The use of deep learning in computer vision tasks such as image classification has led to a rapid increase in the performance of such systems. Due to this substantial increment in the utility of these systems, the use of artificial intelligence in many critical tasks has exploded. In the medical domain, medical image classification systems are being adopted due to their high accuracy and near parity with human physicians in many tasks. However, these artificial intelligence systems are extremely complex and are considered black boxes by scientists, due to the difficulty in interpreting what exactly led to the predictions made by these models. When these systems are being used to assist high-stakes decision-making, it is extremely important to be able to understand, verify and justify the conclusions reached by the model. The research techniques being used to gain insight into the black-box models are in the field of explainable artificial intelligence (XAI). In this paper, we evaluated three different XAI methods across two convolutional neural network models trained to classify lung cancer from histopathological images. We visualized the outputs and analyzed the performance of these methods, in order to better understand how to apply explainable artificial intelligence in the medical domain. △ Less","10 December, 2022",https://arxiv.org/pdf/2212.10565
The Expertise Level,Ron Fulbright,"Computers are quickly gaining on us. Artificial systems are now exceeding the performance of human experts in several domains. However, we do not yet have a deep definition of expertise. This paper examines the nature of expertise and presents an abstract knowledge-level and skill-level description of expertise. A new level lying above the Knowledge Level, called the Expertise Level, is introduced to describe the skills of an expert without having to worry about details of the knowledge required. The Model of Expertise is introduced combining the knowledge-level and expertise-level descriptions. Application of the model to the fields of cognitive architectures and human cognitive augmentation is demonstrated and several famous intelligent systems are analyzed with the model. △ Less","11 November, 2022",https://arxiv.org/pdf/2212.10435
AI applications in forest monitoring need remote sensing benchmark datasets,Emily R. Lines;Matt Allen;Carlos Cabo;Kim Calders;Amandine Debus;Stuart W. D. Grieve;Milto Miltiadou;Adam Noach;Harry J. F. Owen;Stefano Puliti,"With the rise in high resolution remote sensing technologies there has been an explosion in the amount of data available for forest monitoring, and an accompanying growth in artificial intelligence applications to automatically derive forest properties of interest from these datasets. Many studies use their own data at small spatio-temporal scales, and demonstrate an application of an existing or adapted data science method for a particular task. This approach often involves intensive and time-consuming data collection and processing, but generates results restricted to specific ecosystems and sensor types. There is a lack of widespread acknowledgement of how the types and structures of data used affects performance and accuracy of analysis algorithms. To accelerate progress in the field more efficiently, benchmarking datasets upon which methods can be tested and compared are sorely needed. Here, we discuss how lack of standardisation impacts confidence in estimation of key forest properties, and how considerations of data collection need to be accounted for in assessing method performance. We present pragmatic requirements and considerations for the creation of rigorous, useful benchmarking datasets for forest monitoring applications, and discuss how tools from modern data science can improve use of existing data. We list a set of example large-scale datasets that could contribute to benchmarking, and present a vision for how community-driven, representative benchmarking initiatives could benefit the field. △ Less","19 December, 2022",https://arxiv.org/pdf/2212.09937
Natural Language Processing in Customer Service: A Systematic Review,Malak Mashaabi;Areej Alotaibi;Hala Qudaih;Raghad Alnashwan;Hend Al-Khalifa,"Artificial intelligence and natural language processing (NLP) are increasingly being used in customer service to interact with users and answer their questions. The goal of this systematic review is to examine existing research on the use of NLP technology in customer service, including the research domain, applications, datasets used, and evaluation methods. The review also looks at the future direction of the field and any significant limitations. The review covers the time period from 2015 to 2022 and includes papers from five major scientific databases. Chatbots and question-answering systems were found to be used in 10 main fields, with the most common use in general, social networking, and e-commerce areas. Twitter was the second most commonly used dataset, with most research also using their own original datasets. Accuracy, precision, recall, and F1 were the most common evaluation methods. Future work aims to improve the performance and understanding of user behavior and emotions, and address limitations such as the volume, diversity, and quality of datasets. This review includes research on different spoken languages and models and techniques. △ Less","16 December, 2022",https://arxiv.org/pdf/2212.09523
Fast Converging Anytime Model Counting,Yong Lai;Kuldeep S. Meel;Roland H. C. Yap,"Model counting is a fundamental problem which has been influential in many applications, from artificial intelligence to formal verification. Due to the intrinsic hardness of model counting, approximate techniques have been developed to solve real-world instances of model counting. This paper designs a new anytime approach called PartialKC for approximate model counting. The idea is a form of partial knowledge compilation to provide an unbiased estimate of the model count which can converge to the exact count. Our empirical analysis demonstrates that PartialKC achieves significant scalability and accuracy over prior state-of-the-art approximate counters, including satss and STS. Interestingly, the empirical results show that PartialKC reaches convergence for many instances and therefore provides exact model counting performance comparable to state-of-the-art exact counters. △ Less","19 December, 2022",https://arxiv.org/pdf/2212.09390
Robust Anomaly Map Assisted Multiple Defect Detection with Supervised Classification Techniques,Jože M. Rožanec;Patrik Zajec;Spyros Theodoropoulos;Erik Koehorst;Blaž Fortuna;Dunja Mladenić,"Industry 4.0 aims to optimize the manufacturing environment by leveraging new technological advances, such as new sensing capabilities and artificial intelligence. The DRAEM technique has shown state-of-the-art performance for unsupervised classification. The ability to create anomaly maps highlighting areas where defects probably lie can be leveraged to provide cues to supervised classification models and enhance their performance. Our research shows that the best performance is achieved when training a defect detection model by providing an image and the corresponding anomaly map as input. Furthermore, such a setting provides consistent performance when framing the defect detection as a binary or multiclass classification problem and is not affected by class balancing policies. We performed the experiments on three datasets with real-world data provided by Philips Consumer Lifestyle BV. △ Less","19 December, 2022",https://arxiv.org/pdf/2212.09352
Review of security techniques for memristor computing systems,Minhui Zou;Nan Du;Shahar Kvatinsky,"Neural network (NN) algorithms have become the dominant tool in visual object recognition, natural language processing, and robotics. To enhance the computational efficiency of these algorithms, in comparison to the traditional von Neuman computing architectures, researchers have been focusing on memristor computing systems. A major drawback when using memristor computing systems today is that, in the artificial intelligence (AI) era, well-trained NN models are intellectual property and, when loaded in the memristor computing systems, face theft threats, especially when running in edge devices. An adversary may steal the well-trained NN models through advanced attacks such as learning attacks and side-channel analysis. In this paper, we review different security techniques for protecting memristor computing systems. Two threat models are described based on their assumptions regarding the adversary's capabilities: a black-box (BB) model and a white-box (WB) model. We categorize the existing security techniques into five classes in the context of these threat models: thwarting learning attacks (BB), thwarting side-channel attacks (BB), NN model encryption (WB), NN weight transformation (WB), and fingerprint embedding (WB). We also present a cross-comparison of the limitations of the security techniques. This paper could serve as an aid when designing secure memristor computing systems. △ Less","19 December, 2022",https://arxiv.org/pdf/2212.09347
Quantum policy gradient algorithms,Sofiene Jerbi;Arjan Cornelissen;Māris Ozols;Vedran Dunjko,"Understanding the power and limitations of quantum access to data in machine learning tasks is primordial to assess the potential of quantum computing in artificial intelligence. Previous works have already shown that speed-ups in learning are possible when given quantum access to reinforcement learning environments. Yet, the applicability of quantum algorithms in this setting remains very limited, notably in environments with large state and action spaces. In this work, we design quantum algorithms to train state-of-the-art reinforcement learning policies by exploiting quantum interactions with an environment. However, these algorithms only offer full quadratic speed-ups in sample complexity over their classical analogs when the trained policies satisfy some regularity conditions. Interestingly, we find that reinforcement learning policies derived from parametrized quantum circuits are well-behaved with respect to these conditions, which showcases the benefit of a fully-quantum reinforcement learning framework. △ Less","19 December, 2022",https://arxiv.org/pdf/2212.09328
Synthetic Data Augmentation Using GAN For Improved Automated Visual Inspection,Jože M. Rožanec;Patrik Zajec;Spyros Theodoropoulos;Erik Koehorst;Blaž Fortuna;Dunja Mladenić,"Quality control is a crucial activity performed by manufacturing companies to ensure their products conform to the requirements and specifications. The introduction of artificial intelligence models enables to automate the visual quality inspection, speeding up the inspection process and ensuring all products are evaluated under the same criteria. In this research, we compare supervised and unsupervised defect detection techniques and explore data augmentation techniques to mitigate the data imbalance in the context of automated visual inspection. Furthermore, we use Generative Adversarial Networks for data augmentation to enhance the classifiers' discriminative performance. Our results show that state-of-the-art unsupervised defect detection does not match the performance of supervised models but can be used to reduce the labeling workload by more than 50%. Furthermore, the best classification performance was achieved considering GAN-based data generation with AUC ROC scores equal to or higher than 0,9898, even when increasing the dataset imbalance by leaving only 25\% of the images denoting defective products. We performed the research with real-world data provided by Philips Consumer Lifestyle BV. △ Less","19 December, 2022",https://arxiv.org/pdf/2212.09317
"Unified, User and Task (UUT) Centered Artificial Intelligence for Metaverse Edge Computing",Terence Jie Chua;Wenhan Yu;Jun Zhao,"The Metaverse can be considered the extension of the present-day web, which integrates the physical and virtual worlds, delivering hyper-realistic user experiences. The inception of the Metaverse brings forth many ecosystem services such as content creation, social entertainment, in-world value transfer, intelligent traffic, healthcare. These services are compute-intensive and require computation offloading onto a Metaverse edge computing server (MECS). Existing Metaverse edge computing approaches do not efficiently and effectively handle resource allocation to ensure a fluid, seamless and hyper-realistic Metaverse experience required for Metaverse ecosystem services. Therefore, we introduce a new Metaverse-compatible, Unified, User and Task (UUT) centered artificial intelligence (AI)- based mobile edge computing (MEC) paradigm, which serves as a concept upon which future AI control algorithms could be built to develop a more user and task-focused MEC. △ Less","19 December, 2022",https://arxiv.org/pdf/2212.09295
ChatGPT: The End of Online Exam Integrity?,Teo Susnjak,"This study evaluated the ability of ChatGPT, a recently developed artificial intelligence (AI) agent, to perform high-level cognitive tasks and produce text that is indistinguishable from human-generated text. This capacity raises concerns about the potential use of ChatGPT as a tool for academic misconduct in online exams. The study found that ChatGPT is capable of exhibiting critical thinking skills and generating highly realistic text with minimal input, making it a potential threat to the integrity of online exams, particularly in tertiary education settings where such exams are becoming more prevalent. Returning to invigilated and oral exams could form part of the solution, while using advanced proctoring techniques and AI-text output detectors may be effective in addressing this issue, they are not likely to be foolproof solutions. Further research is needed to fully understand the implications of large language models like ChatGPT and to devise strategies for combating the risk of cheating using these tools. It is crucial for educators and institutions to be aware of the possibility of ChatGPT being used for cheating and to investigate measures to address it in order to maintain the fairness and validity of online exams for all students. △ Less","19 December, 2022",https://arxiv.org/pdf/2212.09292
Mobile Edge Computing for the Metaverse,Chang Liu;Yitong Wang;Jun Zhao,"The Metaverse has emerged as the next generation of the Internet. It aims to provide an immersive, persistent virtual space where people can live, learn, work and interact with each other. However, the existing technology is inadequate to guarantee high visual quality and ultra-low latency service for the Metaverse players. Mobile Edge Computing (MEC) is a paradigm where proximal edge servers are utilized to perform computation-intensive and latency-sensitive tasks like image processing and video analysis. In MEC, the large amount of data is processed by edge servers closest to where it is captured, thus significantly reducing the latency and providing almost real-time performance. In this paper, we integrate fundamental elements (5G and 6G wireless communications, Blockchain, digital twin and artificial intelligence) into the MEC framework to facilitate the Metaverse. We also elaborate on the research problems and applications in the MEC-enabled Metaverse. Finally, we provide a case study to establish a thorough knowledge of the user utility maximization problem in a real-world scenario and gain some insights about trends in potential research directions. △ Less","18 December, 2022",https://arxiv.org/pdf/2212.09229
Knowledge Transfer and Reuse: A Case Study of AI-enabled Resource Management in RAN Slicing,Hao Zhou;Melike Erol-Kantarci;Vincent Poor,"An efficient resource management scheme is critical to enable network slicing in 5G networks and in envisioned 6G networks, and artificial intelligence (AI) techniques offer promising solutions. Considering the rapidly emerging new machine learning techniques, such as graph learning, federated learning, and transfer learning, a timely survey is needed to provide an overview of resource management and network slicing techniques of AI-enabled wireless networks. This article provides such a survey along with an application of knowledge transfer in radio access network (RAN) slicing. In particular, we firs provide some background on resource management and network slicing, and review relevant state-of-the-art AI and machine learning (ML) techniques and their applications. Then, we introduce our AI-enabled knowledge transfer and reuse-based resource management (AKRM) scheme, where we apply transfer learning to improve system performance. Compared with most existing works, which focus on the training of standalone agents from scratch, the main difference of AKRM lies in its knowledge transfer and reuse capability between different tasks. Our paper aims to be a roadmap for researchers to use knowledge transfer schemes in AI-enabled wireless networks, and we provide a case study over the resource allocation problem in RAN slicing. △ Less","18 December, 2022",https://arxiv.org/pdf/2212.09172
Disentangling Learnable and Memorizable Data via Contrastive Learning for Semantic Communications,Christina Chaccour;Walid Saad,"Achieving artificially intelligent-native wireless networks is necessary for the operation of future 6G applications such as the metaverse. Nonetheless, current communication schemes are, at heart, a mere reconstruction process that lacks reasoning. One key solution that enables evolving wireless communication to a human-like conversation is semantic communications. In this paper, a novel machine reasoning framework is proposed to pre-process and disentangle source data so as to make it semantic-ready. In particular, a novel contrastive learning framework is proposed, whereby instance and cluster discrimination are performed on the data. These two tasks enable increasing the cohesiveness between data points mapping to semantically similar content elements and disentangling data points of semantically different content elements. Subsequently, the semantic deep clusters formed are ranked according to their level of confidence. Deep semantic clusters of highest confidence are considered learnable, semantic-rich data, i.e., data that can be used to build a language in a semantic communications system. The least confident ones are considered, random, semantic-poor, and memorizable data that must be transmitted classically. Our simulation results showcase the superiority of our contrastive learning approach in terms of semantic impact and minimalism. In fact, the length of the semantic representation achieved is minimized by 57.22% compared to vanilla semantic communication systems, thus achieving minimalist semantic representations. △ Less","18 December, 2022",https://arxiv.org/pdf/2212.09071
IMAGINE: An Integrated Model of Artificial Intelligence-Mediated Communication Effects,Frederic Guerrero-Sole,"Artificial Intelligence (AI) is transforming all fields of knowledge and production. From surgery, autonomous driving, to image and video creation, AI seems to make possible hitherto unimaginable processes of automation and efficient creation. Media and communication are not an exception, and we are currently witnessing the dawn of powerful AI tools capable of creating artistic images from simple keywords, or to capture emotions from facial expression. These examples may be only the beginning of what can be in the future the engines for automatic AI real time creation of media content linked to the emotional and behavioural responses of individuals. Although it may seem we are still far from there, it is already the moment to adapt our theories about media to the hypothetical scenario in which content production can be done without human intervention, and governed by the controlled any reactions of the individual to the exposure to media content. Following that, I propose the definition of the Integrated Model of Artificial Intelligence-Mediated Communication Effects (IMAGINE), and its consequences on the way we understand media evolution (Scolari, 2012) and we think about media effects (Potter, 2010). The conceptual framework proposed is aimed to help scholars theorizing and doing research in a scenario of continuous real-time connection between AI measurement of people's responses to media, and the AI creation of content, with the objective of optimizing and maximizing the processes of influence. Parasocial interaction and real-time beautification are used as examples to model the functioning of the IMAGINE process. △ Less","13 December, 2022",https://arxiv.org/pdf/2212.08658
Huruf: An Application for Arabic Handwritten Character Recognition Using Deep Learning,Minhaz Kamal;Fairuz Shaiara;Chowdhury Mohammad Abdullah;Sabbir Ahmed;Tasnim Ahmed;Md. Hasanul Kabir,"Handwriting Recognition has been a field of great interest in the Artificial Intelligence domain. Due to its broad use cases in real life, research has been conducted widely on it. Prominent work has been done in this field focusing mainly on Latin characters. However, the domain of Arabic handwritten character recognition is still relatively unexplored. The inherent cursive nature of the Arabic characters and variations in writing styles across individuals makes the task even more challenging. We identified some probable reasons behind this and proposed a lightweight Convolutional Neural Network-based architecture for recognizing Arabic characters and digits. The proposed pipeline consists of a total of 18 layers containing four layers each for convolution, pooling, batch normalization, dropout, and finally one Global average pooling and a Dense layer. Furthermore, we thoroughly investigated the different choices of hyperparameters such as the choice of the optimizer, kernel initializer, activation function, etc. Evaluating the proposed architecture on the publicly available 'Arabic Handwritten Character Dataset (AHCD)' and 'Modified Arabic handwritten digits Database (MadBase)' datasets, the proposed model respectively achieved an accuracy of 96.93% and 99.35% which is comparable to the state-of-the-art and makes it a suitable solution for real-life end-level applications. △ Less","24 December, 2022",https://arxiv.org/pdf/2212.08610
Three lines of defense against risks from AI,Jonas Schuett,"Organizations that develop and deploy artificial intelligence (AI) systems need to manage the associated risks - for economic, legal, and ethical reasons. However, it is not always clear who is responsible for AI risk management. The Three Lines of Defense (3LoD) model, which is considered best practice in many industries, might offer a solution. It is a risk management framework that helps organizations to assign and coordinate risk management roles and responsibilities. In this article, I suggest ways in which AI companies could implement the model. I also discuss how the model could help reduce risks from AI: it could identify and close gaps in risk coverage, increase the effectiveness of risk management practices, and enable the board of directors to oversee management more effectively. The article is intended to inform decision-makers at leading AI companies, regulators, and standard-setting bodies. △ Less","16 December, 2022",https://arxiv.org/pdf/2212.08364
Epistemological Equation for Analysing Uncontrollable States in Complex Systems: Quantifying Cyber Risks from the Internet of Things,Petar Radanliev;David De Roure;Pete Burnap;Omar Santos,"To enable quantitative risk assessment of uncontrollable risk states in complex and coupled IoT systems, a new epistemological equation is designed and tested though comparative and empirical analysis. The comparative analysis is conducted on national digital strategies, followed by an empirical analysis of cyber risk assessment approaches. The new epistemological analysis approach enables the assessment of uncontrollable risk states in complex IoT systems, which begin to resemble artificial intelligence, and can be used for a quantitative self-assessment of IoT cyber risk posture. △ Less","15 December, 2022",https://arxiv.org/pdf/2212.08141
"The Role of AI in Drug Discovery: Challenges, Opportunities, and Strategies",Alexandre Blanco-Gonzalez;Alfonso Cabezon;Alejandro Seco-Gonzalez;Daniel Conde-Torres;Paula Antelo-Riveiro;Angel Pineiro;Rebeca Garcia-Fandino,"Artificial intelligence (AI) has the potential to revolutionize the drug discovery process, offering improved efficiency, accuracy, and speed. However, the successful application of AI is dependent on the availability of high-quality data, the addressing of ethical concerns, and the recognition of the limitations of AI-based approaches. In this article, the benefits, challenges and drawbacks of AI in this field are reviewed, and possible strategies and approaches for overcoming the present obstacles are proposed. The use of data augmentation, explainable AI, and the integration of AI with traditional experimental methods, as well as the potential advantages of AI in pharmaceutical research are also discussed. Overall, this review highlights the potential of AI in drug discovery and provides insights into the challenges and opportunities for realizing its potential in this field. Note from the human-authors: This article was created to test the ability of ChatGPT, a chatbot based on the GPT-3.5 language model, to assist human authors in writing review articles. The text generated by the AI following our instructions (see Supporting Information) was used as a starting point, and its ability to automatically generate content was evaluated. After conducting a thorough review, human authors practically rewrote the manuscript, striving to maintain a balance between the original proposal and scientific criteria. The advantages and limitations of using AI for this purpose are discussed in the last section. △ Less","8 December, 2022",https://arxiv.org/pdf/2212.08104
Can REF output quality scores be assigned by AI? Experimental evidence,Mike Thelwall;Kayvan Kousha;Mahshid Abdoli;Emma Stuart;Meiko Makita;Paul Wilson;Jonathan Levitt,This document describes strategies for using Artificial Intelligence (AI) to predict some journal article scores in future research assessment exercises. Five strategies have been assessed.,"11 December, 2022",https://arxiv.org/pdf/2212.08041
Online Handbook of Argumentation for AI: Volume 3,Lars Bengel;Elfia Bezou-Vrakatseli;Lydia Blümel;Federico Castagna;Giulia D'Agostino;Daphne Odekerken;Minal Suresh Patil;Jordan Robinson;Hao Wu;Andreas Xydis,"This volume contains revised versions of the papers selected for the third volume of the Online Handbook of Argumentation for AI (OHAAI). Previously, formal theories of argument and argument interaction have been proposed and studied, and this has led to the more recent study of computational models of argument. Argumentation, as a field within artificial intelligence (AI), is highly relevant for researchers interested in symbolic representations of knowledge and defeasible reasoning. The purpose of this handbook is to provide an open access and curated anthology for the argumentation research community. OHAAI is designed to serve as a research hub to keep track of the latest and upcoming PhD-driven research on the theory and application of argumentation in all areas related to AI. △ Less","15 December, 2022",https://arxiv.org/pdf/2212.07996
Calibrating AI Models for Wireless Communications via Conformal Prediction,Kfir M. Cohen;Sangwoo Park;Osvaldo Simeone;Shlomo Shamai,"When used in complex engineered systems, such as communication networks, artificial intelligence (AI) models should be not only as accurate as possible, but also well calibrated. A well-calibrated AI model is one that can reliably quantify the uncertainty of its decisions, assigning high confidence levels to decisions that are likely to be correct and low confidence levels to decisions that are likely to be erroneous. This paper investigates the application of conformal prediction as a general framework to obtain AI models that produce decisions with formal calibration guarantees. Conformal prediction transforms probabilistic predictors into set predictors that are guaranteed to contain the correct answer with a probability chosen by the designer. Such formal calibration guarantees hold irrespective of the true, unknown, distribution underlying the generation of the variables of interest, and can be defined in terms of ensemble or time-averaged probabilities. In this paper, conformal prediction is applied for the first time to the design of AI for communication systems in conjunction to both frequentist and Bayesian learning, focusing on demodulation, modulation classification, and channel prediction. △ Less","15 December, 2022",https://arxiv.org/pdf/2212.07775
Interpretable ML for Imbalanced Data,Damien A. Dablain;Colin Bellinger;Bartosz Krawczyk;David W. Aha;Nitesh V. Chawla,"Deep learning models are being increasingly applied to imbalanced data in high stakes fields such as medicine, autonomous driving, and intelligence analysis. Imbalanced data compounds the black-box nature of deep networks because the relationships between classes may be highly skewed and unclear. This can reduce trust by model users and hamper the progress of developers of imbalanced learning algorithms. Existing methods that investigate imbalanced data complexity are geared toward binary classification, shallow learning models and low dimensional data. In addition, current eXplainable Artificial Intelligence (XAI) techniques mainly focus on converting opaque deep learning models into simpler models (e.g., decision trees) or mapping predictions for specific instances to inputs, instead of examining global data properties and complexities. Therefore, there is a need for a framework that is tailored to modern deep networks, that incorporates large, high dimensional, multi-class datasets, and uncovers data complexities commonly found in imbalanced data (e.g., class overlap, sub-concepts, and outlier instances). We propose a set of techniques that can be used by both deep learning model users to identify, visualize and understand class prototypes, sub-concepts and outlier instances; and by imbalanced learning algorithm developers to detect features and class exemplars that are key to model performance. Our framework also identifies instances that reside on the border of class decision boundaries, which can carry highly discriminative information. Unlike many existing XAI techniques which map model decisions to gray-scale pixel locations, we use saliency through back-propagation to identify and aggregate image color bands across entire classes. Our framework is publicly available at \url{https://github.com/dd1github/XAI_for_Imbalanced_Learning} △ Less","15 December, 2022",https://arxiv.org/pdf/2212.07743
Build-a-Bot: Teaching Conversational AI Using a Transformer-Based Intent Recognition and Question Answering Architecture,Kate Pearce;Sharifa Alghowinem;Cynthia Breazeal,"As artificial intelligence (AI) becomes a prominent part of modern life, AI literacy is becoming important for all citizens, not just those in technology careers. Previous research in AI education materials has largely focused on the introduction of terminology as well as AI use cases and ethics, but few allow students to learn by creating their own machine learning models. Therefore, there is a need for enriching AI educational tools with more adaptable and flexible platforms for interested educators with any level of technical experience to utilize within their teaching material. As such, we propose the development of an open-source tool (Build-a-Bot) for students and teachers to not only create their own transformer-based chatbots based on their own course material, but also learn the fundamentals of AI through the model creation process. The primary concern of this paper is the creation of an interface for students to learn the principles of artificial intelligence by using a natural language pipeline to train a customized model to answer questions based on their own school curriculums. The model uses contexts given by their instructor, such as chapters of a textbook, to answer questions and is deployed on an interactive chatbot/voice agent. The pipeline teaches students data collection, data augmentation, intent recognition, and question answering by having them work through each of these processes while creating their AI agent, diverging from previous chatbot work where students and teachers use the bots as black-boxes with no abilities for customization or the bots lack AI capabilities, with the majority of dialogue scripts being rule-based. In addition, our tool is designed to make each step of this pipeline intuitive for students at a middle-school level. Further work primarily lies in providing our tool to schools and seeking student and teacher evaluations. △ Less","14 December, 2022",https://arxiv.org/pdf/2212.07542
Speech and Natural Language Processing Technologies for Pseudo-Pilot Simulator,Amrutha Prasad;Juan Zuluaga-Gomez;Petr Motlicek;Saeed Sarfjoo;Iuliia Nigmatulina;Karel Vesely,"This paper describes a simple yet efficient repetition-based modular system for speeding up air-traffic controllers (ATCos) training. E.g., a human pilot is still required in EUROCONTROL's ESCAPE lite simulator (see https://www.eurocontrol.int/simulator/escape) during ATCo training. However, this need can be substituted by an automatic system that could act as a pilot. In this paper, we aim to develop and integrate a pseudo-pilot agent into the ATCo training pipeline by merging diverse artificial intelligence (AI) powered modules. The system understands the voice communications issued by the ATCo, and, in turn, it generates a spoken prompt that follows the pilot's phraseology to the initial communication. Our system mainly relies on open-source AI tools and air traffic control (ATC) databases, thus, proving its simplicity and ease of replicability. The overall pipeline is composed of the following: (1) a submodule that receives and pre-processes the input stream of raw audio, (2) an automatic speech recognition (ASR) system that transforms audio into a sequence of words; (3) a high-level ATC-related entity parser, which extracts relevant information from the communication, i.e., callsigns and commands, and finally, (4) a speech synthesizer submodule that generates responses based on the high-level ATC entities previously extracted. Overall, we show that this system could pave the way toward developing a real proof-of-concept pseudo-pilot system. Hence, speeding up the training of ATCos while drastically reducing its overall cost. △ Less","14 December, 2022",https://arxiv.org/pdf/2212.07164
Explainable Artificial Intelligence in Retinal Imaging for the detection of Systemic Diseases,Ayushi Raj Bhatt;Rajkumar Vaghashiya;Meghna Kulkarni;Dr Prakash Kamaraj,"Explainable Artificial Intelligence (AI) in the form of an interpretable and semiautomatic approach to stage grading ocular pathologies such as Diabetic retinopathy, Hypertensive retinopathy, and other retinopathies on the backdrop of major systemic diseases. The experimental study aims to evaluate an explainable staged grading process without using deep Convolutional Neural Networks (CNNs) directly. Many current CNN-based deep neural networks used for diagnosing retinal disorders might have appreciable performance but fail to pinpoint the basis driving their decisions. To improve these decisions' transparency, we have proposed a clinician-in-the-loop assisted intelligent workflow that performs a retinal vascular assessment on the fundus images to derive quantifiable and descriptive parameters. The retinal vessel parameters meta-data serve as hyper-parameters for better interpretation and explainability of decisions. The semiautomatic methodology aims to have a federated approach to AI in healthcare applications with more inputs and interpretations from clinicians. The baseline process involved in the machine learning pipeline through image processing techniques for optic disc detection, vessel segmentation, and arteriole/venule identification. △ Less","14 December, 2022",https://arxiv.org/pdf/2212.07058
A Survey on Privacy of Personal and Non-Personal Data in B5G/6G Networks,Chamara Sandeepa;Bartlomiej Siniarski;Nicolas Kourtellis;Shen Wang;Madhusanka Liyanage,"The upcoming Beyond 5G (B5G) and 6G networks are expected to provide enhanced capabilities such as ultra-high data rates, dense connectivity, and high scalability. It opens many possibilities for a new generation of services driven by Artificial Intelligence (AI) and billions of interconnected smart devices. However, with this expected massive upgrade, the privacy of people, organizations, and states is becoming a rising concern. The recent introduction of privacy laws and regulations for personal and non-personal data signals that global awareness is emerging in the current privacy landscape. Yet, many gaps need to be identified in the case of two data types. If not detected, they can lead to significant privacy leakages and attacks that will affect billions of people and organizations who utilize B5G/6G. This survey is a comprehensive study of personal and non-personal data privacy in B5G/6G to identify the current progress and future directions to ensure data privacy. We provide a detailed comparison of the two data types and a set of related privacy goals for B5G/6G. Next, we bring data privacy issues with possible solutions. This paper also provides future directions to preserve personal and non-personal data privacy in future networks. △ Less","13 December, 2022",https://arxiv.org/pdf/2212.06987
Enabling the Wireless Metaverse via Semantic Multiverse Communication,Jihong Park;Jinho Choi;Seong-Lyun Kim;Mehdi Bennis,"Metaverse over wireless networks is an emerging use case of the sixth generation (6G) wireless systems, posing unprecedented challenges in terms of its multi-modal data transmissions with stringent latency and reliability requirements. Towards enabling this wireless metaverse, in this article we propose a novel semantic communication (SC) framework by decomposing the metaverse into human/machine agent-specific semantic multiverses (SMs). An SM stored at each agent comprises a semantic encoder and a generator, leveraging recent advances in generative artificial intelligence (AI). To improve communication efficiency, the encoder learns the semantic representations (SRs) of multi-modal data, while the generator learns how to manipulate them for locally rendering scenes and interactions in the metaverse. Since these learned SMs are biased towards local environments, their success hinges on synchronizing heterogeneous SMs in the background while communicating SRs in the foreground, turning the wireless metaverse problem into the problem of semantic multiverse communication (SMC). Based on this SMC architecture, we propose several promising algorithmic and analytic tools for modeling and designing SMC, ranging from distributed learning and multi-agent reinforcement learning (MARL) to signaling games and symbolic AI. △ Less","13 December, 2022",https://arxiv.org/pdf/2212.06908
3rd Continual Learning Workshop Challenge on Egocentric Category and Instance Level Object Understanding,Lorenzo Pellegrini;Chenchen Zhu;Fanyi Xiao;Zhicheng Yan;Antonio Carta;Matthias De Lange;Vincenzo Lomonaco;Roshan Sumbaly;Pau Rodriguez;David Vazquez,"Continual Learning, also known as Lifelong or Incremental Learning, has recently gained renewed interest among the Artificial Intelligence research community. Recent research efforts have quickly led to the design of novel algorithms able to reduce the impact of the catastrophic forgetting phenomenon in deep neural networks. Due to this surge of interest in the field, many competitions have been held in recent years, as they are an excellent opportunity to stimulate research in promising directions. This paper summarizes the ideas, design choices, rules, and results of the challenge held at the 3rd Continual Learning in Computer Vision (CLVision) Workshop at CVPR 2022. The focus of this competition is the complex continual object detection task, which is still underexplored in literature compared to classification tasks. The challenge is based on the challenge version of the novel EgoObjects dataset, a large-scale egocentric object dataset explicitly designed to benchmark continual learning algorithms for egocentric category-/instance-level object understanding, which covers more than 1k unique main objects and 250+ categories in around 100k video frames. △ Less","13 December, 2022",https://arxiv.org/pdf/2212.06833
Classification of Distraction Levels Using Hybrid Deep Neural Networks From EEG Signals,Dae-Hyeok Lee;Sung-Jin Kim;Yeon-Woo Choi,"Non-invasive brain-computer interface technology has been developed for detecting human mental states with high performances. Detection of the pilots' mental states is particularly critical because their abnormal mental states could cause catastrophic accidents. In this study, we presented the feasibility of classifying distraction levels (namely, normal state, low distraction, and high distraction) by applying the deep learning method. To the best of our knowledge, this study is the first attempt to classify distraction levels under a flight environment. We proposed a model for classifying distraction levels. A total of ten pilots conducted the experiment in a simulated flight environment. The grand-average accuracy was 0.8437 for classifying distraction levels across all subjects. Hence, we believe that it will contribute significantly to autonomous driving or flight based on artificial intelligence technology in the future. △ Less","13 December, 2022",https://arxiv.org/pdf/2212.06830
Real-Time Artificial Intelligence Assistance for Safe Laparoscopic Cholecystectomy: Early-Stage Clinical Evaluation,Pietro Mascagni;Deepak Alapatt;Alfonso Lapergola;Armine Vardazaryan;Jean-Paul Mazellier;Bernard Dallemagne;Didier Mutter;Nicolas Padoy,"Artificial intelligence is set to be deployed in operating rooms to improve surgical care. This early-stage clinical evaluation shows the feasibility of concurrently attaining real-time, high-quality predictions from several deep neural networks for endoscopic video analysis deployed for assistance during three laparoscopic cholecystectomies. △ Less","13 December, 2022",https://arxiv.org/pdf/2212.06809
Selected Trends in Artificial Intelligence for Space Applications,Dario Izzo;Gabriele Meoni;Pablo Gómez;Dominik Dold;Alexander Zoechbauer,"The development and adoption of artificial intelligence (AI) technologies in space applications is growing quickly as the consensus increases on the potential benefits introduced. As more and more aerospace engineers are becoming aware of new trends in AI, traditional approaches are revisited to consider the applications of emerging AI technologies. Already at the time of writing, the scope of AI-related activities across academia, the aerospace industry and space agencies is so wide that an in-depth review would not fit in these pages. In this chapter we focus instead on two main emerging trends we believe capture the most relevant and exciting activities in the field: differentiable intelligence and on-board machine learning. Differentiable intelligence, in a nutshell, refers to works making extensive use of automatic differentiation frameworks to learn the parameters of machine learning or related models. Onboard machine learning considers the problem of moving inference, as well as learning, onboard. Within these fields, we discuss a few selected projects originating from the European Space Agency's (ESA) Advanced Concepts Team (ACT), giving priority to advanced topics going beyond the transposition of established AI techniques and practices to the space domain. △ Less","17 December, 2022",https://arxiv.org/pdf/2212.06662
AI Model Utilization Measurements For Finding Class Encoding Patterns,Peter Bajcsy;Antonio Cardone;Chenyi Ling;Philippe Dessauw;Michael Majurski;Tim Blattner;Derek Juba;Walid Keyrouz,"This work addresses the problems of (a) designing utilization measurements of trained artificial intelligence (AI) models and (b) explaining how training data are encoded in AI models based on those measurements. The problems are motivated by the lack of explainability of AI models in security and safety critical applications, such as the use of AI models for classification of traffic signs in self-driving cars. We approach the problems by introducing theoretical underpinnings of AI model utilization measurement and understanding patterns in utilization-based class encodings of traffic signs at the level of computation graphs (AI models), subgraphs, and graph nodes. Conceptually, utilization is defined at each graph node (computation unit) of an AI model based on the number and distribution of unique outputs in the space of all possible outputs (tensor-states). In this work, utilization measurements are extracted from AI models, which include poisoned and clean AI models. In contrast to clean AI models, the poisoned AI models were trained with traffic sign images containing systematic, physically realizable, traffic sign modifications (i.e., triggers) to change a correct class label to another label in a presence of such a trigger. We analyze class encodings of such clean and poisoned AI models, and conclude with implications for trojan injection and detection. △ Less","11 December, 2022",https://arxiv.org/pdf/2212.06576
One-shot Machine Teaching: Cost Very Few Examples to Converge Faster,Chen Zhang;Xiaofeng Cao;Yi Chang;Ivor W Tsang,"Artificial intelligence is to teach machines to take actions like humans. To achieve intelligent teaching, the machine learning community becomes to think about a promising topic named machine teaching where the teacher is to design the optimal (usually minimal) teaching set given a target model and a specific learner. However, previous works usually require numerous teaching examples along with large iterations to guide learners to converge, which is costly. In this paper, we consider a more intelligent teaching paradigm named one-shot machine teaching which costs fewer examples to converge faster. Different from typical teaching, this advanced paradigm establishes a tractable mapping from the teaching set to the model parameter. Theoretically, we prove that this mapping is surjective, which serves to an existence guarantee of the optimal teaching set. Then, relying on the surjective mapping from the teaching set to the parameter, we develop a design strategy of the optimal teaching set under appropriate settings, of which two popular efficiency metrics, teaching dimension and iterative teaching dimension are one. Extensive experiments verify the efficiency of our strategy and further demonstrate the intelligence of this new teaching paradigm. △ Less","13 December, 2022",https://arxiv.org/pdf/2212.06416
ADEV: Sound Automatic Differentiation of Expected Values of Probabilistic Programs,Alexander K. Lew;Mathieu Huot;Sam Staton;Vikash K. Mansinghka,"Optimizing the expected values of probabilistic processes is a central problem in computer science and its applications, arising in fields ranging from artificial intelligence to operations research to statistical computing. Unfortunately, automatic differentiation techniques developed for deterministic programs do not in general compute the correct gradients needed for widely used solutions based on gradient-based optimization. In this paper, we present ADEV, an extension to forward-mode AD that correctly differentiates the expectations of probabilistic processes represented as programs that make random choices. Our algorithm is a source-to-source program transformation on an expressive, higher-order language for probabilistic computation, with both discrete and continuous probability distributions. The result of our transformation is a new probabilistic program, whose expected return value is the derivative of the original program's expectation. This output program can be run to generate unbiased Monte Carlo estimates of the desired gradient, which can then be used within the inner loop of stochastic gradient descent. We prove ADEV correct using logical relations over the denotations of the source and target probabilistic programs. Because it modularly extends forward-mode AD, our algorithm lends itself to a concise implementation strategy, which we exploit to develop a prototype in just a few dozen lines of Haskell (https://github.com/probcomp/adev). △ Less","13 December, 2022",https://arxiv.org/pdf/2212.06386
Towards Seamless Management of AI Models in High-Performance Computing,Sixing Yu;Murali Emani;Chunhua Liao;Pei-Hung Lin;Tristan Vanderbruggen;Xipeng Shen;Ali Jannesari,"With the increasing prevalence of artificial intelligence (AI) in diverse science/engineering communities, AI models emerge on an unprecedented scale among various domains. However, given the complexity and diversity of the software and hardware environments, reusing AI artifacts (models and datasets) is extremely challenging, especially with AI-driven science applications. Building an ecosystem to run and reuse AI applications/datasets at scale efficiently becomes increasingly essential for diverse science and engineering and high-performance computing (HPC) communities. In this paper, we innovate over an HPC-AI ecosystem -- HPCFair, which enables the Findable, Accessible, Interoperable, and Reproducible (FAIR) principles. HPCFair enables the collection of AI models/datasets allowing users to download/upload AI artifacts with authentications. Most importantly, our proposed framework provides user-friendly APIs for users to easily run inference jobs and customize AI artifacts to their tasks as needed. Our results show that, with HPCFair API, users irrespective of technical expertise in AI, can easily leverage AI artifacts to their tasks with minimal effort. △ Less","12 December, 2022",https://arxiv.org/pdf/2212.06352
Generative artificial intelligence-enabled dynamic detection of nicotine-related circuits,Changwei Gong;Changhong Jing;Ye Li;Xinan Liu;Zuxin Chen;Shuqiang Wang,"The identification of addiction-related circuits is critical for explaining addiction processes and developing addiction treatments. And models of functional addiction circuits developed from functional imaging are an effective tool for discovering and verifying addiction circuits. However, analyzing functional imaging data of addiction and detecting functional addiction circuits still have challenges. We have developed a data-driven and end-to-end generative artificial intelligence(AI) framework to address these difficulties. The framework integrates dynamic brain network modeling and novel network architecture networks architecture, including temporal graph Transformer and contrastive learning modules. A complete workflow is formed by our generative AI framework: the functional imaging data, from neurobiological experiments, and computational modeling, to end-to-end neural networks, is transformed into dynamic nicotine addiction-related circuits. It enables the detection of addiction-related brain circuits with dynamic properties and reveals the underlying mechanisms of addiction. △ Less","12 December, 2022",https://arxiv.org/pdf/2212.06330
Dual adaptive training of photonic neural networks,Ziyang Zheng;Zhengyang Duan;Hang Chen;Rui Yang;Sheng Gao;Haiou Zhang;Hongkai Xiong;Xing Lin,"Photonic neural network (PNN) is a remarkable analog artificial intelligence (AI) accelerator that computes with photons instead of electrons to feature low latency, high energy efficiency, and high parallelism. However, the existing training approaches cannot address the extensive accumulation of systematic errors in large-scale PNNs, resulting in a significant decrease in model performance in physical systems. Here, we propose dual adaptive training (DAT) that allows the PNN model to adapt to substantial systematic errors and preserves its performance during the deployment. By introducing the systematic error prediction networks with task-similarity joint optimization, DAT achieves the high similarity mapping between the PNN numerical models and physical systems and high-accurate gradient calculations during the dual backpropagation training. We validated the effectiveness of DAT by using diffractive PNNs and interference-based PNNs on image classification tasks. DAT successfully trained large-scale PNNs under major systematic errors and preserved the model classification accuracies comparable to error-free systems. The results further demonstrated its superior performance over the state-of-the-art in situ training approaches. DAT provides critical support for constructing large-scale PNNs to achieve advanced architectures and can be generalized to other types of AI systems with analog computing errors. △ Less","9 December, 2022",https://arxiv.org/pdf/2212.06141
Reinforcement Learning and Tree Search Methods for the Unit Commitment Problem,Patrick de Mars,"The unit commitment (UC) problem, which determines operating schedules of generation units to meet demand, is a fundamental task in power systems operation. Existing UC methods using mixed-integer programming are not well-suited to highly stochastic systems. Approaches which more rigorously account for uncertainty could yield large reductions in operating costs by reducing spinning reserve requirements; operating power stations at higher efficiencies; and integrating greater volumes of variable renewables. A promising approach to solving the UC problem is reinforcement learning (RL), a methodology for optimal decision-making which has been used to conquer long-standing grand challenges in artificial intelligence. This thesis explores the application of RL to the UC problem and addresses challenges including robustness under uncertainty; generalisability across multiple problem instances; and scaling to larger power systems than previously studied. To tackle these issues, we develop guided tree search, a novel methodology combining model-free RL and model-based planning. The UC problem is formalised as a Markov decision process and we develop an open-source environment based on real data from Great Britain's power system to train RL agents. In problems of up to 100 generators, guided tree search is shown to be competitive with deterministic UC methods, reducing operating costs by up to 1.4\%. An advantage of RL is that the framework can be easily extended to incorporate considerations important to power systems operators such as robustness to generator failure, wind curtailment or carbon prices. When generator outages are considered, guided tree search saves over 2\% in operating costs as compared with methods using conventional N-x reserve criteria. △ Less","12 December, 2022",https://arxiv.org/pdf/2212.06001
Sharing Linkable Learning Objects with the use of Metadata and a Taxonomy Assistant for Categorization,Valentina Franzoni;Sergio Tasso;Simonetta Pallottelli;Damiano Perri,"In this work, a re-design of the Moodledata module functionalities is presented to share learning objects between e-learning content platforms, e.g., Moodle and G-Lorep, in a linkable object format. The e-learning courses content of the Drupal-based Content Management System G-Lorep for academic learning is exchanged designing an object incorporating metadata to support the reuse and the classification in its context. In such an Artificial Intelligence environment, the exchange of Linkable Learning Objects can be used for dialogue between Learning Systems to obtain information, especially with the use of semantic or structural similarity measures to enhance the existent Taxonomy Assistant for advanced automated classification. △ Less","9 December, 2022",https://arxiv.org/pdf/2212.05947
Image-based Artificial Intelligence empowered surrogate model and shape morpher for real-time blank shape optimisation in the hot stamping process,Haosu Zhou;Nan Li,"As the complexity of modern manufacturing technologies increases, traditional trial-and-error design, which requires iterative and expensive simulations, becomes unreliable and time-consuming. This difficulty is especially significant for the design of hot-stamped safety-critical components, such as ultra-high-strength-steel (UHSS) B-pillars. To reduce design costs and ensure manufacturability, scalar-based Artificial-Intelligence-empowered surrogate modelling (SAISM) has been investigated and implemented, which can allow real-time manufacturability-constrained structural design optimisation. However, SAISM suffers from low accuracy and generalisability, and usually requires a high volume of training samples. To solve this problem, an image-based Artificial-intelligence-empowered surrogate modelling (IAISM) approach is developed in this research, in combination with an auto-decoder-based blank shape generator. The IAISM, which is based on a Mask-Res-SE-U-Net architecture, is trained to predict the full thinning field of the as-formed component given an arbitrary blank shape. Excellent prediction performance of IAISM is achieved with only 256 training samples, which indicates the small-data learning nature of engineering AI tasks using structured data representations. The trained auto-decoder, trained Mask-Res-SE-U-Net, and Adam optimiser are integrated to conduct blank optimisation by modifying the latent vector. The optimiser can rapidly find blank shapes that satisfy manufacturability criteria. As a high-accuracy and generalisable surrogate modelling and optimisation tool, the proposed pipeline is promising to be integrated into a full-chain digital twin to conduct real-time, multi-objective design optimisation. △ Less","1 December, 2022",https://arxiv.org/pdf/2212.05885
A Roadmap to Domain Knowledge Integration in Machine Learning,Himel Das Gupta;Victor S. Sheng,"Many machine learning algorithms have been developed in recent years to enhance the performance of a model in different aspects of artificial intelligence. But the problem persists due to inadequate data and resources. Integrating knowledge in a machine learning model can help to overcome these obstacles up to a certain degree. Incorporating knowledge is a complex task though because of various forms of knowledge representation. In this paper, we will give a brief overview of these different forms of knowledge integration and their performance in certain machine learning tasks. △ Less","12 December, 2022",https://arxiv.org/pdf/2212.05712
Energy-based General Sequential Episodic Memory Networks at the Adiabatic Limit,Arjun Karuvally;Terry J. Sejnowski;Hava T. Siegelmann,"The General Associative Memory Model (GAMM) has a constant state-dependant energy surface that leads the output dynamics to fixed points, retrieving single memories from a collection of memories that can be asynchronously preloaded. We introduce a new class of General Sequential Episodic Memory Models (GSEMM) that, in the adiabatic limit, exhibit temporally changing energy surface, leading to a series of meta-stable states that are sequential episodic memories. The dynamic energy surface is enabled by newly introduced asymmetric synapses with signal propagation delays in the network's hidden layer. We study the theoretical and empirical properties of two memory models from the GSEMM class, differing in their activation functions. LISEM has non-linearities in the feature layer, whereas DSEM has non-linearity in the hidden layer. In principle, DSEM has a storage capacity that grows exponentially with the number of neurons in the network. We introduce a learning rule for the synapses based on the energy minimization principle and show it can learn single memories and their sequential relationships online. This rule is similar to the Hebbian learning algorithm and Spike-Timing Dependent Plasticity (STDP), which describe conditions under which synapses between neurons change strength. Thus, GSEMM combines the static and dynamic properties of episodic memory under a single theoretical framework and bridges neuroscience, machine learning, and artificial intelligence. △ Less","11 December, 2022",https://arxiv.org/pdf/2212.05563
Predicting article quality scores with machine learning: The UK Research Excellence Framework,Mike Thelwall;Kayvan Kousha;Mahshid Abdoli;Emma Stuart;Meiko Makita;Paul Wilson;Jonathan Levitt;Petr Knoth;Matteo Cancellieri,"National research evaluation initiatives and incentive schemes have previously chosen between simplistic quantitative indicators and time-consuming peer review, sometimes supported by bibliometrics. Here we assess whether artificial intelligence (AI) could provide a third alternative, estimating article quality using more multiple bibliometric and metadata inputs. We investigated this using provisional three-level REF2021 peer review scores for 84,966 articles submitted to the UK Research Excellence Framework 2021, matching a Scopus record 2014-18 and with a substantial abstract. We found that accuracy is highest in the medical and physical sciences Units of Assessment (UoAs) and economics, reaching 42% above the baseline (72% overall) in the best case. This is based on 1000 bibliometric inputs and half of the articles used for training in each UoA. Prediction accuracies above the baseline for the social science, mathematics, engineering, arts, and humanities UoAs were much lower or close to zero. The Random Forest Classifier (standard or ordinal) and Extreme Gradient Boosting Classifier algorithms performed best from the 32 tested. Accuracy was lower if UoAs were merged or replaced by Scopus broad categories. We increased accuracy with an active learning strategy and by selecting articles with higher prediction probabilities, as estimated by the algorithms, but this substantially reduced the number of scores predicted. △ Less","11 December, 2022",https://arxiv.org/pdf/2212.05415
A systematic literature review of cyberwarfare and state-sponsored hacking teams,Darshan Harsora;Khushalkumar Khoyani,"It is expected that the creation of next-generation wireless networks would result in the availability of high-speed and low-latency connectivity for every part of our life. As a result, it is important that the network is secure. The network's security environment has grown more complicated as a result of the growing number of devices and the diversity of services that 5G will provide. This is why it is important that the development of effective security solutions is carried out early. Our findings of this review have revealed the various directions that will be pursued in the development of next-generation wireless networks. Some of these include the use of Artificial Intelligence and Software Defined Mobile Networks. The threat environment for 5G networks, security weaknesses in the new technology paradigms that 5G will embrace, and provided solutions presented in the key studies in the field of 5G cyber security are all described in this systematic literature review for prospective researchers. Future research directions to protect wireless networks beyond 5G are also covered. △ Less","9 December, 2022",https://arxiv.org/pdf/2212.05166
A perspective on physical reservoir computing with nanomagnetic devices,Dan A Allwood;Matthew O A Ellis;David Griffin;Thomas J Hayward;Luca Manneschi;Mohammad F KH Musameh;Simon O'Keefe;Susan Stepney;Charles Swindells;Martin A Trefzer;Eleni Vasilaki;Guru Venkat;Ian Vidamour;Chester Wringe,"Neural networks have revolutionized the area of artificial intelligence and introduced transformative applications to almost every scientific field and industry. However, this success comes at a great price; the energy requirements for training advanced models are unsustainable. One promising way to address this pressing issue is by developing low-energy neuromorphic hardware that directly supports the algorithm's requirements. The intrinsic non-volatility, non-linearity, and memory of spintronic devices make them appealing candidates for neuromorphic devices. Here we focus on the reservoir computing paradigm, a recurrent network with a simple training algorithm suitable for computation with spintronic devices since they can provide the properties of non-linearity and memory. We review technologies and methods for developing neuromorphic spintronic devices and conclude with critical open issues to address before such devices become widely used. △ Less","9 December, 2022",https://arxiv.org/pdf/2212.04851
Album cover art image generation with Generative Adversarial Networks,Felipe Perez Stoppa;Ester Vidaña-Vila;Joan Navarro,"Generative Adversarial Networks (GANs) were introduced by Goodfellow in 2014, and since then have become popular for constructing generative artificial intelligence models. However, the drawbacks of such networks are numerous, like their longer training times, their sensitivity to hyperparameter tuning, several types of loss and optimization functions and other difficulties like mode collapse. Current applications of GANs include generating photo-realistic human faces, animals and objects. However, I wanted to explore the artistic ability of GANs in more detail, by using existing models and learning from them. This dissertation covers the basics of neural networks and works its way up to the particular aspects of GANs, together with experimentation and modification of existing available models, from least complex to most. The intention is to see if state of the art GANs (specifically StyleGAN2) can generate album art covers and if it is possible to tailor them by genre. This was attempted by first familiarizing myself with 3 existing GANs architectures, including the state of the art StyleGAN2. The StyleGAN2 code was used to train a model with a dataset containing 80K album cover images, then used to style images by picking curated images and mixing their styles. △ Less","9 December, 2022",https://arxiv.org/pdf/2212.04844
"AI-based Fog and Edge Computing: A Systematic Review, Taxonomy and Future Directions",Sundas Iftikhar;Sukhpal Singh Gill;Chenghao Song;Minxian Xu;Mohammad Sadegh Aslanpour;Adel N. Toosi;Junhui Du;Huaming Wu;Shreya Ghosh;Deepraj Chowdhury;Muhammed Golec;Mohit Kumar;Ahmed M. Abdelmoniem;Felix Cuadrado;Blesson Varghese;Omer Rana;Schahram Dustdar;Steve Uhlig,"Resource management in computing is a very challenging problem that involves making sequential decisions. Resource limitations, resource heterogeneity, dynamic and diverse nature of workload, and the unpredictability of fog/edge computing environments have made resource management even more challenging to be considered in the fog landscape. Recently Artificial Intelligence (AI) and Machine Learning (ML) based solutions are adopted to solve this problem. AI/ML methods with the capability to make sequential decisions like reinforcement learning seem most promising for these type of problems. But these algorithms come with their own challenges such as high variance, explainability, and online training. The continuously changing fog/edge environment dynamics require solutions that learn online, adopting changing computing environment. In this paper, we used standard review methodology to conduct this Systematic Literature Review (SLR) to analyze the role of AI/ML algorithms and the challenges in the applicability of these algorithms for resource management in fog/edge computing environments. Further, various machine learning, deep learning and reinforcement learning techniques for edge AI management have been discussed. Furthermore, we have presented the background and current status of AI/ML-based Fog/Edge Computing. Moreover, a taxonomy of AI/ML-based resource management techniques for fog/edge computing has been proposed and compared the existing techniques based on the proposed taxonomy. Finally, open challenges and promising future research directions have been identified and discussed in the area of AI/ML-based fog/edge computing. △ Less","8 December, 2022",https://arxiv.org/pdf/2212.04645
UNet Based Pipeline for Lung Segmentation from Chest X-Ray Images,Shashank Shekhar;Ritika Nandi;H Srikanth Kamath,"Biomedical image segmentation is one of the fastest growing fields which has seen extensive automation through the use of Artificial Intelligence. This has enabled widespread adoption of accurate techniques to expedite the screening and diagnostic processes which would otherwise take several days to finalize. In this paper, we present an end-to-end pipeline to segment lungs from chest X-ray images, training the neural network model on the Japanese Society of Radiological Technology (JSRT) dataset, using UNet to enable faster processing of initial screening for various lung disorders. The pipeline developed can be readily used by medical centers with just the provision of X-Ray images as input. The model will perform the preprocessing, and provide a segmented image as the final output. It is expected that this will drastically reduce the manual effort involved and lead to greater accessibility in resource-constrained locations. △ Less","8 December, 2022",https://arxiv.org/pdf/2212.04617
XRand: Differentially Private Defense against Explanation-Guided Attacks,Truc Nguyen;Phung Lai;NhatHai Phan;My T. Thai,"Recent development in the field of explainable artificial intelligence (XAI) has helped improve trust in Machine-Learning-as-a-Service (MLaaS) systems, in which an explanation is provided together with the model prediction in response to each query. However, XAI also opens a door for adversaries to gain insights into the black-box models in MLaaS, thereby making the models more vulnerable to several attacks. For example, feature-based explanations (e.g., SHAP) could expose the top important features that a black-box model focuses on. Such disclosure has been exploited to craft effective backdoor triggers against malware classifiers. To address this trade-off, we introduce a new concept of achieving local differential privacy (LDP) in the explanations, and from that we establish a defense, called XRand, against such attacks. We show that our mechanism restricts the information that the adversary can learn about the top important features, while maintaining the faithfulness of the explanations. △ Less","14 December, 2022",https://arxiv.org/pdf/2212.04454
A Rubric for Human-like Agents and NeuroAI,Ida Momennejad,"Researchers across cognitive, neuro-, and computer sciences increasingly reference human-like artificial intelligence and neuroAI. However, the scope and use of the terms are often inconsistent. Contributed research ranges widely from mimicking behaviour, to testing machine learning methods as neurally plausible hypotheses at the cellular or functional levels, or solving engineering problems. However, it cannot be assumed nor expected that progress on one of these three goals will automatically translate to progress in others. Here a simple rubric is proposed to clarify the scope of individual contributions, grounded in their commitments to human-like behaviour, neural plausibility, or benchmark/engineering goals. This is clarified using examples of weak and strong neuroAI and human-like agents, and discussing the generative, corroborate, and corrective ways in which the three dimensions interact with one another. The author maintains that future progress in artificial intelligence will need strong interactions across the disciplines, with iterative feedback loops and meticulous validity tests, leading to both known and yet-unknown advances that may span decades to come. △ Less","8 December, 2022",https://arxiv.org/pdf/2212.04401
Lie detection algorithms attract few users but vastly increase accusation rates,Alicia von Schenk;Victor Klockmann;Jean-François Bonnefon;Iyad Rahwan;Nils Köbis,"People are not very good at detecting lies, which may explain why they refrain from accusing others of lying, given the social costs attached to false accusations - both for the accuser and the accused. Here we consider how this social balance might be disrupted by the availability of lie-detection algorithms powered by Artificial Intelligence. Will people elect to use lie detection algorithms that perform better than humans, and if so, will they show less restraint in their accusations? We built a machine learning classifier whose accuracy (67\%) was significantly better than human accuracy (50\%) in a lie-detection task and conducted an incentivized lie-detection experiment in which we measured participants' propensity to use the algorithm, as well as the impact of that use on accusation rates. We find that the few people (33\%) who elect to use the algorithm drastically increase their accusation rates (from 25\% in the baseline condition up to 86% when the algorithm flags a statement as a lie). They make more false accusations (18pp increase), but at the same time, the probability of a lie remaining undetected is much lower in this group (36pp decrease). We consider individual motivations for using lie detection algorithms and the social implications of these algorithms. △ Less","8 December, 2022",https://arxiv.org/pdf/2212.04277
Voice Over Body? Older Adults' Reactions to Robot and Voice Assistant Facilitators of Group Conversation,Katie Seaborn;Takuya Sekiguchi;Seiki Tokunaga;Norihisa P. Miyake;Mihoko Otake-Matsuura,"Intelligent agents have great potential as facilitators of group conversation among older adults. However, little is known about how to design agents for this purpose and user group, especially in terms of agent embodiment. To this end, we conducted a mixed methods study of older adults' reactions to voice and body in a group conversation facilitation agent. Two agent forms with the same underlying artificial intelligence (AI) and voice system were compared: a humanoid robot and a voice assistant. One preliminary study (total n=24) and one experimental study comparing voice and body morphologies (n=36) were conducted with older adults and an experienced human facilitator. Findings revealed that the artificiality of the agent, regardless of its form, was beneficial for the socially uncomfortable task of conversation facilitation. Even so, talkative personality types had a poorer experience with the ""bodied"" robot version. Design implications and supplementary reactions, especially to agent voice, are also discussed. △ Less","8 December, 2022",https://arxiv.org/pdf/2212.04213
Real-Time Counterfactual Explanations For Robotic Systems With Multiple Continuous Outputs,Vilde B. Gjærum;Inga Strümke;Anastasios M. Lekkas;Tim Miller,"Although many machine learning methods, especially from the field of deep learning, have been instrumental in addressing challenges within robotic applications, we cannot take full advantage of such methods before these can provide performance and safety guarantees. The lack of trust that impedes the use of these methods mainly stems from a lack of human understanding of what exactly machine learning models have learned, and how robust their behaviour is. This is the problem the field of explainable artificial intelligence aims to solve. Based on insights from the social sciences, we know that humans prefer contrastive explanations, i.e.\ explanations answering the hypothetical question ""what if?"". In this paper, we show that linear model trees are capable of producing answers to such questions, so-called counterfactual explanations, for robotic systems, including in the case of multiple, continuous inputs and outputs. We demonstrate the use of this method to produce counterfactual explanations for two robotic applications. Additionally, we explore the issue of infeasibility, which is of particular interest in systems governed by the laws of physics. △ Less","8 December, 2022",https://arxiv.org/pdf/2212.04212
Customizing Number Representation and Precision,Olivier Sentieys;Daniel Menard,"There is a growing interest in the use of reduced-precision arithmetic, exacerbated by the recent interest in artificial intelligence, especially with deep learning. Most architectures already provide reduced-precision capabilities (e.g., 8-bit integer, 16-bit floating point). In the context of FPGAs, any number format and bit-width can even be considered.In computer arithmetic, the representation of real numbers is a major issue. Fixed-point (FxP) and floating-point (FlP) are the main options to represent reals, both with their advantages and drawbacks. This chapter presents both FxP and FlP number representations, and draws a fair a comparison between their cost, performance and energy, as well as their impact on accuracy during computations.It is shown that the choice between FxP and FlP is not obvious and strongly depends on the application considered. In some cases, low-precision floating-point arithmetic can be the most effective and provides some benefits over the classical fixed-point choice for energy-constrained applications. △ Less","8 December, 2022",https://arxiv.org/pdf/2212.04184
MixBoost: Improving the Robustness of Deep Neural Networks by Boosting Data Augmentation,Zhendong Liu;Wenyu Jiang;Min guo;Chongjun Wang,"As more and more artificial intelligence (AI) technologies move from the laboratory to real-world applications, the open-set and robustness challenges brought by data from the real world have received increasing attention. Data augmentation is a widely used method to improve model performance, and some recent works have also confirmed its positive effect on the robustness of AI models. However, most of the existing data augmentation methods are heuristic, lacking the exploration of their internal mechanisms. We apply the explainable artificial intelligence (XAI) method, explore the internal mechanisms of popular data augmentation methods, analyze the relationship between game interactions and some widely used robustness metrics, and propose a new proxy for model robustness in the open-set environment. Based on the analysis of the internal mechanisms, we develop a mask-based boosting method for data augmentation that comprehensively improves several robustness measures of AI models and beats state-of-the-art data augmentation approaches. Experiments show that our method can be widely applied to many popular data augmentation methods. Different from the adversarial training, our boosting method not only significantly improves the robustness of models, but also improves the accuracy of test sets. Our code is available at \url{https://github.com/Anonymous_for_submission}. △ Less","7 December, 2022",https://arxiv.org/pdf/2212.04059
DDoD: Dual Denial of Decision Attacks on Human-AI Teams,Benjamin Tag;Niels van Berkel;Sunny Verma;Benjamin Zi Hao Zhao;Shlomo Berkovsky;Dali Kaafar;Vassilis Kostakos;Olga Ohrimenko,"Artificial Intelligence (AI) systems have been increasingly used to make decision-making processes faster, more accurate, and more efficient. However, such systems are also at constant risk of being attacked. While the majority of attacks targeting AI-based applications aim to manipulate classifiers or training data and alter the output of an AI model, recently proposed Sponge Attacks against AI models aim to impede the classifier's execution by consuming substantial resources. In this work, we propose \textit{Dual Denial of Decision (DDoD) attacks against collaborative Human-AI teams}. We discuss how such attacks aim to deplete \textit{both computational and human} resources, and significantly impair decision-making capabilities. We describe DDoD on human and computational resources and present potential risk scenarios in a series of exemplary domains. △ Less","7 December, 2022",https://arxiv.org/pdf/2212.03980
Going Beyond XAI: A Systematic Survey for Explanation-Guided Learning,Yuyang Gao;Siyi Gu;Junji Jiang;Sungsoo Ray Hong;Dazhou Yu;Liang Zhao,"As the societal impact of Deep Neural Networks (DNNs) grows, the goals for advancing DNNs become more complex and diverse, ranging from improving a conventional model accuracy metric to infusing advanced human virtues such as fairness, accountability, transparency (FaccT), and unbiasedness. Recently, techniques in Explainable Artificial Intelligence (XAI) are attracting considerable attention, and have tremendously helped Machine Learning (ML) engineers in understanding AI models. However, at the same time, we started to witness the emerging need beyond XAI among AI communities; based on the insights learned from XAI, how can we better empower ML engineers in steering their DNNs so that the model's reasonableness and performance can be improved as intended? This article provides a timely and extensive literature overview of the field Explanation-Guided Learning (EGL), a domain of techniques that steer the DNNs' reasoning process by adding regularization, supervision, or intervention on model explanations. In doing so, we first provide a formal definition of EGL and its general learning paradigm. Secondly, an overview of the key factors for EGL evaluation, as well as summarization and categorization of existing evaluation procedures and metrics for EGL are provided. Finally, the current and potential future application areas and directions of EGL are discussed, and an extensive experimental study is presented aiming at providing comprehensive comparative studies among existing EGL models in various popular application domains, such as Computer Vision (CV) and Natural Language Processing (NLP) domains. △ Less","7 December, 2022",https://arxiv.org/pdf/2212.03954
Learning Action-Effect Dynamics for Hypothetical Vision-Language Reasoning Task,Shailaja Keyur Sampat;Pratyay Banerjee;Yezhou Yang;Chitta Baral,"'Actions' play a vital role in how humans interact with the world. Thus, autonomous agents that would assist us in everyday tasks also require the capability to perform 'Reasoning about Actions & Change' (RAC). This has been an important research direction in Artificial Intelligence (AI) in general, but the study of RAC with visual and linguistic inputs is relatively recent. The CLEVR_HYP (Sampat et. al., 2021) is one such testbed for hypothetical vision-language reasoning with actions as the key focus. In this work, we propose a novel learning strategy that can improve reasoning about the effects of actions. We implement an encoder-decoder architecture to learn the representation of actions as vectors. We combine the aforementioned encoder-decoder architecture with existing modality parsers and a scene graph question answering model to evaluate our proposed system on the CLEVR_HYP dataset. We conduct thorough experiments to demonstrate the effectiveness of our proposed approach and discuss its advantages over previous baselines in terms of performance, data efficiency, and generalization capability. △ Less","7 December, 2022",https://arxiv.org/pdf/2212.03866
Intent Recognition in Conversational Recommender Systems,Sahar Moradizeyveh,"Any organization needs to improve their products, services, and processes. In this context, engaging with customers and understanding their journey is essential. Organizations have leveraged various techniques and technologies to support customer engagement, from call centres to chatbots and virtual agents. Recently, these systems have used Machine Learning (ML) and Natural Language Processing (NLP) to analyze large volumes of customer feedback and engagement data. The goal is to understand customers in context and provide meaningful answers across various channels. Despite multiple advances in Conversational Artificial Intelligence (AI) and Recommender Systems (RS), it is still challenging to understand the intent behind customer questions during the customer journey. To address this challenge, in this paper, we study and analyze the recent work in Conversational Recommender Systems (CRS) in general and, more specifically, in chatbot-based CRS. We introduce a pipeline to contextualize the input utterances in conversations. We then take the next step towards leveraging reverse feature engineering to link the contextualized input and learning model to support intent recognition. Since performance evaluation is achieved based on different ML models, we use transformer base models to evaluate the proposed approach using a labelled dialogue dataset (MSDialogue) of question-answering interactions between information seekers and answer providers. △ Less","6 December, 2022",https://arxiv.org/pdf/2212.03721
A socio-physics based hybrid metaheuristic for solving complex non-convex constrained optimization problems,Ishaan R Kale;Anand J Kulkarni;Efren Mezura-Montes,"Several Artificial Intelligence based heuristic and metaheuristic algorithms have been developed so far. These algorithms have shown their superiority towards solving complex problems from different domains. However, it is necessary to critically validate these algorithms for solving real-world constrained optimization problems. The search behavior in those problems is different as it involves large number of linear, nonlinear and non-convex type equality and inequality constraints. In this work a 57 real-world constrained optimization problems test suite is solved using two constrained metaheuristic algorithms originated from a socio-based Cohort Intelligence (CI) algorithm. The first CI-based algorithm incorporates a self-adaptive penalty function approach i.e., CI-SAPF. The second algorithm combines CI-SAPF with the intrinsic properties of the physics-based Colliding Bodies Optimization (CBO) referred to CI-SAPF-CBO. The results obtained from CI-SAPF and CI-SAPF-CBO are compared with other constrained optimization algorithms. The superiority of the proposed algorithms is discussed in details followed by future directions to evolve the constrained handling techniques. △ Less","2 September, 2022",https://arxiv.org/pdf/2212.03711
Artificial Intelligence Security Competition (AISC),Yinpeng Dong;Peng Chen;Senyou Deng;Lianji L;Yi Sun;Hanyu Zhao;Jiaxing Li;Yunteng Tan;Xinyu Liu;Yangyi Dong;Enhui Xu;Jincai Xu;Shu Xu;Xuelin Fu;Changfeng Sun;Haoliang Han;Xuchong Zhang;Shen Chen;Zhimin Sun;Junyi Cao;Taiping Yao;Shouhong Ding;Yu Wu;Jian Lin;Tianpeng Wu,"The security of artificial intelligence (AI) is an important research area towards safe, reliable, and trustworthy AI systems. To accelerate the research on AI security, the Artificial Intelligence Security Competition (AISC) was organized by the Zhongguancun Laboratory, China Industrial Control Systems Cyber Emergency Response Team, Institute for Artificial Intelligence, Tsinghua University, and RealAI as part of the Zhongguancun International Frontier Technology Innovation Competition (https://www.zgc-aisc.com/en). The competition consists of three tracks, including Deepfake Security Competition, Autonomous Driving Security Competition, and Face Recognition Security Competition. This report will introduce the competition rules of these three tracks and the solutions of top-ranking teams in each track. △ Less","6 December, 2022",https://arxiv.org/pdf/2212.03412
SAIH: A Scalable Evaluation Methodology for Understanding AI Performance Trend on HPC Systems,Jiangsu Du;Dongsheng Li;Yingpeng Wen;Jiazhi Jiang;Dan Huang;Xiangke Liao;Yutong Lu,"Novel artificial intelligence (AI) technology has expedited various scientific research, e.g., cosmology, physics and bioinformatics, inevitably becoming a significant category of workload on high performance computing (HPC) systems. Existing AI benchmarks tend to customize well-recognized AI applications, so as to evaluate the AI performance of HPC systems under predefined problem size, in terms of datasets and AI models. Due to lack of scalability on the problem size, static AI benchmarks might be under competent to help understand the performance trend of evolving AI applications on HPC systems, in particular, the scientific AI applications on large-scale systems. In this paper, we propose a scalable evaluation methodology (SAIH) for analyzing the AI performance trend of HPC systems with scaling the problem sizes of customized AI applications. To enable scalability, SAIH builds a set of novel mechanisms for augmenting problem sizes. As the data and model constantly scale, we can investigate the trend and range of AI performance on HPC systems, and further diagnose system bottlenecks. To verify our methodology, we augment a cosmological AI application to evaluate a real HPC system equipped with GPUs as a case study of SAIH. △ Less","6 December, 2022",https://arxiv.org/pdf/2212.03410
A Systematic Literature Review on 5G Security,Ishika Sahni;Araftoz Kaur,"It is expected that the creation of next-generation wireless networks would result in the availability of high-speed and low-latency connectivity for every part of our life. As a result, it is important that the network is secure. The network's security environment has grown more complicated as a result of the growing number of devices and the diversity of services that 5G will provide. This is why it is important that the development of effective security solutions is carried out early. Our findings of this review have revealed the various directions that will be pursued in the development of next-generation wireless networks. Some of these include the use of Artificial Intelligence and Software Defined Mobile Networks. The threat environment for 5G networks, security weaknesses in the new technology paradigms that 5G will embrace, and provided solutions presented in the key studies in the field of 5G cyber security are all described in this systematic literature review for prospective researchers. Future research directions to protect wireless networks beyond 5G are also covered. △ Less","6 December, 2022",https://arxiv.org/pdf/2212.03299
From Knowledge Augmentation to Multi-tasking: Towards Human-like Dialogue Systems,Tom Young,"The goal of building dialogue agents that can converse with humans naturally has been a long-standing dream of researchers since the early days of artificial intelligence. The well-known Turing Test proposed to judge the ultimate validity of an artificial intelligence agent on the indistinguishability of its dialogues from humans'. It should come as no surprise that human-level dialogue systems are very challenging to build. But, while early effort on rule-based systems found limited success, the emergence of deep learning enabled great advance on this topic. In this thesis, we focus on methods that address the numerous issues that have been imposing the gap between artificial conversational agents and human-level interlocutors. These methods were proposed and experimented with in ways that were inspired by general state-of-the-art AI methodologies. But they also targeted the characteristics that dialogue systems possess. △ Less","11 December, 2022",https://arxiv.org/pdf/2212.03279
Synthetic Expertise,Ron Fulbright;Grover Walters,"We will soon be surrounded by artificial systems capable of cognitive performance rivaling or exceeding a human expert in specific domains of discourse. However, these cogs need not be capable of full general artificial intelligence nor able to function in a stand-alone manner. Instead, cogs and humans will work together in collaboration each compensating for the weaknesses of the other and together achieve synthetic expertise as an ensemble. This paper reviews the nature of expertise, the Expertise Level to describe the skills required of an expert, and knowledge stores required by an expert. By collaboration, cogs augment human cognitive ability in a human/cog ensemble. This paper introduces six Levels of Cognitive Augmentation to describe the balance of cognitive processing in the human/cog ensemble. Because these cogs will be available to the mass market via common devices and inexpensive applications, they will lead to the Democratization of Expertise and a new cognitive systems era promising to change how we live, work, and play. The future will belong to those best able to communicate, coordinate, and collaborate with cognitive systems. △ Less","11 November, 2022",https://arxiv.org/pdf/2212.03244
A comparative study of emotion recognition methods using facial expressions,Rim EL Cheikh;Hélène Tran;Issam Falih;Engelbert Mephu Nguifo,"Understanding the facial expressions of our interlocutor is important to enrich the communication and to give it a depth that goes beyond the explicitly expressed. In fact, studying one's facial expression gives insight into their hidden emotion state. However, even as humans, and despite our empathy and familiarity with the human emotional experience, we are only able to guess what the other might be feeling. In the fields of artificial intelligence and computer vision, Facial Emotion Recognition (FER) is a topic that is still in full growth mostly with the advancement of deep learning approaches and the improvement of data collection. The main purpose of this paper is to compare the performance of three state-of-the-art networks, each having their own approach to improve on FER tasks, on three FER datasets. The first and second sections respectively describe the three datasets and the three studied network architectures designed for an FER task. The experimental protocol, the results and their interpretation are outlined in the remaining sections. △ Less","5 December, 2022",https://arxiv.org/pdf/2212.03102
A Time Series Approach to Explainability for Neural Nets with Applications to Risk-Management and Fraud Detection,Marc Wildi;Branka Hadji Misheva,"Artificial intelligence is creating one of the biggest revolution across technology driven application fields. For the finance sector, it offers many opportunities for significant market innovation and yet broad adoption of AI systems heavily relies on our trust in their outputs. Trust in technology is enabled by understanding the rationale behind the predictions made. To this end, the concept of eXplainable AI emerged introducing a suite of techniques attempting to explain to users how complex models arrived at a certain decision. For cross-sectional data classical XAI approaches can lead to valuable insights about the models' inner workings, but these techniques generally cannot cope well with longitudinal data (time series) in the presence of dependence structure and non-stationarity. We here propose a novel XAI technique for deep learning methods which preserves and exploits the natural time ordering of the data. △ Less","6 December, 2022",https://arxiv.org/pdf/2212.02906
Counteracting Eavesdropper Attacks Through Reconfigurable Intelligent Surfaces: A New Threat Model and Secrecy Rate Optimization,George C. Alexandropoulos;Konstantinos D. Katsanos;Miaowen Wen;Daniel B. da Costa,"The potential of Reconfigurable Intelligent Surfaces (RISs) for energy-efficient and performance-boosted wireless communications is recently gaining remarkable research attention, motivating their consideration for various 5-th Generation (5G) Advanced and beyond applications. In this paper, we consider a Multiple-Input Multiple-Output (MIMO) Physical Layer Security (PLS) system with multiple data streams including one legitimate passive RIS and one malicious passive RIS, with the former being transparent to the multi-antenna eavesdropper and the latter's presence being unknown at the legitimate multi-antenna transceivers. We first present a novel threat model for the RIS-boosted eavesdropping system and design a joint optimization framework for the eavesdropper's receive combining matrix and the reflection coefficients of the malicious RIS. Focusing next on the secrecy rate maximization problem, we present an RIS-empowered PLS scheme that jointly designs the legitimate precoding matrix and number of data streams, the Artificial Noise (AN) covariance matrix, the receive combining matrix, and the reflection coefficients of the legitimate RIS. The proposed optimization algorithms, whose convergence to at least local optimum points is proved, are based on alternating maximization, minorization-maximization, and manifold optimization, including semi-closed form expressions for the optimization variables. Our extensive simulation results for two representative system setups reveal that, in the absence of a legitimate RIS, transceiver spatial filtering and AN are incapable of offering non-zero secrecy rates, even for malicious RISs with small numbers of elements. However, when an L-element legitimate RIS is deployed, confidential communication can be safeguarded against eavesdropping systems possessing even more than a 5L-element malicious RIS. △ Less","5 December, 2022",https://arxiv.org/pdf/2212.02263
Wearable-based Human Activity Recognition with Spatio-Temporal Spiking Neural Networks,Yuhang Li;Ruokai Yin;Hyoungseob Park;Youngeun Kim;Priyadarshini Panda,"We study the Human Activity Recognition (HAR) task, which predicts user daily activity based on time series data from wearable sensors. Recently, researchers use end-to-end Artificial Neural Networks (ANNs) to extract the features and perform classification in HAR. However, ANNs pose a huge computation burden on wearable devices and lack temporal feature extraction. In this work, we leverage Spiking Neural Networks (SNNs)--an architecture inspired by biological neurons--to HAR tasks. SNNs allow spatio-temporal extraction of features and enjoy low-power computation with binary spikes. We conduct extensive experiments on three HAR datasets with SNNs, demonstrating that SNNs are on par with ANNs in terms of accuracy while reducing up to 94% energy consumption. The code is publicly available in https://github.com/Intelligent-Computing-Lab-Yale/SNN_HAR △ Less","14 November, 2022",https://arxiv.org/pdf/2212.02233
Anomaly Detection in Power Markets and Systems,Ugur Halden;Umit Cali;Ferhat Ozgur Catak;Salvatore D'Arco;Francisco Bilendo,"The widespread use of information and communication technology (ICT) over the course of the last decades has been a primary catalyst behind the digitalization of power systems. Meanwhile, as the utilization rate of the Internet of Things (IoT) continues to rise along with recent advancements in ICT, the need for secure and computationally efficient monitoring of critical infrastructures like the electrical grid and the agents that participate in it is growing. A cyber-physical system, such as the electrical grid, may experience anomalies for a number of different reasons. These may include physical defects, mistakes in measurement and communication, cyberattacks, and other similar occurrences. The goal of this study is to emphasize what the most common incidents are with power systems and to give an overview and classification of the most common ways to find problems, starting with the consumer/prosumer end working up to the primary power producers. In addition, this article aimed to discuss the methods and techniques, such as artificial intelligence (AI) that are used to identify anomalies in the power systems and markets. △ Less","5 December, 2022",https://arxiv.org/pdf/2212.02182
WAIR-D: Wireless AI Research Dataset,Yourui Huangfu;Jian Wang;Shengchen Dai;Rong Li;Jun Wang;Chongwen Huang;Zhaoyang Zhang,"It is a common sense that datasets with high-quality data samples play an important role in artificial intelligence (AI), machine learning (ML) and related studies. However, although AI/ML has been introduced in wireless researches long time ago, few datasets are commonly used in the research community. Without a common dataset, AI-based methods proposed for wireless systems are hard to compare with both the traditional baselines and even each other. The existing wireless AI researches usually rely on datasets generated based on statistical models or ray-tracing simulations with limited environments. The statistical data hinder the trained AI models from further fine-tuning for a specific scenario, and ray-tracing data with limited environments lower down the generalization capability of the trained AI models. In this paper, we present the Wireless AI Research Dataset (WAIR-D)1, which consists of two scenarios. Scenario 1 contains 10,000 environments with sparsely dropped user equipments (UEs), and Scenario 2 contains 100 environments with densely dropped UEs. The environments are randomly picked up from more than 40 cities in the real world map. The large volume of the data guarantees that the trained AI models enjoy good generalization capability, while fine-tuning can be easily carried out on a specific chosen environment. Moreover, both the wireless channels and the corresponding environmental information are provided in WAIR-D, so that extra-information-aided communication mechanism can be designed and evaluated. WAIR-D provides the researchers benchmarks to compare their different designs or reproduce results of others. In this paper, we show the detailed construction of this dataset and examples of using it. △ Less","5 December, 2022",https://arxiv.org/pdf/2212.02159
6G Digital Twin Networks: From Theory to Practice,Xingqin Lin;Lopamudra Kundu;Chris Dick;Emeka Obiodu;Todd Mostak,"Digital twin networks (DTNs) are real-time replicas of physical networks. They are emerging as a powerful technology for design, diagnosis, simulation, what-if-analysis, and artificial intelligence (AI)/machine learning (ML) driven real-time optimization and control of the sixth-generation (6G) wireless networks. Despite the great potential of what digital twins can offer for 6G, realizing the desired capabilities of 6G DTNs requires tackling many design aspects including data, models, and interfaces. In this article, we provide an overview of 6G DTNs by presenting prominent use cases and their service requirements, describing a reference architecture, and discussing fundamental design aspects. We also present a real-world example to illustrate how DTNs can be built upon and operated in a real-time reference development platform - Omniverse. △ Less","5 December, 2022",https://arxiv.org/pdf/2212.02032
HierarchyFL: Heterogeneous Federated Learning via Hierarchical Self-Distillation,Jun Xia;Yi Zhang;Zhihao Yue;Ming Hu;Xian Wei;Mingsong Chen,"Federated learning (FL) has been recognized as a privacy-preserving distributed machine learning paradigm that enables knowledge sharing among various heterogeneous artificial intelligence (AIoT) devices through centralized global model aggregation. FL suffers from model inaccuracy and slow convergence due to the model heterogeneity of the AIoT devices involved. Although various existing methods try to solve the bottleneck of the model heterogeneity problem, most of them improve the accuracy of heterogeneous models in a coarse-grained manner, which makes it still a great challenge to deploy large-scale AIoT devices. To alleviate the negative impact of this problem and take full advantage of the diversity of each heterogeneous model, we propose an efficient framework named HierarchyFL, which uses a small amount of public data for efficient and scalable knowledge across a variety of differently structured models. By using self-distillation and our proposed ensemble library, each hierarchical model can intelligently learn from each other on cloud servers. Experimental results on various well-known datasets show that HierarchyFL can not only maximize the knowledge sharing among various heterogeneous models in large-scale AIoT systems, but also greatly improve the model performance of each involved heterogeneous AIoT device. △ Less","4 December, 2022",https://arxiv.org/pdf/2212.02006
Winning the CityLearn Challenge: Adaptive Optimization with Evolutionary Search under Trajectory-based Guidance,Vanshaj Khattar;Ming Jin,"Modern power systems will have to face difficult challenges in the years to come: frequent blackouts in urban areas caused by high power demand peaks, grid instability exacerbated by intermittent renewable generation, and global climate change amplified by rising carbon emissions. While current practices are growingly inadequate, the path to widespread adoption of artificial intelligence (AI) methods is hindered by missing aspects of trustworthiness. The CityLearn Challenge is an exemplary opportunity for researchers from multiple disciplines to investigate the potential of AI to tackle these pressing issues in the energy domain, collectively modeled as a reinforcement learning (RL) task. Multiple real-world challenges faced by contemporary RL techniques are embodied in the problem formulation. In this paper, we present a novel method using the solution function of optimization as policies to compute actions for sequential decision-making, while notably adapting the parameters of the optimization model from online observations. Algorithmically, this is achieved by an evolutionary algorithm under a novel trajectory-based guidance scheme. Formally, the global convergence property is established. Our agent ranked first in the latest 2021 CityLearn Challenge, being able to achieve superior performance in almost all metrics while maintaining some key aspects of interpretability. △ Less","4 December, 2022",https://arxiv.org/pdf/2212.01939
Harnessing label semantics to extract higher performance under noisy label for Company to Industry matching,Apoorva Jaiswal;Abhishek Mitra,"Assigning appropriate industry tag(s) to a company is a critical task in a financial institution as it impacts various financial machineries. Yet, it remains a complex task. Typically, such industry tags are to be assigned by Subject Matter Experts (SME) after evaluating company business lines against the industry definitions. It becomes even more challenging as companies continue to add new businesses and newer industry definitions are formed. Given the periodicity of the task it is reasonable to assume that an Artificial Intelligent (AI) agent could be developed to carry it out in an efficient manner. While this is an exciting prospect, the challenges appear from the need of historical patterns of such tag assignments (or Labeling). Labeling is often considered the most expensive task in Machine Learning (ML) due its dependency on SMEs and manual efforts. Therefore, often, in enterprise set up, an ML project encounters noisy and dependent labels. Such labels create technical hindrances for ML Models to produce robust tag assignments. We propose an ML pipeline which uses semantic similarity matching as an alternative to multi label text classification, while making use of a Label Similarity Matrix and a minimum labeling strategy. We demonstrate this pipeline achieves significant improvements over the noise and exhibit robust predictive capabilities. △ Less","3 December, 2022",https://arxiv.org/pdf/2212.01685
Onboard Real-Time Multi-Sensor Pose Estimation for Indoor Quadrotor Navigation with Intermittent Communication,Loizos Hadjiloizou;Kyriakos M. Deliparaschos;Evagoras Makridis;Themistoklis Charalambous,"We propose a multisensor fusion framework for onboard real-time navigation of a quadrotor in an indoor environment, by integrating sensor readings from an Inertial Measurement Unit (IMU), a camera-based object detection algorithm, and an Ultra-WideBand (UWB) localization system. The sensor readings from the camera-based object detection algorithm and the UWB localization system arrive intermittently, since the measurements are not readily available. We design a Kalman filter that manages intermittent observations in order to handle and fuse the readings and estimate the pose of the quadrotor for tracking a predefined trajectory. The system is implemented via a Hardware-in-the-loop (HIL) simulation technique, in which the dynamic model of the quadrotor is simulated in an open-source 3D robotics simulator tool, and the whole navigation system is implemented on Artificial Intelligence (AI) enabled edge GPU. The simulation results show that our proposed framework offers low positioning and trajectory errors, while handling intermittent sensor measurements. △ Less","3 December, 2022",https://arxiv.org/pdf/2212.01599
Open RAN Security: Challenges and Opportunities,Madhusanka Liyanage;An Braeken;Shahriar Shahabuddin;Pasika Ranaweera,"Open RAN (ORAN, O-RAN) represents a novel industry-level standard for RAN (Radio Access Network), which defines interfaces that support inter-operation between vendors' equipment and offer network flexibility at a lower cost. Open RAN integrates the benefits and advancements of network softwarization and Artificial Intelligence to enhance the operation of RAN devices and operations. Open RAN offers new possibilities so that different stakeholders can develop the RAN solution in this open ecosystem. However, the benefits of Open RAN bring new security and privacy challenges. As Open RAN offers an entirely different RAN configuration than what exists today, it could lead to severe security and privacy issues if mismanaged, and stakeholders are understandably taking a cautious approach towards the security of Open RAN deployment. In particular, this paper provides a deep analysis of the security and privacy risks and challenges associated with Open RAN architecture. Then, it discusses possible security and privacy solutions to secure Open RAN architecture and presents relevant security standardization efforts relevant to Open RAN security. Finally, we discuss how Open RAN can be used to deploy more advanced security and privacy solutions in 5G and beyond RAN. △ Less","2 December, 2022",https://arxiv.org/pdf/2212.01510
NEAL: An open-source tool for audio annotation,Anthony Gibbons;Ian Donohue;Courtney E. Gorman;Emma King;Andrew Parnell,"Passive acoustic monitoring is used widely in ecology, biodiversity, and conservation studies. Data sets collected via acoustic monitoring are often extremely large and built to be processed automatically using Artificial Intelligence and Machine learning models, which aim to replicate the work of domain experts. These models, being supervised learning algorithms, need to be trained on high quality annotations produced by experts. Since the experts are often resource-limited, a cost-effective process for annotating audio is needed to get maximal use out of the data. We present an open-source interactive audio data annotation tool, NEAL (Nature+Energy Audio Labeller). Built using R and the associated Shiny framework, the tool provides a reactive environment where users can quickly annotate audio files and adjust settings that automatically change the corresponding elements of the user interface. The app has been designed with the goal of having both expert birders and citizen scientists contribute to acoustic annotation projects. The popularity and flexibility of R programming in bioacoustics means that the Shiny app can be modified for other bird labelling data sets, or even to generic audio labelling tasks. We demonstrate the app by labelling data collected from wind farm sites across Ireland. △ Less","8 December, 2022",https://arxiv.org/pdf/2212.01457
5G-NIDD: A Comprehensive Network Intrusion Detection Dataset Generated over 5G Wireless Network,Sehan Samarakoon;Yushan Siriwardhana;Pawani Porambage;Madhusanka Liyanage;Sang-Yoon Chang;Jinoh Kim;Jonghyun Kim;Mika Ylianttila,"With a plethora of new connections, features, and services introduced, the 5th generation (5G) wireless technology reflects the development of mobile communication networks and is here to stay for the next decade. The multitude of services and technologies that 5G incorporates have made modern communication networks very complex and sophisticated in nature. This complexity along with the incorporation of Machine Learning (ML) and Artificial Intelligence (AI) provides the opportunity for the attackers to launch intelligent attacks against the network and network devices. These attacks often traverse undetected due to the lack of intelligent security mechanisms to counter these threats. Therefore, the implementation of real-time, proactive, and self-adaptive security mechanisms throughout the network would be an integral part of 5G as well as future communication systems. Therefore, large amounts of data collected from real networks will play an important role in the training of AI/ML models to identify and detect malicious content in network traffic. This work presents 5G-NIDD, a fully labeled dataset built on a functional 5G test network that can be used by those who develop and test AI/ML solutions. The work further analyses the collected data using common ML models and shows the achieved accuracy levels. △ Less","2 December, 2022",https://arxiv.org/pdf/2212.01298
Olive Branch Learning: A Topology-Aware Federated Learning Framework for Space-Air-Ground Integrated Network,Qingze Fang;Zhiwei Zhai;Shuai Yu;Qiong Wu;Xiaowen Gong;Xu Chen,"The space-air-ground integrated network (SAGIN), one of the key technologies for next-generation mobile communication systems, can facilitate data transmission for users all over the world, especially in some remote areas where vast amounts of informative data are collected by Internet of remote things (IoRT) devices to support various data-driven artificial intelligence (AI) services. However, training AI models centrally with the assistance of SAGIN faces the challenges of highly constrained network topology, inefficient data transmission, and privacy issues. To tackle these challenges, we first propose a novel topology-aware federated learning framework for the SAGIN, namely Olive Branch Learning (OBL). Specifically, the IoRT devices in the ground layer leverage their private data to perform model training locally, while the air nodes in the air layer and the ring-structured low earth orbit (LEO) satellite constellation in the space layer are in charge of model aggregation (synchronization) at different scales.To further enhance communication efficiency and inference performance of OBL, an efficient Communication and Non-IID-aware Air node-Satellite Assignment (CNASA) algorithm is designed by taking the data class distribution of the air nodes as well as their geographic locations into account. Furthermore, we extend our OBL framework and CNASA algorithm to adapt to more complex multi-orbit satellite networks. We analyze the convergence of our OBL framework and conclude that the CNASA algorithm contributes to the fast convergence of the global model. Extensive experiments based on realistic datasets corroborate the superior performance of our algorithm over the benchmark policies. △ Less","2 December, 2022",https://arxiv.org/pdf/2212.01215
Navigating an Ocean of Video Data: Deep Learning for Humpback Whale Classification in YouTube Videos,Michelle Ramirez,"Image analysis technologies empowered by artificial intelligence (AI) have proved images and videos to be an opportune source of data to learn about humpback whale (Megaptera novaeangliae) population sizes and dynamics. With the advent of social media, platforms such as YouTube present an abundance of video data across spatiotemporal contexts documenting humpback whale encounters from users worldwide. In our work, we focus on automating the classification of YouTube videos as relevant or irrelevant based on whether they document a true humpback whale encounter or not via deep learning. We use a CNN-RNN architecture pretrained on the ImageNet dataset for classification of YouTube videos as relevant or irrelevant. We achieve an average 85.7% accuracy, and 84.7% (irrelevant)/ 86.6% (relevant) F1 scores using five-fold cross validation for evaluation on the dataset. We show that deep learning can be used as a time-efficient step to make social media a viable source of image and video data for biodiversity assessments. △ Less","1 December, 2022",https://arxiv.org/pdf/2212.00822
The purpose of qualia: What if human thinking is not (only) information processing?,Martin Korth,"Despite recent breakthroughs in the field of artificial intelligence (AI) - or more specifically machine learning (ML) algorithms for object recognition and natural language processing - it seems to be the majority view that current AI approaches are still no real match for natural intelligence (NI). More importantly, philosophers have collected a long catalogue of features which imply that NI works differently from current AI not only in a gradual sense, but in a more substantial way: NI is closely related to consciousness, intentionality and experiential features like qualia (the subjective contents of mental states) and allows for understanding (e.g., taking insight into causal relationships instead of 'blindly' relying on correlations), as well as aesthetical and ethical judgement beyond what we can put into (explicit or data-induced implicit) rules to program machines with. Additionally, Psychologists find NI to range from unconscious psychological processes to focused information processing, and from embodied and implicit cognition to 'true' agency and creativity. NI thus seems to transcend any neurobiological functionalism by operating on 'bits of meaning' instead of information in the sense of data, quite unlike both the 'good old fashioned', symbolic AI of the past, as well as the current wave of deep neural network based, 'sub-symbolic' AI, which both share the idea of thinking as (only) information processing. In the following I propose an alternative view of NI as information processing plus 'bundle pushing', discuss an example which illustrates how bundle pushing can cut information processing short, and suggest first ideas for scientific experiments in neuro-biology and information theory as further investigations. △ Less","6 December, 2022",https://arxiv.org/pdf/2212.00800
Prioritizing Policies for Furthering Responsible Artificial Intelligence in the United States,Emily Hadley,"Several policy options exist, or have been proposed, to further responsible artificial intelligence (AI) development and deployment. Institutions, including U.S. government agencies, states, professional societies, and private and public sector businesses, are well positioned to implement these policies. However, given limited resources, not all policies can or should be equally prioritized. We define and review nine suggested policies for furthering responsible AI, rank each policy on potential use and impact, and recommend prioritization relative to each institution type. We find that pre-deployment audits and assessments and post-deployment accountability are likely to have the highest impact but also the highest barriers to adoption. We recommend that U.S. government agencies and companies highly prioritize development of pre-deployment audits and assessments, while the U.S. national legislature should highly prioritize post-deployment accountability. We suggest that U.S. government agencies and professional societies should highly prioritize policies that support responsible AI research and that states should highly prioritize support of responsible AI education. We propose that companies can highly prioritize involving community stakeholders in development efforts and supporting diversity in AI development. We advise lower levels of prioritization across institutions for AI ethics statements and databases of AI technologies or incidents. We recognize that no one policy will lead to responsible AI and instead advocate for strategic policy implementation across institutions. △ Less","30 November, 2022",https://arxiv.org/pdf/2212.00740
Understanding the Energy Consumption of HPC Scale Artificial Intelligence,Danilo Carastan dos Santos,"This paper contributes towards better understanding the energy consumption trade-offs of HPC scale Artificial Intelligence (AI), and more specifically Deep Learning (DL) algorithms. For this task we developed benchmark-tracker, a benchmark tool to evaluate the speed and energy consumption of DL algorithms in HPC environments. We exploited hardware counters and Python libraries to collect energy information through software, which enabled us to instrument a known AI benchmark tool, and to evaluate the energy consumption of numerous DL algorithms and models. Through an experimental campaign, we show a case example of the potential of benchmark-tracker to measure the computing speed and the energy consumption for training and inference DL algorithms, and also the potential of Benchmark-Tracker to help better understanding the energy behavior of DL algorithms in HPC platforms. This work is a step forward to better understand the energy consumption of Deep Learning in HPC, and it also contributes with a new tool to help HPC DL developers to better balance the HPC infrastructure in terms of speed and energy consumption. △ Less","14 November, 2022",https://arxiv.org/pdf/2212.00582
"A Survey of Mobile Edge Computing for the Metaverse: Architectures, Applications, and Challenges",Yitong Wang;Jun Zhao,"Metaverse is an emerging virtual universe where humans can have real-time interactions and solid social links like in the physical world, and it opens up a new era of Internet and interactions. In Metaverse, an immersive and photorealistic environment promotes social activities, including education, meetings, and shopping of digital avatars based on critical technologies, including 3D rendering, extended reality, digital twins, artificial intelligence, and Blockchain. However, the limitations of computation, storage, and energy resources restrict the development of Metaverse, and a series of system issues (e.g., latency, security, and battery-life) continue to arise. As a result, how to find corresponding measurements to mitigate unsatisfactory influences becomes the focus. Mobile edge computing (MEC) as a distributed computing paradigm offloads computation-intensive tasks to the edge of the network. It brings the resources as close as possible to the end devices, addressing the shortcomings mentioned above. In this paper, we propose a comprehensive survey of the MEC-based Metaverse. Particular emphasis is given to the technologies convergence, architectures, and application scenarios, e.g., BoundlessXR and CloudXR. Significantly, we introduce the potential future directions for developing Metaverse systems. △ Less","1 December, 2022",https://arxiv.org/pdf/2212.00481
A Dataset with Multibeam Forward-Looking Sonar for Underwater Object Detection,Kaibing Xie;Jian Yang;Kang Qiu,"Multibeam forward-looking sonar (MFLS) plays an important role in underwater detection. There are several challenges to the research on underwater object detection with MFLS. Firstly, the research is lack of available dataset. Secondly, the sonar image, generally processed at pixel level and transformed to sector representation for the visual habits of human beings, is disadvantageous to the research in artificial intelligence (AI) areas. Towards these challenges, we present a novel dataset, the underwater acoustic target detection (UATD) dataset, consisting of over 9000 MFLS images captured using Tritech Gemini 1200ik sonar. Our dataset provides raw data of sonar images with annotation of 10 categories of target objects (cube, cylinder, tyres, etc). The data was collected from lake and shallow water. To verify the practicality of UATD, we apply the dataset to the state-of-the-art detectors and provide corresponding benchmarks for its accuracy and efficiency. △ Less","1 December, 2022",https://arxiv.org/pdf/2212.00352
AI Empowered Net-RCA for 6G,Chengbo Qiu;Kai Yang;Ji Wang;Shenjie Zhao,"6G is envisioned to offer higher data rate, improved reliability, ubiquitous AI services, and support massive scale of connected devices. As a consequence, 6G will be much more complex than its predecessors. The growth of the system scale and complexity as well as the coexistence with the legacy networks and the diversified service requirements will inevitably incur huge maintenance cost and efforts for future 6G networks. Network Root Cause Analysis (Net-RCA) plays a critical role in identifying root causes of network faults. In this article, we first give an introduction about the envisioned 6G networks. Next, we discuss the challenges and potential solutions of 6G network operation and management, and comprehensively survey existing RCA methods. Then we propose an artificial intelligence (AI)-empowered Net-RCA framework for 6G. Performance comparisons on both synthetic and real-world network data are carried out to demonstrate that the proposed method outperforms the existing method considerably. △ Less","4 December, 2022",https://arxiv.org/pdf/2212.00331
Smart Insole: A Gait Analysis Monitoring Platform Targeting Parkinson Disease Patients Based on Insoles,Dimitrios Boucharas;Christos Androutsos;George Gkois;Vassilis Tsakanikas;Vasileios Pezoulas;Dimitrios Manousos;Vasileios Skaramagkas;Chariklia Chatzaki;Stathis Kontogiannis;Christos Spandonidis;Alexandros Pantazis;Nikolaos Tachos;Manolis Tsiknakis;Dimitrios Fotiadis,"During the preceding decades, human gait analysis has been the center of attention for the scientific community, while the association between gait analysis and overall health monitoring has been extensively reported. Technological advances further assisted in this alignment, resulting in access to inexpensive and remote healthcare services. Various assessment tools, such as software platforms and mobile applications, have been proposed by the scientific community and the market that employ sensors to monitor human gait for various purposes ranging from biomechanics to the progression of functional recovery. The framework presented herein offers a valuable digital biomarker for diagnosing and monitoring Parkinson's disease that can help clinical experts in the decision-making process leading to corrective planning or patient-specific treatment. More accurate and reliable decisions can be provided through a wide variety of integrated Artificial Intelligence algorithms and straightforward visualization techniques, including, but not limited to, heatmaps and bar plots. The framework consists of three core components: the insole pair, the mobile application, and the cloud-based platform. The insole pair deploys 16 plantar pressure sensors, an accelerometer, and a gyroscope to acquire gait data. The mobile application formulates the data for the cloud platform, which orchestrates the component interaction through the web application. Utilizing open communication protocols enables the straightforward replacement of one of the core components with a relative one (e.g., a different model of insoles), transparently from the end user, without affecting the overall architecture, resulting in a framework with the flexibility to adjust its modularity. △ Less","23 November, 2022",https://arxiv.org/pdf/2212.00109
Optical multi-task learning using multi-wavelength diffractive deep neural networks,Zhengyang Duan;Hang Chen;Xing Lin,"Photonic neural networks are brain-inspired information processing technology using photons instead of electrons to perform artificial intelligence (AI) tasks. However, existing architectures are designed for a single task but fail to multiplex different tasks in parallel within a single monolithic system due to the task competition that deteriorates the model performance. This paper proposes a novel optical multi-task learning system by designing multi-wavelength diffractive deep neural networks (D2NNs) with the joint optimization method. By encoding multi-task inputs into multi-wavelength channels, the system can increase the computing throughput and significantly alle-viate the competition to perform multiple tasks in parallel with high accuracy. We design the two-task and four-task D2NNs with two and four spectral channels, respectively, for classifying different inputs from MNIST, FMNIST, KMNIST, and EMNIST databases. The numerical evaluations demonstrate that, under the same network size, mul-ti-wavelength D2NNs achieve significantly higher classification accuracies for multi-task learning than single-wavelength D2NNs. Furthermore, by increasing the network size, the multi-wavelength D2NNs for simultaneously performing multiple tasks achieve comparable classification accuracies with respect to the individual training of multiple single-wavelength D2NNs to perform tasks separately. Our work paves the way for developing the wave-length-division multiplexing technology to achieve high-throughput neuromorphic photonic computing and more general AI systems to perform multiple tasks in parallel. △ Less","30 November, 2022",https://arxiv.org/pdf/2212.00022
Multidimensional analysis using sensor arrays with deep learning for high-precision and high-accuracy diagnosis,Julie Payette;Sylvain G. Cloutier;Fabrice Vaussenat,"In the upcoming years, artificial intelligence (AI) is going to transform the practice of medicine in most of its specialties. Deep learning can help achieve better and earlier problem detection, while reducing errors on diagnosis. By feeding a deep neural network (DNN) with the data from a low-cost and low-accuracy sensor array, we demonstrate that it becomes possible to significantly improve the measurements' precision and accuracy. The data collection is done with an array composed of 32 temperature sensors, including 16 analog and 16 digital sensors. All sensors have accuracies between 0.5-2.0^\circC. 800 vectors are extracted, covering a range from to 30 to 45^\circC. In order to improve the temperature readings, we use machine learning to perform a linear regression analysis through a DNN. In an attempt to minimize the model's complexity in order to eventually run inferences locally, the network with the best results involves only three layers using the hyperbolic tangent activation function and the Adam Stochastic Gradient Descent (SGD) optimizer. The model is trained with a randomly-selected dataset using 640 vectors (80% of the data) and tested with 160 vectors (20%). Using the mean squared error as a loss function between the data and the model's prediction, we achieve a loss of only 1.47x10^{-4} on the training set and 1.22x10^{-4} on the test set. As such, we believe this appealing approach offers a new pathway towards significantly better datasets using readily-available ultra low-cost sensors. △ Less","6 December, 2022",https://arxiv.org/pdf/2211.17139
Explaining machine learning models for age classification in human gait analysis,Djordje Slijepcevic;Fabian Horst;Marvin Simak;Sebastian Lapuschkin;Anna-Maria Raberger;Wojciech Samek;Christian Breiteneder;Wolfgang I. Schöllhorn;Matthias Zeppelzauer;Brian Horsak,"Machine learning (ML) models have proven effective in classifying gait analysis data, e.g., binary classification of young vs. older adults. ML models, however, lack in providing human understandable explanations for their predictions. This ""black-box"" behavior impedes the understanding of which input features the model predictions are based on. We investigated an Explainable Artificial Intelligence method, i.e., Layer-wise Relevance Propagation (LRP), for gait analysis data. The research question was: Which input features are used by ML models to classify age-related differences in walking patterns? We utilized a subset of the AIST Gait Database 2019 containing five bilateral ground reaction force (GRF) recordings per person during barefoot walking of healthy participants. Each input signal was min-max normalized before concatenation and fed into a Convolutional Neural Network (CNN). Participants were divided into three age groups: young (20-39 years), middle-aged (40-64 years), and older (65-79 years) adults. The classification accuracy and relevance scores (derived using LRP) were averaged over a stratified ten-fold cross-validation. The mean classification accuracy of 60.1% was clearly higher than the zero-rule baseline of 37.3%. The confusion matrix shows that the CNN distinguished younger and older adults well, but had difficulty modeling the middle-aged adults. △ Less","16 October, 2022",https://arxiv.org/pdf/2211.17016
Explaining automated gender classification of human gait,Fabian Horst;Djordje Slijepcevic;Matthias Zeppelzauer;Anna-Maria Raberger;Sebastian Lapuschkin;Wojciech Samek;Wolfgang I. Schöllhorn;Christian Breiteneder;Brian Horsak,"State-of-the-art machine learning (ML) models are highly effective in classifying gait analysis data, however, they lack in providing explanations for their predictions. This ""black-box"" characteristic makes it impossible to understand on which input patterns, ML models base their predictions. The present study investigates whether Explainable Artificial Intelligence methods, i.e., Layer-wise Relevance Propagation (LRP), can be useful to enhance the explainability of ML predictions in gait classification. The research question was: Which input patterns are most relevant for an automated gender classification model and do they correspond to characteristics identified in the literature? We utilized a subset of the GAITREC dataset containing five bilateral ground reaction force (GRF) recordings per person during barefoot walking of 62 healthy participants: 34 females and 28 males. Each input signal (right and left side) was min-max normalized before concatenation and fed into a multi-layer Convolutional Neural Network (CNN). The classification accuracy was obtained over a stratified ten-fold cross-validation. To identify gender-specific patterns, the input relevance scores were derived using LRP. The mean classification accuracy of the CNN with 83.3% showed a clear superiority over the zero-rule baseline of 54.8%. △ Less","16 October, 2022",https://arxiv.org/pdf/2211.17015
"Interpretability and accessibility of machine learning in selected food processing, agriculture and health applications",N. Ranasinghe;A. Ramanan;S. Fernando;P. N. Hameed;D. Herath;T. Malepathirana;P. Suganthan;M. Niranjan;S. Halgamuge,"Artificial Intelligence (AI) and its data-centric branch of machine learning (ML) have greatly evolved over the last few decades. However, as AI is used increasingly in real world use cases, the importance of the interpretability of and accessibility to AI systems have become major research areas. The lack of interpretability of ML based systems is a major hindrance to widespread adoption of these powerful algorithms. This is due to many reasons including ethical and regulatory concerns, which have resulted in poorer adoption of ML in some areas. The recent past has seen a surge in research on interpretable ML. Generally, designing a ML system requires good domain understanding combined with expert knowledge. New techniques are emerging to improve ML accessibility through automated model design. This paper provides a review of the work done to improve interpretability and accessibility of machine learning in the context of global problems while also being relevant to developing countries. We review work under multiple levels of interpretability including scientific and mathematical interpretation, statistical interpretation and partial semantic interpretation. This review includes applications in three areas, namely food processing, agriculture and health. △ Less","29 November, 2022",https://arxiv.org/pdf/2211.16699
A Blockchain-based Semantic Exchange Framework for Web 3.0 toward Participatory Economy,Yijing Lin;Zhipeng Gao;Yaofeng Tu;Hongyang Du;Dusit Niyato;Jiawen Kang;Hui Yang,"Web 3.0 is the next-generation Internet that enables participants to read, write, and own contents in a decentralized manner. It is mainly driven by blockchain, semantic communication, edge computing, and artificial intelligence, which can construct value networks to achieve participatory economics based on participatory decision making. Web 3.0 can capture the characteristics of blockchain, semantic extraction, and communication to achieve decentralized semantic sharing and transfer information precisely. However, current Web 3.0 solutions focus on the blockchain while overlooking other new technologies' roles in Web 3.0. To further unleash the advantages of semantic extraction and communication in Web 3.0, in this paper, we propose a blockchain-based semantic exchange framework to realize fair and efficient interactions. In this framework, we first attempt to tokenize semantic information into Non-Fungible Token (NFT) for semantic exchange. Then we utilize a Stackelberg game to maximize buying and pricing strategies for semantic trading. We also leverage Zero-Knowledge Proof to share authentic semantic information without publishing it before receiving payments, which can achieve a fair and privacy-preserving trading compared with current NFT marketplaces. A case study about urban planning is given to show clearly the proposed mechanisms. Finally, several challenges and opportunities are identified. △ Less","29 November, 2022",https://arxiv.org/pdf/2211.16662
Sequence learning in a spiking neuronal network with memristive synapses,Younes Bouhadjar;Sebastian Siegel;Tom Tetzlaff;Markus Diesmann;Rainer Waser;Dirk J. Wouters,"Brain-inspired computing proposes a set of algorithmic principles that hold promise for advancing artificial intelligence. They endow systems with self learning capabilities, efficient energy usage, and high storage capacity. A core concept that lies at the heart of brain computation is sequence learning and prediction. This form of computation is essential for almost all our daily tasks such as movement generation, perception, and language. Understanding how the brain performs such a computation is not only important to advance neuroscience but also to pave the way to new technological brain-inspired applications. A previously developed spiking neural network implementation of sequence prediction and recall learns complex, high-order sequences in an unsupervised manner by local, biologically inspired plasticity rules. An emerging type of hardware that holds promise for efficiently running this type of algorithm is neuromorphic hardware. It emulates the way the brain processes information and maps neurons and synapses directly into a physical substrate. Memristive devices have been identified as potential synaptic elements in neuromorphic hardware. In particular, redox-induced resistive random access memories (ReRAM) devices stand out at many aspects. They permit scalability, are energy efficient and fast, and can implement biological plasticity rules. In this work, we study the feasibility of using ReRAM devices as a replacement of the biological synapses in the sequence learning model. We implement and simulate the model including the ReRAM plasticity using the neural simulator NEST. We investigate the effect of different device properties on the performance characteristics of the sequence learning model, and demonstrate resilience with respect to different on-off ratios, conductance resolutions, device variability, and synaptic failure. △ Less","29 November, 2022",https://arxiv.org/pdf/2211.16592
Artificial prediction markets present a novel opportunity for human-AI collaboration,Tatiana Chakravorti;Vaibhav Singh;Sarah Rajtmajer;Michael McLaughlin;Robert Fraleigh;Christopher Griffin;Anthony Kwasnica;David Pennock;C. Lee Giles,"Despite high-profile successes in the field of Artificial Intelligence, machine-driven technologies still suffer important limitations, particularly for complex tasks where creativity, planning, common sense, intuition, or learning from limited data is required. These limitations motivate effective methods for human-machine collaboration. Our work makes two primary contributions. We thoroughly experiment with an artificial prediction market model to understand the effects of market parameters on model performance for benchmark classification tasks. We then demonstrate, through simulation, the impact of exogenous agents in the market, where these exogenous agents represent primitive human behaviors. This work lays the foundation for a novel set of hybrid human-AI machine learning algorithms. △ Less","29 November, 2022",https://arxiv.org/pdf/2211.16590
Improving Commonsense in Vision-Language Models via Knowledge Graph Riddles,Shuquan Ye;Yujia Xie;Dongdong Chen;Yichong Xu;Lu Yuan;Chenguang Zhu;Jing Liao,"This paper focuses on analyzing and improving the commonsense ability of recent popular vision-language (VL) models. Despite the great success, we observe that existing VL-models still lack commonsense knowledge/reasoning ability (e.g., ""Lemons are sour""), which is a vital component towards artificial general intelligence. Through our analysis, we find one important reason is that existing large-scale VL datasets do not contain much commonsense knowledge, which motivates us to improve the commonsense of VL-models from the data perspective. Rather than collecting a new VL training dataset, we propose a more scalable strategy, i.e., ""Data Augmentation with kNowledge graph linearization for CommonsensE capability"" (DANCE). It can be viewed as one type of data augmentation technique, which can inject commonsense knowledge into existing VL datasets on the fly during training. More specifically, we leverage the commonsense knowledge graph (e.g., ConceptNet) and create variants of text description in VL datasets via bidirectional sub-graph sequentialization. For better commonsense evaluation, we further propose the first retrieval-based commonsense diagnostic benchmark. By conducting extensive experiments on some representative VL-models, we demonstrate that our DANCE technique is able to significantly improve the commonsense ability while maintaining the performance on vanilla retrieval tasks. The code and data are available at https://github.com/pleaseconnectwifi/DANCE △ Less","29 November, 2022",https://arxiv.org/pdf/2211.16504
"POLCOVID: a multicenter multiclass chest X-ray database (Poland, 2020-2021)",Aleksandra Suwalska;Joanna Tobiasz;Wojciech Prazuch;Marek Socha;Pawel Foszner;Damian Piotrowski;Katarzyna Gruszczynska;Magdalena Sliwinska;Jerzy Walecki;Tadeusz Popiela;Grzegorz Przybylski;Mateusz Nowak;Piotr Fiedor;Malgorzata Pawlowska;Robert Flisiak;Krzysztof Simon;Gabriela Zapolska;Barbara Gizycka;Edyta Szurowska;POLCOVID Study Group;Michal Marczyk;Andrzej Cieszanowski;Joanna Polanska,"The outbreak of the SARS-CoV-2 pandemic has put healthcare systems worldwide to their limits, resulting in increased waiting time for diagnosis and required medical assistance. With chest radiographs (CXR) being one of the most common COVID-19 diagnosis methods, many artificial intelligence tools for image-based COVID-19 detection have been developed, often trained on a small number of images from COVID-19-positive patients. Thus, the need for high-quality and well-annotated CXR image databases increased. This paper introduces POLCOVID dataset, containing chest X-ray (CXR) images of patients with COVID-19 or other-type pneumonia, and healthy individuals gathered from 15 Polish hospitals. The original radiographs are accompanied by the preprocessed images limited to the lung area and the corresponding lung masks obtained with the segmentation model. Moreover, the manually created lung masks are provided for a part of POLCOVID dataset and the other four publicly available CXR image collections. POLCOVID dataset can help in pneumonia or COVID-19 diagnosis, while the set of matched images and lung masks may serve for the development of lung segmentation solutions. △ Less","15 December, 2022",https://arxiv.org/pdf/2211.16359
"Enhanced artificial intelligence-based diagnosis using CBCT with internal denoising: Clinical validation for discrimination of fungal ball, sinusitis, and normal cases in the maxillary sinus",Kyungsu Kim;Chae Yeon Lim;Joong Bo Shin;Myung Jin Chung;Yong Gi Jung,"The cone-beam computed tomography (CBCT) provides 3D volumetric imaging of a target with low radiation dose and cost compared with conventional computed tomography, and it is widely used in the detection of paranasal sinus disease. However, it lacks the sensitivity to detect soft tissue lesions owing to reconstruction constraints. Consequently, only physicians with expertise in CBCT reading can distinguish between inherent artifacts or noise and diseases, restricting the use of this imaging modality. The development of artificial intelligence (AI)-based computer-aided diagnosis methods for CBCT to overcome the shortage of experienced physicians has attracted substantial attention. However, advanced AI-based diagnosis addressing intrinsic noise in CBCT has not been devised, discouraging the practical use of AI solutions for CBCT. To address this issue, we propose an AI-based computer-aided diagnosis method using CBCT with a denoising module. This module is implemented before diagnosis to reconstruct the internal ground-truth full-dose scan corresponding to an input CBCT image and thereby improve the diagnostic performance. The external validation results for the unified diagnosis of sinus fungal ball, chronic rhinosinusitis, and normal cases show that the proposed method improves the micro-, macro-average AUC, and accuracy by 7.4, 5.6, and 9.6% (from 86.2, 87.0, and 73.4 to 93.6, 92.6, and 83.0%), respectively, compared with a baseline while improving human diagnosis accuracy by 11% (from 71.7 to 83.0%), demonstrating technical differentiation and clinical effectiveness. This pioneering study on AI-based diagnosis using CBCT indicates denoising can improve diagnostic performance and reader interpretability in images from the sinonasal area, thereby providing a new approach and direction to radiographic image reconstruction regarding the development of AI-based diagnostic solutions. △ Less","29 November, 2022",https://arxiv.org/pdf/2211.15950
"Performance Evaluation, Optimization and Dynamic Decision in Blockchain Systems: A Recent Overview",Quan-Lin Li;Yan-Xia Chang;Qing Wang,"With rapid development of blockchain technology as well as integration of various application areas, performance evaluation, performance optimization, and dynamic decision in blockchain systems are playing an increasingly important role in developing new blockchain technology. This paper provides a recent systematic overview of this class of research, and especially, developing mathematical modeling and basic theory of blockchain systems. Important examples include (a) performance evaluation: Markov processes, queuing theory, Markov reward processes, random walks, fluid and diffusion approximations, and martingale theory; (b) performance optimization: Linear programming, nonlinear programming, integer programming, and multi-objective programming; (c) optimal control and dynamic decision: Markov decision processes, and stochastic optimal control; and (d) artificial intelligence: Machine learning, deep reinforcement learning, and federated learning. So far, a little research has focused on these research lines. We believe that the basic theory with mathematical methods, algorithms and simulations of blockchain systems discussed in this paper will strongly support future development and continuous innovation of blockchain technology. △ Less","28 November, 2022",https://arxiv.org/pdf/2211.15907
Development of an Equation-based Parallelization Method for Multiphase Particle-in-Cell Simulations,Mino Woo;Terry Jordan;Tarak Nandi;Jean Francois Dietiker;Christopher Guenther;Dirk Van Essendelft,"Manufacturers have been developing new graphics processing unit (GPU) nodes with large capacity, high bandwidth memory and very high bandwidth intra-node interconnects. This enables moving large amounts of data between GPUs on the same node at low cost. However, small packet bandwidths and latencies have not decreased which makes global dot products expensive. These characteristics favor a new kind of problem decomposition called ""equation decomposition"" rather than traditional domain decomposition. In this approach, each GPU is assigned one equation set to solve in parallel so that the frequent and expensive dot product synchronization points in traditional distributed linear solvers are eliminated. In exchange, the method involves infrequent movement of state variables over the high bandwidth, intra-node interconnects. To test this theory, our flagship code Multiphase Flow with Interphase eXchanges (MFiX) was ported to TensorFlow. This new product is known as MFiX-AI and can produce near identical results to the original version of MFiX with significant acceleration in multiphase particle-in-cell (MP-PIC) simulations. The performance of a single node with 4 NVIDIA A100s connected over NVLINK 2.0 was shown to be competitive to 1000 CPU cores (25 nodes) on the JOULE 2.0 supercomputer, leading to an energy savings of up to 90%. This is a substantial performance benefit for small- to intermediate-sized problems. This benefit is expected to grow as GPU nodes become more powerful. Further, MFiX-AI is poised to accept native artificial intelligence/machine learning models for further acceleration and development. △ Less","28 November, 2022",https://arxiv.org/pdf/2211.15605
AI Enabled Maneuver Identification via the Maneuver Identification Challenge,Kaira Samuel;Matthew LaRosa;Kyle McAlpin;Morgan Schaefer;Brandon Swenson;Devin Wasilefsky;Yan Wu;Dan Zhao;Jeremy Kepner,"Artificial intelligence (AI) has enormous potential to improve Air Force pilot training by providing actionable feedback to pilot trainees on the quality of their maneuvers and enabling instructor-less flying familiarization for early-stage trainees in low-cost simulators. Historically, AI challenges consisting of data, problem descriptions, and example code have been critical to fueling AI breakthroughs. The Department of the Air Force-Massachusetts Institute of Technology AI Accelerator (DAF-MIT AI Accelerator) developed such an AI challenge using real-world Air Force flight simulator data. The Maneuver ID challenge assembled thousands of virtual reality simulator flight recordings collected by actual Air Force student pilots at Pilot Training Next (PTN). This dataset has been publicly released at Maneuver-ID.mit.edu and represents the first of its kind public release of USAF flight training data. Using this dataset, we have applied a variety of AI methods to separate ""good"" vs ""bad"" simulator data and categorize and characterize maneuvers. These data, algorithms, and software are being released as baselines of model performance for others to build upon to enable the AI ecosystem for flight simulator training. △ Less","28 November, 2022",https://arxiv.org/pdf/2211.15552
The Stack: 3 TB of permissively licensed source code,Denis Kocetkov;Raymond Li;Loubna Ben Allal;Jia Li;Chenghao Mou;Carlos Muñoz Ferrandis;Yacine Jernite;Margaret Mitchell;Sean Hughes;Thomas Wolf;Dzmitry Bahdanau;Leandro von Werra;Harm de Vries,"Large Language Models (LLMs) play an ever-increasing role in the field of Artificial Intelligence (AI)--not only for natural language processing but also for code understanding and generation. To stimulate open and responsible research on LLMs for code, we introduce The Stack, a 3.1 TB dataset consisting of permissively licensed source code in 30 programming languages. We describe how we collect the full dataset, construct a permissively licensed subset, present a data governance plan, discuss limitations, and show promising results on text2code benchmarks by training 350M-parameter decoders on different Python subsets. We find that (1) near-deduplicating the data significantly boosts performance across all experiments, and (2) it is possible to match previously reported HumanEval and MBPP performance using only permissively licensed data. We make the dataset available at https://hf.co/BigCode, provide a tool called ""Am I in The Stack"" (https://hf.co/spaces/bigcode/in-the-stack) for developers to search The Stack for copies of their code, and provide a process for code to be removed from the dataset by following the instructions at https://www.bigcode-project.org/docs/about/the-stack/. △ Less","20 November, 2022",https://arxiv.org/pdf/2211.15533
Automated Detection of Dolphin Whistles with Convolutional Networks and Transfer Learning,Burla Nur Korkmaz;Roee Diamant;Gil Danino;Alberto Testolin,"Effective conservation of maritime environments and wildlife management of endangered species require the implementation of efficient, accurate and scalable solutions for environmental monitoring. Ecoacoustics offers the advantages of non-invasive, long-duration sampling of environmental sounds and has the potential to become the reference tool for biodiversity surveying. However, the analysis and interpretation of acoustic data is a time-consuming process that often requires a great amount of human supervision. This issue might be tackled by exploiting modern techniques for automatic audio signal analysis, which have recently achieved impressive performance thanks to the advances in deep learning research. In this paper we show that convolutional neural networks can indeed significantly outperform traditional automatic methods in a challenging detection task: identification of dolphin whistles from underwater audio recordings. The proposed system can detect signals even in the presence of ambient noise, at the same time consistently reducing the likelihood of producing false positives and false negatives. Our results further support the adoption of artificial intelligence technology to improve the automatic monitoring of marine ecosystems. △ Less","28 November, 2022",https://arxiv.org/pdf/2211.15406
Deep Grading based on Collective Artificial Intelligence for AD Diagnosis and Prognosis,Huy-Dung Nguyen;Michaël Clément;Boris Mansencal;Pierrick Coupé,"Accurate diagnosis and prognosis of Alzheimer's disease are crucial to develop new therapies and reduce the associated costs. Recently, with the advances of convolutional neural networks, methods have been proposed to automate these two tasks using structural MRI. However, these methods often suffer from lack of interpretability, generalization, and can be limited in terms of performance. In this paper, we propose a novel deep framework designed to overcome these limitations. Our framework consists of two stages. In the first stage, we propose a deep grading model to extract meaningful features. To enhance the robustness of these features against domain shift, we introduce an innovative collective artificial intelligence strategy for training and evaluating steps. In the second stage, we use a graph convolutional neural network to better capture AD signatures. Our experiments based on 2074 subjects show the competitive performance of our deep framework compared to state-of-the-art methods on different datasets for both AD diagnosis and prognosis. △ Less","28 November, 2022",https://arxiv.org/pdf/2211.15192
Federated Learning Attacks and Defenses: A Survey,Yao Chen;Yijie Gui;Hong Lin;Wensheng Gan;Yongdong Wu,"In terms of artificial intelligence, there are several security and privacy deficiencies in the traditional centralized training methods of machine learning models by a server. To address this limitation, federated learning (FL) has been proposed and is known for breaking down ``data silos"" and protecting the privacy of users. However, FL has not yet gained popularity in the industry, mainly due to its security, privacy, and high cost of communication. For the purpose of advancing the research in this field, building a robust FL system, and realizing the wide application of FL, this paper sorts out the possible attacks and corresponding defenses of the current FL system systematically. Firstly, this paper briefly introduces the basic workflow of FL and related knowledge of attacks and defenses. It reviews a great deal of research about privacy theft and malicious attacks that have been studied in recent years. Most importantly, in view of the current three classification criteria, namely the three stages of machine learning, the three different roles in federated learning, and the CIA (Confidentiality, Integrity, and Availability) guidelines on privacy protection, we divide attack approaches into two categories according to the training stage and the prediction stage in machine learning. Furthermore, we also identify the CIA property violated for each attack method and potential attack role. Various defense mechanisms are then analyzed separately from the level of privacy and security. Finally, we summarize the possible challenges in the application of FL from the aspect of attacks and defenses and discuss the future development direction of FL systems. In this way, the designed FL system has the ability to resist different attacks and is more secure and stable. △ Less","27 November, 2022",https://arxiv.org/pdf/2211.14952
"Metaverse in Education: Vision, Opportunities, and Challenges",Hong Lin;Shicheng Wan;Wensheng Gan;Jiahui Chen;Han-Chieh Chao,"Traditional education has been updated with the development of information technology in human history. Within big data and cyber-physical systems, the Metaverse has generated strong interest in various applications (e.g., entertainment, business, and cultural travel) over the last decade. As a novel social work idea, the Metaverse consists of many kinds of technologies, e.g., big data, interaction, artificial intelligence, game design, Internet computing, Internet of Things, and blockchain. It is foreseeable that the usage of Metaverse will contribute to educational development. However, the architectures of the Metaverse in education are not yet mature enough. There are many questions we should address for the Metaverse in education. To this end, this paper aims to provide a systematic literature review of Metaverse in education. This paper is a comprehensive survey of the Metaverse in education, with a focus on current technologies, challenges, opportunities, and future directions. First, we present a brief overview of the Metaverse in education, as well as the motivation behind its integration. Then, we survey some important characteristics for the Metaverse in education, including the personal teaching environment and the personal learning environment. Next, we envisage what variations of this combination will bring to education in the future and discuss their strengths and weaknesses. We also review the state-of-the-art case studies (including technical companies and educational institutions) for Metaverse in education. Finally, we point out several challenges and issues in this promising area. △ Less","27 November, 2022",https://arxiv.org/pdf/2211.14951
Multi-Label Chest X-Ray Classification via Deep Learning,Aravind Sasidharan Pillai,"In this era of pandemic, the future of healthcare industry has never been more exciting. Artificial intelligence and machine learning (AI & ML) present opportunities to develop solutions that cater for very specific needs within the industry. Deep learning in healthcare had become incredibly powerful for supporting clinics and in transforming patient care in general. Deep learning is increasingly being applied for the detection of clinically important features in the images beyond what can be perceived by the naked human eye. Chest X-ray images are one of the most common clinical method for diagnosing a number of diseases such as pneumonia, lung cancer and many other abnormalities like lesions and fractures. Proper diagnosis of a disease from X-ray images is often challenging task for even expert radiologists and there is a growing need for computerized support systems due to the large amount of information encoded in X-Ray images. The goal of this paper is to develop a lightweight solution to detect 14 different chest conditions from an X ray image. Given an X-ray image as input, our classifier outputs a label vector indicating which of 14 disease classes does the image fall into. Along with the image features, we are also going to use non-image features available in the data such as X-ray view type, age, gender etc. The original study conducted Stanford ML Group is our base line. Original study focuses on predicting 5 diseases. Our aim is to improve upon previous work, expand prediction to 14 diseases and provide insight for future chest radiography research. △ Less","27 November, 2022",https://arxiv.org/pdf/2211.14929
Dynamic Programmable Wireless Environment with UAV-mounted Static Metasurfaces,Prodromos-Vasileios Mekikis;Dimitrios Tyrovolas;Sotiris Tegos;Alexandros Papadopoulos;Alexandros Pitilakis;Sotiris Ioannidis;Ageliki Tsiolaridou;Panagiotis Diamantoulakis;Nikolaos Kantartzis;George K. Karagiannidis;Christos Liaskos,"Reconfigurable intelligent surfaces (RISs) are artificial planar structures able to offer a unique way of manipulating propagated wireless signals. Commonly composed of a number of reconfigurable passive cell components and basic electronic circuits, RISs can almost freely perform a set of wave modification functionalities, in order to realize programmable wireless environments (PWEs). However, a more energy-efficient way to realize a PWE is through dynamically relocating static metasurfaces that perform a unique functionality. In this paper, we employ a UAV swarm to dynamically deploy a set of lowcost passive metasurfaces that are able to perform only one electromagnetic functionality, but with the benefit of requiring no power. Specifically, the UAV-mounted static metasurfaces are carefully positioned across the sky to create cascaded channels for improved user service and security hardening. The performance evaluation results, based on △ Less","27 November, 2022",https://arxiv.org/pdf/2211.14882
Deep Active Learning for Computer Vision: Past and Future,Rinyoichi Takezoe;Xu Liu;Shunan Mao;Marco Tianyu Chen;Zhanpeng Feng;Shiliang Zhang;Xiaoyu Wang,"As an important data selection schema, active learning emerges as the essential component when iterating an Artificial Intelligence (AI) model. It becomes even more critical given the dominance of deep neural network based models, which are composed of a large number of parameters and data hungry, in application. Despite its indispensable role for developing AI models, research on active learning is not as intensive as other research directions. In this paper, we present a review of active learning through deep active learning approaches from the following perspectives: 1) technical advancements in active learning, 2) applications of active learning in computer vision, 3) industrial systems leveraging or with potential to leverage active learning for data iteration, 4) current limitations and future research directions. We expect this paper to clarify the significance of active learning in a modern AI model manufacturing process and to bring additional research attention to active learning. By addressing data automation challenges and coping with automated machine learning systems, active learning will facilitate democratization of AI technologies by boosting model production at scale. △ Less","24 December, 2022",https://arxiv.org/pdf/2211.14819
Conditioning Covert Geo-Location (CGL) Detection on Semantic Class Information,Binoy Saha;Sukhendu Das,"The primary goal of artificial intelligence is to mimic humans. Therefore, to advance toward this goal, the AI community attempts to imitate qualities/skills possessed by humans and imbibes them into machines with the help of datasets/tasks. Earlier, many tasks which require knowledge about the objects present in an image are satisfactorily solved by vision models. Recently, with the aim to incorporate knowledge about non-object image regions (hideouts, turns, and other obscured regions), a task for identification of potential hideouts termed Covert Geo-Location (CGL) detection was proposed by Saha et al. It involves identification of image regions which have the potential to either cause an imminent threat or appear as target zones to be accessed for further investigation to identify any occluded objects. Only certain occluding items belonging to certain semantic classes can give rise to CGLs. This fact was overlooked by Saha et al. and no attempts were made to utilize semantic class information, which is crucial for CGL detection. In this paper, we propose a multitask-learning-based approach to achieve 2 goals - i) extraction of features having semantic class information; ii) robust training of the common encoder, exploiting large standard annotated datasets as training set for the auxiliary task (semantic segmentation). To explicitly incorporate class information in the features extracted by the encoder, we have further employed attention mechanism in a novel manner. We have also proposed a better evaluation metric for CGL detection that gives more weightage to recognition rather than precise localization. Experimental evaluations performed on the CGL dataset, demonstrate a significant increase in performance of about 3% to 14% mIoU and 3% to 16% DaR on split 1, and 1% mIoU and 1% to 2% DaR on split 2 over SOTA, serving as a testimony to the superiority of our approach. △ Less","27 November, 2022",https://arxiv.org/pdf/2211.14750
"Deep Fake Detection, Deterrence and Response: Challenges and Opportunities",Amin Azmoodeh;Ali Dehghantanha,"According to the 2020 cyber threat defence report, 78% of Canadian organizations experienced at least one successful cyberattack in 2020. The consequences of such attacks vary from privacy compromises to immersing damage costs for individuals, companies, and countries. Specialists predict that the global loss from cybercrime will reach 10.5 trillion US dollars annually by 2025. Given such alarming statistics, the need to prevent and predict cyberattacks is as high as ever. Our increasing reliance on Machine Learning(ML)-based systems raises serious concerns about the security and safety of these systems. Especially the emergence of powerful ML techniques to generate fake visual, textual, or audio content with a high potential to deceive humans raised serious ethical concerns. These artificially crafted deceiving videos, images, audio, or texts are known as Deepfakes garnered attention for their potential use in creating fake news, hoaxes, revenge porn, and financial fraud. Diversity and the widespread of deepfakes made their timely detection a significant challenge. In this paper, we first offer background information and a review of previous works on the detection and deterrence of deepfakes. Afterward, we offer a solution that is capable of 1) making our AI systems robust against deepfakes during development and deployment phases; 2) detecting video, image, audio, and textual deepfakes; 3) identifying deepfakes that bypass detection (deepfake hunting); 4) leveraging available intelligence for timely identification of deepfake campaigns launched by state-sponsored hacking teams; 5) conducting in-depth forensic analysis of identified deepfake payloads. Our solution would address important elements of the Canada National Cyber Security Action Plan(2019-2024) in increasing the trustworthiness of our critical services. △ Less","26 November, 2022",https://arxiv.org/pdf/2211.14667
A Survey of Text Representation Methods and Their Genealogy,Philipp Siebers;Christian Janiesch;Patrick Zschech,"In recent years, with the advent of highly scalable artificial-neural-network-based text representation methods the field of natural language processing has seen unprecedented growth and sophistication. It has become possible to distill complex linguistic information of text into multidimensional dense numeric vectors with the use of the distributional hypothesis. As a consequence, text representation methods have been evolving at such a quick pace that the research community is struggling to retain knowledge of the methods and their interrelations. We contribute threefold to this lack of compilation, composition, and systematization by providing a survey of current approaches, by arranging them in a genealogy, and by conceptualizing a taxonomy of text representation methods to examine and explain the state-of-the-art. Our research is a valuable guide and reference for artificial intelligence researchers and practitioners interested in natural language processing applications such as recommender systems, chatbots, and sentiment analysis. △ Less","26 November, 2022",https://arxiv.org/pdf/2211.14591
Deep neuroevolution to predict primary brain tumor grade from functional MRI adjacency matrices,Joseph Stember;Mehrnaz Jenabi;Luca Pasquini;Kyung Peck;Andrei Holodny;Hrithwik Shalu,"Whereas MRI produces anatomic information about the brain, functional MRI (fMRI) tells us about neural activity within the brain, including how various regions communicate with each other. The full chorus of conversations within the brain is summarized elegantly in the adjacency matrix. Although information-rich, adjacency matrices typically provide little in the way of intuition. Whereas trained radiologists viewing anatomic MRI can readily distinguish between different kinds of brain cancer, a similar determination using adjacency matrices would exceed any expert's grasp. Artificial intelligence (AI) in radiology usually analyzes anatomic imaging, providing assistance to radiologists. For non-intuitive data types such as adjacency matrices, AI moves beyond the role of helpful assistant, emerging as indispensible. We sought here to show that AI can learn to discern between two important brain tumor types, high-grade glioma (HGG) and low-grade glioma (LGG), based on adjacency matrices. We trained a convolutional neural networks (CNN) with the method of deep neuroevolution (DNE), because of the latter's recent promising results; DNE has produced remarkably accurate CNNs even when relying on small and noisy training sets, or performing nuanced tasks. After training on just 30 adjacency matrices, our CNN could tell HGG apart from LGG with perfect testing set accuracy. Saliency maps revealed that the network learned highly sophisticated and complex features to achieve its success. Hence, we have shown that it is possible for AI to recognize brain tumor type from functional connectivity. In future work, we will apply DNE to other noisy and somewhat cryptic forms of medical data, including further explorations with fMRI. △ Less","26 November, 2022",https://arxiv.org/pdf/2211.14500
"Deep neuroevolution for limited, heterogeneous data: proof-of-concept application to Neuroblastoma brain metastasis using a small virtual pooled image collection",Subhanik Purkayastha;Hrithwik Shalu;David Gutman;Shakeel Modak;Ellen Basu;Brian Kushner;Kim Kramer;Sofia Haque;Joseph Stember,"Artificial intelligence (AI) in radiology has made great strides in recent years, but many hurdles remain. Overfitting and lack of generalizability represent important ongoing challenges hindering accurate and dependable clinical deployment. If AI algorithms can avoid overfitting and achieve true generalizability, they can go from the research realm to the forefront of clinical work. Recently, small data AI approaches such as deep neuroevolution (DNE) have avoided overfitting small training sets. We seek to address both overfitting and generalizability by applying DNE to a virtually pooled data set consisting of images from various institutions. Our use case is classifying neuroblastoma brain metastases on MRI. Neuroblastoma is well-suited for our goals because it is a rare cancer. Hence, studying this pediatric disease requires a small data approach. As a tertiary care center, the neuroblastoma images in our local Picture Archiving and Communication System (PACS) are largely from outside institutions. These multi-institutional images provide a heterogeneous data set that can simulate real world clinical deployment. As in prior DNE work, we used a small training set, consisting of 30 normal and 30 metastasis-containing post-contrast MRI brain scans, with 37% outside images. The testing set was enriched with 83% outside images. DNE converged to a testing set accuracy of 97%. Hence, the algorithm was able to predict image class with near-perfect accuracy on a testing set that simulates real-world data. Hence, the work described here represents a considerable contribution toward clinically feasible AI. △ Less","26 November, 2022",https://arxiv.org/pdf/2211.14499
A Critical Review of Traffic Signal Control and A Novel Unified View of Reinforcement Learning and Model Predictive Control Approaches for Adaptive Traffic Signal Control,Xiaoyu Wang;Scott Sanner;Baher Abdulhai,"Recent years have witnessed substantial growth in adaptive traffic signal control (ATSC) methodologies that improve transportation network efficiency, especially in branches leveraging artificial intelligence based optimization and control algorithms such as reinforcement learning as well as conventional model predictive control. However, lack of cross-domain analysis and comparison of the effectiveness of applied methods in ATSC research limits our understanding of existing challenges and research directions. This chapter proposes a novel unified view of modern ATSCs to identify common ground as well as differences and shortcomings of existing methodologies with the ultimate goal to facilitate cross-fertilization and advance the state-of-the-art. The unified view applies the mathematical language of the Markov decision process, describes the process of controller design from both the world (problem) and solution modeling perspectives. The unified view also analyses systematic issues commonly ignored in existing studies and suggests future potential directions to resolve these issues. △ Less","25 November, 2022",https://arxiv.org/pdf/2211.14426
Learning Branching Heuristics from Graph Neural Networks,Congsong Zhang;Yong Gao;James Nastos,"Backtracking has been widely used for solving problems in artificial intelligence (AI), including constraint satisfaction problems and combinatorial optimization problems. Good branching heuristics can efficiently improve the performance of backtracking by helping prune the search space and leading the search to the most promising direction. In this paper, we first propose a new graph neural network (GNN) model designed using the probabilistic method. From the GNN model, we introduce an approach to learn a branching heuristic for combinatorial optimization problems. In particular, our GNN model learns appropriate probability distributions on vertices in given graphs from which the branching heuristic is extracted and used in a backtracking search. Our experimental results for the (minimum) dominating-clique problem show that this learned branching heuristic performs better than the minimum-remaining-values heuristic in terms of the number of branches of the whole search tree. Our approach introduces a new way of applying GNNs towards enhancing the classical backtracking algorithm used in AI. △ Less","25 November, 2022",https://arxiv.org/pdf/2211.14405
"Less Data, More Knowledge: Building Next Generation Semantic Communication Networks",Christina Chaccour;Walid Saad;Merouane Debbah;Zhu Han;H. Vincent Poor,"Semantic communication is viewed as a revolutionary paradigm that can potentially transform how we design and operate wireless communication systems. However, despite a recent surge of research activities in this area, the research landscape remains limited. In this tutorial, we present the first rigorous vision of a scalable end-to-end semantic communication network that is founded on novel concepts from artificial intelligence (AI), causal reasoning, and communication theory. We first discuss how the design of semantic communication networks requires a move from data-driven networks towards knowledge-driven ones. Subsequently, we highlight the necessity of creating semantic representations of data that satisfy the key properties of minimalism, generalizability, and efficiency so as to do more with less. We then explain how those representations can form the basis a so-called semantic language. By using semantic representation and languages, we show that the traditional transmitter and receiver now become a teacher and apprentice. Then, we define the concept of reasoning by investigating the fundamentals of causal representation learning and their role in designing semantic communication networks. We demonstrate that reasoning faculties are majorly characterized by the ability to capture causal and associational relationships in datastreams. For such reasoning-driven networks, we propose novel and essential semantic communication metrics that include new ""reasoning capacity"" measures that could go beyond Shannon's bound to capture the convergence of computing and communication. Finally, we explain how semantic communications can be scaled to large-scale networks (6G and beyond). In a nutshell, we expect this tutorial to provide a comprehensive reference on how to properly build, analyze, and deploy future semantic communication networks. △ Less","25 November, 2022",https://arxiv.org/pdf/2211.14343
A Survey of Learning Curves with Bad Behavior: or How More Data Need Not Lead to Better Performance,Marco Loog;Tom Viering,"Plotting a learner's generalization performance against the training set size results in a so-called learning curve. This tool, providing insight in the behavior of the learner, is also practically valuable for model selection, predicting the effect of more training data, and reducing the computational complexity of training. We set out to make the (ideal) learning curve concept precise and briefly discuss the aforementioned usages of such curves. The larger part of this survey's focus, however, is on learning curves that show that more data does not necessarily leads to better generalization performance. A result that seems surprising to many researchers in the field of artificial intelligence. We point out the significance of these findings and conclude our survey with an overview and discussion of open problems in this area that warrant further theoretical and empirical investigation. △ Less","25 November, 2022",https://arxiv.org/pdf/2211.14061
Molecular Joint Representation Learning via Multi-modal Information,Tianyu Wu;Yang Tang;Qiyu Sun;Luolin Xiong,"In recent years, artificial intelligence has played an important role on accelerating the whole process of drug discovery. Various of molecular representation schemes of different modals (e.g. textual sequence or graph) are developed. By digitally encoding them, different chemical information can be learned through corresponding network structures. Molecular graphs and Simplified Molecular Input Line Entry System (SMILES) are popular means for molecular representation learning in current. Previous works have done attempts by combining both of them to solve the problem of specific information loss in single-modal representation on various tasks. To further fusing such multi-modal imformation, the correspondence between learned chemical feature from different representation should be considered. To realize this, we propose a novel framework of molecular joint representation learning via Multi-Modal information of SMILES and molecular Graphs, called MMSG. We improve the self-attention mechanism by introducing bond level graph representation as attention bias in Transformer to reinforce feature correspondence between multi-modal information. We further propose a Bidirectional Message Communication Graph Neural Network (BMC GNN) to strengthen the information flow aggregated from graphs for further combination. Numerous experiments on public property prediction datasets have demonstrated the effectiveness of our model. △ Less","25 November, 2022",https://arxiv.org/pdf/2211.14042
Digital Twin-Driven Computing Resource Management for Vehicular Networks,Mushu Li;Jie Gao;Conghao Zhou;Xuemin;Shen;Weihua Zhuang,"This paper presents a novel approach for computing resource management of edge servers in vehicular networks based on digital twins and artificial intelligence (AI). Specifically, we construct two-tier digital twins tailored for vehicular networks to capture networking-related features of vehicles and edge servers. By exploiting such features, we propose a two-stage computing resource allocation scheme. First, the central controller periodically generates reference policies for real-time computing resource allocation according to the network dynamics and service demands captured by digital twins of edge servers. Second, computing resources of the edge servers are allocated in real time to individual vehicles via low-complexity matching-based allocation that complies with the reference policies. By leveraging digital twins, the proposed scheme can adapt to dynamic service demands and vehicle mobility in a scalable manner. Simulation results demonstrate that the proposed digital twin-driven scheme enables the vehicular network to support more computing tasks than benchmark schemes. △ Less","24 November, 2022",https://arxiv.org/pdf/2211.13818
Generalization of Artificial Intelligence Models in Medical Imaging: A Case-Based Review,Rishi Gadepally;Andrew Gomella;Eric Gingold;Paras Lakhani,"The discussions around Artificial Intelligence (AI) and medical imaging are centered around the success of deep learning algorithms. As new algorithms enter the market, it is important for practicing radiologists to understand the pitfalls of various AI algorithms. This entails having a basic understanding of how algorithms are developed, the kind of data they are trained on, and the settings in which they will be deployed. As with all new technologies, use of AI should be preceded by a fundamental understanding of the risks and benefits to those it is intended to help. This case-based review is intended to point out specific factors practicing radiologists who intend to use AI should consider. △ Less","15 November, 2022",https://arxiv.org/pdf/2211.13230
A Brief Overview of AI Governance for Responsible Machine Learning Systems,Navdeep Gill;Abhishek Mathur;Marcos V. Conde,"Organizations of all sizes, across all industries and domains are leveraging artificial intelligence (AI) technologies to solve some of their biggest challenges around operations, customer experience, and much more. However, due to the probabilistic nature of AI, the risks associated with it are far greater than traditional technologies. Research has shown that these risks can range anywhere from regulatory, compliance, reputational, and user trust, to financial and even societal risks. Depending on the nature and size of the organization, AI technologies can pose a significant risk, if not used in a responsible way. This position paper seeks to present a brief introduction to AI governance, which is a framework designed to oversee the responsible use of AI with the goal of preventing and mitigating risks. Having such a framework will not only manage risks but also gain maximum value out of AI projects and develop consistency for organization-wide adoption of AI. △ Less","21 November, 2022",https://arxiv.org/pdf/2211.13130
Cultural Incongruencies in Artificial Intelligence,Vinodkumar Prabhakaran;Rida Qadri;Ben Hutchinson,"Artificial intelligence (AI) systems attempt to imitate human behavior. How well they do this imitation is often used to assess their utility and to attribute human-like (or artificial) intelligence to them. However, most work on AI refers to and relies on human intelligence without accounting for the fact that human behavior is inherently shaped by the cultural contexts they are embedded in, the values and beliefs they hold, and the social practices they follow. Additionally, since AI technologies are mostly conceived and developed in just a handful of countries, they embed the cultural values and practices of these countries. Similarly, the data that is used to train the models also fails to equitably represent global cultural diversity. Problems therefore arise when these technologies interact with globally diverse societies and cultures, with different values and interpretive practices. In this position paper, we describe a set of cultural dependencies and incongruencies in the context of AI-based language and vision technologies, and reflect on the possibilities of and potential strategies towards addressing these incongruencies. △ Less","19 November, 2022",https://arxiv.org/pdf/2211.13069
Petroleum prices prediction using data mining techniques -- A Review,Kiplang'at Weldon;John Ngechu;Ngatho Everlyne;Nancy Njambi;Kinyua Gikunda,"Over the past 20 years, Kenya's demand for petroleum products has proliferated. This is mainly because this particular commodity is used in many sectors of the country's economy. Exchange rates are impacted by constantly shifting prices, which also impact Kenya's industrial output of commodities. The cost of other items produced and even the expansion of the economy is significantly impacted by any change in the price of petroleum products. Therefore, accurate petroleum price forecasting is critical for devising policies that are suitable to curb fuel-related shocks. Data mining techniques are the tools used to find valuable patterns in data. Data mining techniques used in petroleum price prediction, including artificial neural networks (ANNs), support vector machines (SVMs), and intelligent optimization techniques like the genetic algorithm (GA), have grown increasingly popular. This study provides a comprehensive review of the existing data mining techniques for making predictions on petroleum prices. The data mining techniques are classified into regression models, deep neural network models, fuzzy sets and logic, and hybrid models. A detailed discussion of how these models are developed and the accuracy of the models is provided. △ Less","20 November, 2022",https://arxiv.org/pdf/2211.12964
"Look, Read and Ask: Learning to Ask Questions by Reading Text in Images",Soumya Jahagirdar;Shankar Gangisetty;Anand Mishra,"We present a novel problem of text-based visual question generation or TextVQG in short. Given the recent growing interest of the document image analysis community in combining text understanding with conversational artificial intelligence, e.g., text-based visual question answering, TextVQG becomes an important task. TextVQG aims to generate a natural language question for a given input image and an automatically extracted text also known as OCR token from it such that the OCR token is an answer to the generated question. TextVQG is an essential ability for a conversational agent. However, it is challenging as it requires an in-depth understanding of the scene and the ability to semantically bridge the visual content with the text present in the image. To address TextVQG, we present an OCR consistent visual question generation model that Looks into the visual content, Reads the scene text, and Asks a relevant and meaningful natural language question. We refer to our proposed model as OLRA. We perform an extensive evaluation of OLRA on two public benchmarks and compare them against baselines. Our model OLRA automatically generates questions similar to the public text-based visual question answering datasets that were curated manually. Moreover, we significantly outperform baseline approaches on the performance measures popularly used in text generation literature. △ Less","23 November, 2022",https://arxiv.org/pdf/2211.12950
A Streamlit-based Artificial Intelligence Trust Platform for Next-Generation Wireless Networks,M. Kuzlu;F. O. Catak;S. Sarp;U. Cali;O Gueler,"With the rapid development and integration of artificial intelligence (AI) methods in next-generation networks (NextG), AI algorithms have provided significant advantages for NextG in terms of frequency spectrum usage, bandwidth, latency, and security. A key feature of NextG is the integration of AI, i.e., self-learning architecture based on self-supervised algorithms, to improve the performance of the network. A secure AI-powered structure is also expected to protect NextG networks against cyber-attacks. However, AI itself may be attacked, i.e., model poisoning targeted by attackers, and it results in cybersecurity violations. This paper proposes an AI trust platform using Streamlit for NextG networks that allows researchers to evaluate, defend, certify, and verify their AI models and applications against adversarial threats of evasion, poisoning, extraction, and interference. △ Less","25 October, 2022",https://arxiv.org/pdf/2211.12851
The Impact of Generative AI on the Future of Visual Content Marketing,Shiva Mayahi;Marko Vidrih,"In today's world of marketing, it is necessary to have visually appealing content. Visual material has become an essential area of focus for every company as a result of the widespread availability of gadgets for mass communication and extended visual advancements. Similarly, artificial intelligence is also gaining ground and it is proving to be the most revolutionary technological advancement thus far. The integration of visual content with artificial intelligence is the key to acquiring and retaining loyal customers; its absence from the overarching marketing strategy of any production raises a red flag that could ultimately result in a smaller market share for that company. △ Less","22 November, 2022",https://arxiv.org/pdf/2211.12660
Contextually Aware Intelligent Control Agents for Heterogeneous Swarms,Adam Hepworth;Aya Hussein;Darryn Reid;Hussein Abbass,An emerging challenge in swarm shepherding research is to design effective and efficient artificial intelligence algorithms that maintain a low-computational ceiling while increasing the swarm's abilities to operate in diverse contexts. We propose a methodology to design a context-aware swarm-control intelligent agent. The intelligent control agent (shepherd) first uses swarm metrics to recognise the type of swarm it interacts with to then select a suitable parameterisation from its behavioural library for that particular swarm type. The design principle of our methodology is to increase the situation awareness (i.e. information contents) of the control agent without sacrificing the low-computational cost necessary for efficient swarm control. We demonstrate successful shepherding in both homogeneous and heterogeneous swarms. △ Less,"22 November, 2022",https://arxiv.org/pdf/2211.12560
Smart Agriculture : A Novel Multilevel Approach for Agricultural Risk Assessment over Unstructured Data,Hasna Najmi;Mounia Mikram;Maryem Rhanoui;Siham Yousfi,"Detecting opportunities and threats from massive text data is a challenging task for most. Traditionally, companies would rely mainly on structured data to detect and predict risks, losing a huge amount of information that could be extracted from unstructured text data. Fortunately, artificial intelligence came to remedy this issue by innovating in data extraction and processing techniques, allowing us to understand and make use of Natural Language data and turning it into structures that a machine can process and extract insight from. Uncertainty refers to a state of not knowing what will happen in the future. This paper aims to leverage natural language processing and machine learning techniques to model uncertainties and evaluate the risk level in each uncertainty cluster using massive text data. △ Less","22 November, 2022",https://arxiv.org/pdf/2211.12515
Expansive Participatory AI: Supporting Dreaming within Inequitable Institutions,Michael Alan Chang;Shiran Dudy,"Participatory Artificial Intelligence (PAI) has recently gained interest by researchers as means to inform the design of technology through collective's lived experience. PAI has a greater promise than that of providing useful input to developers, it can contribute to the process of democratizing the design of technology, setting the focus on what should be designed. However, in the process of PAI there existing institutional power dynamics that hinder the realization of expansive dreams and aspirations of the relevant stakeholders. In this work we propose co-design principals for AI that address institutional power dynamics focusing on Participatory AI with youth. △ Less","22 November, 2022",https://arxiv.org/pdf/2211.12434
Photonic Spiking Neural Networks with Highly Efficient Training Protocols for Ultrafast Neuromorphic Computing Systems,Dafydd Owen-Newns;Joshua Robertson;Matej Hejda;Antonio Hurtado,"Photonic technologies offer great prospects for novel ultrafast, energy-efficient and hardware-friendly neuromorphic (brain-like) computing platforms. Moreover, neuromorphic photonic approaches based upon ubiquitous, technology-mature and low-cost Vertical-Cavity Surface Emitting Lasers (VCSELs) (devices found in fibre-optic transmitters, mobile phones, automotive sensors, etc.) are of particular interest. Given VCSELs have shown the ability to realise neuronal optical spiking responses (at ultrafast GHz rates), their use for spike-based information processing systems has been proposed. In this work, Spiking Neural Network (SNN) operation, based on a hardware-friendly photonic system of just one Vertical Cavity Surface Emitting Laser (VCSEL), is reported alongside a novel binary weight 'significance' training scheme that fully capitalises on the discrete nature of the optical spikes used by the SNN to process input information. The VCSEL-based photonic SNN is tested with a highly complex, multivariate, classification task (MADELON) before performance is compared using a traditional least-squares training method and the alternative novel binary weighting scheme. Excellent classification accuracies of >94% are reached by both training methods, exceeding the benchmark performance of the dataset in a fraction of processing time. The newly reported training scheme also dramatically reduces training set size requirements as well as the number of trained nodes (<1% of the total network node count). This VCSEL-based photonic SNN, in combination with the reported 'significance' weighting scheme, therefore grants ultrafast spike-based optical processing with highly reduced training requirements and hardware complexity for potential application in future neuromorphic systems and artificial intelligence applications. △ Less","22 November, 2022",https://arxiv.org/pdf/2211.12239
A Combined Approach of Process Mining and Rule-based AI for Study Planning and Monitoring in Higher Education,Miriam Wagner;Hayyan Helal;Rene Roepke;Sven Judel;Jens Doveren;Sergej Goerzen;Pouya Soudmand;Gerhard Lakemeyer;Ulrik Schroeder;Wil van der Aalst,"This paper presents an approach of using methods of process mining and rule-based artificial intelligence to analyze and understand study paths of students based on campus management system data and study program models. Process mining techniques are used to characterize successful study paths, as well as to detect and visualize deviations from expected plans. These insights are combined with recommendations and requirements of the corresponding study programs extracted from examination regulations. Here, event calculus and answer set programming are used to provide models of the study programs which support planning and conformance checking while providing feedback on possible study plan violations. In its combination, process mining and rule-based artificial intelligence are used to support study planning and monitoring by deriving rules and recommendations for guiding students to more suitable study paths with higher success rates. Two applications will be implemented, one for students and one for study program designers. △ Less","22 November, 2022",https://arxiv.org/pdf/2211.12190
Towards Human-Interpretable Prototypes for Visual Assessment of Image Classification Models,Poulami Sinhamahapatra;Lena Heidemann;Maureen Monnet;Karsten Roscher,"Explaining black-box Artificial Intelligence (AI) models is a cornerstone for trustworthy AI and a prerequisite for its use in safety critical applications such that AI models can reliably assist humans in critical decisions. However, instead of trying to explain our models post-hoc, we need models which are interpretable-by-design built on a reasoning process similar to humans that exploits meaningful high-level concepts such as shapes, texture or object parts. Learning such concepts is often hindered by its need for explicit specification and annotation up front. Instead, prototype-based learning approaches such as ProtoPNet claim to discover visually meaningful prototypes in an unsupervised way. In this work, we propose a set of properties that those prototypes have to fulfill to enable human analysis, e.g. as part of a reliable model assessment case, and analyse such existing methods in the light of these properties. Given a 'Guess who?' game, we find that these prototypes still have a long way ahead towards definite explanations. We quantitatively validate our findings by conducting a user study indicating that many of the learnt prototypes are not considered useful towards human understanding. We discuss about the missing links in the existing methods and present a potential real-world application motivating the need to progress towards truly human-interpretable prototypes. △ Less","22 November, 2022",https://arxiv.org/pdf/2211.12173
The transport problem for non-additive measures,Vicenç Torra,"Non-additive measures, also known as fuzzy measures, capacities, and monotonic games, are increasingly used in different fields. Applications have been built within computer science and artificial intelligence related to e.g. decision making, image processing, machine learning for both classification, and regression. Tools for measure identification have been built. In short, as non-additive measures are more general than additive ones (i.e., than probabilities), they have better modeling capabilities allowing to model situations and problems that cannot be modeled by the latter. See e.g. the application of non-additive measures and the Choquet integral to model both Ellsberg paradox and Allais paradox. Because of that, there is an increasing need to analyze non-additive measures. The need for distances and similarities to compare them is no exception. Some work has been done for defining f-divergence for them. In this work we tackle the problem of defining the optimal transport problem for non-additive measures. Distances for pairs of probability distributions based on the optimal transport are extremely used in practical applications, and they are being studied extensively for their mathematical properties. We consider that it is necessary to provide appropriate definitions with a similar flavour, and that generalize the standard ones, for non-additive measures. We provide definitions based on the Möbius transform, but also based on the (\max, +)-transform that we consider that has some advantages. We will discuss in this paper the problems that arise to define the transport problem for non-additive measures, and discuss ways to solve them. In this paper we provide the definitions of the optimal transport problem, and prove some properties. △ Less","8 December, 2022",https://arxiv.org/pdf/2211.12150
Design of an Autonomous Agriculture Robot for Real Time Weed Detection using CNN,Dhruv Patel;Meet Gandhi;Shankaranarayanan H.;Anand D. Darji,"Agriculture has always remained an integral part of the world. As the human population keeps on rising, the demand for food also increases, and so is the dependency on the agriculture industry. But in today's scenario, because of low yield, less rainfall, etc., a dearth of manpower is created in this agricultural sector, and people are moving to live in the cities, and villages are becoming more and more urbanized. On the other hand, the field of robotics has seen tremendous development in the past few years. The concepts like Deep Learning (DL), Artificial Intelligence (AI), and Machine Learning (ML) are being incorporated with robotics to create autonomous systems for various sectors like automotive, agriculture, assembly line management, etc. Deploying such autonomous systems in the agricultural sector help in many aspects like reducing manpower, better yield, and nutritional quality of crops. So, in this paper, the system design of an autonomous agricultural robot which primarily focuses on weed detection is described. A modified deep-learning model for the purpose of weed detection is also proposed. The primary objective of this robot is the detection of weed on a real-time basis without any human involvement, but it can also be extended to design robots in various other applications involved in farming like weed removal, plowing, harvesting, etc., in turn making the farming industry more efficient. Source code and other details can be found at https://github.com/Dhruv2012/Autonomous-Farm-Robot △ Less","22 November, 2022",https://arxiv.org/pdf/2211.12077
GitFL: Adaptive Asynchronous Federated Learning using Version Control,Ming Hu;Zeke Xia;Zhihao Yue;Jun Xia;Yihao Huang;Yang Liu;Mingsong Chen,"As a promising distributed machine learning paradigm that enables collaborative training without compromising data privacy, Federated Learning (FL) has been increasingly used in AIoT (Artificial Intelligence of Things) design. However, due to the lack of efficient management of straggling devices, existing FL methods greatly suffer from the problems of low inference accuracy and long training time. Things become even worse when taking various uncertain factors (e.g., network delays, performance variances caused by process variation) existing in AIoT scenarios into account. To address this issue, this paper proposes a novel asynchronous FL framework named GitFL, whose implementation is inspired by the famous version control system Git. Unlike traditional FL, the cloud server of GitFL maintains a master model (i.e., the global model) together with a set of branch models indicating the trained local models committed by selected devices, where the master model is updated based on both all the pushed branch models and their version information, and only the branch models after the pull operation are dispatched to devices. By using our proposed Reinforcement Learning (RL)-based device selection mechanism, a pulled branch model with an older version will be more likely to be dispatched to a faster and less frequently selected device for the next round of local training. In this way, GitFL enables both effective control of model staleness and adaptive load balance of versioned models among straggling devices, thus avoiding the performance deterioration. Comprehensive experimental results on well-known models and datasets show that, compared with state-of-the-art asynchronous FL methods, GitFL can achieve up to 2.64X training acceleration and 7.88% inference accuracy improvements in various uncertain scenarios. △ Less","22 November, 2022",https://arxiv.org/pdf/2211.12049
A Short Survey of Systematic Generalization,Yuanpeng Li,"This survey includes systematic generalization and a history of how machine learning addresses it. We aim to summarize and organize the related information of both conventional and recent improvements. We first look at the definition of systematic generalization, then introduce Classicist and Connectionist. We then discuss different types of Connectionists and how they approach the generalization. Two crucial problems of variable binding and causality are discussed. We look into systematic generalization in language, vision, and VQA fields. Recent improvements from different aspects are discussed. Systematic generalization has a long history in artificial intelligence. We could cover only a small portion of many contributions. We hope this paper provides a background and is beneficial for discoveries in future work. △ Less","21 November, 2022",https://arxiv.org/pdf/2211.11956
Predictive Display with Perspective Projection of Surroundings in Vehicle Teleoperation to Account Time-delays,Jai Prakash;Michele Vignati;Daniele Vignarca;Edoardo Sabbioni;Federico Cheli,"Teleoperation provides human operator sophisticated perceptual and cognitive skills into an over the network control loop. It gives hope of addressing some challenges related to vehicular autonomy which is based on artificial intelligence by providing a backup plan. Variable network time delays in data transmission is the major problem in teleoperating a vehicle. On 4G network, variability of these delays is high. Due to this, both video streaming and driving commands encounter variable time delay. This paper presents an approach of providing the human operator a forecast video stream which replicates future perspective of vehicle field of view accounting the delay present in the network. Regarding the image transformation, perspective projection technique is combined with correction given by smith predictor in the control loop. This image transformation accounts current time delay and tries to address both issues, time delays as well as its variability. For experiment sake, only frontward field of view is forecast. Performance is evaluated by performing online vehicle teleoperation on street edge case maneuvers and later comparing the path deviation with and without perspective projection. △ Less","21 November, 2022",https://arxiv.org/pdf/2211.11918
A plea for an upgrade to the digital craft of the historian and digital methodology for discovering the past,Salvatore Spina,"This essay aims to bid analogue historians assume that digitisation is the first step to creating historical heritage based on the new language of Science: Computer Science. As we know, Humanities disciplines cannot easily be encapsulated in a few understandable numbers and names. However, historians must boost Artificial Intelligence (such as Transkribus) and Neural Networks to let the Machine infer meaning from the digitised historical primary source and become the most powerful tool to help historians understand what happened in the Past. Historians (collaborating with data scientists, expert annotators, librarians, archivists, and others, who are crucial to the successful management of digital data collection) have to create the primary ontology, starting from coding manuscripts into digital text, as the Biscari Archive (Italy) study case. △ Less","21 November, 2022",https://arxiv.org/pdf/2211.11861
Improving Multimodal Interactive Agents with Reinforcement Learning from Human Feedback,Josh Abramson;Arun Ahuja;Federico Carnevale;Petko Georgiev;Alex Goldin;Alden Hung;Jessica Landon;Jirka Lhotka;Timothy Lillicrap;Alistair Muldal;George Powell;Adam Santoro;Guy Scully;Sanjana Srivastava;Tamara von Glehn;Greg Wayne;Nathaniel Wong;Chen Yan;Rui Zhu,"An important goal in artificial intelligence is to create agents that can both interact naturally with humans and learn from their feedback. Here we demonstrate how to use reinforcement learning from human feedback (RLHF) to improve upon simulated, embodied agents trained to a base level of competency with imitation learning. First, we collected data of humans interacting with agents in a simulated 3D world. We then asked annotators to record moments where they believed that agents either progressed toward or regressed from their human-instructed goal. Using this annotation data we leveraged a novel method - which we call ""Inter-temporal Bradley-Terry"" (IBT) modelling - to build a reward model that captures human judgments. Agents trained to optimise rewards delivered from IBT reward models improved with respect to all of our metrics, including subsequent human judgment during live interactions with agents. Altogether our results demonstrate how one can successfully leverage human judgments to improve agent behaviour, allowing us to use reinforcement learning in complex, embodied domains without programmatic reward functions. Videos of agent behaviour may be found at https://youtu.be/v_Z9F2_eKk4. △ Less","21 November, 2022",https://arxiv.org/pdf/2211.11602
"Revisiting the Internet of Things: New Trends, Opportunities and Grand Challenges",Khalid Elgazzar;Haytham Khalil;Taghreed Alghamdi;Ahmed Badr;Ghadeer Abdelkader;Abdelrahman Elewah;Rajkumar Buyya,"The Internet of Things (IoT) has brought the dream of ubiquitous data access from physical environments into reality. IoT embeds sensors and actuators in physical objects so that they can communicate and exchange data between themselves to improve efficiency along with enabling real-time intelligent services and offering better quality of life to people. The number of deployed IoT devices has rapidly grown in the past five years in a way that makes IoT the most disruptive technology in recent history. In this paper, we reevaluate the position of IoT in our life and provide deep insights on its enabling technologies, applications, rising trends and grand challenges. The paper also highlights the role of artificial intelligence to make IoT the top transformative technology that has been ever developed in human history. △ Less","14 November, 2022",https://arxiv.org/pdf/2211.11523
"Un discours et un public ""Gilets Jaunes"" au coeur du Grand Débat National? Combinaison des approches IA et textométriques pour l'analyse de discours des plateformes ""Grand Débat National"" et ""Vrai débat""",Suignard Philippe,"In this contribution, we propose to analyze the statements coming from two ''civic tech'' platforms-the governmental platform, ''Grand D{é}bat National'' and, its political and algorithmic response proposed by a Yellow Vest collective, ''Vrai D{é}bat''-, by confronting two families of algorithms dedicated to text analysis. We propose to implement, on the one hand, proven approaches in textual data analysis (Reinert/Iramuteq Method) which have recently shown their interest in the analysis of very large corpora and, on the other hand, new methods resulting from the crossroads of the computer worlds, artificial intelligence and automatic language processing. We will examine the methodological solutions for qualifying the social properties of speakers about whom we have little direct information. Finally, we will attempt to present some research questions at the crossroads of the political sociology of public opinion and data science, which such a confrontation opens up. △ Less","14 November, 2022",https://arxiv.org/pdf/2211.11521
Revealing Hidden Context Bias in Segmentation and Object Detection through Concept-specific Explanations,Maximilian Dreyer;Reduan Achtibat;Thomas Wiegand;Wojciech Samek;Sebastian Lapuschkin,"Applying traditional post-hoc attribution methods to segmentation or object detection predictors offers only limited insights, as the obtained feature attribution maps at input level typically resemble the models' predicted segmentation mask or bounding box. In this work, we address the need for more informative explanations for these predictors by proposing the post-hoc eXplainable Artificial Intelligence method L-CRP to generate explanations that automatically identify and visualize relevant concepts learned, recognized and used by the model during inference as well as precisely locate them in input space. Our method therefore goes beyond singular input-level attribution maps and, as an approach based on the recently published Concept Relevance Propagation technique, is efficiently applicable to state-of-the-art black-box architectures in segmentation and object detection, such as DeepLabV3+ and YOLOv6, among others. We verify the faithfulness of our proposed technique by quantitatively comparing different concept attribution methods, and discuss the effect on explanation complexity on popular datasets such as CityScapes, Pascal VOC and MS COCO 2017. The ability to precisely locate and communicate concepts is used to reveal and verify the use of background features, thereby highlighting possible biases of the model. △ Less","21 November, 2022",https://arxiv.org/pdf/2211.11426
"Intelligent Computing: The Latest Advances, Challenges and Future",Shiqiang Zhu;Ting Yu;Tao Xu;Hongyang Chen;Schahram Dustdar;Sylvain Gigan;Deniz Gunduz;Ekram Hossain;Yaochu Jin;Feng Lin;Bo Liu;Zhiguo Wan;Ji Zhang;Zhifeng Zhao;Wentao Zhu;Zuoning Chen;Tariq Durrani;Huaimin Wang;Jiangxing Wu;Tongyi Zhang;Yunhe Pan,"Computing is a critical driving force in the development of human civilization. In recent years, we have witnessed the emergence of intelligent computing, a new computing paradigm that is reshaping traditional computing and promoting digital revolution in the era of big data, artificial intelligence and internet-of-things with new computing theories, architectures, methods, systems, and applications. Intelligent computing has greatly broadened the scope of computing, extending it from traditional computing on data to increasingly diverse computing paradigms such as perceptual intelligence, cognitive intelligence, autonomous intelligence, and human-computer fusion intelligence. Intelligence and computing have undergone paths of different evolution and development for a long time but have become increasingly intertwined in recent years: intelligent computing is not only intelligence-oriented but also intelligence-driven. Such cross-fertilization has prompted the emergence and rapid advancement of intelligent computing. Intelligent computing is still in its infancy and an abundance of innovations in the theories, systems, and applications of intelligent computing are expected to occur soon. We present the first comprehensive survey of literature on intelligent computing, covering its theory fundamentals, the technological fusion of intelligence and computing, important applications, challenges, and future perspectives. We believe that this survey is highly timely and will provide a comprehensive reference and cast valuable insights into intelligent computing for academic and industrial researchers and practitioners. △ Less","21 November, 2022",https://arxiv.org/pdf/2211.11281
A Novel Uncalibrated Visual Servoing Controller Baesd on Model-Free Adaptive Control Method with Neural Network,Haibin Zeng;Yueyong Lyu;Jiaming Qi;Shuangquan Zou;Tanghao Qin;Wenyu Qin,"Nowadays, with the continuous expansion of application scenarios of robotic arms, there are more and more scenarios where nonspecialist come into contact with robotic arms. However, in terms of robotic arm visual servoing, traditional Position-based Visual Servoing (PBVS) requires a lot of calibration work, which is challenging for the nonspecialist to cope with. To cope with this situation, Uncalibrated Image-Based Visual Servoing (UIBVS) frees people from tedious calibration work. This work applied a model-free adaptive control (MFAC) method which means that the parameters of controller are updated in real time, bringing better ability of suppression changes of system and environment. An artificial intelligent neural network is applied in designs of controller and estimator for hand-eye relationship. The neural network is updated with the knowledge of the system input and output information in MFAC method. Inspired by ""predictive model"" and ""receding-horizon"" in Model Predictive Control (MPC) method and introducing similar structures into our algorithm, we realizes the uncalibrated visual servoing for both stationary targets and moving trajectories. Simulated experiments with a robotic manipulator will be carried out to validate the proposed algorithm. △ Less","21 November, 2022",https://arxiv.org/pdf/2211.11209
Scalable Collaborative Learning via Representation Sharing,Frédéric Berdoz;Abhishek Singh;Martin Jaggi;Ramesh Raskar,"Privacy-preserving machine learning has become a key conundrum for multi-party artificial intelligence. Federated learning (FL) and Split Learning (SL) are two frameworks that enable collaborative learning while keeping the data private (on device). In FL, each data holder trains a model locally and releases it to a central server for aggregation. In SL, the clients must release individual cut-layer activations (smashed data) to the server and wait for its response (during both inference and back propagation). While relevant in several settings, both of these schemes have a high communication cost, rely on server-level computation algorithms and do not allow for tunable levels of collaboration. In this work, we present a novel approach for privacy-preserving machine learning, where the clients collaborate via online knowledge distillation using a contrastive loss (contrastive w.r.t. the labels). The goal is to ensure that the participants learn similar features on similar classes without sharing their input data. To do so, each client releases averaged last hidden layer activations of similar labels to a central server that only acts as a relay (i.e., is not involved in the training or aggregation of the models). Then, the clients download these last layer activations (feature representations) of the ensemble of users and distill their knowledge in their personal model using a contrastive objective. For cross-device applications (i.e., small local datasets and limited computational capacity), this approach increases the utility of the models compared to independent learning and other federated knowledge distillation (FD) schemes, is communication efficient and is scalable with the number of clients. We prove theoretically that our framework is well-posed, and we benchmark its performance against standard FD and FL on various datasets using different model architectures. △ Less","13 December, 2022",https://arxiv.org/pdf/2211.10943
Efficient Meta Reinforcement Learning for Preference-based Fast Adaptation,Zhizhou Ren;Anji Liu;Yitao Liang;Jian Peng;Jianzhu Ma,"Learning new task-specific skills from a few trials is a fundamental challenge for artificial intelligence. Meta reinforcement learning (meta-RL) tackles this problem by learning transferable policies that support few-shot adaptation to unseen tasks. Despite recent advances in meta-RL, most existing methods require the access to the environmental reward function of new tasks to infer the task objective, which is not realistic in many practical applications. To bridge this gap, we study the problem of few-shot adaptation in the context of human-in-the-loop reinforcement learning. We develop a meta-RL algorithm that enables fast policy adaptation with preference-based feedback. The agent can adapt to new tasks by querying human's preference between behavior trajectories instead of using per-step numeric rewards. By extending techniques from information theory, our approach can design query sequences to maximize the information gain from human interactions while tolerating the inherent error of non-expert human oracle. In experiments, we extensively evaluate our method, Adaptation with Noisy OracLE (ANOLE), on a variety of meta-RL benchmark tasks and demonstrate substantial improvement over baseline algorithms in terms of both feedback efficiency and error tolerance. △ Less","19 November, 2022",https://arxiv.org/pdf/2211.10861
Investigating the Potential of Artificial Intelligence Powered Interfaces to Support Different Types of Memory for People with Dementia,Hanuma Teja Maddali;Emma Dixon;Alisha Pradhan;Amanda Lazar,"There has been a growing interest in HCI to understand the specific technological needs of people with dementia and supporting them in self-managing daily activities. One of the most difficult challenges to address is supporting the fluctuating accessibility needs of people with dementia, which vary with the specific type of dementia and the progression of the condition. Researchers have identified auto-personalized interfaces, and more recently, Artificial Intelligence or AI-driven personalization as a potential solution to making commercial technology accessible in a scalable manner for users with fluctuating ability. However, there is a lack of understanding on the perceptions of people with dementia around AI as an aid to their everyday technology use and its role in their overall self-management systems, which include other non-AI technology, and human assistance. In this paper, we present future directions for the design of AI-based systems to personalize an interface for dementia-related changes in different types of memory, along with expectations for AI interactions with the user with dementia. △ Less","19 November, 2022",https://arxiv.org/pdf/2211.10756
Towards Adversarial Robustness of Deep Vision Algorithms,Hanshu Yan,"Deep learning methods have achieved great success in solving computer vision tasks, and they have been widely utilized in artificially intelligent systems for image processing, analysis, and understanding. However, deep neural networks have been shown to be vulnerable to adversarial perturbations in input data. The security issues of deep neural networks have thus come to the fore. It is imperative to study the adversarial robustness of deep vision algorithms comprehensively. This talk focuses on the adversarial robustness of image classification models and image denoisers. We will discuss the robustness of deep vision algorithms from three perspectives: 1) robustness evaluation (we propose the ObsAtk to evaluate the robustness of denoisers), 2) robustness improvement (HAT, TisODE, and CIFS are developed to robustify vision models), and 3) the connection between adversarial robustness and generalization capability to new domains (we find that adversarially robust denoisers can deal with unseen types of real-world noise). △ Less","21 November, 2022",https://arxiv.org/pdf/2211.10670
Do Pre-trained Language Models Indeed Understand Software Engineering Tasks?,Yao Li;Tao Zhang;Xiapu Luo;Haipeng Cai;Sen Fang;Dawei Yuan,"Artificial intelligence (AI) for software engineering (SE) tasks has recently achieved promising performance. In this paper, we investigate to what extent the pre-trained language model truly understands those SE tasks such as code search, code summarization, etc. We conduct a comprehensive empirical study on a board set of AI for SE (AI4SE) tasks by feeding them with variant inputs: 1) with various masking rates and 2) with sufficient input subset method. Then, the trained models are evaluated on different SE tasks, including code search, code summarization, and duplicate bug report detection. Our experimental results show that pre-trained language models are insensitive to the given input, thus they achieve similar performance in these three SE tasks. We refer to this phenomenon as overinterpretation, where a model confidently makes a decision without salient features, or where a model finds some irrelevant relationships between the final decision and the dataset. Our study investigates two approaches to mitigate the overinterpretation phenomenon: whole word mask strategy and ensembling. To the best of our knowledge, we are the first to reveal this overinterpretation phenomenon to the AI4SE community, which is an important reminder for researchers to design the input for the models and calls for necessary future work in understanding and implementing AI4SE tasks. △ Less","19 November, 2022",https://arxiv.org/pdf/2211.10623
Explainable Artificial Intelligence and Causal Inference based ATM Fraud Detection,Yelleti Vivek;Vadlamani Ravi;Abhay Anand Mane;Laveti Ramesh Naidu,"Gaining the trust of customers and providing them empathy are very critical in the financial domain. Frequent occurrence of fraudulent activities affects these two factors. Hence, financial organizations and banks must take utmost care to mitigate them. Among them, ATM fraudulent transaction is a common problem faced by banks. There following are the critical challenges involved in fraud datasets: the dataset is highly imbalanced, the fraud pattern is changing, etc. Owing to the rarity of fraudulent activities, Fraud detection can be formulated as either a binary classification problem or One class classification (OCC). In this study, we handled these techniques on an ATM transactions dataset collected from India. In binary classification, we investigated the effectiveness of various over-sampling techniques, such as the Synthetic Minority Oversampling Technique (SMOTE) and its variants, Generative Adversarial Networks (GAN), to achieve oversampling. Further, we employed various machine learning techniques viz., Naive Bayes (NB), Logistic Regression (LR), Support Vector Machine (SVM), Decision Tree (DT), Random Forest (RF), Gradient Boosting Tree (GBT), Multi-layer perceptron (MLP). GBT outperformed the rest of the models by achieving 0.963 AUC, and DT stands second with 0.958 AUC. DT is the winner if the complexity and interpretability aspects are considered. Among all the oversampling approaches, SMOTE and its variants were observed to perform better. In OCC, IForest attained 0.959 CR, and OCSVM secured second place with 0.947 CR. Further, we incorporated explainable artificial intelligence (XAI) and causal inference (CI) in the fraud detection framework and studied it through various analyses. △ Less","19 November, 2022",https://arxiv.org/pdf/2211.10595
A Transformer Framework for Data Fusion and Multi-Task Learning in Smart Cities,Alexander C. DeRieux;Walid Saad;Wangda Zuo;Rachmawan Budiarto;Mochamad Donny Koerniawan;Dwi Novitasari,"Rapid global urbanization is a double-edged sword, heralding promises of economical prosperity and public health while also posing unique environmental and humanitarian challenges. Smart and connected communities (S&CCs) apply data-centric solutions to these problems by integrating artificial intelligence (AI) and the Internet of Things (IoT). This coupling of intelligent technologies also poses interesting system design challenges regarding heterogeneous data fusion and task diversity. Transformers are of particular interest to address these problems, given their success across diverse fields of natural language processing (NLP), computer vision, time-series regression, and multi-modal data fusion. This begs the question whether Transformers can be further diversified to leverage fusions of IoT data sources for heterogeneous multi-task learning in S&CC trade spaces. In this paper, a Transformer-based AI system for emerging smart cities is proposed. Designed using a pure encoder backbone, and further customized through interchangeable input embedding and output task heads, the system supports virtually any input data and output task types present S&CCs. This generalizability is demonstrated through learning diverse task sets representative of S&CC environments, including multivariate time-series regression, visual plant disease classification, and image-time-series fusion tasks using a combination of Beijing PM2.5 and Plant Village datasets. Simulation results show that the proposed Transformer-based system can handle various input data types via custom sequence embedding techniques, and are naturally suited to learning a diverse set of tasks. The results also show that multi-task learners increase both memory and computational efficiency while maintaining comparable performance to both single-task variants, and non-Transformer baselines. △ Less","18 November, 2022",https://arxiv.org/pdf/2211.10506
Dynamic Interactional And Cooperative Network For Shield Machine,Dazhi Gao;Rongyang Li;Hongbo Wang;Lingfeng Mao;Huansheng Ning,"The shield machine (SM) is a complex mechanical device used for tunneling. However, the monitoring and deciding were mainly done by artificial experience during traditional construction, which brought some limitations, such as hidden mechanical failures, human operator error, and sensor anomalies. To deal with these challenges, many scholars have studied SM intelligent methods. Most of these methods only take SM into account but do not consider the SM operating environment. So, this paper discussed the relationship among SM, geological information, and control terminals. Then, according to the relationship, models were established for the control terminal, including SM rate prediction and SM anomaly detection. The experimental results show that compared with baseline models, the proposed models in this paper perform better. In the proposed model, the R2 and MSE of rate prediction can reach 92.2\%, and 0.0064 respectively. The abnormal detection rate of anomaly detection is up to 98.2\%. △ Less","17 November, 2022",https://arxiv.org/pdf/2211.10473
Computational Short Cuts in Infinite Domain Constraint Satisfaction,Peter Jonsson;Victor Lagerkvist;Sebastian Ordyniak,"A backdoor in a finite-domain CSP instance is a set of variables where each possible instantiation moves the instance into a polynomial-time solvable class. Backdoors have found many applications in artificial intelligence and elsewhere, and the algorithmic problem of finding such backdoors has consequently been intensively studied. Sioutis and Janhunen (Proc. 42nd German Conference on AI (KI-2019)) have proposed a generalised backdoor concept suitable for infinite-domain CSP instances over binary constraints. We generalise their concept into a large class of CSPs that allow for higher-arity constraints. We show that this kind of infinite-domain backdoors have many of the positive computational properties that finite-domain backdoors have: the associated computational problems are fixed-parameter tractable whenever the underlying constraint language is finite. On the other hand, we show that infinite languages make the problems considerably harder: the general backdoor detection problem is W[2]-hard and fixed-parameter tractability is ruled out under standard complexity-theoretic assumptions. We demonstrate that backdoors may have suboptimal behaviour on binary constraints -- this is detrimental from an AI perspective where binary constraints are predominant in, for instance, spatiotemporal applications. In response to this, we introduce sidedoors as an alternative to backdoors. The fundamental computational problems for sidedoors remain fixed-parameter tractable for finite constraint language (possibly also containing non-binary relations). Moreover, the sidedoor approach has appealing computational properties that sometimes leads to faster algorithms than the backdoor approach. △ Less","18 November, 2022",https://arxiv.org/pdf/2211.10144
Intrusion Detection in Internet of Things using Convolutional Neural Networks,Martin Kodys;Zhi Lu;Kar Wai Fok;Vrizlynn L. L. Thing,"Internet of Things (IoT) has become a popular paradigm to fulfil needs of the industry such as asset tracking, resource monitoring and automation. As security mechanisms are often neglected during the deployment of IoT devices, they are more easily attacked by complicated and large volume intrusion attacks using advanced techniques. Artificial Intelligence (AI) has been used by the cyber security community in the past decade to automatically identify such attacks. However, deep learning methods have yet to be extensively explored for Intrusion Detection Systems (IDS) specifically for IoT. Most recent works are based on time sequential models like LSTM and there is short of research in CNNs as they are not naturally suited for this problem. In this article, we propose a novel solution to the intrusion attacks against IoT devices using CNNs. The data is encoded as the convolutional operations to capture the patterns from the sensors data along time that are useful for attacks detection by CNNs. The proposed method is integrated with two classical CNNs: ResNet and EfficientNet, where the detection performance is evaluated. The experimental results show significant improvement in both true positive rate and false positive rate compared to the baseline using LSTM. △ Less","18 November, 2022",https://arxiv.org/pdf/2211.10062
Data-Adaptive Discriminative Feature Localization with Statistically Guaranteed Interpretation,Ben Dai;Xiaotong Shen;Lin Yee Chen;Chunlin Li;Wei Pan,"In explainable artificial intelligence, discriminative feature localization is critical to reveal a blackbox model's decision-making process from raw data to prediction. In this article, we use two real datasets, the MNIST handwritten digits and MIT-BIH Electrocardiogram (ECG) signals, to motivate key characteristics of discriminative features, namely adaptiveness, predictive importance and effectiveness. Then, we develop a localization framework based on adversarial attacks to effectively localize discriminative features. In contrast to existing heuristic methods, we also provide a statistically guaranteed interpretability of the localized features by measuring a generalized partial R^2. We apply the proposed method to the MNIST dataset and the MIT-BIH dataset with a convolutional auto-encoder. In the first, the compact image regions localized by the proposed method are visually appealing. Similarly, in the second, the identified ECG features are biologically plausible and consistent with cardiac electrophysiological principles while locating subtle anomalies in a QRS complex that may not be discernible by the naked eye. Overall, the proposed method compares favorably with state-of-the-art competitors. Accompanying this paper is a Python library dnn-locate (https://dnn-locate.readthedocs.io/en/latest/) that implements the proposed approach. △ Less","18 November, 2022",https://arxiv.org/pdf/2211.10061
Recent Advances in Algebraic Geometry and Bayesian Statistics,Sumio Watanabe,"This article is a review of theoretical advances in the research field of algebraic geometry and Bayesian statistics in the last two decades. Many statistical models and learning machines which contain hierarchical structures or latent variables are called nonidentifiable, because the map from a parameter to a statistical model is not one-to-one. In nonidentifiable models, both the likelihood function and the posterior distribution have singularities in general, hence it was difficult to analyze their statistical properties. However, from the end of the 20th century, new theory and methodology based on algebraic geometry have been established which enables us to investigate such models and machines in the real world. In this article, the following results in recent advances are reported. First, we explain the framework of Bayesian statistics and introduce a new perspective from the birational geometry. Second, two mathematical solutions are derived based on algebraic geometry. An appropriate parameter space can be found by a resolution map, which makes the posterior distribution be normal crossing and the log likelihood ratio function be well-defined. Third, three applications to statistics are introduced. The posterior distribution is represented by the renormalized form, the asymptotic free energy is derived, and the universal formula among the generalization loss, the cross validation, and the information criterion is established. Two mathematical solutions and three applications to statistics based on algebraic geometry reported in this article are now being used in many practical fields in data science and artificial intelligence. △ Less","18 November, 2022",https://arxiv.org/pdf/2211.10049
Potential Auto-driving Threat: Universal Rain-removal Attack,Jinchegn Hu;Jihao Li;Zhuoran Hou;Jingjing Jiang;Cunjia Liu;Yuanjian Zhang,"The problem of robustness in adverse weather conditions is considered a significant challenge for computer vision algorithms in the applicants of autonomous driving. Image rain removal algorithms are a general solution to this problem. They find a deep connection between raindrops/rain-streaks and images by mining the hidden features and restoring information about the rain-free environment based on the powerful representation capabilities of neural networks. However, previous research has focused on architecture innovations and has yet to consider the vulnerability issues that already exist in neural networks. This research gap hints at a potential security threat geared toward the intelligent perception of autonomous driving in the rain. In this paper, we propose a universal rain-removal attack (URA) on the vulnerability of image rain-removal algorithms by generating a non-additive spatial perturbation that significantly reduces the similarity and image quality of scene restoration. Notably, this perturbation is difficult to recognise by humans and is also the same for different target images. Thus, URA could be considered a critical tool for the vulnerability detection of image rain-removal algorithms. It also could be developed as a real-world artificial intelligence attack method. Experimental results show that URA can reduce the scene repair capability by 39.5% and the image generation quality by 26.4%, targeting the state-of-the-art (SOTA) single-image rain-removal algorithms currently available. △ Less","17 November, 2022",https://arxiv.org/pdf/2211.09959
A Review of Deep Learning Techniques for Protein Function Prediction,Divyanshu Aggarwal;Yasha Hasija,"Deep Learning and big data have shown tremendous success in bioinformatics and computational biology in recent years; artificial intelligence methods have also significantly contributed in the task of protein function classification. This review paper analyzes the recent developments in approaches for the task of predicting protein function using deep learning. We explain the importance of determining the protein function and why automating the following task is crucial. Then, after reviewing the widely used deep learning techniques for this task, we continue our review and highlight the emergence of the modern State of The Art (SOTA) deep learning models which have achieved groundbreaking results in the field of computer vision, natural language processing and multi-modal learning in the last few years. We hope that this review will provide a broad view of the current role and advances of deep learning in biological sciences, especially in predicting protein function tasks and encourage new researchers to contribute to this area. △ Less","27 October, 2022",https://arxiv.org/pdf/2211.09705
An Audit Framework for Technical Assessment of Binary Classifiers,Debarati Bhaumik;Diptish Dey,"Multilevel models using logistic regression (MLogRM) and random forest models (RFM) are increasingly deployed in industry for the purpose of binary classification. The European Commission's proposed Artificial Intelligence Act (AIA) necessitates, under certain conditions, that application of such models is fair, transparent, and ethical, which consequently implies technical assessment of these models. This paper proposes and demonstrates an audit framework for technical assessment of RFMs and MLogRMs by focussing on model-, discrimination-, and transparency & explainability-related aspects. To measure these aspects 20 KPIs are proposed, which are paired to a traffic light risk assessment method. An open-source dataset is used to train a RFM and a MLogRM model and these KPIs are computed and compared with the traffic lights. The performance of popular explainability methods such as kernel- and tree-SHAP are assessed. The framework is expected to assist regulatory bodies in performing conformity assessments of binary classifiers and also benefits providers and users deploying such AI-systems to comply with the AIA. △ Less","17 November, 2022",https://arxiv.org/pdf/2211.09500
"Explainable, Domain-Adaptive, and Federated Artificial Intelligence in Medicine",Ahmad Chaddad;Qizong lu;Jiali Li;Yousef Katib;Reem Kateb;Camel Tanougast;Ahmed Bouridane;Ahmed Abdulkadir,"Artificial intelligence (AI) continues to transform data analysis in many domains. Progress in each domain is driven by a growing body of annotated data, increased computational resources, and technological innovations. In medicine, the sensitivity of the data, the complexity of the tasks, the potentially high stakes, and a requirement of accountability give rise to a particular set of challenges. In this review, we focus on three key methodological approaches that address some of the particular challenges in AI-driven medical decision making. (1) Explainable AI aims to produce a human-interpretable justification for each output. Such models increase confidence if the results appear plausible and match the clinicians expectations. However, the absence of a plausible explanation does not imply an inaccurate model. Especially in highly non-linear, complex models that are tuned to maximize accuracy, such interpretable representations only reflect a small portion of the justification. (2) Domain adaptation and transfer learning enable AI models to be trained and applied across multiple domains. For example, a classification task based on images acquired on different acquisition hardware. (3) Federated learning enables learning large-scale models without exposing sensitive personal health information. Unlike centralized AI learning, where the centralized learning machine has access to the entire training data, the federated learning process iteratively updates models across multiple sites by exchanging only parameter updates, not personal health data. This narrative review covers the basic concepts, highlights relevant corner-stone and state-of-the-art research in the field, and discusses perspectives. △ Less","16 November, 2022",https://arxiv.org/pdf/2211.09317
Multi-Timescale Modeling of Human Behavior,Chinmai Basavaraj;Adarsh Pyarelal;Evan Carter,"In recent years, the role of artificially intelligent (AI) agents has evolved from being basic tools to socially intelligent agents working alongside humans towards common goals. In such scenarios, the ability to predict future behavior by observing past actions of their human teammates is highly desirable in an AI agent. Goal-oriented human behavior is complex, hierarchical, and unfolds across multiple timescales. Despite this observation, relatively little attention has been paid towards using multi-timescale features to model such behavior. In this paper, we propose an LSTM network architecture that processes behavioral information at multiple timescales to predict future behavior. We demonstrate that our approach for modeling behavior in multiple timescales substantially improves prediction of future behavior compared to methods that do not model behavior at multiple timescales. We evaluate our architecture on data collected in an urban search and rescue scenario simulated in a virtual Minecraft-based testbed, and compare its performance to that of a number of valid baselines as well as other methods that do not process inputs at multiple timescales. △ Less","16 November, 2022",https://arxiv.org/pdf/2211.09001
"A Comprehensive Survey on Spectrum Sharing Techniques for 5G/B5G Intelligent Wireless Networks: Opportunities, Challenges and Future Research Directions",Anita Patil;Sridhar Iyer;Onel L. A. Lopez;Rahul J Pandya;Krishna Pai;Anshuman Kalla;Rakhee Kallimani,"The increasing popularity of Internet of Everything and small-cell devices has enormously accelerated traffic loads. Consequently, increased bandwidth and high data rate requirements stimulate the operation at the millimeter wave and the Tera-Hertz spectrum bands in the fifth generation (5G) and beyond 5G (B5G) wireless networks. Furthermore, efficient spectrum allocation, maximizing the spectrum utilization, achieving efficient spectrum sharing (SS), and managing the spectrum to enhance the system performance remain challenging. To this end, recent studies have implemented artificial intelligence and machine learning techniques, enabling intelligent and efficient spectrum leveraging. However, despite many recent research advances focused on maximizing utilization of the spectrum bands, achieving efficient sharing, allocation, and management of the enormous available spectrum remains challenging. Therefore, the current article acquaints a comprehensive survey on intelligent SS methodologies for 5G and B5G wireless networks, considering the applications of artificial intelligence for efficient SS. Specifically, a thorough overview of SS methodologies is conferred, following which the various spectrum utilization opportunities arising from the existing SS methodologies in intelligent wireless networks are discussed. Subsequently, to highlight critical limitations of the existing methodologies, recent literature on existing SS methodologies is reviewed in detail, classifying them based on the implemented technology, i.e., cognitive radio, machine learning, blockchain, and multiple other techniques. Moreover, the related SS techniques are reviewed to highlight significant challenges in the B5G intelligent wireless network. Finally, to provide an insight into the prospective research avenues, the article is concluded by presenting several potential research directions and proposed solutions. △ Less","17 November, 2022",https://arxiv.org/pdf/2211.08956
Speeding Up Recommender Systems Using Association Rules,Eyad Kannout;Hung Son Nguyen;Marek Grzegorowski,"Recommender systems are considered one of the most rapidly growing branches of Artificial Intelligence. The demand for finding more efficient techniques to generate recommendations becomes urgent. However, many recommendations become useless if there is a delay in generating and showing them to the user. Therefore, we focus on improving the speed of recommendation systems without impacting the accuracy. In this paper, we suggest a novel recommender system based on Factorization Machines and Association Rules (FMAR). We introduce an approach to generate association rules using two algorithms: (i) apriori and (ii) frequent pattern (FP) growth. These association rules will be utilized to reduce the number of items passed to the factorization machines recommendation model. We show that FMAR has significantly decreased the number of new items that the recommender system has to predict and hence, decreased the required time for generating the recommendations. On the other hand, while building the FMAR tool, we concentrate on making a balance between prediction time and accuracy of generated recommendations to ensure that the accuracy is not significantly impacted compared to the accuracy of using factorization machines without association rules. △ Less","16 November, 2022",https://arxiv.org/pdf/2211.08799
Power-law Scaling to Assist with Key Challenges in Artificial Intelligence,Yuval Meir;Shira Sardi;Shiri Hodassman;Karin Kisos;Itamar Ben-Noam;Amir Goldental;Ido Kanter,"Power-law scaling, a central concept in critical phenomena, is found to be useful in deep learning, where optimized test errors on handwritten digit examples converge as a power-law to zero with database size. For rapid decision making with one training epoch, each example is presented only once to the trained network, the power-law exponent increased with the number of hidden layers. For the largest dataset, the obtained test error was estimated to be in the proximity of state-of-the-art algorithms for large epoch numbers. Power-law scaling assists with key challenges found in current artificial intelligence applications and facilitates an a priori dataset size estimation to achieve a desired test accuracy. It establishes a benchmark for measuring training complexity and a quantitative hierarchy of machine learning tasks and algorithms. △ Less","15 November, 2022",https://arxiv.org/pdf/2211.08430
Participation Interfaces for Human-Centered AI,Sean McGregor,"Emerging artificial intelligence (AI) applications often balance the preferences and impacts among diverse and contentious stakeholder groups. Accommodating these stakeholder groups during system design, development, and deployment requires tools for the elicitation of disparate system interests and collaboration interfaces supporting negotiation balancing those interests. This paper introduces interactive visual ""participation interfaces"" for Markov Decision Processes (MDPs) and collaborative ranking problems as examples restoring a human-centered locus of control. △ Less","15 November, 2022",https://arxiv.org/pdf/2211.08419
Collaborative and AI-aided Exam Question Generation using Wikidata in Education,Philipp Scharpf;Moritz Schubotz;Andreas Spitz;Andre Greiner-Petter;Bela Gipp,"Since the COVID-19 outbreak, the use of digital learning or education platforms has significantly increased. Teachers now digitally distribute homework and provide exercise questions. In both cases, teachers need to continuously develop novel and individual questions. This process can be very time-consuming and should be facilitated and accelerated both through exchange with other teachers and by using Artificial Intelligence (AI) capabilities. To address this need, we propose a multilingual Wikimedia framework that allows for collaborative worldwide teacher knowledge engineering and subsequent AI-aided question generation, test, and correction. As a proof of concept, we present >>PhysWikiQuiz<<, a physics question generation and test engine. Our system (hosted by Wikimedia at https://physwikiquiz.wmflabs.org) retrieves physics knowledge from the open community-curated database Wikidata. It can generate questions in different variations and verify answer values and units using a Computer Algebra System (CAS). We evaluate the performance on a public benchmark dataset at each stage of the system workflow. For an average formula with three variables, the system can generate and correct up to 300 questions for individual students based on a single formula concept name as input by the teacher. △ Less","15 November, 2022",https://arxiv.org/pdf/2211.08361
Deep-Learning-Empowered Inverse Design for Freeform Reconfigurable Metasurfaces,Changhao Liu;Fan Yang;Maokun Li;Shenheng Xu,"The past decade has witnessed the advances of artificial intelligence with various applications in engineering. Recently, artificial neural network empowered inverse design for metasurfaces has been developed that can design on-demand meta-atoms with diverse shapes and high performance, where the design process based on artificial intelligence is fast and automatic. However, once the inverse-designed static meta-atom is fabricated, the function of the metasurface is fixed. Reconfigurable metasurfaces can realize dynamic functions, while applying artificial intelligence to design practical reconfigurable meta-atoms inversely has not been reported yet. Here, we present a deep-learning-empowered inverse design method for freeform reconfigurable metasurfaces, which can generate on-demand reconfigurable coding meta-atoms at self-defined frequency bands. To reduce the scale of dataset, a decoupling method of the reconfigurable meta-atom based on microwave network theory is proposed at first, which can convert the inverse design process for reconfigurable coding meta-atoms to the inverse design for static structures. A convolutional neural network model is trained to predict the responses of free-shaped meta-atoms, and the genetic algorithm is applied to generate the optimal structure patterns rapidly. As a demonstration of concept, several inverse-designed examples are generated with different self-defined spectrum responses in microwave band, and an inverse-designed wideband reconfigurable metasurface prototype is fabricated and measured for beam scanning applications with broad bandwidth. Our work paves the way for the fast and automatic design process of high-performance reconfigurable metasurfaces. △ Less","17 November, 2022",https://arxiv.org/pdf/2211.08296
Low-Thrust Orbital Transfer using Dynamics-Agnostic Reinforcement Learning,Carlos M. Casas;Belen Carro;Antonio Sanchez-Esguevillas,"Low-thrust trajectory design and in-flight control remain two of the most challenging topics for new-generation satellite operations. Most of the solutions currently implemented are based on reference trajectories and lead to sub-optimal fuel usage. Other solutions are based on simple guidance laws that need to be updated periodically, increasing the cost of operations. Whereas some optimization strategies leverage Artificial Intelligence methods, all of the approaches studied so far need either previously generated data or a strong a priori knowledge of the satellite dynamics. This study uses model-free Reinforcement Learning to train an agent on a constrained pericenter raising scenario for a low-thrust medium-Earth-orbit satellite. The agent does not have any prior knowledge of the environment dynamics, which makes it unbiased from classical trajectory optimization patterns. The trained agent is then used to design a trajectory and to autonomously control the satellite during the cruise. Simulations show that a dynamics-agnostic agent is able to learn a quasi-optimal guidance law and responds well to uncertainties in the environment dynamics. The results obtained open the door to the usage of Reinforcement Learning on more complex scenarios, multi-satellite problems, or to explore trajectories in environments where a reference solution is not known △ Less","6 October, 2022",https://arxiv.org/pdf/2211.08272
A Comparative Study of Machine Learning and Deep Learning Techniques for Prediction of Co2 Emission in Cars,Samveg Shah;Shubham Thakar;Kashish Jain;Bhavya Shah;Sudhir Dhage,"The most recent concern of all people on Earth is the increase in the concentration of greenhouse gas in the atmosphere. The concentration of these gases has risen rapidly over the last century and if the trend continues it can cause many adverse climatic changes. There have been ways implemented to curb this by the government by limiting processes that emit a higher amount of CO2, one such greenhouse gas. However, there is mounting evidence that the CO2 numbers supplied by the government do not accurately reflect the performance of automobiles on the road. Our proposal of using artificial intelligence techniques to improve a previously rudimentary process takes a radical tack, but it fits the bill given the situation. To determine which algorithms and models produce the greatest outcomes, we compared them all and explored a novel method of ensembling them. Further, this can be used to foretell the rise in global temperature and to ground crucial policy decisions like the adoption of electric vehicles. To estimate emissions from vehicles, we used machine learning, deep learning, and ensemble learning on a massive dataset. △ Less","15 November, 2022",https://arxiv.org/pdf/2211.08268
Reconocimiento de Objetos a partir de Nube de Puntos en un Veículo Aéreo no Tripulado,Agustina Marion de Freitas Vidal;Anthony Rodriguez;Richard Suarez;André Kelbouscas;Ricardo Grando,"Currently, research in robotics, artificial intelligence and drones are advancing exponentially, they are directly or indirectly related to various areas of the economy, from agriculture to industry. With this context, this project covers these topics guiding them, seeking to provide a framework that is capable of helping to develop new future researchers. For this, we use an aerial vehicle that works autonomously and is capable of mapping the scenario and providing useful information to the end user. This occurs from a communication between a simple programming language (Scratch) and one of the most important and efficient robot operating systems today (ROS). This is how we managed to develop a tool capable of generating a 3D map and detecting objects using the camera attached to the drone. Although this tool can be used in the advanced fields of industry, it is also an important advance for the research sector. The implementation of this tool in intermediate-level institutions is aspired to provide the ability to carry out high-level projects from a simple programming language. △ Less","23 October, 2022",https://arxiv.org/pdf/2211.08190
Design of Unmanned Air Vehicles Using Transformer Surrogate Models,Adam D. Cobb;Anirban Roy;Daniel Elenius;Susmit Jha,"Computer-aided design (CAD) is a promising new area for the application of artificial intelligence (AI) and machine learning (ML). The current practice of design of cyber-physical systems uses the digital twin methodology, wherein the actual physical design is preceded by building detailed models that can be evaluated by physics simulation models. These physics models are often slow and the manual design process often relies on exploring near-by variations of existing designs. AI holds the promise of breaking these design silos and increasing the diversity and performance of designs by accelerating the exploration of the design space. In this paper, we focus on the design of electrical unmanned aerial vehicles (UAVs). The high-density batteries and purely electrical propulsion systems have disrupted the space of UAV design, making this domain an ideal target for AI-based design. In this paper, we develop an AI Designer that synthesizes novel UAV designs. Our approach uses a deep transformer model with a novel domain-specific encoding such that we can evaluate the performance of new proposed designs without running expensive flight dynamics models and CAD tools. We demonstrate that our approach significantly reduces the overall compute requirements for the design process and accelerates the design space exploration. Finally, we identify future research directions to achieve full-scale deployment of AI-assisted CAD for UAVs. △ Less","11 November, 2022",https://arxiv.org/pdf/2211.08138
General Intelligence Requires Rethinking Exploration,Minqi Jiang;Tim Rocktäschel;Edward Grefenstette,"We are at the cusp of a transition from ""learning from data"" to ""learning what data to learn from"" as a central focus of artificial intelligence (AI) research. While the first-order learning problem is not completely solved, large models under unified architectures, such as transformers, have shifted the learning bottleneck from how to effectively train our models to how to effectively acquire and use task-relevant data. This problem, which we frame as exploration, is a universal aspect of learning in open-ended domains, such as the real world. Although the study of exploration in AI is largely limited to the field of reinforcement learning, we argue that exploration is essential to all learning systems, including supervised learning. We propose the problem of generalized exploration to conceptually unify exploration-driven learning between supervised learning and reinforcement learning, allowing us to highlight key similarities across learning settings and open research challenges. Importantly, generalized exploration serves as a necessary objective for maintaining open-ended learning processes, which in continually learning to discover and solve new problems, provides a promising path to more general intelligence. △ Less","14 November, 2022",https://arxiv.org/pdf/2211.07819
Robust Deep Learning for Autonomous Driving,Charles Corbière,"The last decade's research in artificial intelligence had a significant impact on the advance of autonomous driving. Yet, safety remains a major concern when it comes to deploying such systems in high-risk environments. The objective of this thesis is to develop methodological tools which provide reliable uncertainty estimates for deep neural networks. First, we introduce a new criterion to reliably estimate model confidence: the true class probability (TCP). We show that TCP offers better properties for failure prediction than current uncertainty measures. Since the true class is by essence unknown at test time, we propose to learn TCP criterion from data with an auxiliary model, introducing a specific learning scheme adapted to this context. The relevance of the proposed approach is validated on image classification and semantic segmentation datasets. Then, we extend our learned confidence approach to the task of domain adaptation where it improves the selection of pseudo-labels in self-training methods. Finally, we tackle the challenge of jointly detecting misclassification and out-of-distributions samples by introducing a new uncertainty measure based on evidential models and defined on the simplex. △ Less","14 November, 2022",https://arxiv.org/pdf/2211.07772
Evaluating Distribution System Reliability with Hyperstructures Graph Convolutional Nets,Yuzhou Chen;Tian Jiang;Miguel Heleno;Alexandre Moreira;Yulia R. Gel,"Nowadays, it is broadly recognized in the power system community that to meet the ever expanding energy sector's needs, it is no longer possible to rely solely on physics-based models and that reliable, timely and sustainable operation of energy systems is impossible without systematic integration of artificial intelligence (AI) tools. Nevertheless, the adoption of AI in power systems is still limited, while integration of AI particularly into distribution grid investment planning is still an uncharted territory. We make the first step forward to bridge this gap by showing how graph convolutional networks coupled with the hyperstructures representation learning framework can be employed for accurate, reliable, and computationally efficient distribution grid planning with resilience objectives. We further propose a Hyperstructures Graph Convolutional Neural Networks (Hyper-GCNNs) to capture hidden higher order representations of distribution networks with attention mechanism. Our numerical experiments show that the proposed Hyper-GCNNs approach yields substantial gains in computational efficiency compared to the prevailing methodology in distribution grid planning and also noticeably outperforms seven state-of-the-art models from deep learning (DL) community. △ Less","13 November, 2022",https://arxiv.org/pdf/2211.07645
Learning to Answer Multilingual and Code-Mixed Questions,Deepak Gupta,"Question-answering (QA) that comes naturally to humans is a critical component in seamless human-computer interaction. It has emerged as one of the most convenient and natural methods to interact with the web and is especially desirable in voice-controlled environments. Despite being one of the oldest research areas, the current QA system faces the critical challenge of handling multilingual queries. To build an Artificial Intelligent (AI) agent that can serve multilingual end users, a QA system is required to be language versatile and tailored to suit the multilingual environment. Recent advances in QA models have enabled surpassing human performance primarily due to the availability of a sizable amount of high-quality datasets. However, the majority of such annotated datasets are expensive to create and are only confined to the English language, making it challenging to acknowledge progress in foreign languages. Therefore, to measure a similar improvement in the multilingual QA system, it is necessary to invest in high-quality multilingual evaluation benchmarks. In this dissertation, we focus on advancing QA techniques for handling end-user queries in multilingual environments. This dissertation consists of two parts. In the first part, we explore multilingualism and a new dimension of multilingualism referred to as code-mixing. Second, we propose a technique to solve the task of multi-hop question generation by exploiting multiple documents. Experiments show our models achieve state-of-the-art performance on answer extraction, ranking, and generation tasks on multiple domains of MQA, VQA, and language generation. The proposed techniques are generic and can be widely used in various domains and languages to advance QA systems. △ Less","14 November, 2022",https://arxiv.org/pdf/2211.07522
Counterfactual Analysis by Algorithmic Complexity: A metric between possible worlds,Nicholas Kluge Corrêa;Nythamar Fernandes De Oliveira,"Counterfactuals have become an important area of interdisciplinary interest, especially in logic, philosophy of language, epistemology, metaphysics, psychology, decision theory, and even artificial intelligence. In this study, we propose a new form of analysis for counterfactuals: analysis by algorithmic complexity. Inspired by Lewis-Stalnaker's Nicholas Corrêa 2 Manuscrito-Rev. Int. Fil. Campinas, 2022. Possible Worlds Semantics, the proposed method allows for a new interpretation of the debate between David Lewis and Robert Stalnaker regarding the Limit and Singularity assumptions. Besides other results, we offer a new way to answer the problems raised by Goodman and Quine regarding vagueness, context-dependence, and the non-monotonicity of counterfactuals. Engaging in a dialogue with literature, this study will seek to bring new insights and tools to this debate. We hope our method of analysis can make counterfactuals more understandable in an intuitively plausible way, and a philosophically justifiable manner, aligned with the way we usually think about counterfactual propositions and our imaginative reasoning. △ Less","14 November, 2022",https://arxiv.org/pdf/2211.07315
"AI-Based Emotion Recognition: Promise, Peril, and Prescriptions for Prosocial Path",Siddique Latif;Hafiz Shehbaz Ali;Muhammad Usama;Rajib Rana;Björn Schuller;Junaid Qadir,"Automated emotion recognition (AER) technology can detect humans' emotional states in real-time using facial expressions, voice attributes, text, body movements, and neurological signals and has a broad range of applications across many sectors. It helps businesses get a much deeper understanding of their customers, enables monitoring of individuals' moods in healthcare, education, or the automotive industry, and enables identification of violence and threat in forensics, to name a few. However, AER technology also risks using artificial intelligence (AI) to interpret sensitive human emotions. It can be used for economic and political power and against individual rights. Human emotions are highly personal, and users have justifiable concerns about privacy invasion, emotional manipulation, and bias. In this paper, we present the promises and perils of AER applications. We discuss the ethical challenges related to the data and AER systems and highlight the prescriptions for prosocial perspectives for future AER applications. We hope this work will help AI researchers and developers design prosocial AER applications. △ Less","14 November, 2022",https://arxiv.org/pdf/2211.07290
A taxonomic system for failure cause analysis of open source AI incidents,Nikiforos Pittaras;Sean McGregor,"While certain industrial sectors (e.g., aviation) have a long history of mandatory incident reporting complete with analytical findings, the practice of artificial intelligence (AI) safety benefits from no such mandate and thus analyses must be performed on publicly known ``open source'' AI incidents. Although the exact causes of AI incidents are seldom known by outsiders, this work demonstrates how to apply expert knowledge on the population of incidents in the AI Incident Database (AIID) to infer the potential and likely technical causative factors that contribute to reported failures and harms. We present early work on a taxonomic system that covers a cascade of interrelated incident factors, from system goals (nearly always known) to methods / technologies (knowable in many cases) and technical failure causes (subject to expert analysis) of the implicated systems. We pair this ontology structure with a comprehensive classification workflow that leverages expert knowledge and community feedback, resulting in taxonomic annotations grounded by incident data and human expertise. △ Less","14 November, 2022",https://arxiv.org/pdf/2211.07280
Secure Robotics: A Definition and a Brief Review from a Cybersecurity Control and Implementation Methodology Perspective,Adam Haskard;Damith Herath;Zena Assaad,Secure robotics is a multi-disciplinary endeavour for improving the cybersecurity posture of robotic and embodied Artificial Intelligence systems. The article surveys emerging concepts and ideas encapsulating the notion of secure robotics and identifies five Secure Robotics Cybersecurity Control Implementation Layers as a crucial starting point for consideration by practitioners. It also recognises the need for further studies on the relationship between Human-robot trust and the implementation of established and novel cybersecurity controls. △ Less,"14 November, 2022",https://arxiv.org/pdf/2211.07149
Towards a Dynamic Composability Approach for using Heterogeneous Systems in Remote Sensing,Ilkay Altintas;Ismael Perez;Dmitry Mishin;Adrien Trouillaud;Christopher Irving;John Graham;Mahidhar Tatineni;Thomas DeFanti;Shawn Strande;Larry Smarr;Michael L. Norman,"Influenced by the advances in data and computing, the scientific practice increasingly involves machine learning and artificial intelligence driven methods which requires specialized capabilities at the system-, science- and service-level in addition to the conventional large-capacity supercomputing approaches. The latest distributed architectures built around the composability of data-centric applications led to the emergence of a new ecosystem for container coordination and integration. However, there is still a divide between the application development pipelines of existing supercomputing environments, and these new dynamic environments that disaggregate fluid resource pools through accessible, portable and re-programmable interfaces. New approaches for dynamic composability of heterogeneous systems are needed to further advance the data-driven scientific practice for the purpose of more efficient computing and usable tools for specific scientific domains. In this paper, we present a novel approach for using composable systems in the intersection between scientific computing, artificial intelligence (AI), and remote sensing domain. We describe the architecture of a first working example of a composable infrastructure that federates Expanse, an NSF-funded supercomputer, with Nautilus, a Kubernetes-based GPU geo-distributed cluster. We also summarize a case study in wildfire modeling, that demonstrates the application of this new infrastructure in scientific workflows: a composed system that bridges the insights from edge sensing, AI and computing capabilities with a physics-driven simulation. △ Less","13 November, 2022",https://arxiv.org/pdf/2211.06918
Using Features at Multiple Temporal and Spatial Resolutions to Predict Human Behavior in Real Time,Liang Zhang;Justin Lieffers;Adarsh Pyarelal,"When performing complex tasks, humans naturally reason at multiple temporal and spatial resolutions simultaneously. We contend that for an artificially intelligent agent to effectively model human teammates, i.e., demonstrate computational theory of mind (ToM), it should do the same. In this paper, we present an approach for integrating high and low-resolution spatial and temporal information to predict human behavior in real time and evaluate it on data collected from human subjects performing simulated urban search and rescue (USAR) missions in a Minecraft-based environment. Our model composes neural networks for high and low-resolution feature extraction with a neural network for behavior prediction, with all three networks trained simultaneously. The high-resolution extractor encodes dynamically changing goals robustly by taking as input the Manhattan distance difference between the humans' Minecraft avatars and candidate goals in the environment for the latest few actions, computed from a high-resolution gridworld representation. In contrast, the low-resolution extractor encodes participants' historical behavior using a historical state matrix computed from a low-resolution graph representation. Through supervised learning, our model acquires a robust prior for human behavior prediction, and can effectively deal with long-term observations. Our experimental results demonstrate that our method significantly improves prediction accuracy compared to approaches that only use high-resolution information. △ Less","12 November, 2022",https://arxiv.org/pdf/2211.06721
"Explainable Artificial Intelligence in Construction: The Content, Context, Process, Outcome Evaluation Framework",Peter ED Love;Jane Matthews;Weili Fang;Stuart Porter;Hanbin Luo;Lieyun Ding,"Explainable artificial intelligence is an emerging and evolving concept. Its impact on construction, though yet to be realised, will be profound in the foreseeable future. Still, XAI has received limited attention in construction. As a result, no evaluation frameworks have been propagated to enable construction organisations to understand the what, why, how, and when of XAI. Our paper aims to fill this void by developing a content, context, process, and outcome evaluation framework that can be used to justify the adoption and effective management of XAI. After introducing and describing this novel framework, we discuss its implications for future research. While our novel framework is conceptual, it provides a frame of reference for construction organisations to make headway toward realising XAI business value and benefits. △ Less","11 November, 2022",https://arxiv.org/pdf/2211.06561
Improving the Efficiency of the PC Algorithm by Using Model-Based Conditional Independence Tests,Erica Cai;Andrew McGregor;David Jensen,"Learning causal structure is useful in many areas of artificial intelligence, including planning, robotics, and explanation. Constraint-based structure learning algorithms such as PC use conditional independence (CI) tests to infer causal structure. Traditionally, constraint-based algorithms perform CI tests with a preference for smaller-sized conditioning sets, partially because the statistical power of conventional CI tests declines rapidly as the size of the conditioning set increases. However, many modern conditional independence tests are model-based, and these tests use well-regularized models that maintain statistical power even with very large conditioning sets. This suggests an intriguing new strategy for constraint-based algorithms which may result in a reduction of the total number of CI tests performed: Test variable pairs with large conditioning sets first, as a pre-processing step that finds some conditional independencies quickly, before moving on to the more conventional strategy that favors small conditioning sets. We propose such a pre-processing step for the PC algorithm which relies on performing CI tests on a few randomly selected large conditioning sets. We perform an empirical analysis on directed acyclic graphs (DAGs) that correspond to real-world systems and both empirical and theoretical analyses for Erdős-Renyi DAGs. Our results show that Pre-Processing Plus PC (P3PC) performs far fewer CI tests than the original PC algorithm, between 0.5% to 36%, and often less than 10%, of the CI tests that the PC algorithm alone performs. The efficiency gains are particularly significant for the DAGs corresponding to real-world systems. △ Less","11 November, 2022",https://arxiv.org/pdf/2211.06536
Deep Learning Generates Synthetic Cancer Histology for Explainability and Education,James M. Dolezal;Rachelle Wolk;Hanna M. Hieromnimon;Frederick M. Howard;Andrew Srisuwananukorn;Dmitry Karpeyev;Siddhi Ramesh;Sara Kochanny;Jung Woo Kwon;Meghana Agni;Richard C. Simon;Chandni Desai;Raghad Kherallah;Tung D. Nguyen;Jefree J. Schulte;Kimberly Cole;Galina Khramtsova;Marina Chiara Garassino;Aliya N. Husain;Huihua Li;Robert Grossman;Nicole A. Cipriani;Alexander T. Pearson,"Artificial intelligence methods including deep neural networks (DNN) can provide rapid molecular classification of tumors from routine histology with accuracy that matches or exceeds human pathologists. Discerning how neural networks make their predictions remains a significant challenge, but explainability tools help provide insights into what models have learned when corresponding histologic features are poorly defined. Here, we present a method for improving explainability of DNN models using synthetic histology generated by a conditional generative adversarial network (cGAN). We show that cGANs generate high-quality synthetic histology images that can be leveraged for explaining DNN models trained to classify molecularly-subtyped tumors, exposing histologic features associated with molecular state. Fine-tuning synthetic histology through class and layer blending illustrates nuanced morphologic differences between tumor subtypes. Finally, we demonstrate the use of synthetic histology for augmenting pathologist-in-training education, showing that these intuitive visualizations can reinforce and improve understanding of histologic manifestations of tumor biology. △ Less","9 December, 2022",https://arxiv.org/pdf/2211.06522
Rethinking Data-driven Networking with Foundation Models: Challenges and Opportunities,Franck Le;Mudhakar Srivatsa;Raghu Ganti;Vyas Sekar,"Foundational models have caused a paradigm shift in the way artificial intelligence (AI) systems are built. They have had a major impact in natural language processing (NLP), and several other domains, not only reducing the amount of required labeled data or even eliminating the need for it, but also significantly improving performance on a wide range of tasks. We argue foundation models can have a similar profound impact on network traffic analysis, and management. More specifically, we show that network data shares several of the properties that are behind the success of foundational models in linguistics. For example, network data contains rich semantic content, and several of the networking tasks (e.g., traffic classification, generation of protocol implementations from specification text, anomaly detection) can find similar counterparts in NLP (e.g., sentiment analysis, translation from natural language to code, out-of-distribution). However, network settings also present unique characteristics and challenges that must be overcome. Our contribution is in highlighting the opportunities and challenges at the intersection of foundation models and networking. △ Less","11 November, 2022",https://arxiv.org/pdf/2211.06494
A Non-Volatile All-Spin Non-Binary Matrix Multiplier: An Efficient Hardware Accelerator for Machine Learning,Rahnuma Rahman;Supriyo Bandyopadhyay,"We propose and analyze a compact and non-volatile nanomagnetic (all-spin) non-binary matrix multiplier performing the multiply-and-accumulate (MAC) operation using two magnetic tunnel junctions - one activated by strain to act as the multiplier, and the other activated by spin-orbit torque pulses to act as a domain wall synapse that performs the operation of the accumulator. It has two advantages over the usual crossbar-based electronic non-binary matrix multiplier. First, while the crossbar architecture requires N3 devices to multiply two matrices, we require only 2N2 devices. Second, our matrix multiplier is non-volatile and retains the information about the product matrix after being powered off. Here, we present an example where each MAC operation can be performed in ~5 ns and the maximum energy dissipated per operation is ~60Nmax aJ, where Nmax is the largest matrix size. This provides a very useful hardware accelerator for machine learning and artificial intelligence tasks which involve the multiplication of large matrices. The non-volatility allows the matrix multiplier to be embedded in powerful non-von-Neumann architectures, including processor-in-memory. It also allows much of the computing to be done at the edge (of internet-of-things) while reducing the need to access the cloud, thereby making artificial intelligence more resilient against cyberattacks. △ Less","20 November, 2022",https://arxiv.org/pdf/2211.06490
"Calculating Cognitive Augmentation, A Case Study",Ron Fulbright,"We are entering an era in which humans will increasingly work in partnership and collaboration with artificially intelligent entities. For millennia, tools have augmented human physical and mental performance but in the coming era of cognitive systems, human cognitive performance will be augmented. We are only just now beginning to define the fundamental concepts and metrics to describe, characterize, and measure augmented and collaborative cognition. In this paper, the results of a cognitive augmentation experiment are discussed and we calculate the increase in cognitive accuracy and cognitive precision. In the case study, cognitively augmented problem solvers show an increase of 74% in cognitive accuracy (the ability to synthesize desired answers) and a 27% increase in cognitive precision (the ability to synthesize only desired answers). We offer a formal treatment of the case study results and propose cognitive accuracy and cognitive precision as standard metrics to describe and measure human cognitive augmentation. △ Less","11 November, 2022",https://arxiv.org/pdf/2211.06479
On Measuring Cognition and Cognitive Augmentation,Ron Fulbright,"We are at the beginning of a new age in which artificial entities will perform significant amounts of high-level cognitive processing rivaling and even surpassing human thinking. The future belongs to those who can best collaborate with artificial cognitive entities achieving a high degree of cognitive augmenta-tion. However, we currently lack theoretically grounded fundamental metrics able to describe human or artificial cognition much less augmented and combined cognition. How do we measure thinking, cognition, information, and knowledge in an implementation-independent way? How can we tell how much thinking an artificial entity does and how much is done by a human? How can we measure the combined and possible even emergent effect of humans working together with intelligent artificial entities? These are some of the challenges for research-ers in this field. We first define a cognitive process as the transformation of data, information, knowledge, and wisdom. We then review several existing and emerging information metrics based on entropy, processing effort, quantum physics, emergent capacity, and human concept learning. We then discuss how these fail to answer the above questions and provide guidelines for future re-search. △ Less","11 November, 2022",https://arxiv.org/pdf/2211.06477
AI Ethics in Smart Healthcare,Sudeep Pasricha,"This article reviews the landscape of ethical challenges of integrating artificial intelligence (AI) into smart healthcare products, including medical electronic devices. Differences between traditional ethics in the medical domain and emerging ethical challenges with AI-driven healthcare are presented, particularly as they relate to transparency, bias, privacy, safety, responsibility, justice, and autonomy. Open challenges and recommendations are outlined to enable the integration of ethical principles into the design, validation, clinical trials, deployment, monitoring, repair, and retirement of AI-based smart healthcare products. △ Less","2 November, 2022",https://arxiv.org/pdf/2211.06346
Health Guardian Platform: A technology stack to accelerate discovery in Digital Health research,Bo Wen;Vince S. Siu;Italo Buleje;Kuan Yu Hsieh;Takashi Itoh;Lukas Zimmerli;Nigel Hinds;Elif Eyigoz;Bing Dang;Stefan von Cavallar;Jeffrey L. Rogers,"This paper highlights the design philosophy and architecture of the Health Guardian, a platform developed by the IBM Digital Health team to accelerate discoveries of new digital biomarkers and development of digital health technologies. The Health Guardian allows for rapid translation of artificial intelligence (AI) research into cloud-based microservices that can be tested with data from clinical cohorts to understand disease and enable early prevention. The platform can be connected to mobile applications, wearables, or Internet of things (IoT) devices to collect health-related data into a secure database. When the analytics are created, the researchers can containerize and deploy their code on the cloud using pre-defined templates, and validate the models using the data collected from one or more sensing devices. The Health Guardian platform currently supports time-series, text, audio, and video inputs with 70+ analytic capabilities and is used for non-commercial scientific research. We provide an example of the Alzheimer's disease (AD) assessment microservice which uses AI methods to extract linguistic features from audio recordings to evaluate an individual's mini-mental state, the likelihood of having AD, and to predict the onset of AD before turning the age of 85. Today, IBM research teams across the globe use the Health Guardian internally as a test bed for early-stage research ideas, and externally with collaborators to support and enhance AI model development and clinical study efforts. △ Less","10 November, 2022",https://arxiv.org/pdf/2211.06330
Artificial Intelligence and Life in 2030: The One Hundred Year Study on Artificial Intelligence,Peter Stone;Rodney Brooks;Erik Brynjolfsson;Ryan Calo;Oren Etzioni;Greg Hager;Julia Hirschberg;Shivaram Kalyanakrishnan;Ece Kamar;Sarit Kraus;Kevin Leyton-Brown;David Parkes;William Press;AnnaLee Saxenian;Julie Shah;Milind Tambe;Astro Teller,"In September 2016, Stanford's ""One Hundred Year Study on Artificial Intelligence"" project (AI100) issued the first report of its planned long-term periodic assessment of artificial intelligence (AI) and its impact on society. It was written by a panel of 17 study authors, each of whom is deeply rooted in AI research, chaired by Peter Stone of the University of Texas at Austin. The report, entitled ""Artificial Intelligence and Life in 2030,"" examines eight domains of typical urban settings on which AI is likely to have impact over the coming years: transportation, home and service robots, healthcare, education, public safety and security, low-resource communities, employment and workplace, and entertainment. It aims to provide the general public with a scientifically and technologically accurate portrayal of the current state of AI and its potential and to help guide decisions in industry and governments, as well as to inform research and development in the field. The charge for this report was given to the panel by the AI100 Standing Committee, chaired by Barbara Grosz of Harvard University. △ Less","31 October, 2022",https://arxiv.org/pdf/2211.06318
Normative Challenges of Risk Regulation of Artificial Intelligence and Automated Decision-Making,Carsten Orwat;Jascha Bareis;Anja Folberth;Jutta Jahnel;Christian Wadephul,"Recent proposals aiming at regulating artificial intelligence (AI) and automated decision-making (ADM) suggest a particular form of risk regulation, i.e. a risk-based approach. The most salient example is the Artificial Intelligence Act (AIA) proposed by the European Commission. The article addresses challenges for adequate risk regulation that arise primarily from the specific type of risks involved, i.e. risks to the protection of fundamental rights and fundamental societal values. They result mainly from the normative ambiguity of the fundamental rights and societal values in interpreting, specifying or operationalising them for risk assessments. This is exemplified for (1) human dignity, (2) informational self-determination, data protection and privacy, (3) justice and fairness, and (4) the common good. Normative ambiguities require normative choices, which are distributed among different actors in the proposed AIA. Particularly critical normative choices are those of selecting normative conceptions for specifying risks, aggregating and quantifying risks including the use of metrics, balancing of value conflicts, setting levels of acceptable risks, and standardisation. To avoid a lack of democratic legitimacy and legal uncertainty, scientific and political debates are suggested. △ Less","11 November, 2022",https://arxiv.org/pdf/2211.06203
REVEL Framework to measure Local Linear Explanations for black-box models: Deep Learning Image Classification case of study,Iván Sevillano-García;Julián Luengo-Martín;Francisco Herrera,"Explainable artificial intelligence is proposed to provide explanations for reasoning performed by an Artificial Intelligence. There is no consensus on how to evaluate the quality of these explanations, since even the definition of explanation itself is not clear in the literature. In particular, for the widely known Local Linear Explanations, there are qualitative proposals for the evaluation of explanations, although they suffer from theoretical inconsistencies. The case of image is even more problematic, where a visual explanation seems to explain a decision while detecting edges is what it really does. There are a large number of metrics in the literature specialized in quantitatively measuring different qualitative aspects so we should be able to develop metrics capable of measuring in a robust and correct way the desirable aspects of the explanations. In this paper, we propose a procedure called REVEL to evaluate different aspects concerning the quality of explanations with a theoretically coherent development. This procedure has several advances in the state of the art: it standardizes the concepts of explanation and develops a series of metrics not only to be able to compare between them but also to obtain absolute information regarding the explanation itself. The experiments have been carried out on image four datasets as benchmark where we show REVEL's descriptive and analytical power. △ Less","11 November, 2022",https://arxiv.org/pdf/2211.06154
"Identifying, measuring, and mitigating individual unfairness for supervised learning models and application to credit risk models",Rasoul Shahsavarifar;Jithu Chandran;Mario Inchiosa;Amit Deshpande;Mario Schlener;Vishal Gossain;Yara Elias;Vinaya Murali,"In the past few years, Artificial Intelligence (AI) has garnered attention from various industries including financial services (FS). AI has made a positive impact in financial services by enhancing productivity and improving risk management. While AI can offer efficient solutions, it has the potential to bring unintended consequences. One such consequence is the pronounced effect of AI-related unfairness and attendant fairness-related harms. These fairness-related harms could involve differential treatment of individuals; for example, unfairly denying a loan to certain individuals or groups of individuals. In this paper, we focus on identifying and mitigating individual unfairness and leveraging some of the recently published techniques in this domain, especially as applicable to the credit adjudication use case. We also investigate the extent to which techniques for achieving individual fairness are effective at achieving group fairness. Our main contribution in this work is functionalizing a two-step training process which involves learning a fair similarity metric from a group sense using a small portion of the raw data and training an individually ""fair"" classifier using the rest of the data where the sensitive features are excluded. The key characteristic of this two-step technique is related to its flexibility, i.e., the fair metric obtained in the first step can be used with any other individual fairness algorithms in the second step. Furthermore, we developed a second metric (distinct from the fair similarity metric) to determine how fairly a model is treating similar individuals. We use this metric to compare a ""fair"" model against its baseline model in terms of their individual fairness value. Finally, some experimental results corresponding to the individual unfairness mitigation techniques are presented. △ Less","11 November, 2022",https://arxiv.org/pdf/2211.06106
GeoAI for Knowledge Graph Construction: Identifying Causality Between Cascading Events to Support Environmental Resilience Research,Yuanyuan Tian;Wenwen Li,"Knowledge graph technology is considered a powerful and semantically enabled solution to link entities, allowing users to derive new knowledge by reasoning data according to various types of reasoning rules. However, in building such a knowledge graph, events modeling, such as that of disasters, is often limited to single, isolated events. The linkages among cascading events are often missing in existing knowledge graphs. This paper introduces our GeoAI (Geospatial Artificial Intelligence) solutions to identify causality among events, in particular, disaster events, based on a set of spatially and temporally-enabled semantic rules. Through a use case of causal disaster events modeling, we demonstrated how these defined rules, including theme-based identification of correlated events, spatiotemporal co-occurrence constraint, and text mining of event metadata, enable the automatic extraction of causal relationships between different events. Our solution enriches the event knowledge base and allows for the exploration of linked cascading events in large knowledge graphs, therefore empowering knowledge query and discovery. △ Less","11 November, 2022",https://arxiv.org/pdf/2211.06011
Turning disruptive power of Blockchain in the insurance market into innovative opportunities,Wadnerson Boileau,"Insurance has been around for more than centuries. This risk mitigation strategy has been utilized in maritime commerce as early thousand years ago, where Asian merchant seafarers were pooling together their wares in collective funds to pay for damages of individual capsized ship. In 2018, insurance industry made up 6 percent of global GDP while financial industry amounted to about 7 to 9 percent of the US GDP.2020, the industry net premiums totaled USD1.28 trillion, by 2030, blockchain insurance market value is estimated to reach USD39.5 Billion. Despite of growing reform, the insurance market is dominated by intermediaries assisting people to match their insurance needs. While many predictions focused on artificial intelligence, cloud computing, blockchain stands out as the most disruptive technology that can change the driving forces underlying the global economy. This paper presents a blockchain business use case and how insurance industry can turn blockchain disruptive power into innovative opportunities. △ Less","4 December, 2022",https://arxiv.org/pdf/2211.05830
Is Decentralized AI Safer?,Casey Clifton;Richard Blythman;Kartika Tulusan,"Artificial Intelligence (AI) has the potential to significantly benefit or harm humanity. At present, a few for-profit companies largely control the development and use of this technology, and therefore determine its outcomes. In an effort to diversify and democratize work on AI, various groups are building open AI systems, investigating their risks, and discussing their ethics. In this paper, we demonstrate how blockchain technology can facilitate and formalize these efforts. Concretely, we analyze multiple use-cases for blockchain in AI research and development, including decentralized governance, the creation of immutable audit trails, and access to more diverse and representative datasets. We argue that decentralizing AI can help mitigate AI risks and ethical concerns, while also introducing new issues that should be considered in future work. △ Less","3 November, 2022",https://arxiv.org/pdf/2211.05828
Warmup and Transfer Knowledge-Based Federated Learning Approach for IoT Continuous Authentication,Mohamad Wazzeh;Hakima Ould-Slimane;Chamseddine Talhi;Azzam Mourad;Mohsen Guizani,"Continuous behavioural authentication methods add a unique layer of security by allowing individuals to verify their unique identity when accessing a device. Maintaining session authenticity is now feasible by monitoring users' behaviour while interacting with a mobile or Internet of Things (IoT) device, making credential theft and session hijacking ineffective. Such a technique is made possible by integrating the power of artificial intelligence and Machine Learning (ML). Most of the literature focuses on training machine learning for the user by transmitting their data to an external server, subject to private user data exposure to threats. In this paper, we propose a novel Federated Learning (FL) approach that protects the anonymity of user data and maintains the security of his data. We present a warmup approach that provides a significant accuracy increase. In addition, we leverage the transfer learning technique based on feature extraction to boost the models' performance. Our extensive experiments based on four datasets: MNIST, FEMNIST, CIFAR-10 and UMDAA-02-FD, show a significant increase in user authentication accuracy while maintaining user privacy and data security. △ Less","10 November, 2022",https://arxiv.org/pdf/2211.05662
Robust Security Energy Efficiency Optimization for RIS-Aided Cell-Free Networks with Multiple Eavesdroppers,Wanming Hao;Junjie Li;Gangcan Sun;Chongwen Huang;Ming Zeng;Octavia A. Dobre;Chau Yuen,"In this paper, we investigate the energy efficiency (EE) problem under reconfigurable intelligent surface (RIS)-aided secure cell-free networks, where multiple legitimate users and eavesdroppers (Eves) exist. We formulate a max-min secure EE optimization problem by jointly designing the distributed active beamforming and artificial noise at base stations as well as the passive beamforming at RISs under practical constraints. To deal with it, we first divide the original optimization problem into two sub-ones, and then propose an iterative optimization algorithm to solve each sub-problem based on the fractional programming, constrained convex-convex procedure (CCCP) and semi-definite programming (SDP) techniques. After that, these two sub-problems are alternatively solved until convergence, and the final solutions are obtained. Next, we extend to the imperfect channel state information of the Eves' links, and investigate the robust SEE beamforming optimization problem by bringing the outage probability constraints. Based on this, we first transform the uncertain outage probability constraints into the certain ones by the bernstein-type inequality and sphere boundary techniques, and then propose an alternatively iterative algorithm to obtain the solutions of the original problem based on the S-procedure, successive convex approximation, CCCP and SDP techniques. Finally, the simulation results are conducted to show the effectiveness of the proposed schemes. △ Less","10 November, 2022",https://arxiv.org/pdf/2211.05562
Zero-shot Visual Commonsense Immorality Prediction,Yujin Jeong;Seongbeom Park;Suhong Moon;Jinkyu Kim,"Artificial intelligence is currently powering diverse real-world applications. These applications have shown promising performance, but raise complicated ethical issues, i.e. how to embed ethics to make AI applications behave morally. One way toward moral AI systems is by imitating human prosocial behavior and encouraging some form of good behavior in systems. However, learning such normative ethics (especially from images) is challenging mainly due to a lack of data and labeling complexity. Here, we propose a model that predicts visual commonsense immorality in a zero-shot manner. We train our model with an ETHICS dataset (a pair of text and morality annotation) via a CLIP-based image-text joint embedding. In a testing phase, the immorality of an unseen image is predicted. We evaluate our model with existing moral/immoral image datasets and show fair prediction performance consistent with human intuitions. Further, we create a visual commonsense immorality benchmark with more general and extensive immoral visual contents. Codes and dataset are available at https://github.com/ku-vai/Zero-shot-Visual-Commonsense-Immorality-Prediction. Note that this paper might contain images and descriptions that are offensive in nature. △ Less","10 November, 2022",https://arxiv.org/pdf/2211.05521
What is Wrong with Language Models that Can Not Tell a Story?,Ivan P. Yamshchikov;Alexey Tikhonov,"This paper argues that a deeper understanding of narrative and the successful generation of longer subjectively interesting texts is a vital bottleneck that hinders the progress in modern Natural Language Processing (NLP) and may even be in the whole field of Artificial Intelligence. We demonstrate that there are no adequate datasets, evaluation methods, and even operational concepts that could be used to start working on narrative processing. △ Less","10 November, 2022",https://arxiv.org/pdf/2211.05044
Artificial intelligence for improved fitting of trajectories of elementary particles in inhomogeneous dense materials immersed in a magnetic field,Saúl Alonso-Monsalve;Davide Sgalaberna;Xingyu Zhao;Clark McGrew;André Rubbia,"In this article, we use artificial intelligence algorithms to show how to enhance the resolution of the elementary particle track fitting in inhomogeneous dense detectors, such as plastic scintillators. We use deep learning to replace more traditional Bayesian filtering methods, drastically improving the reconstruction of the interacting particle kinematics. We show that a specific form of neural network, inherited from the field of natural language processing, is very close to the concept of a Bayesian filter that adopts a hyper-informative prior. Such a paradigm change can influence the design of future particle physics experiments and their data exploitation. △ Less","9 November, 2022",https://arxiv.org/pdf/2211.04890
Distribution-based Emotion Recognition in Conversation,Wen Wu;Chao Zhang;Philip C. Woodland,"Automatic emotion recognition in conversation (ERC) is crucial for emotion-aware conversational artificial intelligence. This paper proposes a distribution-based framework that formulates ERC as a sequence-to-sequence problem for emotion distribution estimation. The inherent ambiguity of emotions and the subjectivity of human perception lead to disagreements in emotion labels, which is handled naturally in our framework from the perspective of uncertainty estimation in emotion distributions. A Bayesian training loss is introduced to improve the uncertainty estimation by conditioning each emotional state on an utterance-specific Dirichlet prior distribution. Experimental results on the IEMOCAP dataset show that ERC outperformed the single-utterance-based system, and the proposed distribution-based ERC methods have not only better classification accuracy, but also show improved uncertainty estimation. △ Less","9 November, 2022",https://arxiv.org/pdf/2211.04834
Energy System Digitization in the Era of AI: A Three-Layered Approach towards Carbon Neutrality,Le Xie;Tong Huang;Xiangtian Zheng;Yan Liu;Mengdi Wang;Vijay Vittal;P. R. Kumar;Srinivas Shakkottai;Yi Cui,"The transition towards carbon-neutral electricity is one of the biggest game changers in addressing climate change since it addresses the dual challenges of removing carbon emissions from the two largest sectors of emitters: electricity and transportation. The transition to a carbon-neutral electric grid poses significant challenges to conventional paradigms of modern grid planning and operation. Much of the challenge arises from the scale of the decision making and the uncertainty associated with the energy supply and demand. Artificial Intelligence (AI) could potentially have a transformative impact on accelerating the speed and scale of carbon-neutral transition, as many decision making processes in the power grid can be cast as classic, though challenging, machine learning tasks. We point out that to amplify AI's impact on carbon-neutral transition of the electric energy systems, the AI algorithms originally developed for other applications should be tailored in three layers of technology, markets, and policy. △ Less","2 November, 2022",https://arxiv.org/pdf/2211.04584
A study on the ephemeral nature of knowledge shared within multiagent systems,Sanjay Sarma Oruganti Venkata;Ramviyas Parasuraman;Ramana Pidaparti,"Achieving knowledge sharing within an artificial swarm system could lead to significant development in autonomous multiagent and robotic systems research and realize collective intelligence. However, this is difficult to achieve since there is no generic framework to transfer skills between agents other than a query-response-based approach. Moreover, natural living systems have a ""forgetfulness"" property for everything they learn. Analyzing such ephemeral nature (temporal memory properties of new knowledge gained) in artificial systems has never been studied in the literature. We propose a behavior tree-based framework to realize a query-response mechanism for transferring skills encoded as the condition-action control sub-flow of that portion of the knowledge between agents to fill this gap. We simulate a multiagent group with different initial knowledge on a foraging mission. While performing basic operations, each robot queries other robots to respond to an unknown condition. The responding robot shares the control actions by sharing a portion of the behavior tree that addresses the queries. Specifically, we investigate the ephemeral nature of the new knowledge gained through such a framework, where the knowledge gained by the agent is either limited due to memory or is forgotten over time. Our investigations show that knowledge grows proportionally with the duration of remembrance, which is trivial. However, we found minimal impact on knowledge growth due to memory. We compare these cases against a baseline that involved full knowledge pre-coded on all agents. We found that knowledge-sharing strived to match the baseline condition by sharing and achieving knowledge growth as a collective system. △ Less","8 November, 2022",https://arxiv.org/pdf/2211.04433
Heterogeneous Recurrent Spiking Neural Network for Spatio-Temporal Classification,Biswadeep Chakraborty;Saibal Mukhopadhyay,"Spiking Neural Networks are often touted as brain-inspired learning models for the third wave of Artificial Intelligence. Although recent SNNs trained with supervised backpropagation show classification accuracy comparable to deep networks, the performance of unsupervised learning-based SNNs remains much lower. This paper presents a heterogeneous recurrent spiking neural network (HRSNN) with unsupervised learning for spatio-temporal classification of video activity recognition tasks on RGB (KTH, UCF11, UCF101) and event-based datasets (DVS128 Gesture). The key novelty of the HRSNN is that the recurrent layer in HRSNN consists of heterogeneous neurons with varying firing/relaxation dynamics, and they are trained via heterogeneous spike-time-dependent-plasticity (STDP) with varying learning dynamics for each synapse. We show that this novel combination of heterogeneity in architecture and learning method outperforms current homogeneous spiking neural networks. We further show that HRSNN can achieve similar performance to state-of-the-art backpropagation trained supervised SNN, but with less computation (fewer neurons and sparse connection) and less training data. △ Less","22 September, 2022",https://arxiv.org/pdf/2211.04297
Bridging Fairness and Environmental Sustainability in Natural Language Processing,Marius Hessenthaler;Emma Strubell;Dirk Hovy;Anne Lauscher,"Fairness and environmental impact are important research directions for the sustainable development of artificial intelligence. However, while each topic is an active research area in natural language processing (NLP), there is a surprising lack of research on the interplay between the two fields. This lacuna is highly problematic, since there is increasing evidence that an exclusive focus on fairness can actually hinder environmental sustainability, and vice versa. In this work, we shed light on this crucial intersection in NLP by (1) investigating the efficiency of current fairness approaches through surveying example methods for reducing unfair stereotypical bias from the literature, and (2) evaluating a common technique to reduce energy consumption (and thus environmental impact) of English NLP models, knowledge distillation (KD), for its impact on fairness. In this case study, we evaluate the effect of important KD factors, including layer and dimensionality reduction, with respect to: (a) performance on the distillation task (natural language inference and semantic similarity prediction), and (b) multiple measures and dimensions of stereotypical bias (e.g., gender bias measured via the Word Embedding Association Test). Our results lead us to clarify current assumptions regarding the effect of KD on unfair bias: contrary to other findings, we show that KD can actually decrease model fairness. △ Less","8 November, 2022",https://arxiv.org/pdf/2211.04256
SOTIF Entropy: Online SOTIF Risk Quantification and Mitigation for Autonomous Driving,Liang Peng;Boqi Li;Wenhao Yu;Kai Yang;Wenbo Shao;Hong Wang,"Autonomous driving confronts great challenges in complex traffic scenarios, where the risk of Safety of the Intended Functionality (SOTIF) can be triggered by the dynamic operational environment and system insufficiencies. The SOTIF risk is reflected not only intuitively in the collision risk with objects outside the autonomous vehicles (AVs), but also inherently in the performance limitation risk of the implemented algorithms themselves. How to minimize the SOTIF risk for autonomous driving is currently a critical, difficult, and unresolved issue. Therefore, this paper proposes the ""Self-Surveillance and Self-Adaption System"" as a systematic approach to online minimize the SOTIF risk, which aims to provide a systematic solution for monitoring, quantification, and mitigation of inherent and external risks. The core of this system is the risk monitoring of the implemented artificial intelligence algorithms within the AV. As a demonstration of the Self-Surveillance and Self-Adaption System, the risk monitoring of the perception algorithm, i.e., YOLOv5 is highlighted. Moreover, the inherent perception algorithm risk and external collision risk are jointly quantified via SOTIF entropy, which is then propagated downstream to the decision-making module and mitigated. Finally, several challenging scenarios are demonstrated, and the Hardware-in-the-Loop experiments are conducted to verify the efficiency and effectiveness of the system. The results demonstrate that the Self-Surveillance and Self-Adaption System enables dependable online monitoring, quantification, and mitigation of SOTIF risk in real-time critical traffic environments. △ Less","8 November, 2022",https://arxiv.org/pdf/2211.04009
Progress and summary of reinforcement learning on energy management of MPS-EV,Jincheng Hu;Yang Lin;Liang Chu;Zhuoran Hou;Jihan Li;Jingjing Jiang;Yuanjian Zhang,"The high emission and low energy efficiency caused by internal combustion engines (ICE) have become unacceptable under environmental regulations and the energy crisis. As a promising alternative solution, multi-power source electric vehicles (MPS-EVs) introduce different clean energy systems to improve powertrain efficiency. The energy management strategy (EMS) is a critical technology for MPS-EVs to maximize efficiency, fuel economy, and range. Reinforcement learning (RL) has become an effective methodology for the development of EMS. RL has received continuous attention and research, but there is still a lack of systematic analysis of the design elements of RL-based EMS. To this end, this paper presents an in-depth analysis of the current research on RL-based EMS (RL-EMS) and summarizes the design elements of RL-based EMS. This paper first summarizes the previous applications of RL in EMS from five aspects: algorithm, perception scheme, decision scheme, reward function, and innovative training method. The contribution of advanced algorithms to the training effect is shown, the perception and control schemes in the literature are analyzed in detail, different reward function settings are classified, and innovative training methods with their roles are elaborated. Finally, by comparing the development routes of RL and RL-EMS, this paper identifies the gap between advanced RL solutions and existing RL-EMS. Finally, this paper suggests potential development directions for implementing advanced artificial intelligence (AI) solutions in EMS. △ Less","7 November, 2022",https://arxiv.org/pdf/2211.04001
Issues and Challenges in Applications of Artificial Intelligence to Nuclear Medicine -- The Bethesda Report (AI Summit 2022),Arman Rahmim;Tyler J. Bradshaw;Irène Buvat;Joyita Dutta;Abhinav K. Jha;Paul E. Kinahan;Quanzheng Li;Chi Liu;Melissa D. McCradden;Babak Saboury;Eliot Siegel;John J. Sunderland;Richard L. Wahl,"The SNMMI Artificial Intelligence (SNMMI-AI) Summit, organized by the SNMMI AI Task Force, took place in Bethesda, MD on March 21-22, 2022. It brought together various community members and stakeholders from academia, healthcare, industry, patient representatives, and government (NIH, FDA), and considered various key themes to envision and facilitate a bright future for routine, trustworthy use of AI in nuclear medicine. In what follows, essential issues, challenges, controversies and findings emphasized in the meeting are summarized. △ Less","7 November, 2022",https://arxiv.org/pdf/2211.03783
Changes from Classical Statistics to Modern Statistics and Data Science,Kai Zhang;Shan Liu;Momiao Xiong,"A coordinate system is a foundation for every quantitative science, engineering, and medicine. Classical physics and statistics are based on the Cartesian coordinate system. The classical probability and hypothesis testing theory can only be applied to Euclidean data. However, modern data in the real world are from natural language processing, mathematical formulas, social networks, transportation and sensor networks, computer visions, automations, and biomedical measurements. The Euclidean assumption is not appropriate for non Euclidean data. This perspective addresses the urgent need to overcome those fundamental limitations and encourages extensions of classical probability theory and hypothesis testing , diffusion models and stochastic differential equations from Euclidean space to non Euclidean space. Artificial intelligence such as natural language processing, computer vision, graphical neural networks, manifold regression and inference theory, manifold learning, graph neural networks, compositional diffusion models for automatically compositional generations of concepts and demystifying machine learning systems, has been rapidly developed. Differential manifold theory is the mathematic foundations of deep learning and data science as well. We urgently need to shift the paradigm for data analysis from the classical Euclidean data analysis to both Euclidean and non Euclidean data analysis and develop more and more innovative methods for describing, estimating and inferring non Euclidean geometries of modern real datasets. A general framework for integrated analysis of both Euclidean and non Euclidean data, composite AI, decision intelligence and edge AI provide powerful innovative ideas and strategies for fundamentally advancing AI. We are expected to marry statistics with AI, develop a unified theory of modern statistics and drive next generation of AI and data science. △ Less","30 October, 2022",https://arxiv.org/pdf/2211.03756
Multilayer spintronic neural networks with radio-frequency connections,Andrew Ross;Nathan Leroux;Arnaud de Riz;Danijela Marković;Dédalo Sanz-Hernández;Juan Trastoy;Paolo Bortolotti;Damien Querlioz;Leandro Martins;Luana Benetti;Marcel S. Claro;Pedro Anacleto;Alejandro Schulman;Thierry Taris;Jean-Baptiste Begueret;Sylvain Saïghi;Alex S. Jenkins;Ricardo Ferreira;Adrien F. Vincent;Alice Mizrahi;Julie Grollier,"Spintronic nano-synapses and nano-neurons perform complex cognitive computations with high accuracy thanks to their rich, reproducible and controllable magnetization dynamics. These dynamical nanodevices could transform artificial intelligence hardware, provided that they implement state-of-the art deep neural networks. However, there is today no scalable way to connect them in multilayers. Here we show that the flagship nano-components of spintronics, magnetic tunnel junctions, can be connected into multilayer neural networks where they implement both synapses and neurons thanks to their magnetization dynamics, and communicate by processing, transmitting and receiving radio frequency (RF) signals. We build a hardware spintronic neural network composed of nine magnetic tunnel junctions connected in two layers, and show that it natively classifies nonlinearly-separable RF inputs with an accuracy of 97.7%. Using physical simulations, we demonstrate that a large network of nanoscale junctions can achieve state-of the-art identification of drones from their RF transmissions, without digitization, and consuming only a few milliwatts, which is a gain of more than four orders of magnitude in power consumption compared to currently used techniques. This study lays the foundation for deep, dynamical, spintronic neural networks. △ Less","7 November, 2022",https://arxiv.org/pdf/2211.03659
Adaptive User-Centered Multimodal Interaction towards Reliable and Trusted Automotive Interfaces,Amr Gomaa,"With the recently increasing capabilities of modern vehicles, novel approaches for interaction emerged that go beyond traditional touch-based and voice command approaches. Therefore, hand gestures, head pose, eye gaze, and speech have been extensively investigated in automotive applications for object selection and referencing. Despite these significant advances, existing approaches mostly employ a one-model-fits-all approach unsuitable for varying user behavior and individual differences. Moreover, current referencing approaches either consider these modalities separately or focus on a stationary situation, whereas the situation in a moving vehicle is highly dynamic and subject to safety-critical constraints. In this paper, I propose a research plan for a user-centered adaptive multimodal fusion approach for referencing external objects from a moving vehicle. The proposed plan aims to provide an open-source framework for user-centered adaptation and personalization using user observations and heuristics, multimodal fusion, clustering, transfer-of-learning for model adaptation, and continuous learning, moving towards trusted human-centered artificial intelligence. △ Less","7 November, 2022",https://arxiv.org/pdf/2211.03539
End-to-End Evaluation of a Spoken Dialogue System for Learning Basic Mathematics,Eda Okur;Saurav Sahay;Roddy Fuentes Alba;Lama Nachman,"The advances in language-based Artificial Intelligence (AI) technologies applied to build educational applications can present AI for social-good opportunities with a broader positive impact. Across many disciplines, enhancing the quality of mathematics education is crucial in building critical thinking and problem-solving skills at younger ages. Conversational AI systems have started maturing to a point where they could play a significant role in helping students learn fundamental math concepts. This work presents a task-oriented Spoken Dialogue System (SDS) built to support play-based learning of basic math concepts for early childhood education. The system has been evaluated via real-world deployments at school while the students are practicing early math concepts with multimodal interactions. We discuss our efforts to improve the SDS pipeline built for math learning, for which we explore utilizing MathBERT representations for potential enhancement to the Natural Language Understanding (NLU) module. We perform an end-to-end evaluation using real-world deployment outputs from the Automatic Speech Recognition (ASR), Intent Recognition, and Dialogue Manager (DM) components to understand how error propagation affects the overall performance in real-world scenarios. △ Less","7 November, 2022",https://arxiv.org/pdf/2211.03511
Generative Transformers for Design Concept Generation,Qihao Zhu;Jianxi Luo,"Generating novel and useful concepts is essential during the early design stage to explore a large variety of design opportunities, which usually requires advanced design thinking ability and a wide range of knowledge from designers. Growing works on computer-aided tools have explored the retrieval of knowledge and heuristics from design data. However, they only provide stimuli to inspire designers from limited aspects. This study explores the recent advance of the natural language generation (NLG) technique in the artificial intelligence (AI) field to automate the early-stage design concept generation. Specifically, a novel approach utilizing the generative pre-trained transformer (GPT) is proposed to leverage the knowledge and reasoning from textual data and transform them into new concepts in understandable language. Three concept generation tasks are defined to leverage different knowledge and reasoning: domain knowledge synthesis, problem-driven synthesis, and analogy-driven synthesis. The experiments with both human and data-driven evaluation show good performance in generating novel and useful concepts. △ Less","7 November, 2022",https://arxiv.org/pdf/2211.03468
Justification of Recommender Systems Results: A Service-based Approach,Noemi Mauro;Zhongli Filippo Hu;Liliana Ardissono,"With the increasing demand for predictable and accountable Artificial Intelligence, the ability to explain or justify recommender systems results by specifying how items are suggested, or why they are relevant, has become a primary goal. However, current models do not explicitly represent the services and actors that the user might encounter during the overall interaction with an item, from its selection to its usage. Thus, they cannot assess their impact on the user's experience. To address this issue, we propose a novel justification approach that uses service models to (i) extract experience data from reviews concerning all the stages of interaction with items, at different granularity levels, and (ii) organize the justification of recommendations around those stages. In a user study, we compared our approach with baselines reflecting the state of the art in the justification of recommender systems results. The participants evaluated the Perceived User Awareness Support provided by our service-based justification models higher than the one offered by the baselines. Moreover, our models received higher Interface Adequacy and Satisfaction evaluations by users having different levels of Curiosity or low Need for Cognition (NfC). Differently, high NfC participants preferred a direct inspection of item reviews. These findings encourage the adoption of service models to justify recommender systems results but suggest the investigation of personalization strategies to suit diverse interaction needs. △ Less","7 November, 2022",https://arxiv.org/pdf/2211.03452
Named Entity Recognition in Indian court judgments,Prathamesh Kalamkar;Astha Agarwal;Aman Tiwari;Smita Gupta;Saurabh Karn;Vivek Raghavan,"Identification of named entities from legal texts is an essential building block for developing other legal Artificial Intelligence applications. Named Entities in legal texts are slightly different and more fine-grained than commonly used named entities like Person, Organization, Location etc. In this paper, we introduce a new corpus of 46545 annotated legal named entities mapped to 14 legal entity types. The Baseline model for extracting legal named entities from judgment text is also developed. △ Less","7 November, 2022",https://arxiv.org/pdf/2211.03442
B-SMART: A Reference Architecture for Artificially Intelligent Autonomic Smart Buildings,Mikhail Genkin;J. J. McArthur,"The pervasive application of artificial intelligence and machine learning algorithms is transforming many industries and aspects of the human experience. One very important industry trend is the move to convert existing human dwellings to smart buildings, and to create new smart buildings. Smart buildings aim to mitigate climate change by reducing energy consumption and associated carbon emissions. To accomplish this, they leverage artificial intelligence, big data, and machine learning algorithms to learn and optimize system performance. These fields of research are currently very rapidly evolving and advancing, but there has been very little guidance to help engineers and architects working on smart buildings apply artificial intelligence algorithms and technologies in a systematic and effective manner. In this paper we present B-SMART: the first reference architecture for autonomic smart buildings. B-SMART facilitates the application of artificial intelligence techniques and technologies to smart buildings by decoupling conceptually distinct layers of functionality and organizing them into an autonomic control loop. We also present a case study illustrating how B-SMART can be applied to accelerate the introduction of artificial intelligence into an existing smart building. △ Less","6 November, 2022",https://arxiv.org/pdf/2211.03219
A Survey on Influence Maximization: From an ML-Based Combinatorial Optimization,Yandi Li;Haobo Gao;Yunxuan Gao;Jianxiong Guo;Weili Wu,"Influence Maximization (IM) is a classical combinatorial optimization problem, which can be widely used in mobile networks, social computing, and recommendation systems. It aims at selecting a small number of users such that maximizing the influence spread across the online social network. Because of its potential commercial and academic value, there are a lot of researchers focusing on studying the IM problem from different perspectives. The main challenge comes from the NP-hardness of the IM problem and \#P-hardness of estimating the influence spread, thus traditional algorithms for overcoming them can be categorized into two classes: heuristic algorithms and approximation algorithms. However, there is no theoretical guarantee for heuristic algorithms, and the theoretical design is close to the limit. Therefore, it is almost impossible to further optimize and improve their performance. With the rapid development of artificial intelligence, the technology based on Machine Learning (ML) has achieved remarkable achievements in many fields. In view of this, in recent years, a number of new methods have emerged to solve combinatorial optimization problems by using ML-based techniques. These methods have the advantages of fast solving speed and strong generalization ability to unknown graphs, which provide a brand-new direction for solving combinatorial optimization problems. Therefore, we abandon the traditional algorithms based on iterative search and review the recent development of ML-based methods, especially Deep Reinforcement Learning, to solve the IM problem and other variants in social networks. We focus on summarizing the relevant background knowledge, basic principles, common methods, and applied research. Finally, the challenges that need to be solved urgently in future IM research are pointed out. △ Less","6 November, 2022",https://arxiv.org/pdf/2211.03074
MONAI: An open-source framework for deep learning in healthcare,M. Jorge Cardoso;Wenqi Li;Richard Brown;Nic Ma;Eric Kerfoot;Yiheng Wang;Benjamin Murrey;Andriy Myronenko;Can Zhao;Dong Yang;Vishwesh Nath;Yufan He;Ziyue Xu;Ali Hatamizadeh;Andriy Myronenko;Wentao Zhu;Yun Liu;Mingxin Zheng;Yucheng Tang;Isaac Yang;Michael Zephyr;Behrooz Hashemian;Sachidanand Alle;Mohammad Zalbagi Darestani;Charlie Budd,"Artificial Intelligence (AI) is having a tremendous impact across most areas of science. Applications of AI in healthcare have the potential to improve our ability to detect, diagnose, prognose, and intervene on human disease. For AI models to be used clinically, they need to be made safe, reproducible and robust, and the underlying software framework must be aware of the particularities (e.g. geometry, physiology, physics) of medical data being processed. This work introduces MONAI, a freely available, community-supported, and consortium-led PyTorch-based framework for deep learning in healthcare. MONAI extends PyTorch to support medical data, with a particular focus on imaging, and provide purpose-specific AI model architectures, transformations and utilities that streamline the development and deployment of medical AI models. MONAI follows best practices for software-development, providing an easy-to-use, robust, well-documented, and well-tested software framework. MONAI preserves the simple, additive, and compositional approach of its underlying PyTorch libraries. MONAI is being used by and receiving contributions from research, clinical and industrial teams from around the world, who are pursuing applications spanning nearly every aspect of healthcare. △ Less","4 November, 2022",https://arxiv.org/pdf/2211.02701
De novo PROTAC design using graph-based deep generative models,Divya Nori;Connor W. Coley;Rocío Mercado,"PROteolysis TArgeting Chimeras (PROTACs) are an emerging therapeutic modality for degrading a protein of interest (POI) by marking it for degradation by the proteasome. Recent developments in artificial intelligence (AI) suggest that deep generative models can assist with the de novo design of molecules with desired properties, and their application to PROTAC design remains largely unexplored. We show that a graph-based generative model can be used to propose novel PROTAC-like structures from empty graphs. Our model can be guided towards the generation of large molecules (30--140 heavy atoms) predicted to degrade a POI through policy-gradient reinforcement learning (RL). Rewards during RL are applied using a boosted tree surrogate model that predicts a molecule's degradation potential for each POI. Using this approach, we steer the generative model towards compounds with higher likelihoods of predicted degradation activity. Despite being trained on sparse public data, the generative model proposes molecules with substructures found in known degraders. After fine-tuning, predicted activity against a challenging POI increases from 50% to >80% with near-perfect chemical validity for sampled compounds, suggesting this is a promising approach for the optimization of large, PROTAC-like molecules for targeted protein degradation. △ Less","4 November, 2022",https://arxiv.org/pdf/2211.02660
"Pushing AI to Wireless Network Edge: An Overview on Integrated Sensing, Communication, and Computation towards 6G",Guangxu Zhu;Zhonghao Lyu;Xiang Jiao;Peixi Liu;Mingzhe Chen;Jie Xu;Shuguang Cui;Ping Zhang,"Pushing artificial intelligence (AI) from central cloud to network edge has reached board consensus in both industry and academia for materializing the vision of artificial intelligence of things (AIoT) in the sixth-generation (6G) era. This gives rise to an emerging research area known as edge intelligence, which concerns the distillation of human-like intelligence from the huge amount of data scattered at wireless network edge. In general, realizing edge intelligence corresponds to the process of sensing, communication, and computation, which are coupled ingredients for data generation, exchanging, and processing, respectively. However, conventional wireless networks design the sensing, communication, and computation separately in a task-agnostic manner, which encounters difficulties in accommodating the stringent demands of ultra-low latency, ultra-high reliability, and high capacity in emerging AI applications such as auto-driving. This thus prompts a new design paradigm of seamless integrated sensing, communication, and computation (ISCC) in a task-oriented manner, which comprehensively accounts for the use of the data in the downstream AI applications. In view of its growing interest, this article provides a timely overview of ISCC for edge intelligence by introducing its basic concept, design challenges, and enabling techniques, surveying the state-of-the-art development, and shedding light on the road ahead. △ Less","4 November, 2022",https://arxiv.org/pdf/2211.02574
Generation of Chinese classical poetry based on pre-trained model,Ziyao Wang;Lujin Guan;Guanyu Liu,"In order to test whether artificial intelligence can create qualified classical poetry like humans, the author proposes a study of Chinese classical poetry generation based on a pre-trained model. This paper mainly tries to use BART and other pre training models, proposes FS2TEXT and RR2TEXT to generate metrical poetry text and even specific style poetry text, and solves the problem that the user's writing intention gradually reduces the relevance of the generated poetry text. In order to test the model's results, the authors selected ancient poets, by combining it with BART's poetic model work, developed a set of AI poetry Turing problems, it was reviewed by a group of poets and poetry writing researchers. There were more than 600 participants, and the final results showed that, high-level poetry lovers can't distinguish between AI activity and human activity, this indicates that the author's working methods are not significantly different from human activities. The model of poetry generation studied by the author generalizes works that cannot be distinguished from those of advanced scholars. The number of modern Chinese poets has reached 5 million. However, many modern Chinese poets lack language ability and skills as a result of their childhood learning. However, many modern poets have no creative inspiration, and the author's model can help them. They can look at this model when they choose words and phrases and they can write works based on the poems they already have, and they can write their own poems. The importance of poetry lies in the author's thoughts and reflections. It doesn't matter how good AI poetry is. The only thing that matters is for people to see and inspire them. △ Less","4 November, 2022",https://arxiv.org/pdf/2211.02541
Machine Learning Challenges of Biological Factors in Insect Image Data,Nicholas Pellegrino;Zahra Gharaee;Paul Fieguth,"The BIOSCAN project, led by the International Barcode of Life Consortium, seeks to study changes in biodiversity on a global scale. One component of the project is focused on studying the species interaction and dynamics of all insects. In addition to genetically barcoding insects, over 1.5 million images per year will be collected, each needing taxonomic classification. With the immense volume of incoming images, relying solely on expert taxonomists to label the images would be impossible; however, artificial intelligence and computer vision technology may offer a viable high-throughput solution. Additional tasks including manually weighing individual insects to determine biomass, remain tedious and costly. Here again, computer vision may offer an efficient and compelling alternative. While the use of computer vision methods is appealing for addressing these problems, significant challenges resulting from biological factors present themselves. These challenges are formulated in the context of machine learning in this paper. △ Less","4 November, 2022",https://arxiv.org/pdf/2211.02537
The Sustainable Development Goals and Aerospace Engineering: A critical note through Artificial Intelligence,Alejandro Sánchez-Roncero;Òscar Garibo-i-Ortsa;J. Alberto Conejero;Hamidreza Eivazi;Fermín Mallor;Emelie Rosenberg;Francesco Fuso-Nerini;Javier García-Martínez;Ricardo Vinuesa;Sergio Hoyas,"The 2030 Agenda of the United Nations (UN) revolves around the Sustainable Development Goals (SDGs). A critical step towards that objective is identifying whether scientific production aligns with the SDGs' achievement. To assess this, funders and research managers need to manually estimate the impact of their funding agenda on the SDGs, focusing on accuracy, scalability, and objectiveness. With this objective in mind, in this work, we develop ASDG, an easy-to-use artificial-intelligence (AI)-based model for automatically identifying the potential impact of scientific papers on the UN SDGs. As a demonstrator of ASDG, we analyze the alignment of recent aerospace publications with the SDGs. The Aerospace data set analyzed in this paper consists of approximately 820,000 papers published in English from 2011 to 2020 and indexed in the Scopus database. The most-contributed SDGs are 7 (on clean energy), 9 (on industry), 11 (on sustainable cities) and 13 (on climate action). The establishment of the SDGs by the UN in the middle of the 2010 decade did not significantly affect the data. However, we find clear discrepancies among countries, likely indicative of different priorities. Also, different trends can be seen in the most and least cited papers, with clear differences in some SDGs. Finally, the number of abstracts the code cannot identify is decreasing with time, possibly showing the scientific community's awareness of SDG. △ Less","4 November, 2022",https://arxiv.org/pdf/2211.02409
Automated Logging Drone: A Computer Vision Drone Implementation,Aaron Yagnik;Adrian S. -W. Tam,"In recent years, Artificial Intelligence (AI) and Computer Vision (CV) have become the pinnacle of technology with new developments seemingly every day. This technology along with more powerful drone technology have made autonomous surveillance more sought after. Here an overview of the Automated Logging Drone (ALD) project is presented along with examples of how this project can be used with more refining and added features. △ Less","3 November, 2022",https://arxiv.org/pdf/2211.02208
Book Cover Synthesis from the Summary,Emdadul Haque;Md. Faraz Kabir Khan;Mohammad Imrul Jubair;Jarin Anjum;Abrar Zahir Niloy,"The cover is the face of a book and is a point of attraction for the readers. Designing book covers is an essential task in the publishing industry. One of the main challenges in creating a book cover is representing the theme of the book's content in a single image. In this research, we explore ways to produce a book cover using artificial intelligence based on the fact that there exists a relationship between the summary of the book and its cover. Our key motivation is the application of text-to-image synthesis methods to generate images from given text or captions. We explore several existing text-to-image conversion techniques for this purpose and propose an approach to exploit these frameworks for producing book covers from provided summaries. We construct a dataset of English books that contains a large number of samples of summaries of existing books and their cover images. In this paper, we describe our approach to collecting, organizing, and pre-processing the dataset to use it for training models. We apply different text-to-image synthesis techniques to generate book covers from the summary and exhibit the results in this paper. △ Less","3 November, 2022",https://arxiv.org/pdf/2211.02138
Resource-aware Deep Learning for Wireless Fingerprinting Localization,Gregor Cerar;Blaž Bertalanič;Carolina Fortuna,"Location based services, already popular with end users, are now inevitably becoming part of new wireless infrastructures and emerging business processes. The increasingly popular Deep Learning (DL) artificial intelligence methods perform very well in wireless fingerprinting localization based on extensive indoor radio measurement data. However, with the increasing complexity these methods become computationally very intensive and energy hungry, both for their training and subsequent operation. Considering only mobile users, estimated to exceed 7.4 billion by the end of 2025, and assuming that the networks serving these users will need to perform only one localization per user per hour on average, the machine learning models used for the calculation would need to perform 65 \times 10^{12} predictions per year. Add to this equation tens of billions of other connected devices and applications that rely heavily on more frequent location updates, and it becomes apparent that localization will contribute significantly to carbon emissions unless more energy-efficient models are developed and used. In this Chapter, we discuss the latest results and trends in wireless localization and look at paths towards achieving more sustainable AI. We then elaborate on a methodology for computing DL model complexity, energy consumption and carbon footprint and show on a concrete example how to develop a more resource-aware model for fingerprinting. We finally compare relevant works in terms of complexity and training CO_2 footprint. △ Less","12 October, 2022",https://arxiv.org/pdf/2211.01759
Integrated Photonic Tensor Processing Unit for a Matrix Multiply: a Review,Nicola Peserico;Bhavin J. Shastri;Volker J. Sorger,"The explosion of artificial intelligence and machine-learning algorithms, connected to the exponential growth of the exchanged data, is driving a search for novel application-specific hardware accelerators. Among the many, the photonics field appears to be in the perfect spotlight for this global data explosion, thanks to its almost infinite bandwidth capacity associated with limited energy consumption. In this review, we will overview the major advantages that photonics has over electronics for hardware accelerators, followed by a comparison between the major architectures implemented on Photonics Integrated Circuits (PIC) for both the linear and nonlinear parts of Neural Networks. By the end, we will highlight the main driving forces for the next generation of photonic accelerators, as well as the main limits that must be overcome. △ Less","2 November, 2022",https://arxiv.org/pdf/2211.01476
Detecting Emerging Technologies in Artificial Intelligence Scientific Ecosystem Using an Indicator-based Model,Ali Ghaemmaghami;Andrea Schiffauerova;Ashkan Ebadi,"Early identification of emergent topics is of eminent importance due to their potential impacts on society. There are many methods for detecting emerging terms and topics, all with advantages and drawbacks. However, there is no consensus about the attributes and indicators of emergence. In this study, we evaluate emerging topic detection in the field of artificial intelligence using a new method to evaluate emergence. We also introduce two new attributes of collaboration and technological impact which can help us use both paper and patent information simultaneously. Our results confirm that the proposed new method can successfully identify the emerging topics in the period of the study. Moreover, this new method can provide us with the score of each attribute and a final emergence score, which enable us to rank the emerging topics with their emergence scores and each attribute score. △ Less","6 October, 2022",https://arxiv.org/pdf/2211.01348
"Explainable AI over the Internet of Things (IoT): Overview, State-of-the-Art and Future Directions",Senthil Kumar Jagatheesaperumal;Quoc-Viet Pham;Rukhsana Ruby;Zhaohui Yang;Chunmei Xu;Zhaoyang Zhang,"Explainable Artificial Intelligence (XAI) is transforming the field of Artificial Intelligence (AI) by enhancing the trust of end-users in machines. As the number of connected devices keeps on growing, the Internet of Things (IoT) market needs to be trustworthy for the end-users. However, existing literature still lacks a systematic and comprehensive survey work on the use of XAI for IoT. To bridge this lacking, in this paper, we address the XAI frameworks with a focus on their characteristics and support for IoT. We illustrate the widely-used XAI services for IoT applications, such as security enhancement, Internet of Medical Things (IoMT), Industrial IoT (IIoT), and Internet of City Things (IoCT). We also suggest the implementation choice of XAI models over IoT systems in these applications with appropriate examples and summarize the key inferences for future works. Moreover, we present the cutting-edge development in edge XAI structures and the support of sixth-generation (6G) communication services for IoT applications, along with key inferences. In a nutshell, this paper constitutes the first holistic compilation on the development of XAI-based frameworks tailored for the demands of future IoT use cases. △ Less","7 November, 2022",https://arxiv.org/pdf/2211.01036
Spot the fake lungs: Generating Synthetic Medical Images using Neural Diffusion Models,Hazrat Ali;Shafaq Murad;Zubair Shah,"Generative models are becoming popular for the synthesis of medical images. Recently, neural diffusion models have demonstrated the potential to generate photo-realistic images of objects. However, their potential to generate medical images is not explored yet. In this work, we explore the possibilities of synthesis of medical images using neural diffusion models. First, we use a pre-trained DALLE2 model to generate lungs X-Ray and CT images from an input text prompt. Second, we train a stable diffusion model with 3165 X-Ray images and generate synthetic images. We evaluate the synthetic image data through a qualitative analysis where two independent radiologists label randomly chosen samples from the generated data as real, fake, or unsure. Results demonstrate that images generated with the diffusion model can translate characteristics that are otherwise very specific to certain medical conditions in chest X-Ray or CT images. Careful tuning of the model can be very promising. To the best of our knowledge, this is the first attempt to generate lungs X-Ray and CT images using neural diffusion models. This work aims to introduce a new dimension in artificial intelligence for medical imaging. Given that this is a new topic, the paper will serve as an introduction and motivation for the research community to explore the potential of diffusion models for medical image synthesis. We have released the synthetic images on https://www.kaggle.com/datasets/hazrat/awesomelungs. △ Less","2 November, 2022",https://arxiv.org/pdf/2211.00902
Distributed Massive MIMO for LEO Satellite Networks,Mohammed Y. Abdelsadek;Gunes Karabulut Kurt;Halim Yanikomeroglu,"The ultra-dense deployment of interconnected satellites will characterize future low Earth orbit (LEO) mega-constellations. Exploiting this towards a more efficient satellite network (SatNet), this paper proposes a novel LEO SatNet architecture based on distributed massive multiple-input multiple-output (DM-MIMO) technology allowing ground user terminals to be connected to a cluster of satellites. To this end, we investigate various aspects of DM-MIMO-based satellite network design, the benefits of using this architecture, the associated challenges, and the potential solutions. In addition, we propose a distributed joint power allocation and handover management (D-JPAHM) technique that jointly optimizes the power allocation and handover management processes in a cross-layer manner. This framework aims to maximize the network throughput and minimize the handover rate while considering the quality-of-service (QoS) demands of user terminals and the power capabilities of the satellites. Moreover, we devise an artificial intelligence (AI)-based solution to efficiently implement the proposed D-JPAHM framework in a manner suitable for real-time operation and the dynamic SatNet environment. To the best of our knowledge, this is the first work to introduce and study DM-MIMO technology in LEO SatNets. Extensive simulation results reveal the superiority of the proposed architecture and solutions compared to conventional approaches in the literature. △ Less","1 November, 2022",https://arxiv.org/pdf/2211.00832
Internet Of Rights(IOR) In Role Based Block Chain,Yunling Shi;Jie Guan;Junfeng Xiao;Huai Zhang;Qiang Guo;Yu Yuan,"A large amount of data has been accumulated. with the development of the Internet industry. Many problems have been exposed with data explosion: 1. The contradiction between data privacy and data collaborations; 2. The contradiction between data ownership and the right of data usage; 3. The legality of data collection and data usage; 4. The relationship between the governance of data and the governance of rules; 5. Traceability of evidence chain. In order to face such a complicated situation, many algorithms were proposed and developed. This article tries to build a model from the perspective of blockchain to make some breakthroughs.Internet Of Rights(IOR) model uses multi-chain technology to logically break down the consensus mechanism into layers, including storage consensus, permission consensus, role consensus, transaction consensus etc. thus to build a new infrastructure, which enables data sources with complex organizational structures and interactions to collaborate smoothly on the premise of protecting data privacy. With blockchain's nature of decentralization, openness, autonomy, immutability, and controllable anonymity, Internet Of Rights(IOR) model registers the ownership of data, enables applications to build ecosystem based on responsibilities and rights. It also provides cross-domain processing with privacy protection, as well as the separation of data governance and rule governance. With the processing capabilities of artificial intelligence and big data technology, as well as the ubiquitous data collection capabilities of the Internet of Things, Internet Of Rights(IOR) model may provide a new infrastructure concept for realizing swarm intelligence and building a new paradigm of the Internet, i.e. intelligent governance. △ Less","1 November, 2022",https://arxiv.org/pdf/2211.00830
Semi-Supervised Domain Adaptation for Cross-Survey Galaxy Morphology Classification and Anomaly Detection,Aleksandra Ćiprijanović;Ashia Lewis;Kevin Pedro;Sandeep Madireddy;Brian Nord;Gabriel N. Perdue;Stefan M. Wild,"In the era of big astronomical surveys, our ability to leverage artificial intelligence algorithms simultaneously for multiple datasets will open new avenues for scientific discovery. Unfortunately, simply training a deep neural network on images from one data domain often leads to very poor performance on any other dataset. Here we develop a Universal Domain Adaptation method DeepAstroUDA, capable of performing semi-supervised domain alignment that can be applied to datasets with different types of class overlap. Extra classes can be present in any of the two datasets, and the method can even be used in the presence of unknown classes. For the first time, we demonstrate the successful use of domain adaptation on two very different observational datasets (from SDSS and DECaLS). We show that our method is capable of bridging the gap between two astronomical surveys, and also performs well for anomaly detection and clustering of unknown data in the unlabeled dataset. We apply our model to two examples of galaxy morphology classification tasks with anomaly detection: 1) classifying spiral and elliptical galaxies with detection of merging galaxies (three classes including one unknown anomaly class); 2) a more granular problem where the classes describe more detailed morphological properties of galaxies, with the detection of gravitational lenses (ten classes including one unknown anomaly class). △ Less","11 November, 2022",https://arxiv.org/pdf/2211.00677
"minoHealth.ai: A Clinical Evaluation Of Deep Learning Systems For the Diagnosis of Pleural Effusion and Cardiomegaly In Ghana, Vietnam and the United States of America",Darlington Akogo;Benjamin Dabo Sarkodie;Issah Abubakari Samori;Bashiru Babatunde Jimah;Dorothea Akosua Anim;Yaw Boateng Mensah,"A rapid and accurate diagnosis of cardiomegaly and pleural effusion is of the utmost importance to reduce mortality and medical costs. Artificial Intelligence has shown promise in diagnosing medical conditions. With this study, we seek to evaluate how well Artificial Intelligence (AI) systems, developed my minoHealth AI Labs, will perform at diagnosing cardiomegaly and pleural effusion, using chest x-rays from Ghana, Vietnam and the USA, and how well AI systems will perform when compared with radiologists working in Ghana. The evaluation dataset used in this study contained 100 images randomly selected from three datasets. The Deep Learning models were further tested on a larger Ghanaian dataset containing five hundred and sixty one (561) samples. Two AI systems were then evaluated on the evaluation dataset, whilst we also gave the same chest x-ray images within the evaluation dataset to 4 radiologists, with 5 - 20 years experience, to diagnose independently. For cardiomegaly, minoHealth-ai systems scored Area under the Receiver operating characteristic Curve (AUC-ROC) of 0.9 and 0.97 while the AUC-ROC of individual radiologists ranged from 0.77 to 0.87. For pleural effusion, the minoHealth-ai systems scored 0.97 and 0.91 whereas individual radiologists scored between 0.75 and 0.86. On both conditions, the best performing AI model outperforms the best performing radiologist by about 10%. We also evaluate the specificity, sensitivity, negative predictive value (NPV), and positive predictive value (PPV) between the minoHealth-ai systems and radiologists. △ Less","5 November, 2022",https://arxiv.org/pdf/2211.00644
Causal DAG extraction from a library of books or videos/movies,Robert R. Tucci,"Determining a causal DAG (directed acyclic graph) for a problem under consideration, is a major roadblock when doing Judea Pearl's Causal Inference (CI) in Statistics. The same problem arises when doing CI in Artificial Intelligence (AI) and Machine Learning (ML). As with many problems in Science, we think Nature has found an effective solution to this problem. We argue that human and animal brains contain an explicit engine for doing CI, and that such an engine uses as input an atlas (i.e., collection) of causal DAGs. We propose a simple algorithm for constructing such an atlas from a library of books or videos/movies. We illustrate our method by applying it to a database of randomly generated Tic-Tac-Toe games. The software used to generate this Tic-Tac-Toe example is open source and available at GitHub. △ Less","29 October, 2022",https://arxiv.org/pdf/2211.00486
Strategies for Optimizing End-to-End Artificial Intelligence Pipelines on Intel Xeon Processors,Meena Arunachalam;Vrushabh Sanghavi;Yi A Yao;Yi A Zhou;Lifeng A Wang;Zongru Wen;Niroop Ammbashankar;Ning W Wang;Fahim Mohammad,"End-to-end (E2E) artificial intelligence (AI) pipelines are composed of several stages including data preprocessing, data ingestion, defining and training the model, hyperparameter optimization, deployment, inference, postprocessing, followed by downstream analyses. To obtain efficient E2E workflow, it is required to optimize almost all the stages of pipeline. Intel Xeon processors come with large memory capacities, bundled with AI acceleration (e.g., Intel Deep Learning Boost), well suited to run multiple instances of training and inference pipelines in parallel and has low total cost of ownership (TCO). To showcase the performance on Xeon processors, we applied comprehensive optimization strategies coupled with software and hardware acceleration on variety of E2E pipelines in the areas of Computer Vision, NLP, Recommendation systems, etc. We were able to achieve a performance improvement, ranging from 1.8x to 81.7x across different E2E pipelines. In this paper, we will be highlighting the optimization strategies adopted by us to achieve this performance on Intel Xeon processors with a set of eight different E2E pipelines. △ Less","1 November, 2022",https://arxiv.org/pdf/2211.00286
Angular upsampling in diffusion MRI using contextual HemiHex sub-sampling in q-space,Abrar Faiyaz;Md Nasir Uddin;Giovanni Schifitto,"Artificial Intelligence (Deep Learning(DL)/ Machine Learning(ML)) techniques are widely being used to address and overcome all kinds of ill-posed problems in medical imaging which was or in fact is seemingly impossible. Reducing gradient directions but harnessing high angular resolution(HAR) diffusion data in MR that retains clinical features is an important and challenging problem in the field. While the DL/ML approaches are promising, it is important to incorporate relevant context for the data to ensure that maximum prior information is provided for the AI model to infer the posterior. In this paper, we introduce HemiHex (HH) subsampling to suggestively address training data sampling on q-space geometry, followed by a nearest neighbor regression training on the HH-samples to finally upsample the dMRI data. Earlier studies has tried to use regression for up-sampling dMRI data but yields performance issues as it fails to provide structured geometrical measures for inference. Our proposed approach is a geometrically optimized regression technique which infers the unknown q-space thus addressing the limitations in the earlier studies. △ Less","31 October, 2022",https://arxiv.org/pdf/2211.00240
AI Assistants: A Framework for Semi-Automated Data Wrangling,Tomas Petricek;Gerrit J. J. van den Burg;Alfredo Nazábal;Taha Ceritli;Ernesto Jiménez-Ruiz;Christopher K. I. Williams,"Data wrangling tasks such as obtaining and linking data from various sources, transforming data formats, and correcting erroneous records, can constitute up to 80% of typical data engineering work. Despite the rise of machine learning and artificial intelligence, data wrangling remains a tedious and manual task. We introduce AI assistants, a class of semi-automatic interactive tools to streamline data wrangling. An AI assistant guides the analyst through a specific data wrangling task by recommending a suitable data transformation that respects the constraints obtained through interaction with the analyst. We formally define the structure of AI assistants and describe how existing tools that treat data cleaning as an optimization problem fit the definition. We implement AI assistants for four common data wrangling tasks and make AI assistants easily accessible to data analysts in an open-source notebook environment for data science, by leveraging the common structure they follow. We evaluate our AI assistants both quantitatively and qualitatively through three example scenarios. We show that the unified and interactive design makes it easy to perform tasks that would be difficult to do manually or with a fully automatic tool. △ Less","31 October, 2022",https://arxiv.org/pdf/2211.00192
Improving Fairness in Image Classification via Sketching,Ruichen Yao;Ziteng Cui;Xiaoxiao Li;Lin Gu,"Fairness is a fundamental requirement for trustworthy and human-centered Artificial Intelligence (AI) system. However, deep neural networks (DNNs) tend to make unfair predictions when the training data are collected from different sub-populations with different attributes (i.e. color, sex, age), leading to biased DNN predictions. We notice that such a troubling phenomenon is often caused by data itself, which means that bias information is encoded to the DNN along with the useful information (i.e. class information, semantic information). Therefore, we propose to use sketching to handle this phenomenon. Without losing the utility of data, we explore the image-to-sketching methods that can maintain useful semantic information for the target classification while filtering out the useless bias information. In addition, we design a fair loss to further improve the model fairness. We evaluate our method through extensive experiments on both general scene dataset and medical scene dataset. Our results show that the desired image-to-sketching method improves model fairness and achieves satisfactory results among state-of-the-art. △ Less","31 October, 2022",https://arxiv.org/pdf/2211.00168
"Large scale traffic forecasting with gradient boosting, Traffic4cast 2022 challenge",Martin Lumiste;Andrei Ilie,"Accurate traffic forecasting is of the utmost importance for optimal travel planning and for efficient city mobility. IARAI (The Institute of Advanced Research in Artificial Intelligence) organizes Traffic4cast, a yearly traffic prediction competition based on real-life data [https://www.iarai.ac.at/traffic4cast/], aiming to leverage artificial intelligence advances for producing accurate traffic estimates. We present our solution to the IARAI Traffic4cast 2022 competition, in which the goal is to develop algorithms for predicting road graph edge congestion classes and supersegment-level travel times. In contrast to the previous years, this year's competition focuses on modelling graph edge level behaviour, rather than more coarse aggregated grid-based traffic movies. Due to this, we leverage a method familiar from tabular data modelling -- gradient-boosted decision tree ensembles. We reduce the dimensionality of the input data representing traffic counters with the help of the classic PCA method and feed it as input to a LightGBM model. This simple, fast, and scalable technique allowed us to win second place in the core competition. The source code and references to trained model files and submissions are available at https://github.com/skandium/t4c22 . △ Less","31 October, 2022",https://arxiv.org/pdf/2211.00157
Towards Human Cognition Level-based Experiment Design for Counterfactual Explanations (XAI),Muhammad Suffian;Muhammad Yaseen Khan;Alessandro Bogliolo,"Explainable Artificial Intelligence (XAI) has recently gained a swell of interest, as many Artificial Intelligence (AI) practitioners and developers are compelled to rationalize how such AI-based systems work. Decades back, most XAI systems were developed as knowledge-based or expert systems. These systems assumed reasoning for the technical description of an explanation, with little regard for the user's cognitive capabilities. The emphasis of XAI research appears to have turned to a more pragmatic explanation approach for better understanding. An extensive area where cognitive science research may substantially influence XAI advancements is evaluating user knowledge and feedback, which are essential for XAI system evaluation. To this end, we propose a framework to experiment with generating and evaluating the explanations on the grounds of different cognitive levels of understanding. In this regard, we adopt Bloom's taxonomy, a widely accepted model for assessing the user's cognitive capability. We utilize the counterfactual explanations as an explanation-providing medium encompassed with user feedback to validate the levels of understanding about the explanation at each cognitive level and improvise the explanation generation methods accordingly. △ Less","31 October, 2022",https://arxiv.org/pdf/2211.00103
Road Damages Detection and Classification with YOLOv7,Vung Pham;Du Nguyen;Christopher Donan,"Maintaining the roadway infrastructure is one of the essential factors in enabling a safe, economic, and sustainable transportation system. Manual roadway damage data collection is laborious and unsafe for humans to perform. This area is poised to benefit from the rapid advance and diffusion of artificial intelligence technologies. Specifically, deep learning advancements enable the detection of road damages automatically from the collected road images. This work proposes to collect and label road damage data using Google Street View and use YOLOv7 (You Only Look Once version 7) together with coordinate attention and related accuracy fine-tuning techniques such as label smoothing and ensemble method to train deep learning models for automatic road damage detection and classification. The proposed approaches are applied to the Crowdsensing-based Road Damage Detection Challenge (CRDDC2022), IEEE BigData 2022. The results show that the data collection from Google Street View is efficient, and the proposed deep learning approach results in F1 scores of 81.7% on the road damage data collected from the United States using Google Street View and 74.1% on all test images of this dataset. △ Less","31 October, 2022",https://arxiv.org/pdf/2211.00091
AI Explainability and Governance in Smart Energy Systems: A Review,Roba Alsaigh;Rashid Mehmood;Iyad Katib,"Traditional electrical power grids have long suffered from operational unreliability, instability, inflexibility, and inefficiency. Smart grids (or smart energy systems) continue to transform the energy sector with emerging technologies, renewable energy sources, and other trends. Artificial intelligence (AI) is being applied to smart energy systems to process massive and complex data in this sector and make smart and timely decisions. However, the lack of explainability and governability of AI is a major concern for stakeholders hindering a fast uptake of AI in the energy sector. This paper provides a review of AI explainability and governance in smart energy systems. We collect 3,568 relevant papers from the Scopus database, automatically discover 15 parameters or themes for AI governance in energy and elaborate the research landscape by reviewing over 150 papers and providing temporal progressions of the research. The methodology for discovering parameters or themes is based on ""deep journalism"", our data-driven deep learning-based big data analytics approach to automatically discover and analyse cross-sectional multi-perspective information to enable better decision-making and develop better instruments for governance. The findings show that research on AI explainability in energy systems is segmented and narrowly focussed on a few AI traits and energy system problems. This paper deepens our knowledge of AI governance in energy and is expected to help governments, industry, academics, energy prosumers, and other stakeholders to understand the landscape of AI in the energy sector, leading to better design, operations, utilisation, and risk management of energy systems. △ Less","14 December, 2022",https://arxiv.org/pdf/2211.00069
Artificial Intelligence and Arms Control,Paul Scharre;Megan Lamberth,"Potential advancements in artificial intelligence (AI) could have profound implications for how countries research and develop weapons systems, and how militaries deploy those systems on the battlefield. The idea of AI-enabled military systems has motivated some activists to call for restrictions or bans on some weapon systems, while others have argued that AI may be too diffuse to control. This paper argues that while a ban on all military applications of AI is likely infeasible, there may be specific cases where arms control is possible. Throughout history, the international community has attempted to ban or regulate weapons or military systems for a variety of reasons. This paper analyzes both successes and failures and offers several criteria that seem to influence why arms control works in some cases and not others. We argue that success or failure depends on the desirability (i.e., a weapon's military value versus its perceived horribleness) and feasibility (i.e., sociopolitical factors that influence its success) of arms control. Based on these criteria, and the historical record of past attempts at arms control, we analyze the potential for AI arms control in the future and offer recommendations for what policymakers can do today. △ Less","22 October, 2022",https://arxiv.org/pdf/2211.00065
"Trends in Energy Estimates for Computing in AI/Machine Learning Accelerators, Supercomputers, and Compute-Intensive Applications",Sadasivan Shankar;Albert Reuther,"We examine the computational energy requirements of different systems driven by the geometrical scaling law, and increasing use of Artificial Intelligence or Machine Learning (AI-ML) over the last decade. With more scientific and technology applications based on data-driven discovery, machine learning methods, especially deep neural networks, have become widely used. In order to enable such applications, both hardware accelerators and advanced AI-ML methods have led to the introduction of new architectures, system designs, algorithms, and software. Our analysis of energy trends indicates three important observations: 1) Energy efficiency due to geometrical scaling is slowing down; 2) The energy efficiency at the bit-level does not translate into efficiency at the instruction-level, or at the system-level for a variety of systems, especially for large-scale AI-ML accelerators or supercomputers; 3) At the application level, general-purpose AI-ML methods can be computationally energy intensive, off-setting the gains in energy from geometrical scaling and special purpose accelerators. Further, our analysis provides specific pointers for integrating energy efficiency with performance analysis for enabling high-performance and sustainable computing in the future. △ Less","12 October, 2022",https://arxiv.org/pdf/2210.17331
LAD-RCNN:A Powerful Tool for Livestock Face Detection and Normalization,Ling Sun;Guiqiong Liu;Xunping Jiang;Junrui Liu;Xu Wang;Han Yang;Shiping Yang,"With the demand for standardized large-scale livestock farming and the development of artificial intelligence technology, a lot of research in area of animal face recognition were carried on pigs, cattle, sheep and other livestock. Face recognition consists of three sub-task: face detection, face normalizing and face identification. Most of animal face recognition study focuses on face detection and face identification. Animals are often uncooperative when taking photos, so the collected animal face images are often in arbitrary directions. The use of non-standard images may significantly reduce the performance of face recognition system. However, there is no study on normalizing of the animal face image with arbitrary directions. In this study, we developed a light-weight angle detection and region-based convolutional network (LAD-RCNN) containing a new rotation angle coding method that can detect the rotation angle and the location of animal face in one-stage. LAD-RCNN has a frame rate of 72.74 FPS (including all steps) on a single GeForce RTX 2080 Ti GPU. LAD-RCNN has been evaluated on multiple dataset including goat dataset and gaot infrared image. Evaluation result show that the AP of face detection was more than 95% and the deviation between the detected rotation angle and the ground-truth rotation angle were less than 0.036 (i.e. 6.48°) on all the test dataset. This shows that LAD-RCNN has excellent performance on livestock face and its direction detection, and therefore it is very suitable for livestock face detection and Normalizing. Code is available at https://github.com/SheepBreedingLab-HZAU/LAD-RCNN/ △ Less","5 November, 2022",https://arxiv.org/pdf/2210.17146
DanZero: Mastering GuanDan Game with Reinforcement Learning,Yudong Lu;Jian Zhao;Youpeng Zhao;Wengang Zhou;Houqiang Li,"Card game AI has always been a hot topic in the research of artificial intelligence. In recent years, complex card games such as Mahjong, DouDizhu and Texas Hold'em have been solved and the corresponding AI programs have reached the level of human experts. In this paper, we are devoted to developing an AI program for a more complex card game, GuanDan, whose rules are similar to DouDizhu but much more complicated. To be specific, the characteristics of large state and action space, long length of one episode and the unsure number of players in the GuanDan pose great challenges for the development of the AI program. To address these issues, we propose the first AI program DanZero for GuanDan using reinforcement learning technique. Specifically, we utilize a distributed framework to train our AI system. In the actor processes, we carefully design the state features and agents generate samples by self-play. In the learner process, the model is updated by Deep Monte-Carlo Method. After training for 30 days using 160 CPUs and 1 GPU, we get our DanZero bot. We compare it with 8 baseline AI programs which are based on heuristic rules and the results reveal the outstanding performance of DanZero. We also test DanZero with human players and demonstrate its human-level performance. △ Less","31 October, 2022",https://arxiv.org/pdf/2210.17087
LearningGroup: A Real-Time Sparse Training on FPGA via Learnable Weight Grouping for Multi-Agent Reinforcement Learning,Je Yang;JaeUk Kim;Joo-Young Kim,"Multi-agent reinforcement learning (MARL) is a powerful technology to construct interactive artificial intelligent systems in various applications such as multi-robot control and self-driving cars. Unlike supervised model or single-agent reinforcement learning, which actively exploits network pruning, it is obscure that how pruning will work in multi-agent reinforcement learning with its cooperative and interactive characteristics. \par In this paper, we present a real-time sparse training acceleration system named LearningGroup, which adopts network pruning on the training of MARL for the first time with an algorithm/architecture co-design approach. We create sparsity using a weight grouping algorithm and propose on-chip sparse data encoding loop (OSEL) that enables fast encoding with efficient implementation. Based on the OSEL's encoding format, LearningGroup performs efficient weight compression and computation workload allocation to multiple cores, where each core handles multiple sparse rows of the weight matrix simultaneously with vector processing units. As a result, LearningGroup system minimizes the cycle time and memory footprint for sparse data generation up to 5.72x and 6.81x. Its FPGA accelerator shows 257.40-3629.48 GFLOPS throughput and 7.10-100.12 GFLOPS/W energy efficiency for various conditions in MARL, which are 7.13x higher and 12.43x more energy efficient than Nvidia Titan RTX GPU, thanks to the fully on-chip training and highly optimized dataflow/data format provided by FPGA. Most importantly, the accelerator shows speedup up to 12.52x for processing sparse data over the dense case, which is the highest among state-of-the-art sparse training accelerators. △ Less","29 October, 2022",https://arxiv.org/pdf/2210.16624
ODNet: A Convolutional Neural Network for Asteroid Occultation Detection,Dorian Cazeneuve;Franck Marchis;Guillaume Blaclard;Paul A. Dalba;Victor Martin;Joé Asencioa,"We propose to design and build an algorithm that will use a Convolutional Neural Network (CNN) and observations from the Unistellar network to reliably detect asteroid occultations. The Unistellar Network, made of more than 10,000 digital telescopes owned by citizen scientists, and is regularly used to record asteroid occultations. In order to process the increasing amount of observational produced by this network, we need a quick and reliable way to analyze occultations. In an effort to solve this problem, we trained a CNN with artificial images of stars with twenty different types of photometric signals. Inputs to the network consists of two stacks of snippet images of stars, one around the star that is supposed to be occulted and a reference star used for comparison. We need the reference star to distinguish between a true occultation and artefacts introduced by poor atmospheric condition. Our Occultation Detection Neural Network (ODNet), can analyze three sequence of stars per second with 91\% of precision and 87\% of recall. The algorithm is sufficiently fast and robust so we can envision incorporating onboard the eVscopes to deliver real-time results. We conclude that citizen science represents an important opportunity for the future studies and discoveries in the occultations, and that application of artificial intelligence will permit us to to take better advantage of the ever-growing quantity of data to categorize asteroids. △ Less","28 October, 2022",https://arxiv.org/pdf/2210.16440
Big Data Meets Metaverse: A Survey,Jiayi Sun;Wensheng Gan;Zefeng Chen;Junhui Li;Philip S. Yu,"We are living in the era of big data. The Metaverse is an emerging technology in the future, and it has a combination of big data, AI (artificial intelligence), VR (Virtual Reality), AR (Augmented Reality), MR (mixed reality), and other technologies that will diminish the difference between online and real-life interaction. It has the goal of becoming a platform where we can work, go shopping, play around, and socialize. Each user who enters the Metaverse interacts with the virtual world in a data way. With the development and application of the Metaverse, the data will continue to grow, thus forming a big data network, which will bring huge data processing pressure to the digital world. Therefore, big data processing technology is one of the key technologies to implement the Metaverse. In this survey, we provide a comprehensive review of how Metaverse is changing big data. Moreover, we discuss the key security and privacy of Metaverse big data in detail. Finally, we summarize the open problems and opportunities of Metaverse, as well as the future of Metaverse with big data. We hope that this survey will provide researchers with the research direction and prospects of applying big data in the Metaverse. △ Less","28 October, 2022",https://arxiv.org/pdf/2210.16282
SEMPAI: a Self-Enhancing Multi-Photon Artificial Intelligence for prior-informed assessment of muscle function and pathology,Alexander Mühlberg;Paul Ritter;Simon Langer;Chloë Goossens;Stefanie Nübler;Dominik Schneidereit;Oliver Taubmann;Felix Denzinger;Dominik Nörenberg;Michael Haug;Wolfgang H. Goldmann;Andreas K. Maier;Oliver Friedrich;Lucas Kreiss,"Deep learning (DL) shows notable success in biomedical studies. However, most DL algorithms work as a black box, exclude biomedical experts, and need extensive data. We introduce the Self-Enhancing Multi-Photon Artificial Intelligence (SEMPAI), that integrates hypothesis-driven priors in a data-driven DL approach for research on multiphoton microscopy (MPM) of muscle fibers. SEMPAI utilizes meta-learning to optimize prior integration, data representation, and neural network architecture simultaneously. This allows hypothesis testing and provides interpretable feedback about the origin of biological information in MPM images. SEMPAI performs joint learning of several tasks to enable prediction for small datasets. The method is applied on an extensive multi-study dataset resulting in the largest joint analysis of pathologies and function for single muscle fibers. SEMPAI outperforms state-of-the-art biomarkers in six of seven predictive tasks, including those with scarce data. SEMPAI's DL models with integrated priors are superior to those without priors and to prior-only machine learning approaches. △ Less","28 October, 2022",https://arxiv.org/pdf/2210.16273
BI AVAN: Brain inspired Adversarial Visual Attention Network,Heng Huang;Lin Zhao;Xintao Hu;Haixing Dai;Lu Zhang;Dajiang Zhu;Tianming Liu,"Visual attention is a fundamental mechanism in the human brain, and it inspires the design of attention mechanisms in deep neural networks. However, most of the visual attention studies adopted eye-tracking data rather than the direct measurement of brain activity to characterize human visual attention. In addition, the adversarial relationship between the attention-related objects and attention-neglected background in the human visual system was not fully exploited. To bridge these gaps, we propose a novel brain-inspired adversarial visual attention network (BI-AVAN) to characterize human visual attention directly from functional brain activity. Our BI-AVAN model imitates the biased competition process between attention-related/neglected objects to identify and locate the visual objects in a movie frame the human brain focuses on in an unsupervised manner. We use independent eye-tracking data as ground truth for validation and experimental results show that our model achieves robust and promising results when inferring meaningful human visual attention and mapping the relationship between brain activities and visual stimuli. Our BI-AVAN model contributes to the emerging field of leveraging the brain's functional architecture to inspire and guide the model design in artificial intelligence (AI), e.g., deep neural networks. △ Less","27 October, 2022",https://arxiv.org/pdf/2210.15790
"Gathering Strength, Gathering Storms: The One Hundred Year Study on Artificial Intelligence (AI100) 2021 Study Panel Report",Michael L. Littman;Ifeoma Ajunwa;Guy Berger;Craig Boutilier;Morgan Currie;Finale Doshi-Velez;Gillian Hadfield;Michael C. Horowitz;Charles Isbell;Hiroaki Kitano;Karen Levy;Terah Lyons;Melanie Mitchell;Julie Shah;Steven Sloman;Shannon Vallor;Toby Walsh,"In September 2021, the ""One Hundred Year Study on Artificial Intelligence"" project (AI100) issued the second report of its planned long-term periodic assessment of artificial intelligence (AI) and its impact on society. It was written by a panel of 17 study authors, each of whom is deeply rooted in AI research, chaired by Michael Littman of Brown University. The report, entitled ""Gathering Strength, Gathering Storms,"" answers a set of 14 questions probing critical areas of AI development addressing the major risks and dangers of AI, its effects on society, its public perception and the future of the field. The report concludes that AI has made a major leap from the lab to people's lives in recent years, which increases the urgency to understand its potential negative effects. The questions were developed by the AI100 Standing Committee, chaired by Peter Stone of the University of Texas at Austin, consisting of a group of AI leaders with expertise in computer science, sociology, ethics, economics, and other disciplines. △ Less","27 October, 2022",https://arxiv.org/pdf/2210.15767
Solving the Schrodinger equation with genetic algorithms: a practical approach,Rafael Lahoz-Beltra,"The Schrodinger equation is one of the most important equations in physics and chemistry and can be solved in the simplest cases by computer numerical methods. Since the beginning of the 70s of the last century the computer began to be used to solve this equation in elementary quantum systems, e.g. and in the most complex case a hydrogen-like system. Obtaining the solution means finding the wave function, which allows predicting the physical and chemical properties of the quantum system. However, when a quantum system is more complex than a hydrogen-like system then we must be satisfied with an approximate solution of the equation. During the last decade the application of algorithms and principles of quantum computation in disciplines other than physics and chemistry, such as biology and artificial intelligence, has led to the search for alternative techniques with which to obtain approximate solutions of the Schrodinger equation. In this paper, we review and illustrate the application of genetic algorithms, i.e. stochastic optimization procedures inspired by Darwinian evolution, in elementary quantum systems and in quantum models of artificial intelligence. In this last field, we illustrate with two toy models how to solve the Schrodinger equation in an elementary model of a quantum neuron and in the synthesis of quantum circuits controlling the behavior of a Braitenberg vehicle. △ Less","27 October, 2022",https://arxiv.org/pdf/2210.15720
On the Efficiency of Ethics as a Governing Tool for Artificial Intelligence,Nicholas Kluge Corrêa;Nythamar De Oliveira;Diogo Massmann,"The 4th Industrial Revolution is the culmination of the digital age. Nowadays, technologies such as robotics, nanotechnology, genetics, and artificial intelligence promise to transform our world and the way we live. Artificial Intelligence Ethics and Safety is an emerging research field that has been gaining popularity in recent years. Several private, public and non-governmental organizations have published guidelines proposing ethical principles for regulating the use and development of autonomous intelligent systems. Meta-analyses of the AI Ethics research field point to convergence on certain principles that supposedly govern the AI industry. However, little is known about the effectiveness of this form of Ethics. In this paper, we would like to conduct a critical analysis of the current state of AI Ethics and suggest that this form of governance based on principled ethical guidelines is not sufficient to norm the AI industry and its developers. We believe that drastic changes are necessary, both in the training processes of professionals in the fields related to the development of software and intelligent systems and in the increased regulation of these professionals and their industry. To this end, we suggest that law should benefit from recent contributions from bioethics, to make the contributions of AI ethics to governance explicit in legal terms. △ Less","27 October, 2022",https://arxiv.org/pdf/2210.15289
Conversing with Copilot: Exploring Prompt Engineering for Solving CS1 Problems Using Natural Language,Paul Denny;Viraj Kumar;Nasser Giacaman,"GitHub Copilot is an artificial intelligence model for automatically generating source code from natural language problem descriptions. Since June 2022, Copilot has officially been available for free to all students as a plug-in to development environments like Visual Studio Code. Prior work exploring OpenAI Codex, the underlying model that powers Copilot, has shown it performs well on typical CS1 problems thus raising concerns about the impact it will have on how introductory programming courses are taught. However, little is known about the types of problems for which Copilot does not perform well, or about the natural language interactions that a student might have with Copilot when resolving errors. We explore these questions by evaluating the performance of Copilot on a publicly available dataset of 166 programming problems. We find that it successfully solves around half of these problems on its very first attempt, and that it solves 60\% of the remaining problems using only natural language changes to the problem description. We argue that this type of prompt engineering, which we believe will become a standard interaction between human and Copilot when it initially fails, is a potentially useful learning activity that promotes computational thinking skills, and is likely to change the nature of code writing skill development. △ Less","26 October, 2022",https://arxiv.org/pdf/2210.15157
Fully Automated Deep Learning-enabled Detection for Hepatic Steatosis on Computed Tomography: A Multicenter International Validation Study,Zhongyi Zhang;Guixia Li;Ziqiang Wang;Feng Xia;Ning Zhao;Huibin Nie;Zezhong Ye;Joshua Lin;Yiyi Hui;Xiangchun Liu,"Despite high global prevalence of hepatic steatosis, no automated diagnostics demonstrated generalizability in detecting steatosis on multiple international datasets. Traditionally, hepatic steatosis detection relies on clinicians selecting the region of interest (ROI) on computed tomography (CT) to measure liver attenuation. ROI selection demands time and expertise, and therefore is not routinely performed in populations. To automate the process, we validated an existing artificial intelligence (AI) system for 3D liver segmentation and used it to purpose a novel method: AI-ROI, which could automatically select the ROI for attenuation measurements. AI segmentation and AI-ROI method were evaluated on 1,014 non-contrast enhanced chest CT images from eight international datasets: LIDC-IDRI, NSCLC-Lung1, RIDER, VESSEL12, RICORD-1A, RICORD-1B, COVID-19-Italy, and COVID-19-China. AI segmentation achieved a mean dice coefficient of 0.957. Attenuations measured by AI-ROI showed no significant differences (p = 0.545) and a reduction of 71% time compared to expert measurements. The area under the curve (AUC) of the steatosis classification of AI-ROI is 0.921 (95% CI: 0.883 - 0.959). If performed as a routine screening method, our AI protocol could potentially allow early non-invasive, non-pharmacological preventative interventions for hepatic steatosis. 1,014 expert-annotated liver segmentations of patients with hepatic steatosis annotations can be downloaded here: https://drive.google.com/drive/folders/1-g_zJeAaZXYXGqL1OeF6pUjr6KB0igJX. △ Less","6 November, 2022",https://arxiv.org/pdf/2210.15149
Federated Continual Learning to Detect Accounting Anomalies in Financial Auditing,Marco Schreyer;Hamed Hemati;Damian Borth;Miklos A. Vasarhelyi,"The International Standards on Auditing require auditors to collect reasonable assurance that financial statements are free of material misstatement. At the same time, a central objective of Continuous Assurance is the real-time assessment of digital accounting journal entries. Recently, driven by the advances in artificial intelligence, Deep Learning techniques have emerged in financial auditing to examine vast quantities of accounting data. However, learning highly adaptive audit models in decentralised and dynamic settings remains challenging. It requires the study of data distribution shifts over multiple clients and time periods. In this work, we propose a Federated Continual Learning framework enabling auditors to learn audit models from decentral clients continuously. We evaluate the framework's ability to detect accounting anomalies in common scenarios of organizational activity. Our empirical results, using real-world datasets and combined federated continual learning strategies, demonstrate the learned model's ability to detect anomalies in audit settings of data distribution shifts. △ Less","26 October, 2022",https://arxiv.org/pdf/2210.15051
"ClipBot: an educational, physically impaired robot that learns to walk via genetic algorithm optimization",Diego Ulisse Pizzagalli;Ilaria Arini;Mauro Prevostini,"Educational robots allow experimenting with a variety of principles from mechanics, electronics, and informatics. Here we propose ClipBot, a low-cost, do-it-yourself, robot whose skeleton is made of two paper clips. An Arduino nano microcontroller actuates two servo motors that move the paper clips. However, such mechanical configuration confers physical impairments to movement. This creates the need for and allows experimenting with artificial intelligence methods to overcome hardware limitations. We report our experience in the usage of this robot during the study week 'fascinating informatics', organized by the Swiss Foundation Schweizer Jugend Forscht (www.sjf.ch). Students at the high school level were asked to implement a genetic algorithm to optimize the movements of the robot until it learned to walk. Such a methodology allowed the robot to learn the motor actuation scheme yielding straight movement in the forward direction using less than 20 iterations. △ Less","26 October, 2022",https://arxiv.org/pdf/2210.14703
"A Survey on Artificial Intelligence for Music Generation: Agents, Domains and Perspectives",Carlos Hernandez-Olivan;Javier Hernandez-Olivan;Jose R. Beltran,"Music is one of the Gardner's intelligences in his theory of multiple intelligences. How humans perceive and understand music is still being studied and is crucial to develop artificial intelligence models that imitate such processes. Music generation with Artificial Intelligence is an emerging field that is gaining much attention in the recent years. In this paper, we describe how humans compose music and how new AI systems could imitate such process by comparing past and recent advances in the field with music composition techniques. To understand how AI models and algorithms generate music and the potential applications that might appear in the future, we explore, analyze and describe the agents that take part of the music generation process: the datasets, models, interfaces, the users and the generated music. We mention possible applications that might benefit from this field and we also propose new trends and future research directions that could be explored in the future. △ Less","3 November, 2022",https://arxiv.org/pdf/2210.13944
Graded-Q Reinforcement Learning with Information-Enhanced State Encoder for Hierarchical Collaborative Multi-Vehicle Pursuit,Yiying Yang;Xinhang Li;Zheng Yuan;Qinwen Wang;Chen Xu;Lin Zhang,"The multi-vehicle pursuit (MVP), as a problem abstracted from various real-world scenarios, is becoming a hot research topic in Intelligent Transportation System (ITS). The combination of Artificial Intelligence (AI) and connected vehicles has greatly promoted the research development of MVP. However, existing works on MVP pay little attention to the importance of information exchange and cooperation among pursuing vehicles under the complex urban traffic environment. This paper proposed a graded-Q reinforcement learning with information-enhanced state encoder (GQRL-IESE) framework to address this hierarchical collaborative multi-vehicle pursuit (HCMVP) problem. In the GQRL-IESE, a cooperative graded Q scheme is proposed to facilitate the decision-making of pursuing vehicles to improve pursuing efficiency. Each pursuing vehicle further uses a deep Q network (DQN) to make decisions based on its encoded state. A coordinated Q optimizing network adjusts the individual decisions based on the current environment traffic information to obtain the global optimal action set. In addition, an information-enhanced state encoder is designed to extract critical information from multiple perspectives and uses the attention mechanism to assist each pursuing vehicle in effectively determining the target. Extensive experimental results based on SUMO indicate that the total timestep of the proposed GQRL-IESE is less than other methods on average by 47.64%, which demonstrates the excellent pursuing efficiency of the GQRL-IESE. Codes are outsourced in https://github.com/ANT-ITS/GQRL-IESE. △ Less","24 October, 2022",https://arxiv.org/pdf/2210.13470
Artificial Intelligence-Based Methods for Fusion of Electronic Health Records and Imaging Data,Farida Mohsen;Hazrat Ali;Nady El Hajj;Zubair Shah,"Healthcare data are inherently multimodal, including electronic health records (EHR), medical images, and multi-omics data. Combining these multimodal data sources contributes to a better understanding of human health and provides optimal personalized healthcare. Advances in artificial intelligence (AI) technologies, particularly machine learning (ML), enable the fusion of these different data modalities to provide multimodal insights. To this end, in this scoping review, we focus on synthesizing and analyzing the literature that uses AI techniques to fuse multimodal medical data for different clinical applications. More specifically, we focus on studies that only fused EHR with medical imaging data to develop various AI methods for clinical applications. We present a comprehensive analysis of the various fusion strategies, the diseases and clinical outcomes for which multimodal fusion was used, the ML algorithms used to perform multimodal fusion for each clinical application, and the available multimodal medical datasets. We followed the PRISMA-ScR guidelines. We searched Embase, PubMed, Scopus, and Google Scholar to retrieve relevant studies. We extracted data from 34 studies that fulfilled the inclusion criteria. In our analysis, a typical workflow was observed: feeding raw data, fusing different data modalities by applying conventional machine learning (ML) or deep learning (DL) algorithms, and finally, evaluating the multimodal fusion through clinical outcome predictions. Specifically, early fusion was the most used technique in most applications for multimodal learning (22 out of 34 studies). We found that multimodality fusion models outperformed traditional single-modality models for the same task. Disease diagnosis and prediction were the most common clinical outcomes (reported in 20 and 10 studies, respectively) from a clinical outcome perspective. △ Less","23 October, 2022",https://arxiv.org/pdf/2210.13462
Secure and Trustworthy Artificial Intelligence-Extended Reality (AI-XR) for Metaverses,Adnan Qayyum;Muhammad Atif Butt;Hassan Ali;Muhammad Usman;Osama Halabi;Ala Al-Fuqaha;Qammer H. Abbasi;Muhammad Ali Imran;Junaid Qadir,"Metaverse is expected to emerge as a new paradigm for the next-generation Internet, providing fully immersive and personalised experiences to socialize, work, and play in self-sustaining and hyper-spatio-temporal virtual world(s). The advancements in different technologies like augmented reality, virtual reality, extended reality (XR), artificial intelligence (AI), and 5G/6G communication will be the key enablers behind the realization of AI-XR metaverse applications. While AI itself has many potential applications in the aforementioned technologies (e.g., avatar generation, network optimization, etc.), ensuring the security of AI in critical applications like AI-XR metaverse applications is profoundly crucial to avoid undesirable actions that could undermine users' privacy and safety, consequently putting their lives in danger. To this end, we attempt to analyze the security, privacy, and trustworthiness aspects associated with the use of various AI techniques in AI-XR metaverse applications. Specifically, we discuss numerous such challenges and present a taxonomy of potential solutions that could be leveraged to develop secure, private, robust, and trustworthy AI-XR applications. To highlight the real implications of AI-associated adversarial threats, we designed a metaverse-specific case study and analyzed it through the adversarial lens. Finally, we elaborate upon various open issues that require further research interest from the community. △ Less","24 October, 2022",https://arxiv.org/pdf/2210.13289
GeoAI at ACM SIGSPATIAL: The New Frontier of Geospatial Artificial Intelligence Research,Dalton Lunga;Yingjie Hu;Shawn Newsam;Song Gao;Bruno Martins;Lexie Yang;Xueqing Deng,"Geospatial Artificial Intelligence (GeoAI) is an interdisciplinary field enjoying tremendous adoption. However, the efficient design and implementation of GeoAI systems face many open challenges. This is mainly due to the lack of non-standardized approaches to artificial intelligence tool development, inadequate platforms, and a lack of multidisciplinary engagements, which all motivate domain experts to seek a shared stage with scientists and engineers to solve problems of significant impact on society. Since its inception in 2017, the GeoAI series of workshops has been co-located with the Association for Computing Machinery International Conference on Advances in Geographic Information Systems. The workshop series has fostered a nexus for geoscientists, computer scientists, engineers, entrepreneurs, and decision-makers, from academia, industry, and government to engage in artificial intelligence, spatiotemporal data computing, and geospatial data science research, motivated by various challenges. In this article, we revisit and discuss the state of GeoAI open research directions, the recent developments, and an emerging agenda calling for a continued cross-disciplinary community engagement. △ Less","20 October, 2022",https://arxiv.org/pdf/2210.13207
Investigating the detection of Tortured Phrases in Scientific Literature,Puthineath Lay;Martin Lentschat;Cyril Labbé,"With the help of online tools, unscrupulous authors can today generate a pseudo-scientific article and attempt to publish it. Some of these tools work by replacing or paraphrasing existing texts to produce new content, but they have a tendency to generate nonsensical expressions. A recent study introduced the concept of 'tortured phrase', an unexpected odd phrase that appears instead of the fixed expression. E.g. counterfeit consciousness instead of artificial intelligence. The present study aims at investigating how tortured phrases, that are not yet listed, can be detected automatically. We conducted several experiments, including non-neural binary classification, neural binary classification and cosine similarity comparison of the phrase tokens, yielding noticeable results. △ Less","24 October, 2022",https://arxiv.org/pdf/2210.13024
"Deep Edge Intelligence: Architecture, Key Features, Enabling Technologies and Challenges",Prabath Abeysekara;Hai Dong;A. K. Qin,"With the breakthroughs in Deep Learning, recent years have witnessed a massive surge in Artificial Intelligence applications and services. Meanwhile, the rapid advances in Mobile Computing and Internet of Things has also given rise to billions of mobile and smart sensing devices connected to the Internet, generating zettabytes of data at the network edge. The opportunity to combine these two domains of technologies to power interconnected devices with intelligence is likely to pave the way for a new wave of technology revolutions. Embracing this technology revolution, in this article, we present a novel computing vision named Deep Edge Intelligence (DEI). DEI employs Deep Learning, Artificial Intelligence, Cloud and Edge Computing, 5G/6G networks, Internet of Things, Microservices, etc. aiming to provision reliable and secure intelligence services to every person and organisation at any place with better user experience. The vision, system architecture, key layers and features of DEI are also detailed. Finally, we reveal the key enabling technologies and research challenges associated with it. △ Less","24 October, 2022",https://arxiv.org/pdf/2210.12944
Symmetry and Variance: Generative Parametric Modelling of Historical Brick Wall Patterns,Sevgi Altun;Mustafa Cem Gunes;Yusuf H. Sahin;Alican Mertan;Gozde Unal;Mine Ozkar,"This study integrates artificial intelligence and computational design tools to extract information from architectural heritage. Photogrammetry-based point cloud models of brick walls from the Anatolian Seljuk period are analysed in terms of the interrelated units of construction, simultaneously considering both the inherent symmetries and irregularities. The real-world data is used as input for acquiring the stochastic parameters of spatial relations and a set of parametric shape rules to recreate designs of existing and hypothetical brick walls within the style. The motivation is to be able to generate large data sets for machine learning of the style and to devise procedures for robotic production of such designs with repetitive units. △ Less","23 October, 2022",https://arxiv.org/pdf/2210.12856
A Temporal Type-2 Fuzzy System for Time-dependent Explainable Artificial Intelligence,Mehrin Kiani;Javier Andreu-Perez;Hani Hagras,"Explainable Artificial Intelligence (XAI) is a paradigm that delivers transparent models and decisions, which are easy to understand, analyze, and augment by a non-technical audience. Fuzzy Logic Systems (FLS) based XAI can provide an explainable framework, while also modeling uncertainties present in real-world environments, which renders it suitable for applications where explainability is a requirement. However, most real-life processes are not characterized by high levels of uncertainties alone; they are inherently time-dependent as well, i.e., the processes change with time. In this work, we present novel Temporal Type-2 FLS Based Approach for time-dependent XAI (TXAI) systems, which can account for the likelihood of a measurement's occurrence in the time domain using (the measurement's) frequency of occurrence. In Temporal Type-2 Fuzzy Sets (TT2FSs), a four-dimensional (4D) time-dependent membership function is developed where relations are used to construct the inter-relations between the elements of the universe of discourse and its frequency of occurrence. The TXAI system manifested better classification prowess, with 10-fold test datasets, with a mean recall of 95.40\% than a standard XAI system (based on non-temporal general type-2 (GT2) fuzzy sets) that had a mean recall of 87.04\%. TXAI also performed significantly better than most non-explainable AI systems between 3.95\%, to 19.04\% improvement gain in mean recall. In addition, TXAI can also outline the most likely time-dependent trajectories using the frequency of occurrence values embedded in the TXAI model; viz. given a rule at a determined time interval, what will be the next most likely rule at a subsequent time interval. In this regard, the proposed TXAI system can have profound implications for delineating the evolution of real-life time-dependent processes, such as behavioural or biological processes. △ Less","22 October, 2022",https://arxiv.org/pdf/2210.12571
Estimating oil and gas recovery factors via machine learning: Database-dependent accuracy and reliability,Alireza Roustazadeh;Behzad Ghanbarian;Mohammad B. Shadmand;Vahid Taslimitehrani;Larry W. Lake,"With recent advances in artificial intelligence, machine learning (ML) approaches have become an attractive tool in petroleum engineering, particularly for reservoir characterizations. A key reservoir property is hydrocarbon recovery factor (RF) whose accurate estimation would provide decisive insights to drilling and production strategies. Therefore, this study aims to estimate the hydrocarbon RF for exploration from various reservoir characteristics, such as porosity, permeability, pressure, and water saturation via the ML. We applied three regression-based models including the extreme gradient boosting (XGBoost), support vector machine (SVM), and stepwise multiple linear regression (MLR) and various combinations of three databases to construct ML models and estimate the oil and/or gas RF. Using two databases and the cross-validation method, we evaluated the performance of the ML models. In each iteration 90 and 10% of the data were respectively used to train and test the models. The third independent database was then used to further assess the constructed models. For both oil and gas RFs, we found that the XGBoost model estimated the RF for the train and test datasets more accurately than the SVM and MLR models. However, the performance of all the models were unsatisfactory for the independent databases. Results demonstrated that the ML algorithms were highly dependent and sensitive to the databases based on which they were trained. Statistical tests revealed that such unsatisfactory performances were because the distributions of input features and target variables in the train datasets were significantly different from those in the independent databases (p-value < 0.05). △ Less","22 October, 2022",https://arxiv.org/pdf/2210.12491
AI-based Arabic Language and Speech Tutor,Sicong Shao;Saleem Alharir;Salim Hariri;Pratik Satam;Sonia Shiri;Abdessamad Mbarki,"In the past decade, we have observed a growing interest in using technologies such as artificial intelligence (AI), machine learning, and chatbots to provide assistance to language learners, especially in second language learning. By using AI and natural language processing (NLP) and chatbots, we can create an intelligent self-learning environment that goes beyond multiple-choice questions and/or fill in the blank exercises. In addition, NLP allows for learning to be adaptive in that it offers more than an indication that an error has occurred. It also provides a description of the error, uses linguistic analysis to isolate the source of the error, and then suggests additional drills to achieve optimal individualized learning outcomes. In this paper, we present our approach for developing an Artificial Intelligence-based Arabic Language and Speech Tutor (AI-ALST) for teaching the Moroccan Arabic dialect. The AI-ALST system is an intelligent tutor that provides analysis and assessment of students learning the Moroccan dialect at University of Arizona (UA). The AI-ALST provides a self-learned environment to practice each lesson for pronunciation training. In this paper, we present our initial experimental evaluation of the AI-ALST that is based on MFCC (Mel frequency cepstrum coefficient) feature extraction, bidirectional LSTM (Long Short-Term Memory), attention mechanism, and a cost-based strategy for dealing with class-imbalance learning. We evaluated our tutor on the word pronunciation of lesson 1 of the Moroccan Arabic dialect class. The experimental results show that the AI-ALST can effectively and successfully detect pronunciation errors and evaluate its performance by using F_1-score, accuracy, precision, and recall. △ Less","22 October, 2022",https://arxiv.org/pdf/2210.12346
Feature selection intelligent algorithm with mutual information and steepest ascent strategy,Elkebir Sarhrouni;Ahmed Hammouch;Driss Aboutajdine,"Remote sensing is a higher technology to produce knowledge for data mining applications. In principle hyperspectral images (HSIs) is a remote sensing tool that provides precise classification of regions. The HSI contains more than a hundred of images of the ground truth (GT) map. Some images are carrying relevant information, but others describe redundant information, or they are affected by atmospheric noise. The aim is to reduce dimensionality of HSI. Many studies use mutual information (MI) or normalised forms of MI to select appropriate bands. In this paper we design an algorithm based also on MI, and we combine MI with steepest ascent algorithm, to improve a symmetric uncertainty coefficient-based strategy to select relevant bands for classification of HSI. This algorithm is a feature selection tool and a wrapper strategy. We perform our study on HSI AVIRIS 92AV3C. This is an artificial intelligent system to control redundancy; we had to clear the difference of the result's algorithm and the human decision, and this can be viewed as case study which human decision is perhaps different to an intelligent algorithm. Index Terms - Hyperspectral images, Classification, Fea-ture selection, Mutual Information, Redundancy, Steepest Ascent. Artificial Intelligence △ Less","21 October, 2022",https://arxiv.org/pdf/2210.12296
Benchmarking GPU and TPU Performance with Graph Neural Networks,xiangyang Ju;Yunsong Wang;Daniel Murnane;Nicholas Choma;Steven Farrell;Paolo Calafiura,"Many artificial intelligence (AI) devices have been developed to accelerate the training and inference of neural networks models. The most common ones are the Graphics Processing Unit (GPU) and Tensor Processing Unit (TPU). They are highly optimized for dense data representations. However, sparse representations such as graphs are prevalent in many domains, including science. It is therefore important to characterize the performance of available AI accelerators on sparse data. This work analyzes and compares the GPU and TPU performance training a Graph Neural Network (GNN) developed to solve a real-life pattern recognition problem. Characterizing the new class of models acting on sparse data may prove helpful in optimizing the design of deep learning libraries and future AI accelerators. △ Less","21 October, 2022",https://arxiv.org/pdf/2210.12247
EDUKG: a Heterogeneous Sustainable K-12 Educational Knowledge Graph,Bowen Zhao;Jiuding Sun;Bin Xu;Xingyu Lu;Yuchen Li;Jifan Yu;Minghui Liu;Tingjian Zhang;Qiuyang Chen;Hanming Li;Lei Hou;Juanzi Li,"Web and artificial intelligence technologies, especially semantic web and knowledge graph (KG), have recently raised significant attention in educational scenarios. Nevertheless, subject-specific KGs for K-12 education still lack sufficiency and sustainability from knowledge and data perspectives. To tackle these issues, we propose EDUKG, a heterogeneous sustainable K-12 Educational Knowledge Graph. We first design an interdisciplinary and fine-grained ontology for uniformly modeling knowledge and resource in K-12 education, where we define 635 classes, 445 object properties, and 1314 datatype properties in total. Guided by this ontology, we propose a flexible methodology for interactively extracting factual knowledge from textbooks. Furthermore, we establish a general mechanism based on our proposed generalized entity linking system for EDUKG's sustainable maintenance, which can dynamically index numerous heterogeneous resources and data with knowledge topics in EDUKG. We further evaluate EDUKG to illustrate its sufficiency, richness, and variability. We publish EDUKG with more than 252 million entities and 3.86 billion triplets. Our code and data repository is now available at https://github.com/THU-KEG/EDUKG. △ Less","21 October, 2022",https://arxiv.org/pdf/2210.12228
High-Fidelity Visual Structural Inspections through Transformers and Learnable Resizers,Kareem Eltouny;Seyedomid Sajedi;Xiao Liang,"Visual inspection is the predominant technique for evaluating the condition of civil infrastructure. The recent advances in unmanned aerial vehicles (UAVs) and artificial intelligence have made the visual inspections faster, safer, and more reliable. Camera-equipped UAVs are becoming the new standard in the industry by collecting massive amounts of visual data for human inspectors. Meanwhile, there has been significant research on autonomous visual inspections using deep learning algorithms, including semantic segmentation. While UAVs can capture high-resolution images of buildings' façades, high-resolution segmentation is extremely challenging due to the high computational memory demands. Typically, images are uniformly downsized at the price of losing fine local details. Contrarily, breaking the images into multiple smaller patches can cause a loss of global contextual in-formation. We propose a hybrid strategy that can adapt to different inspections tasks by managing the global and local semantics trade-off. The framework comprises a compound, high-resolution deep learning architecture equipped with an attention-based segmentation model and learnable downsampler-upsampler modules designed for optimal efficiency and in-formation retention. The framework also utilizes vision transformers on a grid of image crops aiming for high precision learning without downsizing. An augmented inference technique is used to boost the performance and re-duce the possible loss of context due to grid cropping. Comprehensive experiments have been performed on 3D physics-based graphics models synthetic environments in the Quake City dataset. The proposed framework is evaluated using several metrics on three segmentation tasks: component type, component damage state, and global damage (crack, rebar, spalling). △ Less","21 October, 2022",https://arxiv.org/pdf/2210.12175
Grid cells and their potential application in AI,Jason Toy,"Since their Nobel Prize winning discovery in 2005, grid cells have been studied extensively by neuroscientists. Their multi-scale periodic firing rates tiling the environment as the animal moves around has been shown as critical for path integration. Multiple experiments have shown that grid cells also fire for other representations such as olfactory, attention mechanisms, imagined movement, and concept organization potentially acting as a form of neural recycling and showing the possible brain mechanism for cognitive maps that Tolman envisioned in 1948. Grid cell integration into artificial neural networks may enable more robust, generalized, and smarter computers. In this paper we give an overview of grid cell research since their discovery, their role in neuroscience and cognitive science, and possible future directions of artificial intelligence research. △ Less","12 October, 2022",https://arxiv.org/pdf/2210.12068
AI-HRI Brings New Dimensions to Human-Aware Design for Human-Aware AI,Richard G. Freedman,"Since the first AI-HRI held at the 2014 AAAI Fall Symposium Series, a lot of the presented research and discussions have emphasized how artificial intelligence (AI) developments can benefit human-robot interaction (HRI). This portrays HRI as an application, a source of domain-specific problems to solve, to the AI community. Likewise, this portrays AI as a tool, a source of solutions available for relevant problems, to the HRI community. However, members of the AI-HRI research community will point out that the relationship has a deeper synergy than matchmaking problems and solutions -- there are insights from each field that impact how the other one thinks about the world and performs scientific research. There is no greater opportunity for sharing perspectives at the moment than human-aware AI, which studies how to account for the fact that people are more than a source of data or part of an algorithm. We will explore how AI-HRI can change the way researchers think about human-aware AI, from observation through validation, to make even the algorithmic design process human-aware. △ Less","21 October, 2022",https://arxiv.org/pdf/2210.11832
ProSky: NEAT Meets NOMA-mmWave in the Sky of 6G,Ahmed Benfaid;Nadia Adem;Abdurrahman Elmaghbub,"Rendering to their abilities to provide ubiquitous connectivity, flexibly and cost effectively, unmanned aerial vehicles (UAVs) have been getting more and more research attention. To take the UAVs' performance to the next level, however, they need to be merged with some other technologies like non-orthogonal multiple access (NOMA) and millimeter wave (mmWave), which both promise high spectral efficiency (SE). As managing UAVs efficiently may not be possible using model-based techniques, another key innovative technology that UAVs will inevitably need to leverage is artificial intelligence (AI). Designing an AI-based technique that adaptively allocates radio resources and places UAVs in 3D space to meet certain communication objectives, however, is a tough row to hoe. In this paper, we propose a neuroevolution of augmenting topologies NEAT framework, referred to as ProSky, to manage NOMA-mmWave-UAV networks. ProSky exhibits a remarkable performance improvement over a model-based method. Moreover, ProSky learns 5.3 times faster than and outperforms, in both SE and energy efficiency EE while being reasonably fair, a deep reinforcement learning DRL based scheme. The ProSky source code is accessible to use here: https://github.com/Fouzibenfaid/ProSky △ Less","12 October, 2022",https://arxiv.org/pdf/2210.11406
OCR-VQGAN: Taming Text-within-Image Generation,Juan A. Rodriguez;David Vazquez;Issam Laradji;Marco Pedersoli;Pau Rodriguez,"Synthetic image generation has recently experienced significant improvements in domains such as natural image or art generation. However, the problem of figure and diagram generation remains unexplored. A challenging aspect of generating figures and diagrams is effectively rendering readable texts within the images. To alleviate this problem, we present OCR-VQGAN, an image encoder, and decoder that leverages OCR pre-trained features to optimize a text perceptual loss, encouraging the architecture to preserve high-fidelity text and diagram structure. To explore our approach, we introduce the Paper2Fig100k dataset, with over 100k images of figures and texts from research papers. The figures show architecture diagrams and methodologies of articles available at arXiv.org from fields like artificial intelligence and computer vision. Figures usually include text and discrete objects, e.g., boxes in a diagram, with lines and arrows that connect them. We demonstrate the effectiveness of OCR-VQGAN by conducting several experiments on the task of figure reconstruction. Additionally, we explore the qualitative and quantitative impact of weighting different perceptual metrics in the overall loss function. We release code, models, and dataset at https://github.com/joanrod/ocr-vqgan. △ Less","21 October, 2022",https://arxiv.org/pdf/2210.11248
The State-of-the-Art in AI-Based Malware Detection Techniques: A Review,Adam Wolsey,"Artificial Intelligence techniques have evolved rapidly in recent years, revolutionising the approaches used to fight against cybercriminals. But as the cyber security field has progressed, so has malware development, making it an economic imperative to strengthen businesses' defensive capability against malware attacks. This review aims to outline the state-of-the-art AI techniques used in malware detection and prevention, providing an in-depth analysis of the latest studies in this field. The algorithms investigated consist of Shallow Learning, Deep Learning and Bio-Inspired Computing, applied to a variety of platforms, such as PC, cloud, Android and IoT. This survey also touches on the rapid adoption of AI by cybercriminals as a means to create ever more advanced malware and exploit the AI algorithms designed to defend against them. △ Less","12 October, 2022",https://arxiv.org/pdf/2210.11239
"Robótica Móvel e Inteligência Artificial para Investigação, Competição e Automatização de Sistemas Industriais",Hiago Jacobs Sodre Pereira;Pablo Ezequiel Moraes;André Da Silva Kelbouscas;Ricardo Grando,"The implementation of robots to enhance some processes has become popular in recent years due to the accelerated way of production in some factories. Within this context was where robotics has emerged, firstly with stationary robots and more recently mobile robots, namely aerial and terrestrial robots. They can be used for delimited processes within a function, mainly the stationary robots, but also for research in wider areas and even competition. This work summarizes the construction of a model of terrestrial mobile robot that makes the use of artificial intelligence for the purpose of research and competitions, all of that with the basic sensing that can be used in industry. △ Less","20 October, 2022",https://arxiv.org/pdf/2210.11195
From Modelling to Understanding Children's Behaviour in the Context of Robotics and Social Artificial Intelligence,Serge Thill;Vicky Charisi;Tony Belpaeme;Ana Paiva,"Understanding and modelling children's cognitive processes and their behaviour in the context of their interaction with robots and social artificial intelligence systems is a fundamental prerequisite for meaningful and effective robot interventions. However, children's development involve complex faculties such as exploration, creativity and curiosity which are challenging to model. Also, often children express themselves in a playful way which is different from a typical adult behaviour. Different children also have different needs, and it remains a challenge in the current state of the art that those of neurodiverse children are under-addressed. With this workshop, we aim to promote a common ground among different disciplines such as developmental sciences, artificial intelligence and social robotics and discuss cutting-edge research in the area of user modelling and adaptive systems for children. △ Less","20 October, 2022",https://arxiv.org/pdf/2210.11161
Co-Training an Observer and an Evading Target,André Brandenburger;Folker Hoffmann;Alexander Charlish,"Reinforcement learning (RL) is already widely applied to applications such as robotics, but it is only sparsely used in sensor management. In this paper, we apply the popular Proximal Policy Optimization (PPO) approach to a multi-agent UAV tracking scenario. While recorded data of real scenarios can accurately reflect the real world, the required amount of data is not always available. Simulation data, however, is typically cheap to generate, but the utilized target behavior is often naive and only vaguely represents the real world. In this paper, we utilize multi-agent RL to jointly generate protagonistic and antagonistic policies and overcome the data generation problem, as the policies are generated on-the-fly and adapt continuously. This way, we are able to clearly outperform baseline methods and robustly generate competitive policies. In addition, we investigate explainable artificial intelligence (XAI) by interpreting feature saliency and generating an easy-to-read decision tree as a simplified policy. △ Less","20 October, 2022",https://arxiv.org/pdf/2210.11126
Palm up: Playing in the Latent Manifold for Unsupervised Pretraining,Hao Liu;Tom Zahavy;Volodymyr Mnih;Satinder Singh,"Large and diverse datasets have been the cornerstones of many impressive advancements in artificial intelligence. Intelligent creatures, however, learn by interacting with the environment, which changes the input sensory signals and the state of the environment. In this work, we aim to bring the best of both worlds and propose an algorithm that exhibits an exploratory behavior whilst it utilizes large diverse datasets. Our key idea is to leverage deep generative models that are pretrained on static datasets and introduce a dynamic model in the latent space. The transition dynamics simply mixes an action and a random sampled latent. It then applies an exponential moving average for temporal persistency, the resulting latent is decoded to image using pretrained generator. We then employ an unsupervised reinforcement learning algorithm to explore in this environment and perform unsupervised representation learning on the collected data. We further leverage the temporal information of this data to pair data points as a natural supervision for representation learning. Our experiments suggest that the learned representations can be successfully transferred to downstream tasks in both vision and reinforcement learning domains. △ Less","21 October, 2022",https://arxiv.org/pdf/2210.10913
Review of the state of the art in autonomous artificial intelligence,Petar Radanliev;David De Roure,"This article presents a new design for autonomous artificial intelligence (AI), based on the state-of-the-art algorithms, and describes a new autonomous AI system called AutoAI. The methodology is used to assemble the design founded on self-improved algorithms that use new and emerging sources of data (NEFD). The objective of the article is to conceptualise the design of a novel AutoAI algorithm. The conceptual approach is used to advance into building new and improved algorithms. The article integrates and consolidates the findings from existing literature and advances the AutoAI design into (1) using new and emerging sources of data for teaching and training AI algorithms and (2) enabling AI algorithms to use automated tools for training new and improved algorithms. This approach is going beyond the state-of-the-art in AI algorithms and suggests a design that enables autonomous algorithms to self-optimise and self-adapt, and on a higher level, be capable to self-procreate. △ Less","17 October, 2022",https://arxiv.org/pdf/2210.10659
RLM-Tracking: Online Multi-Pedestrian Tracking Supported by Relative Location Mapping,Kai Ren;Chuanping Hu,"The problem of multi-object tracking is a fundamental computer vision research focus, widely used in public safety, transport, autonomous vehicles, robotics, and other regions involving artificial intelligence. Because of the complexity of natural scenes, object occlusion and semi-occlusion usually occur in fundamental tracking tasks. These can easily lead to ID switching, object loss, detect errors, and misaligned limitation boxes. These conditions have a significant impact on the precision of multi-object tracking. In this paper, we design a new multi-object tracker for the above issues that contains an object \textbf{Relative Location Mapping} (RLM) model and \textbf{Target Region Density} (TRD) model. The new tracker is more sensitive to the differences in position relationships between objects. It can introduce low-score detection frames into different regions in real-time according to the density of object regions in the video. This improves the accuracy of object tracking without consuming extensive arithmetic resources. Our study shows that the proposed model has considerably enhanced the HOTA and DF1 measurements on the MOT17 and MOT20 data sets when applied to the advanced MOT method. △ Less","19 October, 2022",https://arxiv.org/pdf/2210.10477
ELASTIC: Numerical Reasoning with Adaptive Symbolic Compiler,Jiaxin Zhang;Yashar Moshfeghi,"Numerical reasoning over text is a challenging task of Artificial Intelligence (AI), requiring reading comprehension and numerical reasoning abilities. Previous approaches use numerical reasoning programs to represent the reasoning process. However, most works do not separate the generation of operators and operands, which are key components of a numerical reasoning program, thus limiting their ability to generate such programs for complicated tasks. In this paper, we introduce the numEricaL reASoning with adapTive symbolIc Compiler (ELASTIC) model, which is constituted of the RoBERTa as the Encoder and a Compiler with four modules: Reasoning Manager, Operator Generator, Operands Generator, and Memory Register. ELASTIC is robust when conducting complicated reasoning. Also, it is domain agnostic by supporting the expansion of diverse operators without caring about the number of operands it contains. Experiments show that ELASTIC achieves 68.96 and 65.21 of execution accuracy and program accuracy on the FinQA dataset and 83.00 program accuracy on the MathQA dataset, outperforming previous state-of-the-art models significantly. △ Less","20 October, 2022",https://arxiv.org/pdf/2210.10105
Vision Paper: Causal Inference for Interpretable and Robust Machine Learning in Mobility Analysis,Yanan Xin;Natasa Tagasovska;Fernando Perez-Cruz;Martin Raubal,"Artificial intelligence (AI) is revolutionizing many areas of our lives, leading a new era of technological advancement. Particularly, the transportation sector would benefit from the progress in AI and advance the development of intelligent transportation systems. Building intelligent transportation systems requires an intricate combination of artificial intelligence and mobility analysis. The past few years have seen rapid development in transportation applications using advanced deep neural networks. However, such deep neural networks are difficult to interpret and lack robustness, which slows the deployment of these AI-powered algorithms in practice. To improve their usability, increasing research efforts have been devoted to developing interpretable and robust machine learning methods, among which the causal inference approach recently gained traction as it provides interpretable and actionable information. Moreover, most of these methods are developed for image or sequential data which do not satisfy specific requirements of mobility data analysis. This vision paper emphasizes research challenges in deep learning-based mobility analysis that require interpretability and robustness, summarizes recent developments in using causal inference for improving the interpretability and robustness of machine learning methods, and highlights opportunities in developing causally-enabled machine learning models tailored for mobility analysis. This research direction will make AI in the transportation sector more interpretable and reliable, thus contributing to safer, more efficient, and more sustainable future transportation systems. △ Less","18 October, 2022",https://arxiv.org/pdf/2210.10010
Generalizing in the Real World with Representation Learning,Tegan Maharaj,"Machine learning (ML) formalizes the problem of getting computers to learn from experience as optimization of performance according to some metric(s) on a set of data examples. This is in contrast to requiring behaviour specified in advance (e.g. by hard-coded rules). Formalization of this problem has enabled great progress in many applications with large real-world impact, including translation, speech recognition, self-driving cars, and drug discovery. But practical instantiations of this formalism make many assumptions - for example, that data are i.i.d.: independent and identically distributed - whose soundness is seldom investigated. And in making great progress in such a short time, the field has developed many norms and ad-hoc standards, focused on a relatively small range of problem settings. As applications of ML, particularly in artificial intelligence (AI) systems, become more pervasive in the real world, we need to critically examine these assumptions, norms, and problem settings, as well as the methods that have become de-facto standards. There is much we still do not understand about how and why deep networks trained with stochastic gradient descent are able to generalize as well as they do, why they fail when they do, and how they will perform on out-of-distribution data. In this thesis I cover some of my work towards better understanding deep net generalization, identify several ways assumptions and problem settings fail to generalize to the real world, and propose ways to address those failures in practice. △ Less","18 October, 2022",https://arxiv.org/pdf/2210.09925
"Graphs, Constraints, and Search for the Abstraction and Reasoning Corpus",Yudong Xu;Elias B. Khalil;Scott Sanner,"The Abstraction and Reasoning Corpus (ARC) aims at benchmarking the performance of general artificial intelligence algorithms. The ARC's focus on broad generalization and few-shot learning has made it difficult to solve using pure machine learning. A more promising approach has been to perform program synthesis within an appropriately designed Domain Specific Language (DSL). However, these too have seen limited success. We propose Abstract Reasoning with Graph Abstractions (ARGA), a new object-centric framework that first represents images using graphs and then performs a search for a correct program in a DSL that is based on the abstracted graph space. The complexity of this combinatorial search is tamed through the use of constraint acquisition, state hashing, and Tabu search. An extensive set of experiments demonstrates the promise of ARGA in tackling some of the complicated object-centric tasks of the ARC rather efficiently, producing programs that are correct and easy to understand. △ Less","1 December, 2022",https://arxiv.org/pdf/2210.09880
"Near Real-time CO_2
Emissions Based on Carbon Satellite and Artificial Intelligence",Zhengwen Zhang;Jinjin Gu;Junhua Zhao;Jianwei Huang;Haifeng Wu,"To limit global warming to pre-industrial levels, global governments, industry and academia are taking aggressive efforts to reduce carbon emissions. The evaluation of anthropogenic carbon dioxide (CO_2) emissions, however, depends on the self-reporting information that is not always reliable. Society need to develop an objective, independent, and generalized system to meter CO_2 emissions. Satellite CO_2 observation from space that reports column-average regional CO_2 dry-air mole fractions has gradually indicated its potential to build such a system. Nevertheless, estimating anthropogenic CO_2 emissions from CO_2 observing satellite is bottlenecked by the influence of the highly complicated physical characteristics of atmospheric activities. Here we provide the first method that combines the advanced artificial intelligence (AI) techniques and the carbon satellite monitor to quantify anthropogenic CO_2 emissions. We propose an integral AI based pipeline that contains both a data retrieval algorithm and a two-step data-driven solution. First, the data retrieval algorithm can generate effective datasets from multi-modal data including carbon satellite, the information of carbon sources, and several environmental factors. Second, the two-step data-driven solution that applies the powerful representation of deep learning techniques to learn to quantify anthropogenic CO_2 emissions from satellite CO_2 observation with other factors. Our work unmasks the potential of quantifying CO_2 emissions based on the combination of deep learning algorithms and the carbon satellite monitor. △ Less","22 October, 2022",https://arxiv.org/pdf/2210.09850
Bridging the Gap between Artificial Intelligence and Artificial General Intelligence: A Ten Commandment Framework for Human-Like Intelligence,Ananta Nair;Farnoush Banaei-Kashani,"The field of artificial intelligence has seen explosive growth and exponential success. The last phase of development showcased deep learnings ability to solve a variety of difficult problems across a multitude of domains. Many of these networks met and exceeded human benchmarks by becoming experts in the domains in which they are trained. Though the successes of artificial intelligence have begun to overshadow its failures, there is still much that separates current artificial intelligence tools from becoming the exceptional general learners that humans are. In this paper, we identify the ten commandments upon which human intelligence is systematically and hierarchically built. We believe these commandments work collectively to serve as the essential ingredients that lead to the emergence of higher-order cognition and intelligence. This paper discusses a computational framework that could house these ten commandments and suggests new architectural modifications that could lead to the development of smarter, more explainable, and generalizable artificial systems inspired by a neuromorphic approach. △ Less","17 October, 2022",https://arxiv.org/pdf/2210.09366
Embodying the Glitch: Perspectives on Generative AI in Dance Practice,Benedikte Wallace;Charles P. Martin,"What role does the break from realism play in the potential for generative artificial intelligence as a creative tool? Through exploration of glitch, we examine the prospective value of these artefacts in creative practice. This paper describes findings from an exploration of AI-generated ""mistakes"" when using movement produced by a generative deep learning model as an inspiration source in dance composition. △ Less","5 October, 2022",https://arxiv.org/pdf/2210.09291
Artificial Intelligence Nomenclature Identified From Delphi Study on Key Issues Related to Trust and Barriers to Adoption for Autonomous Systems,Thomas E. Doyle;Victoria Tucci;Calvin Zhu;Yifei Zhang;Basem Yassa;Sajjad Rashidiani;Md Asif Khan;Reza Samavi;Michael Noseworthy;Steven Yule,"The rapid integration of artificial intelligence across traditional research domains has generated an amalgamation of nomenclature. As cross-discipline teams work together on complex machine learning challenges, finding a consensus of basic definitions in the literature is a more fundamental problem. As a step in the Delphi process to define issues with trust and barriers to the adoption of autonomous systems, our study first collected and ranked the top concerns from a panel of international experts from the fields of engineering, computer science, medicine, aerospace, and defence, with experience working with artificial intelligence. This document presents a summary of the literature definitions for nomenclature derived from expert feedback. △ Less","14 October, 2022",https://arxiv.org/pdf/2210.09086
Good AI for Good: How AI Strategies of the Nordic Countries Address the Sustainable Development Goals,Andreas Theodorou;Juan Carlos Nieves;Virginia Dignum,"Developed and used responsibly Artificial Intelligence (AI) is a force for global sustainable development. Given this opportunity, we expect that the many of the existing guidelines and recommendations for trustworthy or responsible AI will provide explicit guidance on how AI can contribute to the achievement of United Nations' Sustainable Development Goals (SDGs). This would in particular be the case for the AI strategies of the Nordic countries, at least given their high ranking and overall political focus when it comes to the achievement of the SDGs. In this paper, we present an analysis of existing AI recommendations from 10 different countries or organisations based on topic modelling techniques to identify how much these strategy documents refer to the SDGs. The analysis shows no significant difference on how much these documents refer to SDGs. Moreover, the Nordic countries are not different from the others albeit their long-term commitment to SDGs. More importantly, references to \textit{gender equality} (SDG 5) and \textit{inequality} (SDG 10), as well as references to environmental impact of AI development and use, and in particular the consequences for life on earth, are notably missing from the guidelines. △ Less","8 October, 2022",https://arxiv.org/pdf/2210.09010
A Symbolic Representation of Human Posture for Interpretable Learning and Reasoning,Richard G. Freedman;Joseph B. Mueller;Jack Ladwig;Steven Johnston;David McDonald;Helen Wauck;Ruta Wheelock;Hayley Borck,"Robots that interact with humans in a physical space or application need to think about the person's posture, which typically comes from visual sensors like cameras and infra-red. Artificial intelligence and machine learning algorithms use information from these sensors either directly or after some level of symbolic abstraction, and the latter usually partitions the range of observed values to discretize the continuous signal data. Although these representations have been effective in a variety of algorithms with respect to accuracy and task completion, the underlying models are rarely interpretable, which also makes their outputs more difficult to explain to people who request them. Instead of focusing on the possible sensor values that are familiar to a machine, we introduce a qualitative spatial reasoning approach that describes the human posture in terms that are more familiar to people. This paper explores the derivation of our symbolic representation at two levels of detail and its preliminary use as features for interpretable activity recognition. △ Less","23 October, 2022",https://arxiv.org/pdf/2210.08998
"Decentralized nation, solving the web identity crisis",Frederic Jumelle;Timothy Pagett;Ryan Lemand,"The web of today whether you prefer to call it web 2.0, web 3.0, web 5.0 or even the metaverse is at a critical stage of evolution and challenge, largely centered around its crisis of identity. Like teenagers who cannot assess properly their reason for being and do not seem ready to take responsibility for their actions, we are constantly blaming the very system we are trying to get away from. To truly realize the benefits from innovation and technology, this crisis has to be resolved, not just through tactical solutions but through developments that enhance the sustainability of the web and its benefits. Significant strides are being made in the evolution of digital services enabled by technology, regulation, and the sheer pace of societal change. The journey to the decentralized web is mirroring the convergence of the physical and digital worlds across all economies and is increasingly embracing the digital native world. Technology has provided the foundational platform for individuals and entities to create and manage wealth, potentially without the need for big institutions. Ironically, despite all of the advancements, we are still facing an unprecedented and increasing wealth gap. Clearly, the system is broken, not just around the edges but at the very core of the democratic underpinning of our society. In this whitepaper, we propose how artificial intelligence on blockchain can be used to generate a new class of identity through direct human computer interaction. We demonstrate how this, combined with new perspectives for sustaining community and governance embedded within the use of blockchain technology, will underpin a sustainable solution to protect identity, authorship and privacy at the same time while contributing to restore trust amongst members of a future decentralized nation and hence contribute to solving the web most significant identity crisis. △ Less","2 October, 2022",https://arxiv.org/pdf/2210.08978
Coordinated Science Laboratory 70th Anniversary Symposium: The Future of Computing,Klara Nahrstedt;Naresh Shanbhag;Vikram Adve;Nancy Amato;Romit Roy Choudhury;Carl Gunter;Nam Sung Kim;Olgica Milenkovic;Sayan Mitra;Lav Varshney;Yurii Vlasov;Sarita Adve;Rashid Bashir;Andreas Cangellaris;James DiCarlo;Katie Driggs-Campbell;Nick Feamster;Mattia Gazzola;Karrie Karahalios;Sanmi Koyejo;Paul Kwiat;Bo Li;Negar Mehr;Ravish Mehra;Andrew Miller,"In 2021, the Coordinated Science Laboratory CSL, an Interdisciplinary Research Unit at the University of Illinois Urbana-Champaign, hosted the Future of Computing Symposium to celebrate its 70th anniversary. CSL's research covers the full computing stack, computing's impact on society and the resulting need for social responsibility. In this white paper, we summarize the major technological points, insights, and directions that speakers brought forward during the Future of Computing Symposium. Participants discussed topics related to new computing paradigms, technologies, algorithms, behaviors, and research challenges to be expected in the future. The symposium focused on new computing paradigms that are going beyond traditional computing and the research needed to support their realization. These needs included stressing security and privacy, the end to end human cyber physical systems and with them the analysis of the end to end artificial intelligence needs. Furthermore, advances that enable immersive environments for users, the boundaries between humans and machines will blur and become seamless. Particular integration challenges were made clear in the final discussion on the integration of autonomous driving, robo taxis, pedestrians, and future cities. Innovative approaches were outlined to motivate the next generation of researchers to work on these challenges. The discussion brought out the importance of considering not just individual research areas, but innovations at the intersections between computing research efforts and relevant application domains, such as health care, transportation, energy systems, and manufacturing. △ Less","4 October, 2022",https://arxiv.org/pdf/2210.08974
Artificial Intelligence and Innovation to Reduce the Impact of Extreme Weather Events on Sustainable Production,Derrick Effah;Chunguang Bai;Matthew Quayson,"Frequent occurrences of extreme weather events substantially impact the lives of the less privileged in our societies, particularly in agriculture-inclined economies. The unpredictability of extreme fires, floods, drought, cyclones, and others endangers sustainable production and life on land (SDG goal 15), which translates into food insecurity and poorer populations. Fortunately, modern technologies such as Artificial Intelligent (AI), the Internet of Things (IoT), blockchain, 3D printing, and virtual and augmented reality (VR and AR) are promising to reduce the risk and impact of extreme weather in our societies. However, research directions on how these technologies could help reduce the impact of extreme weather are unclear. This makes it challenging to emploring digital technologies within the spheres of extreme weather. In this paper, we employed the Delphi Best Worst method and Machine learning approaches to identify and assess the push factors of technology. The BWM evaluation revealed that predictive nature was AI's most important criterion and role, while the mass-market potential was the less important criterion. Based on this outcome, we tested the predictive ability of machine elarning on a publilcly available dataset to affrm the predictive rols of AI. We presented the managerial and methodological implications of the study, which are crucial for research and practice. The methodology utilized in this study could aid decision-makers in devising strategies and interventions to safeguard sustainable production. This will also facilitate allocating scarce resources and investment in improving AI techniques to reduce the adverse impacts of extreme events. Correspondingly, we put forward the limitations of this, which necessitate future research. △ Less","21 September, 2022",https://arxiv.org/pdf/2210.08962
Chat Control or Child Protection?,Ross Anderson,"Ian Levy and Crispin Robinson's position paper ""Thoughts on child safety on commodity platforms"" is to be welcomed for extending the scope of the debate about the extent to which child safety concerns justify legal limits to online privacy. Their paper's context is the laws proposed in both the UK and the EU to give the authorities the power to undermine end-to-end cryptography in online communications services, with a justification of preventing and detecting of child abuse and terrorist recruitment. Both jurisdictions plan to make it easier to get service firms to take down a range of illegal material from their servers; but they also propose to mandate client-side scanning - not just for known illegal images, but for text messages indicative of sexual grooming or terrorist recruitment. In this initial response, I raise technical issues about the capabilities of the technologies the authorities propose to mandate, and a deeper strategic issue: that we should view the child safety debate from the perspective of children at risk of violence, rather than from that of the security and intelligence agencies and the firms that sell surveillance software. The debate on terrorism similarly needs to be grounded in the context in which young people are radicalised. Both political violence and violence against children tend to be politicised and as a result are often poorly policed. Effective policing, particularly of crimes embedded in wicked social problems, must be locally led and involve multiple stakeholders; the idea of using 'artificial intelligence' to replace police officers, social workers and teachers is just the sort of magical thinking that leads to bad policy. The debate must also be conducted within the boundary conditions set by human rights and privacy law, and to be pragmatic must also consider reasonable police priorities. △ Less","11 October, 2022",https://arxiv.org/pdf/2210.08958
A.I. Robustness: a Human-Centered Perspective on Technological Challenges and Opportunities,Andrea Tocchetti;Lorenzo Corti;Agathe Balayn;Mireia Yurrita;Philip Lippmann;Marco Brambilla;Jie Yang,"Despite the impressive performance of Artificial Intelligence (AI) systems, their robustness remains elusive and constitutes a key issue that impedes large-scale adoption. Robustness has been studied in many domains of AI, yet with different interpretations across domains and contexts. In this work, we systematically survey the recent progress to provide a reconciled terminology of concepts around AI robustness. We introduce three taxonomies to organize and describe the literature both from a fundamental and applied point of view: 1) robustness by methods and approaches in different phases of the machine learning pipeline; 2) robustness for specific model architectures, tasks, and systems; and in addition, 3) robustness assessment methodologies and insights, particularly the trade-offs with other trustworthiness properties. Finally, we identify and discuss research gaps and opportunities and give an outlook on the field. We highlight the central role of humans in evaluating and enhancing AI robustness, considering the necessary knowledge humans can provide, and discuss the need for better understanding practices and developing supportive tools in the future. △ Less","19 October, 2022",https://arxiv.org/pdf/2210.08906
COFAR: Commonsense and Factual Reasoning in Image Search,Prajwal Gatti;Abhirama Subramanyam Penamakuri;Revant Teotia;Anand Mishra;Shubhashis Sengupta;Roshni Ramnani,"One characteristic that makes humans superior to modern artificially intelligent models is the ability to interpret images beyond what is visually apparent. Consider the following two natural language search queries - (i) ""a queue of customers patiently waiting to buy ice cream"" and (ii) ""a queue of tourists going to see a famous Mughal architecture in India."" Interpreting these queries requires one to reason with (i) Commonsense such as interpreting people as customers or tourists, actions as waiting to buy or going to see; and (ii) Fact or world knowledge associated with named visual entities, for example, whether the store in the image sells ice cream or whether the landmark in the image is a Mughal architecture located in India. Such reasoning goes beyond just visual recognition. To enable both commonsense and factual reasoning in the image search, we present a unified framework, namely Knowledge Retrieval-Augmented Multimodal Transformer (KRAMT), that treats the named visual entities in an image as a gateway to encyclopedic knowledge and leverages them along with natural language query to ground relevant knowledge. Further, KRAMT seamlessly integrates visual content and grounded knowledge to learn alignment between images and search queries. This unified framework is then used to perform image search requiring commonsense and factual reasoning. The retrieval performance of KRAMT is evaluated and compared with related approaches on a new dataset we introduce - namely COFAR. We make our code and dataset available at https://vl2g.github.io/projects/cofar △ Less","16 October, 2022",https://arxiv.org/pdf/2210.08554
Taxonomy of A Decision Support System for Adaptive Experimental Design in Field Robotics,Jason M. Gregory;Sarah Al-Hussaini;Ali-akbar Agha-mohammadi;Satyandra K. Gupta,"Experimental design in field robotics is an adaptive human-in-the-loop decision-making process in which an experimenter learns about system performance and limitations through interactions with a robot in the form of constructed experiments. This can be challenging because of system complexity, the need to operate in unstructured environments, and the competing objectives of maximizing information gain while simultaneously minimizing experimental costs. Based on the successes in other domains, we propose the use of a Decision Support System (DSS) to amplify the human's decision-making abilities, overcome their inherent shortcomings, and enable principled decision-making in field experiments. In this work, we propose common terminology and a six-stage taxonomy of DSSs specifically for adaptive experimental design of more informative tests and reduced experimental costs. We construct and present our taxonomy using examples and trends from DSS literature, including works involving artificial intelligence and Intelligent DSSs. Finally, we identify critical technical gaps and opportunities for future research to direct the scientific community in the pursuit of next-generation DSSs for experimental design. △ Less","15 October, 2022",https://arxiv.org/pdf/2210.08397
A Scalable Reinforcement Learning Approach for Attack Allocation in Swarm to Swarm Engagement Problems,Umut Demir;Nazim Kemal Ure,"In this work we propose a reinforcement learning (RL) framework that controls the density of a large-scale swarm for engaging with adversarial swarm attacks. Although there is a significant amount of existing work in applying artificial intelligence methods to swarm control, analysis of interactions between two adversarial swarms is a rather understudied area. Most of the existing work in this subject develop strategies by making hard assumptions regarding the strategy and dynamics of the adversarial swarm. Our main contribution is the formulation of the swarm to swarm engagement problem as a Markov Decision Process and development of RL algorithms that can compute engagement strategies without the knowledge of strategy/dynamics of the adversarial swarm. Simulation results show that the developed framework can handle a wide array of large-scale engagement scenarios in an efficient manner. △ Less","15 October, 2022",https://arxiv.org/pdf/2210.08319
On Trustworthy Decision-Making Process of Human Drivers from the View of Perceptual Uncertainty Reduction,Huanjie Wang;Haibin Liu;Wenshuo Wang;Lijun Sun,"Humans are experts in making decisions for challenging driving tasks with uncertainties. Many efforts have been made to model the decision-making process of human drivers at the behavior level. However, limited studies explain how human drivers actively make reliable sequential decisions to complete interactive driving tasks in an uncertain environment. This paper argues that human drivers intently search for actions to reduce the uncertainty of their perception of the environment, i.e., perceptual uncertainty, to a low level that allows them to make a trustworthy decision easily. This paper provides a proof of concept framework to empirically reveal that human drivers' perceptual uncertainty decreases when executing interactive tasks with uncertainties. We first introduce an explainable-artificial intelligence approach (i.e., SHapley Additive exPlanation, SHAP) to determine the salient features on which human drivers make decisions. Then, we use entropy-based measures to quantify the drivers' perceptual changes in these ranked salient features across the decision-making process, reflecting the changes in uncertainties. The validation and verification of our proposed method are conducted in the highway on-ramp merging scenario with congested traffic using the INTERACTION dataset. Experimental results support that human drivers intentionally seek information to reduce their perceptual uncertainties in the number and rank of salient features of their perception of environments to make a trustworthy decision. △ Less","15 October, 2022",https://arxiv.org/pdf/2210.08256
VHetNets for AI and AI for VHetNets: An Anomaly Detection Case Study for Ubiquitous IoT,Weili Wang;Omid Abbasi;Halim Yanikomeroglu;Chengchao Liang;Lun Tang;Qianbin Chen,"Vertical heterogenous networks (VHetNets) and artificial intelligence (AI) play critical roles in 6G and beyond networks. This article presents an AI-native VHetNets architecture to enable the synergy of VHetNets and AI, thereby supporting varieties of AI services while facilitating automatic and intelligent network management. Anomaly detection in Internet of Things (IoT) is a major AI service required by many fields, including intrusion detection, state monitoring, device-activity analysis, security supervision and so on. Conventional anomaly detection technologies mainly consider the anomaly detection as a standalone service that is independent of any other network management functionalities, which cannot be used directly in ubiquitous IoT due to the resource constrained end nodes and decentralized data distribution. In this article, we develop an AI-native VHetNets-enabled framework to provide the anomaly detection service for ubiquitous IoT, whose implementation is assisted by intelligent network management functionalities. We first discuss the possibilities of VHetNets used for distributed AI model training to provide anomaly detection service for ubiquitous IoT, i.e., VHetNets for AI. After that, we study the application of AI approaches in helping provide automatic and intelligent network management functionalities for VHetNets, i.e., AI for VHetNets, whose aim is to facilitate the efficient implementation of anomaly detection service. Finally, a case study is presented to demonstrate the efficiency and effectiveness of the proposed AI-native VHetNets-enabled anomaly detection framework. △ Less","14 October, 2022",https://arxiv.org/pdf/2210.08132
Wild Animal Classifier Using CNN,Sahil Faizal;Sanjay Sundaresan,"Classification and identification of wild animals for tracking and protection purposes has become increasingly important with the deterioration of the environment, and technology is the agent of change which augments this process with novel solutions. Computer vision is one such technology which uses the abilities of artificial intelligence and machine learning models on visual inputs. Convolution neural networks (CNNs) have multiple layers which have different weights for the purpose of prediction of a particular input. The precedent for classification, however, is set by the image processing techniques which provide nearly ideal input images that produce optimal results. Image segmentation is one such widely used image processing method which provides a clear demarcation of the areas of interest in the image, be it regions or objects. The Efficiency of CNN can be related to the preprocessing done before training. Further, it is a well-established fact that heterogeneity in image sources is detrimental to the performance of CNNs. Thus, the added functionality of heterogeneity elimination is performed by the image processing techniques, introducing a level of consistency that sets the tone for the excellent feature extraction and eventually in classification. △ Less","3 October, 2022",https://arxiv.org/pdf/2210.07973
Towards Trustworthy AI-Empowered Real-Time Bidding for Online Advertisement Auctioning,Xiaoli Tang;Han Yu,"Artificial intelligence-empowred Real-Time Bidding (AIRTB) is regarded as one of the most enabling technologies for online advertising. It has attracted significant research attention from diverse fields such as pattern recognition, game theory and mechanism design. Despite of its remarkable development and deployment, the AIRTB system can sometimes harm the interest of its participants (e.g., depleting the advertisers' budget with various kinds of fraud). As such, building trustworthy AIRTB auctioning systems has emerged as an important direction of research in this field in recent years. Due to the highly interdisciplinary nature of this field and a lack of a comprehensive survey, it is a challenge for researchers to enter this field and contribute towards building trustworthy AIRTB technologies. This paper bridges this important gap in trustworthy AIRTB literature. We start by analysing the key concerns of various AIRTB stakeholders and identify three main dimensions of trust building in AIRTB, namely security, robustness and fairness. For each of these dimensions, we propose a unique taxonomy of the state of the art, trace the root causes of possible breakdown of trust, and discuss the necessity of the given dimension. This is followed by a comprehensive review of existing strategies for fulfilling the requirements of each trust dimension. In addition, we discuss the promising future directions of research essential towards building trustworthy AIRTB systems to benefit the field of online advertising. △ Less","21 September, 2022",https://arxiv.org/pdf/2210.07770
Machine Learning in Transaction Monitoring: The Prospect of xAI,Julie Gerlings;Ioanna Constantiou,"Banks hold a societal responsibility and regulatory requirements to mitigate the risk of financial crimes. Risk mitigation primarily happens through monitoring customer activity through Transaction Monitoring (TM). Recently, Machine Learning (ML) has been proposed to identify suspicious customer behavior, which raises complex socio-technical implications around trust and explainability of ML models and their outputs. However, little research is available due to its sensitivity. We aim to fill this gap by presenting empirical research exploring how ML supported automation and augmentation affects the TM process and stakeholders' requirements for building eXplainable Artificial Intelligence (xAI). Our study finds that xAI requirements depend on the liable party in the TM process which changes depending on augmentation or automation of TM. Context-relatable explanations can provide much-needed support for auditing and may diminish bias in the investigator's judgement. These results suggest a use case-specific approach for xAI to adequately foster the adoption of ML in TM. △ Less","28 December, 2022",https://arxiv.org/pdf/2210.07648
InFIP: An Explainable DNN Intellectual Property Protection Method based on Intrinsic Features,Mingfu Xue;Xin Wang;Yinghao Wu;Shifeng Ni;Yushu Zhang;Weiqiang Liu,"Intellectual property (IP) protection for Deep Neural Networks (DNNs) has raised serious concerns in recent years. Most existing works embed watermarks in the DNN model for IP protection, which need to modify the model and lack of interpretability. In this paper, for the first time, we propose an interpretable intellectual property protection method for DNN based on explainable artificial intelligence. Compared with existing works, the proposed method does not modify the DNN model, and the decision of the ownership verification is interpretable. We extract the intrinsic features of the DNN model by using Deep Taylor Decomposition. Since the intrinsic feature is composed of unique interpretation of the model's decision, the intrinsic feature can be regarded as fingerprint of the model. If the fingerprint of a suspected model is the same as the original model, the suspected model is considered as a pirated model. Experimental results demonstrate that the fingerprints can be successfully used to verify the ownership of the model and the test accuracy of the model is not affected. Furthermore, the proposed method is robust to fine-tuning attack, pruning attack, watermark overwriting attack, and adaptive attack. △ Less","13 October, 2022",https://arxiv.org/pdf/2210.07481
Machine Learning vs. Deep Learning in 5G Networks -- A Comparison of Scientific Impact,Ilker Turker;Serhat Orkun Tan,"Introduction of fifth generation (5G) wireless network technology has matched the crucial need for high capacity and speed needs of the new generation mobile applications. Recent advances in Artificial Intelligence (AI) also empowered 5G cellular networks with two mainstreams as machine learning (ML) and deep learning (DL) techniques. Our study aims to uncover the differences in scientific impact for these two techniques by the means of statistical bibliometrics. The performed analysis includes citation performance with respect to indexing types, funding availability, journal or conference publishing options together with distributions of these metrics along years to evaluate the popularity trends in a detailed manner. Web of Science (WoS) database host 2245 papers for ML and 1407 papers for DL-related studies. DL studies, starting with 9% rate in 2013, has reached to 45% rate in 2022 among all DL and ML-related studies. Results related to scientific impact indicate that DL studies get slightly more average normalized citation (2.256) compared to ML studies (2.118) in 5G, while SCI-Expanded indexed papers in both sides tend to have similar citation performance (3.165 and 3.162 respectively). ML-related studies those are indexed in ESCI show twice citation performance compared to DL. Conference papers in DL domain and journal papers in ML domain are superior in scientific interest to their counterparts with minor differences. Highest citation performance for ML studies is achieved for year 2014, while this peak is observed for 2017 for DL studies. We can conclude that both publication and citation rate for DL-related papers tend to increase and outperform ML-based studies in 5G domain by the means of citation metrics. △ Less","13 October, 2022",https://arxiv.org/pdf/2210.07327
Advancing the cybersecurity of the healthcare system with self-optimising and self-adaptative artificial intelligence (part 2),Petar Radanliev;David De Roure,"This article advances the knowledge on teaching and training new artificial intelligence algorithms, for securing, preparing, and adapting the healthcare system to cope with future pandemics. The core objective is to develop a concept healthcare system supported by autonomous artificial intelligence that can use edge health devices with real-time data. The article constructs two case scenarios for applying cybersecurity with autonomous artificial intelligence for (1) self-optimising predictive cyber risk analytics of failures in healthcare systems during a Disease X event (i.e., undefined future pandemic), and (2) self-adaptive forecasting of medical production and supply chain bottlenecks during future pandemics. To construct the two testing scenarios, the article uses the case of Covid-19 to synthesise data for the algorithms i.e., for optimising and securing digital healthcare systems in anticipation of disease X. The testing scenarios are built to tackle the logistical challenges and disruption of complex production and supply chains for vaccine distribution with optimisation algorithms. △ Less","30 August, 2022",https://arxiv.org/pdf/2210.07065
Anonymizing Speech with Generative Adversarial Networks to Preserve Speaker Privacy,Sarina Meyer;Pascal Tilli;Pavel Denisov;Florian Lux;Julia Koch;Ngoc Thang Vu,"In order to protect the privacy of speech data, speaker anonymization aims for hiding the identity of a speaker by changing the voice in speech recordings. This typically comes with a privacy-utility trade-off between protection of individuals and usability of the data for downstream applications. One of the challenges in this context is to create non-existent voices that sound as natural as possible. In this work, we propose to tackle this issue by generating speaker embeddings using a generative adversarial network with Wasserstein distance as cost function. By incorporating these artificial embeddings into a speech-to-text-to-speech pipeline, we outperform previous approaches in terms of privacy and utility. According to standard objective metrics and human evaluation, our approach generates intelligible and content-preserving yet privacy-protecting versions of the original recordings. △ Less","20 October, 2022",https://arxiv.org/pdf/2210.07002
Parallel photonic accelerator for decision making using optical spatiotemporal chaos,Kensei Morijiri;Kento Takehana;Takatomo Mihana;Kazutaka Kanno;Makoto Naruse;Atsushi Uchida,"Photonic accelerators have attracted increasing attention in artificial intelligence applications. The multi-armed bandit problem is a fundamental problem of decision making using reinforcement learning. However, the scalability of photonic decision making has not yet been demonstrated in experiments, owing to technical difficulties in physical realization. We propose a parallel photonic decision-making system for solving large-scale multi-armed bandit problems using optical spatiotemporal chaos. We solve a 512-armed bandit problem online, which is much larger than previous experiments by two orders of magnitude. The scaling property for correct decision making is examined as a function of the number of slot machines, evaluated as an exponent of 0.86. This exponent is smaller than that in previous work, indicating the superiority of the proposed parallel principle. This experimental demonstration facilitates photonic decision making to solve large-scale multi-armed bandit problems for future photonic accelerators. △ Less","12 October, 2022",https://arxiv.org/pdf/2210.06976
On-Premise Artificial Intelligence as a Service for Small and Medium Size Setups,Carolina Fortuna;Din Mušić;Gregor Cerar;Andrej Čampa;Panagiotis Kapsalis;Mihael Mohorčič,"Artificial Intelligence (AI) technologies are moving from customized deployments in specific domains towards generic solutions horizontally permeating vertical domains and industries. For instance, decisions on when to perform maintenance of roads or bridges or how to optimize public lighting in view of costs and safety in smart cities are increasingly informed by AI models. While various commercial solutions offer user friendly and easy to use AI as a Service (AIaaS), functionality-wise enabling the democratization of such ecosystems, open-source equivalent ecosystems are lagging behind. In this chapter, we discuss AIaaS functionality and corresponding technology stack and analyze possible realizations using open source user friendly technologies that are suitable for on-premise set-ups of small and medium sized users allowing full control over the data and technological platform without any third-party dependence or vendor lock-in. △ Less","12 October, 2022",https://arxiv.org/pdf/2210.06956
Agent-Based Modelling for Urban Analytics: State of the Art and Challenges,Nick Malleson;Mark Birkin;Daniel Birks;Jiaqi Ge;Alison Heppenstall;Ed Manley;Josie McCulloch;Patricia Ternes,"Agent-based modelling (ABM) is a facet of wider Multi-Agent Systems (MAS) research that explores the collective behaviour of individual `agents', and the implications that their behaviour and interactions have for wider systemic behaviour. The method has been shown to hold considerable value in exploring and understanding human societies, but is still largely confined to use in academia. This is particularly evident in the field of Urban Analytics; one that is characterised by the use of new forms of data in combination with computational approaches to gain insight into urban processes. In Urban Analytics, ABM is gaining popularity as a valuable method for understanding the low-level interactions that ultimately drive cities, but as yet is rarely used by stakeholders (planners, governments, etc.) to address real policy problems. This paper presents the state-of-the-art in the application of ABM at the interface of MAS and Urban Analytics by a group of ABM researchers who are affiliated with the Urban Analytics programme of the Alan Turing Institute in London (UK). It addresses issues around modelling behaviour, the use of new forms of data, the calibration of models under high uncertainty, real-time modelling, the use of AI techniques, large-scale models, and the implications for modelling policy. The discussion also contextualises current research in wider debates around Data Science, Artificial Intelligence, and MAS more broadly. △ Less","13 October, 2022",https://arxiv.org/pdf/2210.06955
"Multi-Player Immersive Communications and Interactions in Metaverse: Challenges, Architecture, and Future Directions",Yakun Huang;Xiuquan Qiao;Haowen Wang;Xiang Su;Schahram Dustdar;Ping Zhang,"The metaverse has awakened users' expectations of an immersive interaction that fuses the virtual digital world and the physical world across space and time. However, the metaverse is still in its infancy, typically expanding multi-player applications (e.g., multi-player games) to implement a prototype with the help of 5G/Beyond 5G, Artificial Intelligence, digital twin, and other enabling technologies. This article reviews the characteristics, key enabling technologies, and driving applications of the state-of-the-art metaverse. We focus on the immersive interactions perspective of the metaverse from the tasks, inputs, and feedback across the users, digital world, and physical world and reveal the key challenges. Afterwards, we present a multi-player interaction prototype platform based on a cloud-edge-device collaborative framework. Also, we evaluate it with centralized and device-to-device (D2D) approaches to verify the efficiency and flexibility of interactions. Finally, we point out future research approaches and discuss potential solutions to enable more stable and higher quality multi-player interactions for metaverse services. △ Less","13 October, 2022",https://arxiv.org/pdf/2210.06802
Neuro-symbolic Explainable Artificial Intelligence Twin for Zero-touch IoE in Wireless Network,Md. Shirajum Munir;Ki Tae Kim;Apurba Adhikary;Walid Saad;Sachin Shetty;Seong-Bae Park;Choong Seon Hong,"Explainable artificial intelligence (XAI) twin systems will be a fundamental enabler of zero-touch network and service management (ZSM) for sixth-generation (6G) wireless networks. A reliable XAI twin system for ZSM requires two composites: an extreme analytical ability for discretizing the physical behavior of the Internet of Everything (IoE) and rigorous methods for characterizing the reasoning of such behavior. In this paper, a novel neuro-symbolic explainable artificial intelligence twin framework is proposed to enable trustworthy ZSM for a wireless IoE. The physical space of the XAI twin executes a neural-network-driven multivariate regression to capture the time-dependent wireless IoE environment while determining unconscious decisions of IoE service aggregation. Subsequently, the virtual space of the XAI twin constructs a directed acyclic graph (DAG)-based Bayesian network that can infer a symbolic reasoning score over unconscious decisions through a first-order probabilistic language model. Furthermore, a Bayesian multi-arm bandits-based learning problem is proposed for reducing the gap between the expected explained score and the current obtained score of the proposed neuro-symbolic XAI twin. To address the challenges of extensible, modular, and stateless management functions in ZSM, the proposed neuro-symbolic XAI twin framework consists of two learning systems: 1) an implicit learner that acts as an unconscious learner in physical space, and 2) an explicit leaner that can exploit symbolic reasoning based on implicit learner decisions and prior evidence. Experimental results show that the proposed neuro-symbolic XAI twin can achieve around 96.26% accuracy while guaranteeing from 18% to 44% more trust score in terms of reasoning and closed-loop automation. △ Less","12 October, 2022",https://arxiv.org/pdf/2210.06649
BLADERUNNER: Rapid Countermeasure for Synthetic (AI-Generated) StyleGAN Faces,Adam Dorian Wong,"StyleGAN is the open-sourced TensorFlow implementation made by NVIDIA. It has revolutionized high quality facial image generation. However, this democratization of Artificial Intelligence / Machine Learning (AI/ML) algorithms has enabled hostile threat actors to establish cyber personas or sock-puppet accounts in social media platforms. These ultra-realistic synthetic faces. This report surveys the relevance of AI/ML with respect to Cyber & Information Operations. The proliferation of AI/ML algorithms has led to a rise in DeepFakes and inauthentic social media accounts. Threats are analyzed within the Strategic and Operational Environments. Existing methods of identifying synthetic faces exists, but they rely on human beings to visually scrutinize each photo for inconsistencies. However, through use of the DLIB 68-landmark pre-trained file, it is possible to analyze and detect synthetic faces by exploiting repetitive behaviors in StyleGAN images. Project Blade Runner encompasses two scripts necessary to counter StyleGAN images. Through PapersPlease acting as the analyzer, it is possible to derive indicators-of-attack (IOA) from scraped image samples. These IOAs can be fed back into AmongUs acting as the detector to identify synthetic faces from live operational samples. The opensource copy of Blade Runner may lack additional unit tests and some functionality, but the open-source copy is a redacted version, far leaner, better optimized, and a proof-of-concept for the information security community. The desired end-state will be to incrementally add automation to stay on-par with its closed-source predecessor. △ Less","28 October, 2022",https://arxiv.org/pdf/2210.06587
Microscopy is All You Need,Sergei V. Kalinin;Rama Vasudevan;Yongtao Liu;Ayana Ghosh;Kevin Roccapriore;Maxim Ziatdinov,"We pose that microscopy offers an ideal real-world experimental environment for the development and deployment of active Bayesian and reinforcement learning methods. Indeed, the tremendous progress achieved by machine learning (ML) and artificial intelligence over the last decade has been largely achieved via the utilization of static data sets, from the paradigmatic MNIST to the bespoke corpora of text and image data used to train large models such as GPT3, DALLE and others. However, it is now recognized that continuous, minute improvements to state-of-the-art do not necessarily translate to advances in real-world applications. We argue that a promising pathway for the development of ML methods is via the route of domain-specific deployable algorithms in areas such as electron and scanning probe microscopy and chemical imaging. This will benefit both fundamental physical studies and serve as a test bed for more complex autonomous systems such as robotics and manufacturing. Favorable environment characteristics of scanning and electron microscopy include low risk, extensive availability of domain-specific priors and rewards, relatively small effects of exogeneous variables, and often the presence of both upstream first principles as well as downstream learnable physical models for both statics and dynamics. Recent developments in programmable interfaces, edge computing, and access to APIs facilitating microscope control, all render the deployment of ML codes on operational microscopes straightforward. We discuss these considerations and hope that these arguments will lead to creating a novel set of development targets for the ML community by accelerating both real-world ML applications and scientific progress. △ Less","12 October, 2022",https://arxiv.org/pdf/2210.06526
Non-Axiomatic Term Logic: A Computational Theory of Cognitive Symbolic Reasoning,Kotaro Funakoshi,"This paper presents Non-Axiomatic Term Logic (NATL) as a theoretical computational framework of humanlike symbolic reasoning in artificial intelligence. NATL unites a discrete syntactic system inspired from Aristotle's term logic and a continuous semantic system based on the modern idea of distributed representations, or embeddings. This paper positions the proposed approach in the phylogeny and the literature of logic, and explains the framework. As it is yet no more than a theory and it requires much further elaboration to implement it, no quantitative evaluation is presented. Instead, qualitative analyses of arguments using NATL, some applications to possible cognitive science/robotics-related research, and remaining issues towards a machinery implementation are discussed. △ Less","12 October, 2022",https://arxiv.org/pdf/2210.06316
The evolution of AI approaches for motor imagery EEG-based BCIs,Aurora Saibene;Silvia Corchs;Mirko Caglioni;Francesca Gasparini,"The Motor Imagery (MI) electroencephalography (EEG) based Brain Computer Interfaces (BCIs) allow the direct communication between humans and machines by exploiting the neural pathways connected to motor imagination. Therefore, these systems open the possibility of developing applications that could span from the medical field to the entertainment industry. In this context, Artificial Intelligence (AI) approaches become of fundamental importance especially when wanting to provide a correct and coherent feedback to BCI users. Moreover, publicly available datasets in the field of MI EEG-based BCIs have been widely exploited to test new techniques from the AI domain. In this work, AI approaches applied to datasets collected in different years and with different devices but with coherent experimental paradigms are investigated with the aim of providing a concise yet sufficiently comprehensive survey on the evolution and influence of AI techniques on MI EEG-based BCI data. △ Less","11 October, 2022",https://arxiv.org/pdf/2210.06290
Can Artificial Intelligence Reconstruct Ancient Mosaics?,Fernando Moral-Andrés;Elena Merino-Gómez;Pedro Reviriego;Fabrizio Lombardi,"A large number of ancient mosaics have not reached us because they have been destroyed by erosion, earthquakes, looting or even used as materials in newer construction. To make things worse, among the small fraction of mosaics that we have been able to recover, many are damaged or incomplete. Therefore, restoration and reconstruction of mosaics play a fundamental role to preserve cultural heritage and to understand the role of mosaics in ancient cultures. This reconstruction has traditionally been done manually and more recently using computer graphics programs but always by humans. In the last years, Artificial Intelligence (AI) has made impressive progress in the generation of images from text descriptions and reference images. State of the art AI tools such as DALL-E2 can generate high quality images from text prompts and can take a reference image to guide the process. In august 2022, DALL-E2 launched a new feature called outpainting that takes as input an incomplete image and a text prompt and then generates a complete image filling the missing parts. In this paper, we explore whether this innovative technology can be used to reconstruct mosaics with missing parts. Hence a set of ancient mosaics have been used and reconstructed using DALL-E2; results are promising showing that AI is able to interpret the key features of the mosaics and is able to produce reconstructions that capture the essence of the scene. However, in some cases AI fails to reproduce some details, geometric forms or introduces elements that are not consistent with the rest of the mosaic. This suggests that as AI image generation technology matures in the next few years, it could be a valuable tool for mosaic reconstruction going forward. △ Less","7 October, 2022",https://arxiv.org/pdf/2210.06145
Application of Deep Learning on Single-Cell RNA-sequencing Data Analysis: A Review,Matthew Brendel;Chang Su;Zilong Bai;Hao Zhang;Olivier Elemento;Fei Wang,"Single-cell RNA-sequencing (scRNA-seq) has become a routinely used technique to quantify the gene expression profile of thousands of single cells simultaneously. Analysis of scRNA-seq data plays an important role in the study of cell states and phenotypes, and has helped elucidate biological processes, such as those occurring during development of complex organisms and improved our understanding of disease states, such as cancer, diabetes, and COVID, among others. Deep learning, a recent advance of artificial intelligence that has been used to address many problems involving large datasets, has also emerged as a promising tool for scRNA-seq data analysis, as it has a capacity to extract informative, compact features from noisy, heterogeneous, and high-dimensional scRNA-seq data to improve downstream analysis. The present review aims at surveying recently developed deep learning techniques in scRNA-seq data analysis, identifying key steps within the scRNA-seq data analysis pipeline that have been advanced by deep learning, and explaining the benefits of deep learning over more conventional analysis tools. Finally, we summarize the challenges in current deep learning approaches faced within scRNA-seq data and discuss potential directions for improvements in deep algorithms for scRNA-seq data analysis. △ Less","11 October, 2022",https://arxiv.org/pdf/2210.05677
"T5 for Hate Speech, Augmented Data and Ensemble",Tosin Adewumi;Sana Sabah Sabry;Nosheen Abid;Foteini Liwicki;Marcus Liwicki,"We conduct relatively extensive investigations of automatic hate speech (HS) detection using different state-of-the-art (SoTA) baselines over 11 subtasks of 6 different datasets. Our motivation is to determine which of the recent SoTA models is best for automatic hate speech detection and what advantage methods like data augmentation and ensemble may have on the best model, if any. We carry out 6 cross-task investigations. We achieve new SoTA on two subtasks - macro F1 scores of 91.73% and 53.21% for subtasks A and B of the HASOC 2020 dataset, where previous SoTA are 51.52% and 26.52%, respectively. We achieve near-SoTA on two others - macro F1 scores of 81.66% for subtask A of the OLID 2019 dataset and 82.54% for subtask A of the HASOC 2021 dataset, where SoTA are 82.9% and 83.05%, respectively. We perform error analysis and use two explainable artificial intelligence (XAI) algorithms (IG and SHAP) to reveal how two of the models (Bi-LSTM and T5) make the predictions they do by using examples. Other contributions of this work are 1) the introduction of a simple, novel mechanism for correcting out-of-class (OOC) predictions in T5, 2) a detailed description of the data augmentation methods, 3) the revelation of the poor data annotations in the HASOC 2021 dataset by using several examples and XAI (buttressing the need for better quality control), and 4) the public release of our model checkpoints and codes to foster transparency. △ Less","11 October, 2022",https://arxiv.org/pdf/2210.05480
Multi-site Diagnostic Classification Of Schizophrenia Using 3D CNN On Aggregated Task-based fMRI Data,Vigneshwaran Shankaran;Bhaskaran V,"In spite of years of research, the mechanisms that underlie the development of schizophrenia, as well as its relapse, symptomatology, and treatment, continue to be a mystery. The absence of appropriate analytic tools to deal with the variable and complicated nature of schizophrenia may be one of the factors that contribute to the development of this disorder. Deep learning is a subfield of artificial intelligence that was inspired by the nervous system. In recent years, deep learning has made it easier to model and analyse complicated, high-dimensional, and nonlinear systems. Research on schizophrenia is one of the many areas of study that has been revolutionised as a result of the outstanding accuracy that deep learning algorithms have demonstrated in classification and prediction tasks. Deep learning has the potential to become a powerful tool for understanding the mechanisms that are at the root of schizophrenia. In addition, a growing variety of techniques aimed at improving model interpretability and causal reasoning are contributing to this trend. Using multi-site fMRI data and a variety of deep learning approaches, this study seeks to identify different types of schizophrenia. Our proposed method of temporal aggregation of the 4D fMRI data outperforms existing work. In addition, this study aims to shed light on the strength of connections between various brain areas in schizophrenia individuals. △ Less","11 October, 2022",https://arxiv.org/pdf/2210.05240
On Explainability in AI-Solutions: A Cross-Domain Survey,Simon Daniel Duque Anton;Daniel Schneider;Hans Dieter Schotten,"Artificial Intelligence (AI) increasingly shows its potential to outperform predicate logic algorithms and human control alike. In automatically deriving a system model, AI algorithms learn relations in data that are not detectable for humans. This great strength, however, also makes use of AI methods dubious. The more complex a model, the more difficult it is for a human to understand the reasoning for the decisions. As currently, fully automated AI algorithms are sparse, every algorithm has to provide a reasoning for human operators. For data engineers, metrics such as accuracy and sensitivity are sufficient. However, if models are interacting with non-experts, explanations have to be understandable. This work provides an extensive survey of literature on this topic, which, to a large part, consists of other surveys. The findings are mapped to ways of explaining decisions and reasons for explaining decisions. It shows that the heterogeneity of reasons and methods of and for explainability lead to individual explanatory frameworks. △ Less","11 October, 2022",https://arxiv.org/pdf/2210.05173
Leveraging Artificial Intelligence on Binary Code Comprehension,Yifan Zhang,"Understanding binary code is an essential but complex software engineering task for reverse engineering, malware analysis, and compiler optimization. Unlike source code, binary code has limited semantic information, which makes it challenging for human comprehension. At the same time, compiling source to binary code, or transpiling among different programming languages (PLs) can provide a way to introduce external knowledge into binary comprehension. We propose to develop Artificial Intelligence (AI) models that aid human comprehension of binary code. Specifically, we propose to incorporate domain knowledge from large corpora of source code (e.g., variable names, comments) to build AI models that capture a generalizable representation of binary code. Lastly, we will investigate metrics to assess the performance of models that apply to binary code by using human studies of comprehension. △ Less","10 October, 2022",https://arxiv.org/pdf/2210.05103
Using Whole Slide Image Representations from Self-Supervised Contrastive Learning for Melanoma Concordance Regression,Sean Grullon;Vaughn Spurrier;Jiayi Zhao;Corey Chivers;Yang Jiang;Kiran Motaparthi;Michael Bonham;Julianna Ianni,"Although melanoma occurs more rarely than several other skin cancers, patients' long term survival rate is extremely low if the diagnosis is missed. Diagnosis is complicated by a high discordance rate among pathologists when distinguishing between melanoma and benign melanocytic lesions. A tool that provides potential concordance information to healthcare providers could help inform diagnostic, prognostic, and therapeutic decision-making for challenging melanoma cases. We present a melanoma concordance regression deep learning model capable of predicting the concordance rate of invasive melanoma or melanoma in-situ from digitized Whole Slide Images (WSIs). The salient features corresponding to melanoma concordance were learned in a self-supervised manner with the contrastive learning method, SimCLR. We trained a SimCLR feature extractor with 83,356 WSI tiles randomly sampled from 10,895 specimens originating from four distinct pathology labs. We trained a separate melanoma concordance regression model on 990 specimens with available concordance ground truth annotations from three pathology labs and tested the model on 211 specimens. We achieved a Root Mean Squared Error (RMSE) of 0.28 +/- 0.01 on the test set. We also investigated the performance of using the predicted concordance rate as a malignancy classifier, and achieved a precision and recall of 0.85 +/- 0.05 and 0.61 +/- 0.06, respectively, on the test set. These results are an important first step for building an artificial intelligence (AI) system capable of predicting the results of consulting a panel of experts and delivering a score based on the degree to which the experts would agree on a particular diagnosis. Such a system could be used to suggest additional testing or other action such as ordering additional stains or genetic tests. △ Less","10 October, 2022",https://arxiv.org/pdf/2210.04803
Utilizing Explainable AI for improving the Performance of Neural Networks,Huawei Sun;Lorenzo Servadei;Hao Feng;Michael Stephan;Robert Wille;Avik Santra,"Nowadays, deep neural networks are widely used in a variety of fields that have a direct impact on society. Although those models typically show outstanding performance, they have been used for a long time as black boxes. To address this, Explainable Artificial Intelligence (XAI) has been developing as a field that aims to improve the transparency of the model and increase their trustworthiness. We propose a retraining pipeline that consistently improves the model predictions starting from XAI and utilizing state-of-the-art techniques. To do that, we use the XAI results, namely SHapley Additive exPlanations (SHAP) values, to give specific training weights to the data samples. This leads to an improved training of the model and, consequently, better performance. In order to benchmark our method, we evaluate it on both real-life and public datasets. First, we perform the method on a radar-based people counting scenario. Afterward, we test it on the CIFAR-10, a public Computer Vision dataset. Experiments using the SHAP-based retraining approach achieve a 4% more accuracy w.r.t. the standard equal weight retraining for people counting tasks. Moreover, on the CIFAR-10, our SHAP-based weighting strategy ends up with a 3% accuracy rate than the training procedure with equal weighted samples. △ Less","7 October, 2022",https://arxiv.org/pdf/2210.04686
Integrating Digital Twin and Advanced Intelligent Technologies to Realize the Metaverse,Moayad Aloqaily;Ouns Bouachir;Fakhri Karray;Ismaeel Al Ridhawi;Abdulmotaleb El Saddik,"The advances in Artificial Intelligence (AI) have led to technological advancements in a plethora of domains. Healthcare, education, and smart city services are now enriched with AI capabilities. These technological advancements would not have been realized without the assistance of fast, secure, and fault-tolerant communication media. Traditional processing, communication and storage technologies cannot maintain high levels of scalability and user experience for immersive services. The metaverse is an immersive three-dimensional (3D) virtual world that integrates fantasy and reality into a virtual environment using advanced virtual reality (VR) and augmented reality (AR) devices. Such an environment is still being developed and requires extensive research in order for it to be realized to its highest attainable levels. In this article, we discuss some of the key issues required in order to attain realization of metaverse services. We propose a framework that integrates digital twin (DT) with other advanced technologies such as the sixth generation (6G) communication network, blockchain, and AI, to maintain continuous end-to-end metaverse services. This article also outlines requirements for an integrated, DT-enabled metaverse framework and provides a look ahead into the evolving topic. △ Less","3 October, 2022",https://arxiv.org/pdf/2210.04606
"Actor-Critic Network for O-RAN Resource Allocation: xApp Design, Deployment, and Analysis",Mohammadreza Kouchaki;Vuk Marojevic,"Open Radio Access Network (O-RAN) has introduced an emerging RAN architecture that enables openness, intelligence, and automated control. The RAN Intelligent Controller (RIC) provides the platform to design and deploy RAN controllers. xApps are the applications which will take this responsibility by leveraging machine learning (ML) algorithms and acting in near-real time. Despite the opportunities provided by this new architecture, the progress of practical artificial intelligence (AI)-based solutions for network control and automation has been slow. This is mostly because of the lack of an endto-end solution for designing, deploying, and testing AI-based xApps fully executable in real O-RAN network. In this paper we introduce an end-to-end O-RAN design and evaluation procedure and provide a detailed discussion of developing a Reinforcement Learning (RL) based xApp by using two different RL approaches and considering the latest released O-RAN architecture and interfaces. △ Less","26 September, 2022",https://arxiv.org/pdf/2210.04604
Local Interpretable Model Agnostic Shap Explanations for machine learning models,P. Sai Ram Aditya;Mayukha Pal,"With the advancement of technology for artificial intelligence (AI) based solutions and analytics compute engines, machine learning (ML) models are getting more complex day by day. Most of these models are generally used as a black box without user interpretability. Such complex ML models make it more difficult for people to understand or trust their predictions. There are variety of frameworks using explainable AI (XAI) methods to demonstrate explainability and interpretability of ML models to make their predictions more trustworthy. In this manuscript, we propose a methodology that we define as Local Interpretable Model Agnostic Shap Explanations (LIMASE). This proposed ML explanation technique uses Shapley values under the LIME paradigm to achieve the following (a) explain prediction of any model by using a locally faithful and interpretable decision tree model on which the Tree Explainer is used to calculate the shapley values and give visually interpretable explanations. (b) provide visually interpretable global explanations by plotting local explanations of several data points. (c) demonstrate solution for the submodular optimization problem. (d) also bring insight into regional interpretation e) faster computation compared to use of kernel explainer. △ Less","10 October, 2022",https://arxiv.org/pdf/2210.04533
The Guilty (Silicon) Mind: Blameworthiness and Liability in Human-Machine Teaming,Dr Brendan Walker-Munro;Dr Zena Assaad,"As human science pushes the boundaries towards the development of artificial intelligence (AI), the sweep of progress has caused scholars and policymakers alike to question the legality of applying or utilising AI in various human endeavours. For example, debate has raged in international scholarship about the legitimacy of applying AI to weapon systems to form lethal autonomous weapon systems (LAWS). Yet the argument holds true even when AI is applied to a military autonomous system that is not weaponised: how does one hold a machine accountable for a crime? What about a tort? Can an artificial agent understand the moral and ethical content of its instructions? These are thorny questions, and in many cases these questions have been answered in the negative, as artificial entities lack any contingent moral agency. So what if the AI is not alone, but linked with or overseen by a human being, with their own moral and ethical understandings and obligations? Who is responsible for any malfeasance that may be committed? Does the human bear the legal risks of unethical or immoral decisions by an AI? These are some of the questions this manuscript seeks to engage with. △ Less","10 October, 2022",https://arxiv.org/pdf/2210.04456
MRI-based classification of IDH mutation and 1p/19q codeletion status of gliomas using a 2.5D hybrid multi-task convolutional neural network,Satrajit Chakrabarty;Pamela LaMontagne;Joshua Shimony;Daniel S. Marcus;Aristeidis Sotiras,"Isocitrate dehydrogenase (IDH) mutation and 1p/19q codeletion status are important prognostic markers for glioma. Currently, they are determined using invasive procedures. Our goal was to develop artificial intelligence-based methods to non-invasively determine these molecular alterations from MRI. For this purpose, pre-operative MRI scans of 2648 patients with gliomas (grade II-IV) were collected from Washington University School of Medicine (WUSM; n = 835) and publicly available datasets viz. Brain Tumor Segmentation (BraTS; n = 378), LGG 1p/19q (n = 159), Ivy Glioblastoma Atlas Project (Ivy GAP; n = 41), The Cancer Genome Atlas (TCGA; n = 461), and the Erasmus Glioma Database (EGD; n = 774). A 2.5D hybrid convolutional neural network was proposed to simultaneously localize the tumor and classify its molecular status by leveraging imaging features from MR scans and prior knowledge features from clinical records and tumor location. The models were tested on one internal (TCGA) and two external (WUSM and EGD) test sets. For IDH, the best-performing model achieved areas under the receiver operating characteristic (AUROC) of 0.925, 0.874, 0.933 and areas under the precision-recall curves (AUPRC) of 0.899, 0.702, 0.853 on the internal, WUSM, and EGD test sets, respectively. For 1p/19q, the best model achieved AUROCs of 0.782, 0.754, 0.842, and AUPRCs of 0.588, 0.713, 0.782, on those three data-splits, respectively. The high accuracy of the model on unseen data showcases its generalization capabilities and suggests its potential to perform a 'virtual biopsy' for tailoring treatment planning and overall clinical management of gliomas. △ Less","7 October, 2022",https://arxiv.org/pdf/2210.03779
Exploring Effectiveness of Explanations for Appropriate Trust: Lessons from Cognitive Psychology,Ruben S. Verhagen;Siddharth Mehrotra;Mark A. Neerincx;Catholijn M. Jonker;Myrthe L. Tielman,"The rapid development of Artificial Intelligence (AI) requires developers and designers of AI systems to focus on the collaboration between humans and machines. AI explanations of system behavior and reasoning are vital for effective collaboration by fostering appropriate trust, ensuring understanding, and addressing issues of fairness and bias. However, various contextual and subjective factors can influence an AI system explanation's effectiveness. This work draws inspiration from findings in cognitive psychology to understand how effective explanations can be designed. We identify four components to which explanation designers can pay special attention: perception, semantics, intent, and user & context. We illustrate the use of these four explanation components with an example of estimating food calories by combining text with visuals, probabilities with exemplars, and intent communication with both user and context in mind. We propose that the significant challenge for effective AI explanations is an additional step between explanation generation using algorithms not producing interpretable explanations and explanation communication. We believe this extra step will benefit from carefully considering the four explanation components outlined in our work, which can positively affect the explanation's effectiveness. △ Less","5 October, 2022",https://arxiv.org/pdf/2210.03737
Artificial Intelligence and Natural Language Processing and Understanding in Space: A Methodological Framework and Four ESA Case Studies,José Manuel Gómez-Pérez;Andrés García-Silva;Rosemarie Leone;Mirko Albani;Moritz Fontaine;Charles Poncet;Leopold Summerer;Alessandro Donati;Ilaria Roma;Stefano Scaglioni,"The European Space Agency is well known as a powerful force for scientific discovery in numerous areas related to Space. The amount and depth of the knowledge produced throughout the different missions carried out by ESA and their contribution to scientific progress is enormous, involving large collections of documents like scientific publications, feasibility studies, technical reports, and quality management procedures, among many others. Through initiatives like the Open Space Innovation Platform, ESA also acts as a hub for new ideas coming from the wider community across different challenges, contributing to a virtuous circle of scientific discovery and innovation. Handling such wealth of information, of which large part is unstructured text, is a colossal task that goes beyond human capabilities, hence requiring automation. In this paper, we present a methodological framework based on artificial intelligence and natural language processing and understanding to automatically extract information from Space documents, generating value from it, and illustrate such framework through several case studies implemented across different functional areas of ESA, including Mission Design, Quality Assurance, Long-Term Data Preservation, and the Open Space Innovation Platform. In doing so, we demonstrate the value of these technologies in several tasks ranging from effortlessly searching and recommending Space information to automatically determining how innovative an idea can be, answering questions about Space, and generating quizzes regarding quality procedures. Each of these accomplishments represents a step forward in the application of increasingly intelligent AI systems in Space, from structuring and facilitating information access to intelligent systems capable to understand and reason with such information. △ Less","24 October, 2022",https://arxiv.org/pdf/2210.03640
"1st ICLR International Workshop on Privacy, Accountability, Interpretability, Robustness, Reasoning on Structured Data (PAIR^2Struct)",Hao Wang;Wanyu Lin;Hao He;Di Wang;Chengzhi Mao;Muhan Zhang,"Recent years have seen advances on principles and guidance relating to accountable and ethical use of artificial intelligence (AI) spring up around the globe. Specifically, Data Privacy, Accountability, Interpretability, Robustness, and Reasoning have been broadly recognized as fundamental principles of using machine learning (ML) technologies on decision-critical and/or privacy-sensitive applications. On the other hand, in tremendous real-world applications, data itself can be well represented as various structured formalisms, such as graph-structured data (e.g., networks), grid-structured data (e.g., images), sequential data (e.g., text), etc. By exploiting the inherently structured knowledge, one can design plausible approaches to identify and use more relevant variables to make reliable decisions, thereby facilitating real-world deployments. △ Less","7 October, 2022",https://arxiv.org/pdf/2210.03612
Synthetic Voice Detection and Audio Splicing Detection using SE-Res2Net-Conformer Architecture,Lei Wang;Benedict Yeoh;Jun Wah Ng,"Synthetic voice and splicing audio clips have been generated to spoof Internet users and artificial intelligence (AI) technologies such as voice authentication. Existing research work treats spoofing countermeasures as a binary classification problem: bonafide vs. spoof. This paper extends the existing Res2Net by involving the recent Conformer block to further exploit the local patterns on acoustic features. Experimental results on ASVspoof 2019 database show that the proposed SE-Res2Net-Conformer architecture is able to improve the spoofing countermeasures performance for the logical access scenario. In addition, this paper also proposes to re-formulate the existing audio splicing detection problem. Instead of identifying the complete splicing segments, it is more useful to detect the boundaries of the spliced segments. Moreover, a deep learning approach can be used to solve the problem, which is different from the previous signal processing techniques. △ Less","29 November, 2022",https://arxiv.org/pdf/2210.03581
AI-Driven Road Maintenance Inspection v2: Reducing Data Dependency & Quantifying Road Damage,Haris Iqbal;Hemang Chawla;Arnav Varma;Terence Brouns;Ahmed Badar;Elahe Arani;Bahram Zonooz,"Road infrastructure maintenance inspection is typically a labor-intensive and critical task to ensure the safety of all road users. Existing state-of-the-art techniques in Artificial Intelligence (AI) for object detection and segmentation help automate a huge chunk of this task given adequate annotated data. However, annotating videos from scratch is cost-prohibitive. For instance, it can take an annotator several days to annotate a 5-minute video recorded at 30 FPS. Hence, we propose an automated labelling pipeline by leveraging techniques like few-shot learning and out-of-distribution detection to generate labels for road damage detection. In addition, our pipeline includes a risk factor assessment for each damage by instance quantification to prioritize locations for repairs which can lead to optimal deployment of road maintenance machinery. We show that the AI models trained with these techniques can not only generalize better to unseen real-world data with reduced requirement for human annotation but also provide an estimate of maintenance urgency, thereby leading to safer roads. △ Less","7 October, 2022",https://arxiv.org/pdf/2210.03570
Multi-Agent Systems for Computational Economics and Finance,Michael Kampouridis;Panagiotis Kanellopoulos;Maria Kyropoulou;Themistoklis Melissourgos;Alexandros A. Voudouris,"In this article we survey the main research topics of our group at the University of Essex. Our research interests lie at the intersection of theoretical computer science, artificial intelligence, and economic theory. In particular, we focus on the design and analysis of mechanisms for systems involving multiple strategic agents, both from a theoretical and an applied perspective. We present an overview of our group's activities, as well as its members, and then discuss in detail past, present, and future work in multi-agent systems. △ Less","7 October, 2022",https://arxiv.org/pdf/2210.03540
An Overview of Affective Speech Synthesis and Conversion in the Deep Learning Era,Andreas Triantafyllopoulos;Björn W. Schuller;Gökçe İymen;Metin Sezgin;Xiangheng He;Zijiang Yang;Panagiotis Tzirakis;Shuo Liu;Silvan Mertes;Elisabeth André;Ruibo Fu;Jianhua Tao,"Speech is the fundamental mode of human communication, and its synthesis has long been a core priority in human-computer interaction research. In recent years, machines have managed to master the art of generating speech that is understandable by humans. But the linguistic content of an utterance encompasses only a part of its meaning. Affect, or expressivity, has the capacity to turn speech into a medium capable of conveying intimate thoughts, feelings, and emotions -- aspects that are essential for engaging and naturalistic interpersonal communication. While the goal of imparting expressivity to synthesised utterances has so far remained elusive, following recent advances in text-to-speech synthesis, a paradigm shift is well under way in the fields of affective speech synthesis and conversion as well. Deep learning, as the technology which underlies most of the recent advances in artificial intelligence, is spearheading these efforts. In the present overview, we outline ongoing trends and summarise state-of-the-art approaches in an attempt to provide a comprehensive overview of this exciting field. △ Less","6 October, 2022",https://arxiv.org/pdf/2210.03538
Algorithmic Trading Using Continuous Action Space Deep Reinforcement Learning,Naseh Majidi;Mahdi Shamsi;Farokh Marvasti,"Price movement prediction has always been one of the traders' concerns in financial market trading. In order to increase their profit, they can analyze the historical data and predict the price movement. The large size of the data and complex relations between them lead us to use algorithmic trading and artificial intelligence. This paper aims to offer an approach using Twin-Delayed DDPG (TD3) and the daily close price in order to achieve a trading strategy in the stock and cryptocurrency markets. Unlike previous studies using a discrete action space reinforcement learning algorithm, the TD3 is continuous, offering both position and the number of trading shares. Both the stock (Amazon) and cryptocurrency (Bitcoin) markets are addressed in this research to evaluate the performance of the proposed algorithm. The achieved strategy using the TD3 is compared with some algorithms using technical analysis, reinforcement learning, stochastic, and deterministic strategies through two standard metrics, Return and Sharpe ratio. The results indicate that employing both position and the number of trading shares can improve the performance of a trading system based on the mentioned metrics. △ Less","7 October, 2022",https://arxiv.org/pdf/2210.03469
AutoML for Climate Change: A Call to Action,Renbo Tu;Nicholas Roberts;Vishak Prasad;Sibasis Nayak;Paarth Jain;Frederic Sala;Ganesh Ramakrishnan;Ameet Talwalkar;Willie Neiswanger;Colin White,"The challenge that climate change poses to humanity has spurred a rapidly developing field of artificial intelligence research focused on climate change applications. The climate change AI (CCAI) community works on a diverse, challenging set of problems which often involve physics-constrained ML or heterogeneous spatiotemporal data. It would be desirable to use automated machine learning (AutoML) techniques to automatically find high-performing architectures and hyperparameters for a given dataset. In this work, we benchmark popular AutoML libraries on three high-leverage CCAI applications: climate modeling, wind power forecasting, and catalyst discovery. We find that out-of-the-box AutoML libraries currently fail to meaningfully surpass the performance of human-designed CCAI models. However, we also identify a few key weaknesses, which stem from the fact that most AutoML techniques are tailored to computer vision and NLP applications. For example, while dozens of search spaces have been designed for image and language data, none have been designed for spatiotemporal data. Addressing these key weaknesses can lead to the discovery of novel architectures that yield substantial performance gains across numerous CCAI applications. Therefore, we present a call to action to the AutoML community, since there are a number of concrete, promising directions for future work in the space of AutoML for CCAI. We release our code and a list of resources at https://github.com/climate-change-automl/climate-change-automl. △ Less","7 October, 2022",https://arxiv.org/pdf/2210.03324
Perspectives on a 6G Architecture,Rainer Liebhart;Mansoor Shafi;Gajan Shivanandan;Devaki Chandramouli;Laurent Thiebaut,"Mobile communications have been undergoing a generational change every ten years. Whilst we are just beginning to roll out 5G networks, significant efforts are planned to standardize 6G that is expected to be commercially introduced by 2030. This paper looks at the use cases for 6G and their impact on the network architecture to meet the anticipated performance requirements. The new architecture is based on integrating various network functions in virtual cloud environments, leveraging the advancement of artificial intelligence in all domains, integrating different sub-networks constituting the 6G system, and on enhanced means of exposing data and services to third parties. △ Less","6 October, 2022",https://arxiv.org/pdf/2210.03286
TCNL: Transparent and Controllable Network Learning Via Embedding Human-Guided Concepts,Zhihao Wang;Chuang Zhu,"Explaining deep learning models is of vital importance for understanding artificial intelligence systems, improving safety, and evaluating fairness. To better understand and control the CNN model, many methods for transparency-interpretability have been proposed. However, most of these works are less intuitive for human understanding and have insufficient human control over the CNN model. We propose a novel method, Transparent and Controllable Network Learning (TCNL), to overcome such challenges. Towards the goal of improving transparency-interpretability, in TCNL, we define some concepts for specific classification tasks through scientific human-intuition study and incorporate concept information into the CNN model. In TCNL, the shallow feature extractor gets preliminary features first. Then several concept feature extractors are built right after the shallow feature extractor to learn high-dimensional concept representations. The concept feature extractor is encouraged to encode information related to the predefined concepts. We also build the concept mapper to visualize features extracted by the concept extractor in a human-intuitive way. TCNL provides a generalizable approach to transparency-interpretability. Researchers can define concepts corresponding to certain classification tasks and encourage the model to encode specific concept information, which to a certain extent improves transparency-interpretability and the controllability of the CNN model. The datasets (with concept sets) for our experiments will also be released (https://github.com/bupt-ai-cz/TCNL). △ Less","21 November, 2022",https://arxiv.org/pdf/2210.03274
Integrative Imaging Informatics for Cancer Research: Workflow Automation for Neuro-oncology (I3CR-WANO),Satrajit Chakrabarty;Syed Amaan Abidi;Mina Mousa;Mahati Mokkarala;Isabelle Hren;Divya Yadav;Matthew Kelsey;Pamela LaMontagne;John Wood;Michael Adams;Yuzhuo Su;Sherry Thorpe;Caroline Chung;Aristeidis Sotiras;Daniel S. Marcus,"Efforts to utilize growing volumes of clinical imaging data to generate tumor evaluations continue to require significant manual data wrangling owing to the data heterogeneity. Here, we propose an artificial intelligence-based solution for the aggregation and processing of multisequence neuro-oncology MRI data to extract quantitative tumor measurements. Our end-to-end framework i) classifies MRI sequences using an ensemble classifier, ii) preprocesses the data in a reproducible manner, iii) delineates tumor tissue subtypes using convolutional neural networks, and iv) extracts diverse radiomic features. Moreover, it is robust to missing sequences and adopts an expert-in-the-loop approach, where the segmentation results may be manually refined by radiologists. Following the implementation of the framework in Docker containers, it was applied to two retrospective glioma datasets collected from the Washington University School of Medicine (WUSM; n = 384) and the M.D. Anderson Cancer Center (MDA; n = 30) comprising preoperative MRI scans from patients with pathologically confirmed gliomas. The scan-type classifier yielded an accuracy of over 99%, correctly identifying sequences from 380/384 and 30/30 sessions from the WUSM and MDA datasets, respectively. Segmentation performance was quantified using the Dice Similarity Coefficient between the predicted and expert-refined tumor masks. Mean Dice scores were 0.882 (\pm0.244) and 0.977 (\pm0.04) for whole tumor segmentation for WUSM and MDA, respectively. This streamlined framework automatically curated, processed, and segmented raw MRI data of patients with varying grades of gliomas, enabling the curation of large-scale neuro-oncology datasets and demonstrating a high potential for integration as an assistive tool in clinical practice. △ Less","6 October, 2022",https://arxiv.org/pdf/2210.03151
Fault Diagnosis using eXplainable AI: a Transfer Learning-based Approach for Rotating Machinery exploiting Augmented Synthetic Data,Lucas Costa Brito;Gian Antonio Susto;Jorge Nei Brito;Marcus Antonio Viana Duarte,"Artificial Intelligence (AI) is one of the approaches that has been proposed to analyze the collected data (e.g., vibration signals) providing a diagnosis of the asset's operating condition. It is known that models trained with labeled data (supervised) achieve excellent results, but two main problems make their application in production processes difficult: (i) impossibility or long time to obtain a sample of all operational conditions (since faults seldom happen) and (ii) high cost of experts to label all acquired data. Another limitating factor for the applicability of AI approaches in this context is the lack of interpretability of the models (black-boxes), which reduces the confidence of the diagnosis and trust/adoption from users. To overcome these problems, a new generic and interpretable approach for classifying faults in rotating machinery based on transfer learning from augmented synthetic data to real rotating machinery is here proposed, namelly FaultD-XAI (Fault Diagnosis using eXplainable AI). To provide scalability using transfer learning, synthetic vibration signals are created mimicking the characteristic behavior of failures in operation. The application of Gradient-weighted Class Activation Mapping (Grad-CAM) with 1D Convolutional Neural Network (1D CNN) allows the interpretation of results, supporting the user in decision making and increasing diagnostic confidence. The proposed approach not only obtained promising diagnostic performance, but was also able to learn characteristics used by experts to identify conditions in a source domain and apply them in another target domain. The experimental results suggest a promising approach on exploiting transfer learning, synthetic data and explainable artificial intelligence for fault diagnosis. Lastly, to guarantee reproducibility and foster research in the field, the developed dataset is made publicly available. △ Less","11 October, 2022",https://arxiv.org/pdf/2210.02974
Why Should I Choose You? AutoXAI: A Framework for Selecting and Tuning eXplainable AI Solutions,Robin Cugny;Julien Aligon;Max Chevalier;Geoffrey Roman Jimenez;Olivier Teste,"In recent years, a large number of XAI (eXplainable Artificial Intelligence) solutions have been proposed to explain existing ML (Machine Learning) models or to create interpretable ML models. Evaluation measures have recently been proposed and it is now possible to compare these XAI solutions. However, selecting the most relevant XAI solution among all this diversity is still a tedious task, especially when meeting specific needs and constraints. In this paper, we propose AutoXAI, a framework that recommends the best XAI solution and its hyperparameters according to specific XAI evaluation metrics while considering the user's context (dataset, ML model, XAI needs and constraints). It adapts approaches from context-aware recommender systems and strategies of optimization and evaluation from AutoML (Automated Machine Learning). We apply AutoXAI to two use cases, and show that it recommends XAI solutions adapted to the user's needs with the best hyperparameters matching the user's constraints. △ Less","10 October, 2022",https://arxiv.org/pdf/2210.02795
When not to use machine learning: a perspective on potential and limitations,M. R. Carbone,"The unparalleled success of artificial intelligence (AI) in the technology sector has catalyzed an enormous amount of research in the scientific community. It has proven to be a powerful tool, but as with any rapidly developing field, the deluge of information can be overwhelming, confusing and sometimes misleading. This can make it easy to become lost in the same hype cycles that have historically ended in the periods of scarce funding and depleted expectations known as AI Winters. Furthermore, while the importance of innovative, high-risk research cannot be overstated, it is also imperative to understand the fundamental limits of available techniques, especially in young fields where the rules appear to be constantly rewritten and as the likelihood of application to high-stakes scenarios increases. In this perspective, we highlight the guiding principles of data-driven modeling, how these principles imbue models with almost magical predictive power, and how they also impose limitations on the scope of problems they can address. Particularly, understanding when not to use data-driven techniques, such as machine learning, is not something commonly explored, but is just as important as knowing how to apply the techniques properly. We hope that the discussion to follow provides researchers throughout the sciences with a better understanding of when said techniques are appropriate, the pitfalls to watch for, and most importantly, the confidence to leverage the power they can provide. △ Less","6 October, 2022",https://arxiv.org/pdf/2210.02666
MechRetro is a chemical-mechanism-driven graph learning framework for interpretable retrosynthesis prediction and pathway planning,Yu Wang;Chao Pang;Yuzhe Wang;Yi Jiang;Junru Jin;Sirui Liang;Quan Zou;Leyi Wei,"Leveraging artificial intelligence for automatic retrosynthesis speeds up organic pathway planning in digital laboratories. However, existing deep learning approaches are unexplainable, like ""black box"" with few insights, notably limiting their applications in real retrosynthesis scenarios. Here, we propose MechRetro, a chemical-mechanism-driven graph learning framework for interpretable retrosynthetic prediction and pathway planning, which learns several retrosynthetic actions to simulate a reverse reaction via elaborate self-adaptive joint learning. By integrating chemical knowledge as prior information, we design a novel Graph Transformer architecture to adaptively learn discriminative and chemically meaningful molecule representations, highlighting the strong capacity in molecule feature representation learning. We demonstrate that MechRetro outperforms the state-of-the-art approaches for retrosynthetic prediction with a large margin on large-scale benchmark datasets. Extending MechRetro to the multi-step retrosynthesis analysis, we identify efficient synthetic routes via an interpretable reasoning mechanism, leading to a better understanding in the realm of knowledgeable synthetic chemists. We also showcase that MechRetro discovers a novel pathway for protokylol, along with energy scores for uncertainty assessment, broadening the applicability for practical scenarios. Overall, we expect MechRetro to provide meaningful insights for high-throughput automated organic synthesis in drug discovery. △ Less","5 October, 2022",https://arxiv.org/pdf/2210.02630
The Influence of Explainable Artificial Intelligence: Nudging Behaviour or Boosting Capability?,Matija Franklin,"This article aims to provide a theoretical account and corresponding paradigm for analysing how explainable artificial intelligence (XAI) influences people's behaviour and cognition. It uses insights from research on behaviour change. Two notable frameworks for thinking about behaviour change techniques are nudges - aimed at influencing behaviour - and boosts - aimed at fostering capability. It proposes that local and concept-based explanations are more adjacent to nudges, while global and counterfactual explanations are more adjacent to boosts. It outlines a method for measuring XAI influence and argues for the benefits of understanding it for optimal, safe and ethical human-AI collaboration. △ Less","5 October, 2022",https://arxiv.org/pdf/2210.02407
Emotion Twenty Questions Dialog System for Lexical Emotional Intelligence,Abe Kazemzadeh;Adedamola Sanusi;Huihui;Nie,"This paper presents a web-based demonstration of Emotion Twenty Questions (EMO20Q), a dialog game whose purpose is to study how people describe emotions. EMO20Q can also be used to develop artificially intelligent dialog agents that can play the game. In previous work, an EMO20Q agent used a sequential Bayesian machine learning model and could play the question-asking role. Newer transformer-based neural machine learning models have made it possible to develop an agent for the question-answering role. This demo paper describes the recent developments in the question-answering role of the EMO20Q game, which requires the agent to respond to more open-ended inputs. Furthermore, we also describe the design of the system, including the web-based front-end, agent architecture and programming, and updates to earlier software used. The demo system will be available to collect pilot data during the ACII conference and this data will be used to inform future experiments and system design. △ Less","5 October, 2022",https://arxiv.org/pdf/2210.02400
A Liquid Democracy System for Human-Computer Societies,Anton Kolonin;Ben Goertzel;Cassio Pennachin;Deborah Duong;Marco Argentieri;Matt Iklé;Nejc Znidar,"Problem of reliable democratic governance is critical for survival of any community, and it will be critical for communities powered with Artificial Intelligence (AI) systems upon developments of the latter. Apparently, it will be getting more and more critical because of increasing speeds and scales of electronic communications and decreasing latencies in system responses. In order to address this need, we present design and implementation of a reputation system supporting ""liquid democracy"" principle. The system is based on ""weighted liquid rank"" algorithm employing different sorts of explicit and implicit ratings being exchanged by members of the society as well as implicit assessments of of the members based on measures of their activity in the society. The system is evaluated against live social network data with help of simulation modelling for an online marketplace case. △ Less","5 October, 2022",https://arxiv.org/pdf/2210.02356
Game Theoretic Rating in N-player general-sum games with Equilibria,Luke Marris;Marc Lanctot;Ian Gemp;Shayegan Omidshafiei;Stephen McAleer;Jerome Connor;Karl Tuyls;Thore Graepel,"Rating strategies in a game is an important area of research in game theory and artificial intelligence, and can be applied to any real-world competitive or cooperative setting. Traditionally, only transitive dependencies between strategies have been used to rate strategies (e.g. Elo), however recent work has expanded ratings to utilize game theoretic solutions to better rate strategies in non-transitive games. This work generalizes these ideas and proposes novel algorithms suitable for N-player, general-sum rating of strategies in normal-form games according to the payoff rating system. This enables well-established solution concepts, such as equilibria, to be leveraged to efficiently rate strategies in games with complex strategic interactions, which arise in multiagent training and real-world interactions between many agents. We empirically validate our methods on real world normal-form data (Premier League) and multiagent reinforcement learning agent evaluation. △ Less","5 October, 2022",https://arxiv.org/pdf/2210.02205
On the Influence of Cognitive Styles on Users' Understanding of Explanations,Lara Riefle;Patrick Hemmer;Carina Benz;Michael Vössing;Jannik Pries,"Artificial intelligence (AI) is becoming increasingly complex, making it difficult for users to understand how the AI has derived its prediction. Using explainable AI (XAI)-methods, researchers aim to explain AI decisions to users. So far, XAI-based explanations pursue a technology-focused approach - neglecting the influence of users' cognitive abilities and differences in information processing on the understanding of explanations. Hence, this study takes a human-centered perspective and incorporates insights from cognitive psychology. In particular, we draw on the psychological construct of cognitive styles that describe humans' characteristic modes of processing information. Applying a between-subject experiment design, we investigate how users' rational and intuitive cognitive styles affect their objective and subjective understanding of different types of explanations provided by an AI. Initial results indicate substantial differences in users' understanding depending on their cognitive style. We expect to contribute to a more nuanced view of the interrelation of human factors and XAI design. △ Less","5 October, 2022",https://arxiv.org/pdf/2210.02123
Ten Years after ImageNet: A 360° Perspective on AI,Sanjay Chawla;Preslav Nakov;Ahmed Ali;Wendy Hall;Issa Khalil;Xiaosong Ma;Husrev Taha Sencar;Ingmar Weber;Michael Wooldridge;Ting Yu,"It is ten years since neural networks made their spectacular comeback. Prompted by this anniversary, we take a holistic perspective on Artificial Intelligence (AI). Supervised Learning for cognitive tasks is effectively solved - provided we have enough high-quality labeled data. However, deep neural network models are not easily interpretable, and thus the debate between blackbox and whitebox modeling has come to the fore. The rise of attention networks, self-supervised learning, generative modeling, and graph neural networks has widened the application space of AI. Deep Learning has also propelled the return of reinforcement learning as a core building block of autonomous decision making systems. The possible harms made possible by new AI technologies have raised socio-technical issues such as transparency, fairness, and accountability. The dominance of AI by Big-Tech who control talent, computing resources, and most importantly, data may lead to an extreme AI divide. Failure to meet high expectations in high profile, and much heralded flagship projects like self-driving vehicles could trigger another AI winter. △ Less","30 September, 2022",https://arxiv.org/pdf/2210.01797
Predicting the traffic flux in the city of Valencia with Deep Learning,Miguel G. Folgado;Veronica Sanz;Johannes Hirn;Edgar G. Lorenzo;Javier F. Urchueguia,"Traffic congestion is a major urban issue due to its adverse effects on health and the environment, so much so that reducing it has become a priority for urban decision-makers. In this work, we investigate whether a high amount of data on traffic flow throughout a city and the knowledge of the road city network allows an Artificial Intelligence to predict the traffic flux far enough in advance in order to enable emission reduction measures such as those linked to the Low Emission Zone policies. To build a predictive model, we use the city of Valencia traffic sensor system, one of the densest in the world, with nearly 3500 sensors distributed throughout the city. In this work we train and characterize an LSTM (Long Short-Term Memory) Neural Network to predict temporal patterns of traffic in the city using historical data from the years 2016 and 2017. We show that the LSTM is capable of predicting future evolution of the traffic flux in real-time, by extracting patterns out of the measured data. △ Less","4 October, 2022",https://arxiv.org/pdf/2210.01630
Understanding Older Adults' Perceptions and Challenges in Using AI-enabled Everyday Technologies,Esha Shandilya;Mingming Fan,"Artificial intelligence (AI)-enabled everyday technologies could help address age-related challenges like physical impairments and cognitive decline. While recent research studied older adults' experiences with specific AI-enabled products (e.g., conversational agents and assistive robots), it remains unknown how older adults perceive and experience current AI-enabled everyday technologies in general, which could impact their adoption of future AI-enabled products. We conducted a survey study (N=41) and semi-structured interviews (N=15) with older adults to understand their experiences and perceptions of AI. We found that older adults were enthusiastic about learning and using AI-enabled products, but they lacked learning avenues. Additionally, they worried when AI-enabled products outwitted their expectations, intruded on their privacy, or impacted their decision-making skills. Therefore, they held mixed views towards AI-enabled products such as AI, an aid, or an adversary. We conclude with design recommendations that make older adults feel inclusive, secure, and in control of their interactions with AI-enabled products. △ Less","4 October, 2022",https://arxiv.org/pdf/2210.01369
Estimating productivity gains in digital automation,Mauricio Jacobo-Romero;Danilo S. Carvalho;André Freitas,"This paper proposes a novel productivity estimation model to evaluate the effects of adopting Artificial Intelligence (AI) components in a production chain. Our model provides evidence to address the ""AI's"" Solow's Paradox. We provide (i) theoretical and empirical evidence to explain Solow's dichotomy; (ii) a data-driven model to estimate and asses productivity variations; (iii) a methodology underpinned on process mining datasets to determine the business process, BP, and productivity; (iv) a set of computer simulation parameters; (v) and empirical analysis on labour-distribution. These provide data on why we consider AI Solow's paradox a consequence of metric mismeasurement. △ Less","8 October, 2022",https://arxiv.org/pdf/2210.01252
CaiRL: A High-Performance Reinforcement Learning Environment Toolkit,Per-Arne Andersen;Morten Goodwin;Ole-Christoffer Granmo,"This paper addresses the dire need for a platform that efficiently provides a framework for running reinforcement learning (RL) experiments. We propose the CaiRL Environment Toolkit as an efficient, compatible, and more sustainable alternative for training learning agents and propose methods to develop more efficient environment simulations. There is an increasing focus on developing sustainable artificial intelligence. However, little effort has been made to improve the efficiency of running environment simulations. The most popular development toolkit for reinforcement learning, OpenAI Gym, is built using Python, a powerful but slow programming language. We propose a toolkit written in C++ with the same flexibility level but works orders of magnitude faster to make up for Python's inefficiency. This would drastically cut climate emissions. CaiRL also presents the first reinforcement learning toolkit with a built-in JVM and Flash support for running legacy flash games for reinforcement learning research. We demonstrate the effectiveness of CaiRL in the classic control benchmark, comparing the execution speed to OpenAI Gym. Furthermore, we illustrate that CaiRL can act as a drop-in replacement for OpenAI Gym to leverage significantly faster training speeds because of the reduced environment computation time. △ Less","3 October, 2022",https://arxiv.org/pdf/2210.01235
"Green Learning: Introduction, Examples and Outlook",C. -C. Jay Kuo;Azad M. Madni,"Rapid advances in artificial intelligence (AI) in the last decade have largely been built upon the wide applications of deep learning (DL). However, the high carbon footprint yielded by larger and larger DL networks becomes a concern for sustainability. Furthermore, DL decision mechanism is somewhat obsecure and can only be verified by test data. Green learning (GL) has been proposed as an alternative paradigm to address these concerns. GL is characterized by low carbon footprints, small model sizes, low computational complexity, and logical transparency. It offers energy-effective solutions in cloud centers as well as mobile/edge devices. GL also provides a clear and logical decision-making process to gain people's trust. Several statistical tools have been developed to achieve this goal in recent years. They include subspace approximation, unsupervised and supervised representation learning, supervised discriminant feature selection, and feature space partitioning. We have seen a few successful GL examples with performance comparable with state-of-the-art DL solutions. This paper offers an introduction to GL, its demonstrated applications, and future outlook. △ Less","3 October, 2022",https://arxiv.org/pdf/2210.00965
Predicting the Future of AI with AI: High-quality link prediction in an exponentially growing knowledge network,Mario Krenn;Lorenzo Buffoni;Bruno Coutinho;Sagi Eppel;Jacob Gates Foster;Andrew Gritsevskiy;Harlin Lee;Yichao Lu;Joao P. Moutinho;Nima Sanjabi;Rishi Sonthalia;Ngoc Mai Tran;Francisco Valente;Yangxinyu Xie;Rose Yu;Michael Kopp,"A tool that could suggest new personalized research directions and ideas by taking insights from the scientific literature could significantly accelerate the progress of science. A field that might benefit from such an approach is artificial intelligence (AI) research, where the number of scientific publications has been growing exponentially over the last years, making it challenging for human researchers to keep track of the progress. Here, we use AI techniques to predict the future research directions of AI itself. We develop a new graph-based benchmark based on real-world data -- the Science4Cast benchmark, which aims to predict the future state of an evolving semantic network of AI. For that, we use more than 100,000 research papers and build up a knowledge network with more than 64,000 concept nodes. We then present ten diverse methods to tackle this task, ranging from pure statistical to pure learning methods. Surprisingly, the most powerful methods use a carefully curated set of network features, rather than an end-to-end AI approach. It indicates a great potential that can be unleashed for purely ML approaches without human knowledge. Ultimately, better predictions of new future research directions will be a crucial component of more advanced research suggestion tools. △ Less","23 September, 2022",https://arxiv.org/pdf/2210.00881
Dancing with the Unexpected and Beyond: The Use of AI Assistance in Design Fiction Creation,Yiying Wu;Yunye Yu;Pengcheng An,"The creation process of design fiction is going participatory and inclusive with non experts. Recognizing the potential of artificial intelligence in creativity support, we explore the use of AI assistance in creating design fiction. This investigation is based on a workshop on future work in 2040 with Chinese youth. We look into fiction quality, participants experiences with the AI agent, and their ways of incorporating those texts into writing. Our findings show that human writers while responding to messy and unexpected AI-generated texts, can elevate the richness and creativity in writing and initiate joyful and inspirational interactions. Furthermore, for the design of AI assistance in creativity support, we suggest two implications of enhancing interactional quality between human and AI and prompt programming. Our study indicates the potential of applying design fiction outside the design context using a more inclusive approach for future speculation with critical reflection on technology. △ Less","3 October, 2022",https://arxiv.org/pdf/2210.00829
Dual Gradient Descent EMF-Aware MU-MIMO Beamforming in RIS-Aided 6G Networks,Yi Yu;Rita Ibrahim;Dinh-Thuy Phan-Huy,"Reconfigurable Intelligent Surface (RIS) is one of the key technologies for the upcoming 6th Generation (6G) communications, which can improve the signal strength at the receivers by adding artificial propagation paths. In the context of Downlink (DL) Multi-User Multiple-Input Multiple-Output (MU-MIMO) communications, designing an appropriate Beamforming (BF) scheme to take full advantage of this reconfigured propagation environment and improve the network capacity is a major challenge. Due to the spatial dimension provided by MIMO systems, independent data streams can be transmitted to multiple users simultaneously on the same radio resources. It is important to note that serving the same subset of users over a period of time may lead to undesired areas where the average Electromagnetic Field Exposure (EMFE) exceeds regulatory limits. To address this challenge, in this paper, we propose a Dual Gradient Descent (Dual-GD)-based Electromagnetic Field (EMF)-aware MU-MIMO BF scheme that aims to optimize the overall capacity under EMFE constraints in RIS-aided 6G cellular networks. △ Less","3 October, 2022",https://arxiv.org/pdf/2210.00766
The boundaries of meaning: a case study in neural machine translation,Yuri Balashov,"The success of deep learning in natural language processing raises intriguing questions about the nature of linguistic meaning and ways in which it can be processed by natural and artificial systems. One such question has to do with subword segmentation algorithms widely employed in language modeling, machine translation, and other tasks since 2016. These algorithms often cut words into semantically opaque pieces, such as 'period', 'on', 't', and 'ist' in 'period|on|t|ist'. The system then represents the resulting segments in a dense vector space, which is expected to model grammatical relations among them. This representation may in turn be used to map 'period|on|t|ist' (English) to 'par|od|ont|iste' (French). Thus, instead of being modeled at the lexical level, translation is reformulated more generally as the task of learning the best bilingual mapping between the sequences of subword segments of two languages; and sometimes even between pure character sequences: 'p|e|r|i|o|d|o|n|t|i|s|t' \rightarrow 'p|a|r|o|d|o|n|t|i|s|t|e'. Such subword segmentations and alignments are at work in highly efficient end-to-end machine translation systems, despite their allegedly opaque nature. The computational value of such processes is unquestionable. But do they have any linguistic or philosophical plausibility? I attempt to cast light on this question by reviewing the relevant details of the subword segmentation algorithms and by relating them to important philosophical and linguistic debates, in the spirit of making artificial intelligence more transparent and explainable. △ Less","2 October, 2022",https://arxiv.org/pdf/2210.00613
Belief propagation generalizes backpropagation,Frederik Eaton,"The two most important algorithms in artificial intelligence are backpropagation and belief propagation. In spite of their importance, the connection between them is poorly characterized. We show that when an input to backpropagation is converted into an input to belief propagation so that (loopy) belief propagation can be run on it, then the result of belief propagation encodes the result of backpropagation; thus backpropagation is recovered as a special case of belief propagation. In other words, we prove for apparently the first time that belief propagation generalizes backpropagation. Our analysis is a theoretical contribution, which we motivate with the expectation that it might reconcile our understandings of each of these algorithms, and serve as a guide to engineering researchers seeking to improve the behavior of systems that use one or the other. △ Less","2 October, 2022",https://arxiv.org/pdf/2210.00610
"Establishing Meta-Decision-Making for AI: An Ontology of Relevance, Representation and Reasoning",Cosmin Badea;Leilani Gilpin,"We propose an ontology of building decision-making systems, with the aim of establishing Meta-Decision-Making for Artificial Intelligence (AI), improving autonomy, and creating a framework to build metrics and benchmarks upon. To this end, we propose the three parts of Relevance, Representation, and Reasoning, and discuss their value in ensuring safety and mitigating risk in the context of third wave cognitive systems. Our nomenclature reflects the literature on decision-making, and our ontology allows researchers that adopt it to frame their work in relation to one or more of these parts. △ Less","2 October, 2022",https://arxiv.org/pdf/2210.00608
Composition of Differential Privacy & Privacy Amplification by Subsampling,Thomas Steinke,"This chapter is meant to be part of the book ""Differential Privacy for Artificial Intelligence Applications."" We give an introduction to the most important property of differential privacy -- composition: running multiple independent analyses on the data of a set of people will still be differentially private as long as each of the analyses is private on its own -- as well as the related topic of privacy amplification by subsampling. This chapter introduces the basic concepts and gives proofs of the key results needed to apply these tools in practice. △ Less","26 October, 2022",https://arxiv.org/pdf/2210.00597
RISC-V Toolchain and Agile Development based Open-source Neuromorphic Processor,Jiulong Wang;Ruopu Wu;Guokai Chen;Xuhao Chen;Boran Liu;Jixiang Zong;Di Zhao,"In recent decades, neuromorphic computing aiming to imitate brains' behaviors has been developed in various fields of computer science. The Artificial Neural Network (ANN) is an important concept in Artificial Intelligence (AI). It is utilized in recognition and classification. To explore a better way to simulate obtained brain behaviors, which is fast and energy-efficient, on hardware, researchers need an advanced method such as neuromorphic computing. In this case, Spiking Neural Network (SNN) becomes an optimal choice in hardware implementation. Recent works are focusing on accelerating SNN computing. However, most accelerator solutions are based on CPU-accelerator architecture which is energy-inefficient due to the complex control flows in this structure. This paper proposes Wenquxing 22A, a low-power neuromorphic processor that combines general-purpose CPU functions and SNN to efficiently compute it with RISC-V SNN extension instructions. The main idea of Wenquxing 22A is to integrate the SNN calculation unit into the pipeline of a general-purpose CPU to achieve low-power computing with customized RISC-V SNN instructions version 1.0 (RV-SNN V1.0), Streamlined Leaky Integrate-and-Fire (LIF) model, and the binary stochastic Spike-timing-dependent-plasticity (STDP). The source code of Wenquxing 22A is released online on Gitee and GitHub. We apply Wenquxing 22A to the recognition of the MNIST dataset to make a comparison with other SNN systems. Our experiment results show that Wenquxing 22A improves the energy expenses by 5.13 times over the accelerator solution, ODIN, with approximately classification accuracy, 85.00% for 3-bit ODIN online learning, and 91.91% for 1-bit Wenquxing 22A. △ Less","6 October, 2022",https://arxiv.org/pdf/2210.00562
Robust Bayesian optimization with reinforcement learned acquisition functions,Zijing Liu;Xiyao Qu;Xuejun Liu;Hongqiang Lyu,"In Bayesian optimization (BO) for expensive black-box optimization tasks, acquisition function (AF) guides sequential sampling and plays a pivotal role for efficient convergence to better optima. Prevailing AFs usually rely on artificial experiences in terms of preferences for exploration or exploitation, which runs a risk of a computational waste or traps in local optima and resultant re-optimization. To address the crux, the idea of data-driven AF selection is proposed, and the sequential AF selection task is further formalized as a Markov decision process (MDP) and resort to powerful reinforcement learning (RL) technologies. Appropriate selection policy for AFs is learned from superior BO trajectories to balance between exploration and exploitation in real time, which is called reinforcement-learning-assisted Bayesian optimization (RLABO). Competitive and robust BO evaluations on five benchmark problems demonstrate RL's recognition of the implicit AF selection pattern and imply the proposal's potential practicality for intelligent AF selection as well as efficient optimization in expensive black-box problems. △ Less","2 October, 2022",https://arxiv.org/pdf/2210.00476
Citation Trajectory Prediction via Publication Influence Representation Using Temporal Knowledge Graph,Chang Zong;Yueting Zhuang;Weiming Lu;Jian Shao;Siliang Tang,"Predicting the impact of publications in science and technology has become an important research area, which is useful in various real world scenarios such as technology investment, research direction selection, and technology policymaking. Citation trajectory prediction is one of the most popular tasks in this area. Existing approaches mainly rely on mining temporal and graph data from academic articles. Some recent methods are capable of handling cold-start prediction by aggregating metadata features of new publications. However, the implicit factors causing citations and the richer information from handling temporal and attribute features still need to be explored. In this paper, we propose CTPIR, a new citation trajectory prediction framework that is able to represent the influence (the momentum of citation) of either new or existing publications using the history information of all their attributes. Our framework is composed of three modules: difference-preserved graph embedding, fine-grained influence representation, and learning-based trajectory calculation. To test the effectiveness of our framework in more situations, we collect and construct a new temporal knowledge graph dataset from the real world, named AIPatent, which stems from global patents in the field of artificial intelligence. Experiments are conducted on both the APS academic dataset and our contributed AIPatent dataset. The results demonstrate the strengths of our approach in the citation trajectory prediction task. △ Less","2 October, 2022",https://arxiv.org/pdf/2210.00450
A Decade of Knowledge Graphs in Natural Language Processing: A Survey,Phillip Schneider;Tim Schopf;Juraj Vladika;Mikhail Galkin;Elena Simperl;Florian Matthes,"In pace with developments in the research field of artificial intelligence, knowledge graphs (KGs) have attracted a surge of interest from both academia and industry. As a representation of semantic relations between entities, KGs have proven to be particularly relevant for natural language processing (NLP), experiencing a rapid spread and wide adoption within recent years. Given the increasing amount of research work in this area, several KG-related approaches have been surveyed in the NLP research community. However, a comprehensive study that categorizes established topics and reviews the maturity of individual research streams remains absent to this day. Contributing to closing this gap, we systematically analyzed 507 papers from the literature on KGs in NLP. Our survey encompasses a multifaceted review of tasks, research types, and contributions. As a result, we present a structured overview of the research landscape, provide a taxonomy of tasks, summarize our findings, and highlight directions for future work. △ Less","30 September, 2022",https://arxiv.org/pdf/2210.00105
Digital Twin and Artificial Intelligence Incorporated With Surrogate Modeling for Hybrid and Sustainable Energy Systems,Abid Hossain Khan;Salauddin Omar;Nadia Mushtary;Richa Verma;Dinesh Kumar;Syed Alam,"Surrogate modeling has brought about a revolution in computation in the branches of science and engineering. Backed by Artificial Intelligence, a surrogate model can present highly accurate results with a significant reduction in computation time than computer simulation of actual models. Surrogate modeling techniques have found their use in numerous branches of science and engineering, energy system modeling being one of them. Since the idea of hybrid and sustainable energy systems is spreading rapidly in the modern world for the paradigm of the smart energy shift, researchers are exploring the future application of artificial intelligence-based surrogate modeling in analyzing and optimizing hybrid energy systems. One of the promising technologies for assessing applicability for the energy system is the digital twin, which can leverage surrogate modeling. This work presents a comprehensive framework/review on Artificial Intelligence-driven surrogate modeling and its applications with a focus on the digital twin framework and energy systems. The role of machine learning and artificial intelligence in constructing an effective surrogate model is explained. After that, different surrogate models developed for different sustainable energy sources are presented. Finally, digital twin surrogate models and associated uncertainties are described. △ Less","30 September, 2022",https://arxiv.org/pdf/2210.00073
Towards General-Purpose Representation Learning of Polygonal Geometries,Gengchen Mai;Chiyu Jiang;Weiwei Sun;Rui Zhu;Yao Xuan;Ling Cai;Krzysztof Janowicz;Stefano Ermon;Ni Lao,"Neural network representation learning for spatial data is a common need for geographic artificial intelligence (GeoAI) problems. In recent years, many advancements have been made in representation learning for points, polylines, and networks, whereas little progress has been made for polygons, especially complex polygonal geometries. In this work, we focus on developing a general-purpose polygon encoding model, which can encode a polygonal geometry (with or without holes, single or multipolygons) into an embedding space. The result embeddings can be leveraged directly (or finetuned) for downstream tasks such as shape classification, spatial relation prediction, and so on. To achieve model generalizability guarantees, we identify a few desirable properties: loop origin invariance, trivial vertex invariance, part permutation invariance, and topology awareness. We explore two different designs for the encoder: one derives all representations in the spatial domain; the other leverages spectral domain representations. For the spatial domain approach, we propose ResNet1D, a 1D CNN-based polygon encoder, which uses circular padding to achieve loop origin invariance on simple polygons. For the spectral domain approach, we develop NUFTspec based on Non-Uniform Fourier Transformation (NUFT), which naturally satisfies all the desired properties. We conduct experiments on two tasks: 1) shape classification based on MNIST; 2) spatial relation prediction based on two new datasets - DBSR-46K and DBSR-cplx46K. Our results show that NUFTspec and ResNet1D outperform multiple existing baselines with significant margins. While ResNet1D suffers from model performance degradation after shape-invariance geometry modifications, NUFTspec is very robust to these modifications due to the nature of the NUFT. △ Less","29 September, 2022",https://arxiv.org/pdf/2209.15458
Accurate Long-term Air Temperature Prediction with a Fusion of Artificial Intelligence and Data Reduction Techniques,Dušan Fister;Jorge Pérez-Aracil;César Peláez-Rodríguez;Javier Del Ser;Sancho Salcedo-Sanz,"In this paper three customised Artificial Intelligence (AI) frameworks, considering Deep Learning (convolutional neural networks), Machine Learning algorithms and data reduction techniques are proposed, for a problem of long-term summer air temperature prediction. Specifically, the prediction of average air temperature in the first and second August fortnights, using input data from previous months, at two different locations, Paris (France) and Córdoba (Spain), is considered. The target variable, mainly in the first August fortnight, can contain signals of extreme events such as heatwaves, like the mega-heatwave of 2003, which affected France and the Iberian Peninsula. Thus, an accurate prediction of long-term air temperature may be valuable also for different problems related to climate change, such as attribution of extreme events, and in other problems related to renewable energy. The analysis carried out this work is based on Reanalysis data, which are first processed by a correlation analysis among different prediction variables and the target (average air temperature in August first and second fortnights). An area with the largest correlation is located, and the variables within, after a feature selection process, are the input of different deep learning and ML algorithms. The experiments carried out show a very good prediction skill in the three proposed AI frameworks, both in Paris and Córdoba regions. △ Less","29 September, 2022",https://arxiv.org/pdf/2209.15424
Higher-order Neural Additive Models: An Interpretable Machine Learning Model with Feature Interactions,Minkyu Kim;Hyun-Soo Choi;Jinho Kim,"Black-box models, such as deep neural networks, exhibit superior predictive performances, but understanding their behavior is notoriously difficult. Many explainable artificial intelligence methods have been proposed to reveal the decision-making processes of black box models. However, their applications in high-stakes domains remain limited. Recently proposed neural additive models (NAM) have achieved state-of-the-art interpretable machine learning. NAM can provide straightforward interpretations with slight performance sacrifices compared with multi-layer perceptron. However, NAM can only model 1^{\text{st}}-order feature interactions; thus, it cannot capture the co-relationships between input features. To overcome this problem, we propose a novel interpretable machine learning method called higher-order neural additive models (HONAM) and a feature interaction method for high interpretability. HONAM can model arbitrary orders of feature interactions. Therefore, it can provide the high predictive performance and interpretability that high-stakes domains need. In addition, we propose a novel hidden unit to effectively learn sharp-shape functions. We conducted experiments using various real-world datasets to examine the effectiveness of HONAM. Furthermore, we demonstrate that HONAM can achieve fair AI with a slight performance sacrifice. The source code for HONAM is publicly available. △ Less","30 September, 2022",https://arxiv.org/pdf/2209.15409
A Multivariate Complexity Analysis of Qualitative Reasoning Problems,Leif Eriksson;Victor Lagerkvist,"Qualitative reasoning is an important subfield of artificial intelligence where one describes relationships with qualitative, rather than numerical, relations. Many such reasoning tasks, e.g., Allen's interval algebra, can be solved in 2^{O(n \cdot \log n)} time, but single-exponential running times 2^{O(n)} are currently far out of reach. In this paper we consider single-exponential algorithms via a multivariate analysis consisting of a fine-grained parameter n (e.g., the number of variables) and a coarse-grained parameter k expected to be relatively small. We introduce the classes FPE and XE of problems solvable in f(k) \cdot 2^{O(n)}, respectively f(k)^n, time, and prove several fundamental properties of these classes. We proceed by studying temporal reasoning problems and (1) show that the Partially Ordered Time problem of effective width k is solvable in 16^{kn} time and is thus included in XE, and (2) that the network consistency problem for Allen's interval algebra with no interval overlapping with more than k others is solvable in (2nk)^{2k} \cdot 2^{n} time and is included in FPE. Our multivariate approach is in no way limited to these to specific problems and may be a generally useful approach for obtaining single-exponential algorithms. △ Less","30 September, 2022",https://arxiv.org/pdf/2209.15275
OAK4XAI: Model towards Out-Of-Box eXplainable Artificial Intelligence for Digital Agriculture,Quoc Hung Ngo;Tahar Kechadi;Nhien-An Le-Khac,"Recent machine learning approaches have been effective in Artificial Intelligence (AI) applications. They produce robust results with a high level of accuracy. However, most of these techniques do not provide human-understandable explanations for supporting their results and decisions. They usually act as black boxes, and it is not easy to understand how decisions have been made. Explainable Artificial Intelligence (XAI), which has received much interest recently, tries to provide human-understandable explanations for decision-making and trained AI models. For instance, in digital agriculture, related domains often present peculiar or input features with no link to background knowledge. The application of the data mining process on agricultural data leads to results (knowledge), which are difficult to explain. In this paper, we propose a knowledge map model and an ontology design as an XAI framework (OAK4XAI) to deal with this issue. The framework does not only consider the data analysis part of the process, but it takes into account the semantics aspect of the domain knowledge via an ontology and a knowledge map model, provided as modules of the framework. Many ongoing XAI studies aim to provide accurate and verbalizable accounts for how given feature values contribute to model decisions. The proposed approach, however, focuses on providing consistent information and definitions of concepts, algorithms, and values involved in the data mining models. We built an Agriculture Computing Ontology (AgriComO) to explain the knowledge mined in agriculture. AgriComO has a well-designed structure and includes a wide range of concepts and transformations suitable for agriculture and computing domains. △ Less","29 September, 2022",https://arxiv.org/pdf/2209.15104
Greybox XAI: a Neural-Symbolic learning framework to produce interpretable predictions for image classification,Adrien Bennetot;Gianni Franchi;Javier Del Ser;Raja Chatila;Natalia Diaz-Rodriguez,"Although Deep Neural Networks (DNNs) have great generalization and prediction capabilities, their functioning does not allow a detailed explanation of their behavior. Opaque deep learning models are increasingly used to make important predictions in critical environments, and the danger is that they make and use predictions that cannot be justified or legitimized. Several eXplainable Artificial Intelligence (XAI) methods that separate explanations from machine learning models have emerged, but have shortcomings in faithfulness to the model actual functioning and robustness. As a result, there is a widespread agreement on the importance of endowing Deep Learning models with explanatory capabilities so that they can themselves provide an answer to why a particular prediction was made. First, we address the problem of the lack of universal criteria for XAI by formalizing what an explanation is. We also introduced a set of axioms and definitions to clarify XAI from a mathematical perspective. Finally, we present the Greybox XAI, a framework that composes a DNN and a transparent model thanks to the use of a symbolic Knowledge Base (KB). We extract a KB from the dataset and use it to train a transparent model (i.e., a logistic regression). An encoder-decoder architecture is trained on RGB images to produce an output similar to the KB used by the transparent model. Once the two models are trained independently, they are used compositionally to form an explainable predictive model. We show how this new architecture is accurate and explainable in several datasets. △ Less","26 September, 2022",https://arxiv.org/pdf/2209.14974
Domain-Unified Prompt Representations for Source-Free Domain Generalization,Hongjing Niu;Hanting Li;Feng Zhao;Bin Li,"Domain generalization (DG), aiming to make models work on unseen domains, is a surefire way toward general artificial intelligence. Limited by the scale and diversity of current DG datasets, it is difficult for existing methods to scale to diverse domains in open-world scenarios (e.g., science fiction and pixelate style). Therefore, the source-free domain generalization (SFDG) task is necessary and challenging. To address this issue, we propose an approach based on large-scale vision-language pretraining models (e.g., CLIP), which exploits the extensive domain information embedded in it. The proposed scheme generates diverse prompts from a domain bank that contains many more diverse domains than existing DG datasets. Furthermore, our method yields domain-unified representations from these prompts, thus being able to cope with samples from open-world domains. Extensive experiments on mainstream DG datasets, namely PACS, VLCS, OfficeHome, and DomainNet, show that the proposed method achieves competitive performance compared to state-of-the-art (SOTA) DG methods that require source domain data for training. Besides, we collect a small datasets consists of two domains to evaluate the open-world domain generalization ability of the proposed method. The source code and the dataset will be made publicly available at https://github.com/muse1998/Source-Free-Domain-Generalization △ Less","29 September, 2022",https://arxiv.org/pdf/2209.14926
DR.BENCH: Diagnostic Reasoning Benchmark for Clinical Natural Language Processing,Yanjun Gao;Dmitriy Dligach;Timothy Miller;John Caskey;Brihat Sharma;Matthew M Churpek;Majid Afshar,"The meaningful use of electronic health records (EHR) continues to progress in the digital era with clinical decision support systems augmented by artificial intelligence. A priority in improving provider experience is to overcome information overload and reduce the cognitive burden so fewer medical errors and cognitive biases are introduced during patient care. One major type of medical error is diagnostic error due to systematic or predictable errors in judgment that rely on heuristics. The potential for clinical natural language processing (cNLP) to model diagnostic reasoning in humans with forward reasoning from data to diagnosis and potentially reduce the cognitive burden and medical error has not been investigated. Existing tasks to advance the science in cNLP have largely focused on information extraction and named entity recognition through classification tasks. We introduce a novel suite of tasks coined as Diagnostic Reasoning Benchmarks, DR.BENCH, as a new benchmark for developing and evaluating cNLP models with clinical diagnostic reasoning ability. The suite includes six tasks from ten publicly available datasets addressing clinical text understanding, medical knowledge reasoning, and diagnosis generation. DR.BENCH is the first clinical suite of tasks designed to be a natural language generation framework to evaluate pre-trained language models. Experiments with state-of-the-art pre-trained generative language models using large general domain models and models that were continually trained on a medical corpus demonstrate opportunities for improvement when evaluated in DR. BENCH. We share DR. BENCH as a publicly available GitLab repository with a systematic approach to load and evaluate models for the cNLP community. △ Less","13 December, 2022",https://arxiv.org/pdf/2209.14901
EMF-Aware MU-MIMO Beamforming in RIS-Aided Cellular Networks,Yi Yu;Rita Ibrahim;Dinh-Thuy Phan-Huy,"Reconfigurable Intelligent Surfaces (RISs) are one of the key emerging 6th Generation (6G) technologies that are expected to improve the link budgets between transmitters and receivers by adding artificial propagation paths. In such re-configured propagation environment, Downlink (DL) Multi-User Multi-Input Multi-Output (MU-MIMO) brings capacity improvement to cellular networks. It benefits from the spatial dimension offered by MIMO systems to enable simultaneous transmission of independent data streams to multiple users on the same radio resources by applying appropriate Beamforming (BF) schemes. However, in some cases, serving the same subset of users for a long period of time may cause some undesired regions where the average Electromagnetic Field Exposure (EMFE) exceeds the regulatory limits. To address this challenge, we propose in this paper a novel Electromagnetic Field (EMF) aware MU-MIMO BF scheme that aims to optimize the overall capacity under EMF constraints in RIS-aided cellular networks. △ Less","29 September, 2022",https://arxiv.org/pdf/2209.14785
Neural Methods for Logical Reasoning Over Knowledge Graphs,Alfonso Amayuelas;Shuai Zhang;Susie Xi Rao;Ce Zhang,"Reasoning is a fundamental problem for computers and deeply studied in Artificial Intelligence. In this paper, we specifically focus on answering multi-hop logical queries on Knowledge Graphs (KGs). This is a complicated task because, in real-world scenarios, the graphs tend to be large and incomplete. Most previous works have been unable to create models that accept full First-Order Logical (FOL) queries, which include negative queries, and have only been able to process a limited set of query structures. Additionally, most methods present logic operators that can only perform the logical operation they are made for. We introduce a set of models that use Neural Networks to create one-point vector embeddings to answer the queries. The versatility of neural networks allows the framework to handle FOL queries with Conjunction (\wedge), Disjunction (\vee) and Negation (\neg) operators. We demonstrate experimentally the performance of our model through extensive experimentation on well-known benchmarking datasets. Besides having more versatile operators, the models achieve a 10\% relative increase over the best performing state of the art and more than 30\% over the original method based on single-point vector embeddings. △ Less","28 September, 2022",https://arxiv.org/pdf/2209.14464
Biological connectomes as a representation for the architecture of artificial neural networks,Samuel Schmidgall;Catherine Schuman;Maryam Parsa,"Grand efforts in neuroscience are working toward mapping the connectomes of many new species, including the near completion of the Drosophila melanogaster. It is important to ask whether these models could benefit artificial intelligence. In this work we ask two fundamental questions: (1) where and when biological connectomes can provide use in machine learning, (2) which design principles are necessary for extracting a good representation of the connectome. Toward this end, we translate the motor circuit of the C. Elegans nematode into artificial neural networks at varying levels of biophysical realism and evaluate the outcome of training these networks on motor and non-motor behavioral tasks. We demonstrate that biophysical realism need not be upheld to attain the advantages of using biological circuits. We also establish that, even if the exact wiring diagram is not retained, the architectural statistics provide a valuable prior. Finally, we show that while the C. Elegans locomotion circuit provides a powerful inductive bias on locomotion problems, its structure may hinder performance on tasks unrelated to locomotion such as visual classification problems. △ Less","5 October, 2022",https://arxiv.org/pdf/2209.14406
Automatic Analysis of Available Source Code of Top Artificial Intelligence Conference Papers,Jialiang Lin;Yingmin Wang;Yao Yu;Yu Zhou;Yidong Chen;Xiaodong Shi,"Source code is essential for researchers to reproduce the methods and replicate the results of artificial intelligence (AI) papers. Some organizations and researchers manually collect AI papers with available source code to contribute to the AI community. However, manual collection is a labor-intensive and time-consuming task. To address this issue, we propose a method to automatically identify papers with available source code and extract their source code repository URLs. With this method, we find that 20.5% of regular papers of 10 top AI conferences published from 2010 to 2019 are identified as papers with available source code and that 8.1% of these source code repositories are no longer accessible. We also create the XMU NLP Lab README Dataset, the largest dataset of labeled README files for source code document research. Through this dataset, we have discovered that quite a few README files have no installation instructions or usage tutorials provided. Further, a large-scale comprehensive statistical analysis is made for a general picture of the source code of AI conference papers. The proposed solution can also go beyond AI conference papers to analyze other scientific papers from both journals and conferences to shed light on more domains. △ Less","28 September, 2022",https://arxiv.org/pdf/2209.14155
"Mobile Edge Computing, Metaverse, 6G Wireless Communications, Artificial Intelligence, and Blockchain: Survey and Their Convergence",Yitong Wang;Jun Zhao,"With the advances of the Internet of Things (IoT) and 5G/6G wireless communications, the paradigms of mobile computing have developed dramatically in recent years, from centralized mobile cloud computing to distributed fog computing and mobile edge computing (MEC). MEC pushes compute-intensive assignments to the edge of the network and brings resources as close to the endpoints as possible, addressing the shortcomings of mobile devices with regard to storage space, resource optimisation, computational performance and efficiency. Compared to cloud computing, as the distributed and closer infrastructure, the convergence of MEC with other emerging technologies, including the Metaverse, 6G wireless communications, artificial intelligence (AI), and blockchain, also solves the problems of network resource allocation, more network load as well as latency requirements. Accordingly, this paper investigates the computational paradigms used to meet the stringent requirements of modern applications. The application scenarios of MEC in mobile augmented reality (MAR) are provided. Furthermore, this survey presents the motivation of MEC-based Metaverse and introduces the applications of MEC to the Metaverse. Particular emphasis is given on a set of technical fusions mentioned above, e.g., 6G with MEC paradigm, MEC strengthened by blockchain, etc. △ Less","28 September, 2022",https://arxiv.org/pdf/2209.14147
On the visual analytic intelligence of neural networks,Stanisław Woźniak;Hlynur Jónsson;Giovanni Cherubini;Angeliki Pantazi;Evangelos Eleftheriou,"Visual oddity task was conceived as a universal ethnic-independent analytic intelligence test for humans. Advancements in artificial intelligence led to important breakthroughs, yet competing with humans on such analytic intelligence tasks remains challenging and typically resorts to non-biologically-plausible architectures. We present a biologically realistic system that receives inputs from synthetic eye movements - saccades, and processes them with neurons incorporating dynamics of neocortical neurons. We introduce a procedurally generated visual oddity dataset to train an architecture extending conventional relational networks and our proposed system. Both approaches surpass the human accuracy, and we uncover that both share the same essential underlying mechanism of reasoning. Finally, we show that the biologically inspired network achieves superior accuracy, learns faster and requires fewer parameters than the conventional network. △ Less","28 September, 2022",https://arxiv.org/pdf/2209.14017
What Does DALL-E 2 Know About Radiology?,Lisa C. Adams;Felix Busch;Daniel Truhn;Marcus R. Makowski;Hugo JWL. Aerts;Keno K. Bressem,"Generative models such as DALL-E 2 could represent a promising future tool for image generation, augmentation, and manipulation for artificial intelligence research in radiology provided that these models have sufficient medical domain knowledge. Here we show that DALL-E 2 has learned relevant representations of X-ray images with promising capabilities in terms of zero-shot text-to-image generation of new images, continuation of an image beyond its original boundaries, or removal of elements, while pathology generation or CT, MRI, and ultrasound images are still limited. The use of generative models for augmenting and generating radiological data thus seems feasible, even if further fine-tuning and adaptation of these models to the respective domain is required beforehand. △ Less","27 September, 2022",https://arxiv.org/pdf/2209.13696
"Artificial Intelligence for Cybersecurity: Threats, Attacks and Mitigation",Abhilash Chakraborty;Anupam Biswas;Ajoy Kumar Khan,"With the advent of the digital era, every day-to-day task is automated due to technological advances. However, technology has yet to provide people with enough tools and safeguards. As the internet connects more-and-more devices around the globe, the question of securing the connected devices grows at an even spiral rate. Data thefts, identity thefts, fraudulent transactions, password compromises, and system breaches are becoming regular everyday news. The surging menace of cyber-attacks got a jolt from the recent advancements in Artificial Intelligence. AI is being applied in almost every field of different sciences and engineering. The intervention of AI not only automates a particular task but also improves efficiency by many folds. So it is evident that such a scrumptious spread would be very appetizing to cybercriminals. Thus the conventional cyber threats and attacks are now ``intelligent"" threats. This article discusses cybersecurity and cyber threats along with both conventional and intelligent ways of defense against cyber-attacks. Furthermore finally, end the discussion with the potential prospects of the future of AI in cybersecurity. △ Less","27 September, 2022",https://arxiv.org/pdf/2209.13454
FedStack: Personalized activity monitoring using stacked federated learning,Thanveer Shaik;Xiaohui Tao;Niall Higgins;Raj Gururajan;Yuefeng Li;Xujuan Zhou;U Rajendra Acharya,"Recent advances in remote patient monitoring (RPM) systems can recognize various human activities to measure vital signs, including subtle motions from superficial vessels. There is a growing interest in applying artificial intelligence (AI) to this area of healthcare by addressing known limitations and challenges such as predicting and classifying vital signs and physical movements, which are considered crucial tasks. Federated learning is a relatively new AI technique designed to enhance data privacy by decentralizing traditional machine learning modeling. However, traditional federated learning requires identical architectural models to be trained across the local clients and global servers. This limits global model architecture due to the lack of local models heterogeneity. To overcome this, a novel federated learning architecture, FedStack, which supports ensembling heterogeneous architectural client models was proposed in this study. This work offers a protected privacy system for hospitalized in-patients in a decentralized approach and identifies optimum sensor placement. The proposed architecture was applied to a mobile health sensor benchmark dataset from 10 different subjects to classify 12 routine activities. Three AI models, ANN, CNN, and Bi-LSTM were trained on individual subject data. The federated learning architecture was applied to these models to build local and global models capable of state of the art performances. The local CNN model outperformed ANN and Bi-LSTM models on each subject data. Our proposed work has demonstrated better performance for heterogeneous stacking of the local models compared to homogeneous stacking. This work sets the stage to build an enhanced RPM system that incorporates client privacy to assist with clinical observations for patients in an acute mental health facility and ultimately help to prevent unexpected death. △ Less","26 September, 2022",https://arxiv.org/pdf/2209.13080
Mitigating Attacks on Artificial Intelligence-based Spectrum Sensing for Cellular Network Signals,Ferhat Ozgur Catak;Murat Kuzlu;Salih Sarp;Evren Catak;Umit Cali,"Cellular networks (LTE, 5G, and beyond) are dramatically growing with high demand from consumers and more promising than the other wireless networks with advanced telecommunication technologies. The main goal of these networks is to connect billions of devices, systems, and users with high-speed data transmission, high cell capacity, and low latency, as well as to support a wide range of new applications, such as virtual reality, metaverse, telehealth, online education, autonomous and flying vehicles, advanced manufacturing, and many more. To achieve these goals, spectrum sensing has been paid more attention, along with new approaches using artificial intelligence (AI) methods for spectrum management in cellular networks. This paper provides a vulnerability analysis of spectrum sensing approaches using AI-based semantic segmentation models for identifying cellular network signals under adversarial attacks with and without defensive distillation methods. The results showed that mitigation methods can significantly reduce the vulnerabilities of AI-based spectrum sensing models against adversarial attacks. △ Less","27 September, 2022",https://arxiv.org/pdf/2209.13007
"A Comprehensive Review of Trends, Applications and Challenges In Out-of-Distribution Detection",Navid Ghassemi;Ehsan Fazl-Ersi,"With recent advancements in artificial intelligence, its applications can be seen in every aspect of humans' daily life. From voice assistants to mobile healthcare and autonomous driving, we rely on the performance of AI methods for many critical tasks; therefore, it is essential to assert the performance of models in proper means to prevent damage. One of the shortfalls of AI models in general, and deep machine learning in particular, is a drop in performance when faced with shifts in the distribution of data. Nonetheless, these shifts are always expected in real-world applications; thus, a field of study has emerged, focusing on detecting out-of-distribution data subsets and enabling a more comprehensive generalization. Furthermore, as many deep learning based models have achieved near-perfect results on benchmark datasets, the need to evaluate these models' reliability and trustworthiness for pushing towards real-world applications is felt more strongly than ever. This has given rise to a growing number of studies in the field of out-of-distribution detection and domain generalization, which begs the need for surveys that compare these studies from various perspectives and highlight their straightens and weaknesses. This paper presents a survey that, in addition to reviewing more than 70 papers in this field, presents challenges and directions for future works and offers a unifying look into various types of data shifts and solutions for better generalization. △ Less","26 September, 2022",https://arxiv.org/pdf/2209.12935
Environmental and Social Sustainability of Creative-Ai,André Holzapfel;Petra Jääskeläinen;Anna-Kaisa Kaila,"The recent developments of artificial intelligence increase its capability for the creation of arts in both largely autonomous and collaborative contexts. In both contexts, Ai aims to imitate, combine, and extend existing artistic styles, and can transform creative practices. In our ongoing research, we investigate such Creative-Ai from sustainability and ethical perspectives. The two main focus areas are understanding the environmental sustainability aspects (material, practices) in the context of artistic processes that involve Creative-Ai, and ethical issues related to who gets to be involved in the creation process (power, authorship, ownership). This paper provides an outline of our ongoing research in these two directions. We will present our interdisciplinary approach, which combines interviews, workshops, online ethnography, and energy measurements, to address our research questions: How is Creative-Ai currently used by artist communities, and which future applications do artists imagine? When Ai is applied to creating art, how might it impact the economy and environment? And, how can answers to these questions guide requirements for intellectual property regimes for Creative-Ai? △ Less","28 September, 2022",https://arxiv.org/pdf/2209.12879
"Language and Intelligence, Artificial vs. Natural or What Can and What Cannot AI Do with NL?",Gyula Klima,"In this talk, I argue that there are certain pragmatic features of natural language (that I will call 'productivity' and 'malleability', on top of syntactical generativity and semantical compositionality), which are not only hard, but even impossible to capture in an artificial language used by an AI system, and the reason for this is to be found in certain deep, metaphysical differences between artificial and natural intelligence, accounting for the differences in their respective processes of concept-formation. △ Less","31 August, 2022",https://arxiv.org/pdf/2209.12829
AI-powered Language Assessment Tools for Dementia,Mahboobeh Parsapoor;Muhammad Raisul Alam;Alex Mihailidis,"The main objective of this paper is to propose an approach for developing an Artificial Intelligence (AI)-powered Language Assessment (LA) tool. Such tools can be used to assess language impairments associated with dementia in older adults. The Machine Learning (ML) classifiers are the main parts of our proposed approach, therefore to develop an accurate tool with high sensitivity and specificity, we consider different binary classifiers and evaluate their performances. We also assess the reliability and validity of our approach by comparing the impact of different types of language tasks, features, and recording media on the performance of ML classifiers. △ Less","13 September, 2022",https://arxiv.org/pdf/2209.12652
An Artificial Intelligence Outlook for Colorectal Cancer Screening,Panagiotis Katrakazas;Aristotelis Ballas;Marco Anisetti;Ilias Spais,"Colorectal cancer is the third most common tumor in men and the second in women, accounting for 10% of all tumors worldwide. It ranks second in cancer-related deaths with 9.4%, following lung cancer. The decrease in mortality rate documented over the last 20 years has shown signs of slowing down since 2017, necessitating concentrated actions on specific measures that have exhibited considerable potential. As such, the technical foundation and research evidence for blood-derived protein markers have been set, pending comparative validation, clinical implementation and integration into an artificial intelligence enabled decision support framework that also considers knowledge on risk factors. The current paper aspires to constitute the driving force for creating change in colorectal cancer screening by reviewing existing medical practices through accessible and non-invasive risk estimation, employing a straightforward artificial intelligence outlook. △ Less","5 September, 2022",https://arxiv.org/pdf/2209.12624
Cognitive Architecture for Co-Evolutionary Hybrid Intelligence,Kirill Krinkin;Yulia Shichkina,"This paper questions the feasibility of a strong (general) data-centric artificial intelligence (AI). The disadvantages of this type of intelligence are discussed. As an alternative, the concept of co-evolutionary hybrid intelligence is proposed. It is based on the cognitive interoperability of man and machine. An analysis of existing approaches to the construction of cognitive architectures is given. An architecture seamlessly incorporates a human into the loop of intelligent problem solving is considered. The article is organized as follows. The first part contains a critique of data-centric intelligent systems. The reasons why it is impossible to create a strong artificial intelligence based on this type of intelligence are indicated. The second part briefly presents the concept of co-evolutionary hybrid intelligence and shows its advantages. The third part gives an overview and analysis of existing cognitive architectures. It is concluded that many do not consider humans part of the intelligent data processing process. The next part discusses the cognitive architecture for co-evolutionary hybrid intelligence, providing integration with humans. It finishes with general conclusions about the feasibility of developing intelligent systems with humans in the problem-solving loop. △ Less","5 September, 2022",https://arxiv.org/pdf/2209.12623
Survey on Applications of Neurosymbolic Artificial Intelligence,Djallel Bouneffouf;Charu C. Aggarwal,"In recent years, the Neurosymbolic framework has attracted a lot of attention in various applications, from recommender systems and information retrieval to healthcare and finance. This success is due to its stellar performance combined with attractive properties, such as learning and reasoning. The new emerging Neurosymbolic field is currently experiencing a renaissance, as novel frameworks and algorithms motivated by various practical applications are being introduced, building on top of the classical neural and reasoning problem setting. This article aims to provide a comprehensive review of significant recent developments in real-world applications of Neurosymbolic Artificial Intelligence. Specifically, we introduce a taxonomy of common Neurosymbolic applications and summarize the state-of-the-art for each of those domains. Furthermore, we identify important current trends and provide new perspectives pertaining to the future of this burgeoning field. △ Less","8 September, 2022",https://arxiv.org/pdf/2209.12618
Improving Document Image Understanding with Reinforcement Finetuning,Bao-Sinh Nguyen;Dung Tien Le;Hieu M. Vu;Tuan Anh D. Nguyen;Minh-Tien Nguyen;Hung Le,"Successful Artificial Intelligence systems often require numerous labeled data to extract information from document images. In this paper, we investigate the problem of improving the performance of Artificial Intelligence systems in understanding document images, especially in cases where training data is limited. We address the problem by proposing a novel finetuning method using reinforcement learning. Our approach treats the Information Extraction model as a policy network and uses policy gradient training to update the model to maximize combined reward functions that complement the traditional cross-entropy losses. Our experiments on four datasets using labels and expert feedback demonstrate that our finetuning mechanism consistently improves the performance of a state-of-the-art information extractor, especially in the small training data regime. △ Less","26 September, 2022",https://arxiv.org/pdf/2209.12561
Delayed Geometric Discounts: An Alternative Criterion for Reinforcement Learning,Firas Jarboui;Ahmed Akakzia,"The endeavor of artificial intelligence (AI) is to design autonomous agents capable of achieving complex tasks. Namely, reinforcement learning (RL) proposes a theoretical background to learn optimal behaviors. In practice, RL algorithms rely on geometric discounts to evaluate this optimality. Unfortunately, this does not cover decision processes where future returns are not exponentially less valuable. Depending on the problem, this limitation induces sample-inefficiency (as feed-backs are exponentially decayed) and requires additional curricula/exploration mechanisms (to deal with sparse, deceptive or adversarial rewards). In this paper, we tackle these issues by generalizing the discounted problem formulation with a family of delayed objective functions. We investigate the underlying RL problem to derive: 1) the optimal stationary solution and 2) an approximation of the optimal non-stationary control. The devised algorithms solved hard exploration problems on tabular environment and improved sample-efficiency on classic simulated robotics benchmarks. △ Less","26 September, 2022",https://arxiv.org/pdf/2209.12483
Certifying Parity Reasoning Efficiently Using Pseudo-Boolean Proofs,Stephan Gocht;Jakob Nordström,"The dramatic improvements in combinatorial optimization algorithms over the last decades have had a major impact in artificial intelligence, operations research, and beyond, but the output of current state-of-the-art solvers is often hard to verify and is sometimes wrong. For Boolean satisfiability (SAT) solvers proof logging has been introduced as a way to certify correctness, but the methods used seem hard to generalize to stronger paradigms. What is more, even for enhanced SAT techniques such as parity (XOR) reasoning, cardinality detection, and symmetry handling, it has remained beyond reach to design practically efficient proofs in the standard DRAT format. In this work, we show how to instead use pseudo-Boolean inequalities with extension variables to concisely justify XOR reasoning. Our experimental evaluation of a SAT solver integration shows a dramatic decrease in proof logging and verification time compared to existing DRAT methods. Since our method is a strict generalization of DRAT, and readily lends itself to expressing also 0-1 programming and even constraint programming problems, we hope this work points the way towards a unified approach for efficient machine-verifiable proofs for a rich class of combinatorial optimization paradigms. △ Less","25 September, 2022",https://arxiv.org/pdf/2209.12185
Machine Learning and Artificial Intelligence-Driven Multi-Scale Modeling for High Burnup Accident-Tolerant Fuels for Light Water-Based SMR Applications,Md. Shamim Hassan;Abid Hossain Khan;Richa Verma;Dinesh Kumar;Kazuma Kobayashi;Shoaib Usman;Syed Alam,"The concept of small modular reactor has changed the outlook for tackling future energy crises. This new reactor technology is very promising considering its lower investment requirements, modularity, design simplicity, and enhanced safety features. The application of artificial intelligence-driven multi-scale modeling (neutronics, thermal hydraulics, fuel performance, etc.) incorporating Digital Twin and associated uncertainties in the research of small modular reactors is a recent concept. In this work, a comprehensive study is conducted on the multiscale modeling of accident-tolerant fuels. The application of these fuels in the light water-based small modular reactors is explored. This chapter also focuses on the application of machine learning and artificial intelligence in the design optimization, control, and monitoring of small modular reactors. Finally, a brief assessment of the research gap on the application of artificial intelligence to the development of high burnup composite accident-tolerant fuels is provided. Necessary actions to fulfill these gaps are also discussed. △ Less","25 September, 2022",https://arxiv.org/pdf/2209.12146
Asset Pricing and Deep Learning,Chen Zhang,"Traditional machine learning methods have been widely studied in financial innovation. My study focuses on the application of deep learning methods on asset pricing. I investigate various deep learning methods for asset pricing, especially for risk premia measurement. All models take the same set of predictive signals (firm characteristics, systematic risks and macroeconomics). I demonstrate high performance of all kinds of state-of-the-art (SOTA) deep learning methods, and figure out that RNNs with memory mechanism and attention have the best performance in terms of predictivity. Furthermore, I demonstrate large economic gains to investors using deep learning forecasts. The results of my comparative experiments highlight the importance of domain knowledge and financial theory when designing deep learning models. I also show return prediction tasks bring new challenges to deep learning. The time varying distribution causes distribution shift problem, which is essential for financial time series prediction. I demonstrate that deep learning methods can improve asset risk premium measurement. Due to the booming deep learning studies, they can constantly promote the study of underlying financial mechanisms behind asset pricing. I also propose a promising research method that learning from data and figuring out the underlying economic mechanisms through explainable artificial intelligence (AI) methods. My findings not only justify the value of deep learning in blooming fintech development, but also highlight their prospects and advantages over traditional machine learning methods. △ Less","24 September, 2022",https://arxiv.org/pdf/2209.12014
"Taking the Intentional Stance Seriously, or ""Intending"" to Improve Cognitive Systems",Will Bridewell,"Finding claims that researchers have made considerable progress in artificial intelligence over the last several decades is easy. However, our everyday interactions with cognitive systems (e.g., Siri, Alexa, DALL-E) quickly move from intriguing to frustrating. One cause of those frustrations rests in a mismatch between the expectations we have due to our inherent, folk-psychological theories and the real limitations we experience with existing computer programs. The software does not understand that people have goals, beliefs about how to achieve those goals, and intentions to act accordingly. One way to align cognitive systems with our expectations is to imbue them with mental states that mirror those we use to predict and explain human behavior. This paper discusses these concerns and illustrates the challenge of following this route by analyzing the mental state 'intention.' That analysis is joined with high-level methodological suggestions that support progress in this endeavor. △ Less","8 November, 2022",https://arxiv.org/pdf/2209.11764
Assessment of cognitive characteristics in intelligent systems and predictive ability,Oleg V. Kubryak;Sergey V. Kovalchuk;Nadezhda G. Bagdasaryan,"The article proposes a universal dual-axis intelligent systems assessment scale. The scale considers the properties of intelligent systems within the environmental context, which develops over time. In contrast to the frequent consideration of the 'mind' of artificial intelligent systems on a scale from 'weak' to 'strong', we highlight the modulating influences of anticipatory ability on their 'brute force'. In addition, the complexity, the 'weight' of the cognitive task and the ability to critically assess it beforehand determine the actual set of cognitive tools, the use of which provides the best result in these conditions. In fact, the presence of 'common sense' options is what connects the ability to solve a problem with the correct use of such an ability itself. The degree of 'correctness' and 'adequacy' is determined by the combination of a suitable solution with the temporal characteristics of the event, phenomenon, object or subject under study. △ Less","16 September, 2022",https://arxiv.org/pdf/2209.11761
"SERF: Interpretable Sleep Staging using Embeddings, Rules, and Features",Irfan Al-Hussaini;Cassie S. Mitchell,"The accuracy of recent deep learning based clinical decision support systems is promising. However, lack of model interpretability remains an obstacle to widespread adoption of artificial intelligence in healthcare. Using sleep as a case study, we propose a generalizable method to combine clinical interpretability with high accuracy derived from black-box deep learning. Clinician-determined sleep stages from polysomnogram (PSG) remain the gold standard for evaluating sleep quality. However, PSG manual annotation by experts is expensive and time-prohibitive. We propose SERF, interpretable Sleep staging using Embeddings, Rules, and Features to read PSG. SERF provides interpretation of classified sleep stages through meaningful features derived from the AASM Manual for the Scoring of Sleep and Associated Events. In SERF, the embeddings obtained from a hybrid of convolutional and recurrent neural networks are transposed to the interpretable feature space. These representative interpretable features are used to train simple models like a shallow decision tree for classification. Model results are validated on two publicly available datasets. SERF surpasses the current state-of-the-art for interpretable sleep staging by 2%. Using Gradient Boosted Trees as the classifier, SERF obtains 0.766 κ and 0.870 AUC-ROC, within 2% of the current state-of-the-art black-box models. △ Less","25 September, 2022",https://arxiv.org/pdf/2209.11174
Seen to Unseen: When Fuzzy Inference System Predicts IoT Device Positioning Labels That Had Not Appeared in Training Phase,Han Xu;Zheming Zuo;Jie Li;Victor Chang,"Situating at the core of Artificial Intelligence (AI), Machine Learning (ML), and more specifically, Deep Learning (DL) have embraced great success in the past two decades. However, unseen class label prediction is far less explored due to missing classes being invisible in training ML or DL models. In this work, we propose a fuzzy inference system to cope with such a challenge by adopting TSK+ fuzzy inference engine in conjunction with the Curvature-based Feature Selection (CFS) method. The practical feasibility of our system has been evaluated by predicting the positioning labels of networking devices within the realm of the Internet of Things (IoT). Competitive prediction performance confirms the efficiency and efficacy of our system, especially when a large number of continuous class labels are unseen during the model training stage. △ Less","21 September, 2022",https://arxiv.org/pdf/2209.10627
Current and Near-Term AI as a Potential Existential Risk Factor,Benjamin S. Bucknall;Shiri Dori-Hacohen,"There is a substantial and ever-growing corpus of evidence and literature exploring the impacts of Artificial intelligence (AI) technologies on society, politics, and humanity as a whole. A separate, parallel body of work has explored existential risks to humanity, including but not limited to that stemming from unaligned Artificial General Intelligence (AGI). In this paper, we problematise the notion that current and near-term artificial intelligence technologies have the potential to contribute to existential risk by acting as intermediate risk factors, and that this potential is not limited to the unaligned AGI scenario. We propose the hypothesis that certain already-documented effects of AI can act as existential risk factors, magnifying the likelihood of previously identified sources of existential risk. Moreover, future developments in the coming decade hold the potential to significantly exacerbate these risk factors, even in the absence of artificial general intelligence. Our main contribution is a (non-exhaustive) exposition of potential AI risk factors and the causal relationships between them, focusing on how AI can affect power dynamics and information security. This exposition demonstrates that there exist causal pathways from AI systems to existential risks that do not presuppose hypothetical future AI capabilities. △ Less","21 September, 2022",https://arxiv.org/pdf/2209.10604
Artificial Intelligence-Based Image Reconstruction in Cardiac Magnetic Resonance,Chen Qin;Daniel Rueckert,"Artificial intelligence (AI) and Machine Learning (ML) have shown great potential in improving the medical imaging workflow, from image acquisition and reconstruction to disease diagnosis and treatment. Particularly, in recent years, there has been a significant growth in the use of AI and ML algorithms, especially Deep Learning (DL) based methods, for medical image reconstruction. DL techniques have shown to be competitive and often superior over conventional reconstruction methods in terms of both reconstruction quality and computational efficiency. The use of DL-based image reconstruction also provides promising opportunities to transform the way cardiac images are acquired and reconstructed. In this chapter, we will review recent advances in DL-based reconstruction techniques for cardiac imaging, with emphasis on cardiac magnetic resonance (CMR) image reconstruction. We mainly focus on supervised DL methods for the application, including image post-processing techniques, model-driven approaches and k-space based methods. Current limitations, challenges and future opportunities of DL for cardiac image reconstruction are also discussed. △ Less","21 September, 2022",https://arxiv.org/pdf/2209.10298
Deep Learning based pipeline for anomaly detection and quality enhancement in industrial binder jetting processes,Alexander Zeiser;Bas van Stein;Thomas Bäck,"Anomaly detection describes methods of finding abnormal states, instances or data points that differ from a normal value space. Industrial processes are a domain where predicitve models are needed for finding anomalous data instances for quality enhancement. A main challenge, however, is absence of labels in this environment. This paper contributes to a data-centric way of approaching artificial intelligence in industrial production. With a use case from additive manufacturing for automotive components we present a deep-learning-based image processing pipeline. Additionally, we integrate the concept of domain randomisation and synthetic data in the loop that shows promising results for bridging advances in deep learning and its application to real-world, industrial production processes. △ Less","23 September, 2022",https://arxiv.org/pdf/2209.10178
Leak Detection in Natural Gas Pipeline Using Machine Learning Models,Adebayo Oshingbesan,"Leak detection in gas pipelines is an important and persistent problem in the Oil and Gas industry. This is particularly important as pipelines are the most common way of transporting natural gas. This research aims to study the ability of data-driven intelligent models to detect small leaks for a natural gas pipeline using basic operational parameters and then compare the intelligent models among themselves using existing performance metrics. This project applies the observer design technique to detect leaks in natural gas pipelines using a regressoclassification hierarchical model where an intelligent model acts as a regressor and a modified logistic regression model acts as a classifier. Five intelligent models (gradient boosting, decision trees, random forest, support vector machine and artificial neural network) are studied in this project using a pipeline data stream of four weeks. The results shows that while support vector machine and artificial neural networks are better regressors than the others, they do not provide the best results in leak detection due to their internal complexities and the volume of data used. The random forest and decision tree models are the most sensitive as they can detect a leak of 0.1% of nominal flow in about 2 hours. All the intelligent models had high reliability with zero false alarm rate in testing phase. The average time to leak detection for all the intelligent models was compared to a real time transient model in literature. The results show that intelligent models perform relatively well in the problem of leak detection. This result suggests that intelligent models could be used alongside a real time transient model to significantly improve leak detection results. △ Less","21 September, 2022",https://arxiv.org/pdf/2209.10121
NeurOLight: A Physics-Agnostic Neural Operator Enabling Parametric Photonic Device Simulation,Jiaqi Gu;Zhengqi Gao;Chenghao Feng;Hanqing Zhu;Ray T. Chen;Duane S. Boning;David Z. Pan,"Optical computing is an emerging technology for next-generation efficient artificial intelligence (AI) due to its ultra-high speed and efficiency. Electromagnetic field simulation is critical to the design, optimization, and validation of photonic devices and circuits. However, costly numerical simulation significantly hinders the scalability and turn-around time in the photonic circuit design loop. Recently, physics-informed neural networks have been proposed to predict the optical field solution of a single instance of a partial differential equation (PDE) with predefined parameters. Their complicated PDE formulation and lack of efficient parametrization mechanisms limit their flexibility and generalization in practical simulation scenarios. In this work, for the first time, a physics-agnostic neural operator-based framework, dubbed NeurOLight, is proposed to learn a family of frequency-domain Maxwell PDEs for ultra-fast parametric photonic device simulation. We balance the efficiency and generalization of NeurOLight via several novel techniques. Specifically, we discretize different devices into a unified domain, represent parametric PDEs with a compact wave prior, and encode the incident light via masked source modeling. We design our model with parameter-efficient cross-shaped NeurOLight blocks and adopt superposition-based augmentation for data-efficient learning. With these synergistic approaches, NeurOLight generalizes to a large space of unseen simulation settings, demonstrates 2-orders-of-magnitude faster simulation speed than numerical solvers, and outperforms prior neural network models by ~54% lower prediction error with ~44% fewer parameters. Our code is available at https://github.com/JeremieMelo/NeurOLight. △ Less","19 September, 2022",https://arxiv.org/pdf/2209.10098
Setting the rhythm scene: deep learning-based drum loop generation from arbitrary language cues,Ignacio J. Tripodi,"Generative artificial intelligence models can be a valuable aid to music composition and live performance, both to aid the professional musician and to help democratize the music creation process for hobbyists. Here we present a novel method that, given an English word or phrase, generates 2 compasses of a 4-piece drum pattern that embodies the ""mood"" of the given language cue, or that could be used for an audiovisual scene described by the language cue. We envision this tool as composition aid for electronic music and audiovisual soundtrack production, or an improvisation tool for live performance. In order to produce the training samples for this model, besides manual annotation of the ""scene"" or ""mood"" terms, we have designed a novel method to extract the consensus drum track of any song. This consists of a 2-bar, 4-piece drum pattern that represents the main percussive motif of a song, which could be imported into any music loop device or live looping software. These two key components (drum pattern generation from a generalizable input, and consensus percussion extraction) present a novel approach to computer-aided composition and provide a stepping stone for more comprehensive rhythm generation. △ Less","20 September, 2022",https://arxiv.org/pdf/2209.10016
Target-Guided Open-Domain Conversation Planning,Yosuke Kishinami;Reina Akama;Shiki Sato;Ryoko Tokuhisa;Jun Suzuki;Kentaro Inui,"Prior studies addressing target-oriented conversational tasks lack a crucial notion that has been intensively studied in the context of goal-oriented artificial intelligence agents, namely, planning. In this study, we propose the task of Target-Guided Open-Domain Conversation Planning (TGCP) task to evaluate whether neural conversational agents have goal-oriented conversation planning abilities. Using the TGCP task, we investigate the conversation planning abilities of existing retrieval models and recent strong generative models. The experimental results reveal the challenges facing current technology. △ Less","20 September, 2022",https://arxiv.org/pdf/2209.09746
Artificial Intelligence in Concrete Materials: A Scientometric View,Zhanzhao Li;Aleksandra Radlińska,"Artificial intelligence (AI) has emerged as a transformative and versatile tool, breaking new frontiers across scientific domains. Among its most promising applications, AI research is blossoming in concrete science and engineering, where it has offered new insights towards mixture design optimization and service life prediction of cementitious systems. This chapter aims to uncover the main research interests and knowledge structure of the existing literature on AI for concrete materials. To begin with, a total of 389 journal articles published from 1990 to 2020 were retrieved from the Web of Science. Scientometric tools such as keyword co-occurrence analysis and documentation co-citation analysis were adopted to quantify features and characteristics of the research field. The findings bring to light pressing questions in data-driven concrete research and suggest future opportunities for the concrete community to fully utilize the capabilities of AI techniques. △ Less","17 September, 2022",https://arxiv.org/pdf/2209.09636
Review of data types and model dimensionality for cardiac DTI SMS-related artefact removal,Michael Tanzer;Sea Hee Yook;Guang Yang;Daniel Rueckert;Sonia Nielles-Vallespin,"As diffusion tensor imaging (DTI) gains popularity in cardiac imaging due to its unique ability to non-invasively assess the cardiac microstructure, deep learning-based Artificial Intelligence is becoming a crucial tool in mitigating some of its drawbacks, such as the long scan times. As it often happens in fast-paced research environments, a lot of emphasis has been put on showing the capability of deep learning while often not enough time has been spent investigating what input and architectural properties would benefit cardiac DTI acceleration the most. In this work, we compare the effect of several input types (magnitude images vs complex images), multiple dimensionalities (2D vs 3D operations), and multiple input types (single slice vs multi-slice) on the performance of a model trained to remove artefacts caused by a simultaneous multi-slice (SMS) acquisition. Despite our initial intuition, our experiments show that, for a fixed number of parameters, simpler 2D real-valued models outperform their more advanced 3D or complex counterparts. The best performance is although obtained by a real-valued model trained using both the magnitude and phase components of the acquired data. We believe this behaviour to be due to real-valued models making better use of the lower number of parameters, and to 3D models not being able to exploit the spatial information because of the low SMS acceleration factor used in our experiments. △ Less","20 September, 2022",https://arxiv.org/pdf/2209.09522
Deep learning at the edge enables real-time streaming ptychographic imaging,Anakha V Babu;Tao Zhou;Saugat Kandel;Tekin Bicer;Zhengchun Liu;William Judge;Daniel J. Ching;Yi Jiang;Sinisa Veseli;Steven Henke;Ryan Chard;Yudong Yao;Ekaterina Sirazitdinova;Geetika Gupta;Martin V. Holt;Ian T. Foster;Antonino Miceli;Mathew J. Cherukara,"Coherent microscopy techniques provide an unparalleled multi-scale view of materials across scientific and technological fields, from structural materials to quantum devices, from integrated circuits to biological cells. Driven by the construction of brighter sources and high-rate detectors, coherent X-ray microscopy methods like ptychography are poised to revolutionize nanoscale materials characterization. However, associated significant increases in data and compute needs mean that conventional approaches no longer suffice for recovering sample images in real-time from high-speed coherent imaging experiments. Here, we demonstrate a workflow that leverages artificial intelligence at the edge and high-performance computing to enable real-time inversion on X-ray ptychography data streamed directly from a detector at up to 2 kHz. The proposed AI-enabled workflow eliminates the sampling constraints imposed by traditional ptychography, allowing low dose imaging using orders of magnitude less data than required by traditional methods. △ Less","19 September, 2022",https://arxiv.org/pdf/2209.09408
Robustness of an Artificial Intelligence Solution for Diagnosis of Normal Chest X-Rays,Tom Dyer;Jordan Smith;Gaetan Dissez;Nicole Tay;Qaiser Malik;Tom Naunton Morgan;Paul Williams;Liliana Garcia-Mondragon;George Pearse;Simon Rasalingham,"Purpose: Artificial intelligence (AI) solutions for medical diagnosis require thorough evaluation to demonstrate that performance is maintained for all patient sub-groups and to ensure that proposed improvements in care will be delivered equitably. This study evaluates the robustness of an AI solution for the diagnosis of normal chest X-rays (CXRs) by comparing performance across multiple patient and environmental subgroups, as well as comparing AI errors with those made by human experts. Methods: A total of 4,060 CXRs were sampled to represent a diverse dataset of NHS patients and care settings. Ground-truth labels were assigned by a 3-radiologist panel. AI performance was evaluated against assigned labels and sub-groups analysis was conducted against patient age and sex, as well as CXR view, modality, device manufacturer and hospital site. Results: The AI solution was able to remove 18.5% of the dataset by classification as High Confidence Normal (HCN). This was associated with a negative predictive value (NPV) of 96.0%, compared to 89.1% for diagnosis of normal scans by radiologists. In all AI false negative (FN) cases, a radiologist was found to have also made the same error when compared to final ground-truth labels. Subgroup analysis showed no statistically significant variations in AI performance, whilst reduced normal classification was observed in data from some hospital sites. Conclusion: We show the AI solution could provide meaningful workload savings by diagnosis of 18.5% of scans as HCN with a superior NPV to human readers. The AI solution is shown to perform well across patient subgroups and error cases were shown to be subjective or subtle in nature. △ Less","31 August, 2022",https://arxiv.org/pdf/2209.09204
Generating detailed saliency maps using model-agnostic methods,Maciej Sakowicz,"The emerging field of Explainable Artificial Intelligence focuses on researching methods of explaining the decision making processes of complex machine learning models. In the field of explainability for Computer Vision, explanations are provided as saliency maps, which visualize the importance of individual pixels of the input w.r.t. the model's prediction. In this work we focus on a perturbation-based, model-agnostic explainability method called RISE, elaborate on observed shortcomings of its grid-based approach and propose two modifications: replacement of square occlusions with convex polygonal occlusions based on cells of a Voronoi mesh and addition of an informativeness guarantee to the occlusion mask generator. These modifications, collectively called VRISE (Voronoi-RISE), are meant to, respectively, improve the accuracy of maps generated using large occlusions and accelerate convergence of saliency maps in cases where sampling density is either very low or very high. We perform a quantitative comparison of accuracy of saliency maps produced by VRISE and RISE on the validation split of ILSVRC2012, using a saliency-guided content insertion/deletion metric and a localization metric based on bounding boxes. Additionally, we explore the space of configurable occlusion pattern parameters to better understand their influence on saliency maps produced by RISE and VRISE. We also describe and demonstrate two effects observed over the course of experimentation, arising from the random sampling approach of RISE: ""feature slicing"" and ""saliency misattribution"". Our results show that convex polygonal occlusions yield more accurate maps for coarse occlusion meshes and multi-object images, but improvement is not guaranteed in other cases. The informativeness guarantee is shown to increase the convergence rate without incurring a significant computational overhead. △ Less","4 September, 2022",https://arxiv.org/pdf/2209.09202
Development and Clinical Evaluation of an AI Support Tool for Improving Telemedicine Photo Quality,Kailas Vodrahalli;Justin Ko;Albert S. Chiou;Roberto Novoa;Abubakar Abid;Michelle Phung;Kiana Yekrang;Paige Petrone;James Zou;Roxana Daneshjou,"Telemedicine utilization was accelerated during the COVID-19 pandemic, and skin conditions were a common use case. However, the quality of photographs sent by patients remains a major limitation. To address this issue, we developed TrueImage 2.0, an artificial intelligence (AI) model for assessing patient photo quality for telemedicine and providing real-time feedback to patients for photo quality improvement. TrueImage 2.0 was trained on 1700 telemedicine images annotated by clinicians for photo quality. On a retrospective dataset of 357 telemedicine images, TrueImage 2.0 effectively identified poor quality images (Receiver operator curve area under the curve (ROC-AUC) =0.78) and the reason for poor quality (Blurry ROC-AUC=0.84, Lighting issues ROC-AUC=0.70). The performance is consistent across age, gender, and skin tone. Next, we assessed whether patient-TrueImage 2.0 interaction led to an improvement in submitted photo quality through a prospective clinical pilot study with 98 patients. TrueImage 2.0 reduced the number of patients with a poor-quality image by 68.0%. △ Less","12 September, 2022",https://arxiv.org/pdf/2209.09105
Disentangling Shape and Pose for Object-Centric Deep Active Inference Models,Stefano Ferraro;Toon Van de Maele;Pietro Mazzaglia;Tim Verbelen;Bart Dhoedt,"Active inference is a first principles approach for understanding the brain in particular, and sentient agents in general, with the single imperative of minimizing free energy. As such, it provides a computational account for modelling artificial intelligent agents, by defining the agent's generative model and inferring the model parameters, actions and hidden state beliefs. However, the exact specification of the generative model and the hidden state space structure is left to the experimenter, whose design choices influence the resulting behaviour of the agent. Recently, deep learning methods have been proposed to learn a hidden state space structure purely from data, alleviating the experimenter from this tedious design task, but resulting in an entangled, non-interpreteable state space. In this paper, we hypothesize that such a learnt, entangled state space does not necessarily yield the best model in terms of free energy, and that enforcing different factors in the state space can yield a lower model complexity. In particular, we consider the problem of 3D object representation, and focus on different instances of the ShapeNet dataset. We propose a model that factorizes object shape, pose and category, while still learning a representation for each factor using a deep neural network. We show that models, with best disentanglement properties, perform best when adopted by an active agent in reaching preferred observations. △ Less","16 September, 2022",https://arxiv.org/pdf/2209.09097
Reconfigurable Intelligent Surface-assisted Classification of Modulations using Deep Learning,Mir Lodro;Hamidreza Taghvaee;Jean Baptiste Gros;Steve Greedy;Geofrroy Lerosey;Gabriele Gradoni,"The fifth generating (5G) of wireless networks will be more adaptive and heterogeneous. Reconfigurable intelligent surface technology enables the 5G to work on multistrand waveforms. However, in such a dynamic network, the identification of specific modulation types is of paramount importance. We present a RIS-assisted digital classification method based on artificial intelligence. We train a convolutional neural network to classify digital modulations. The proposed method operates and learns features directly on the received signal without feature extraction. The features learned by the convolutional neural network are presented and analyzed. Furthermore, the robust features of the received signals at a specific SNR range are studied. The accuracy of the proposed classification method is found to be remarkable, particularly for low levels of SNR. △ Less","17 September, 2022",https://arxiv.org/pdf/2209.08388
The BLue Amazon Brain (BLAB): A Modular Architecture of Services about the Brazilian Maritime Territory,Paulo Pirozelli;Ais B. R. Castro;Ana Luiza C. de Oliveira;André S. Oliveira;Flávio N. Cação;Igor C. Silveira;João G. M. Campos;Laura C. Motheo;Leticia F. Figueiredo;Lucas F. A. O. Pellicer;Marcelo A. José;Marcos M. José;Pedro de M. Ligabue;Ricardo S. Grava;Rodrigo M. Tavares;Vinícius B. Matos;Yan V. Sym;Anna H. R. Costa;Anarosa A. F. Brandão;Denis D. Mauá;Fabio G. Cozman;Sarajane M. Peres,"We describe the first steps in the development of an artificial agent focused on the Brazilian maritime territory, a large region within the South Atlantic also known as the Blue Amazon. The ""BLue Amazon Brain"" (BLAB) integrates a number of services aimed at disseminating information about this region and its importance, functioning as a tool for environmental awareness. The main service provided by BLAB is a conversational facility that deals with complex questions about the Blue Amazon, called BLAB-Chat; its central component is a controller that manages several task-oriented natural language processing modules (e.g., question answering and summarizer systems). These modules have access to an internal data lake as well as to third-party databases. A news reporter (BLAB-Reporter) and a purposely-developed wiki (BLAB-Wiki) are also part of the BLAB service architecture. In this paper, we describe our current version of BLAB's architecture (interface, backend, web services, NLP modules, and resources) and comment on the challenges we have faced so far, such as the lack of training data and the scattered state of domain information. Solving these issues presents a considerable challenge in the development of artificial intelligence for technical domains. △ Less","6 September, 2022",https://arxiv.org/pdf/2209.07928
Power to the People? Opportunities and Challenges for Participatory AI,Abeba Birhane;William Isaac;Vinodkumar Prabhakaran;Mark Díaz;Madeleine Clare Elish;Iason Gabriel;Shakir Mohamed,"Participatory approaches to artificial intelligence (AI) and machine learning (ML) are gaining momentum: the increased attention comes partly with the view that participation opens the gateway to an inclusive, equitable, robust, responsible and trustworthy AI.Among other benefits, participatory approaches are essential to understanding and adequately representing the needs, desires and perspectives of historically marginalized communities. However, there currently exists lack of clarity on what meaningful participation entails and what it is expected to do. In this paper we first review participatory approaches as situated in historical contexts as well as participatory methods and practices within the AI and ML pipeline. We then introduce three case studies in participatory AI.Participation holds the potential for beneficial, emancipatory and empowering technology design, development and deployment while also being at risk for concerns such as cooptation and conflation with other activities. We lay out these limitations and concerns and argue that as participatory AI/ML becomes in vogue, a contextual and nuanced understanding of the term as well as consideration of who the primary beneficiaries of participatory activities ought to be constitute crucial factors to realizing the benefits and opportunities that participation brings. △ Less","15 September, 2022",https://arxiv.org/pdf/2209.07572
Extended Intelligence,David L Barack;Andrew Jaegle,"We argue that intelligence, construed as the disposition to perform tasks successfully, is a property of systems composed of agents and their contexts. This is the thesis of extended intelligence. We argue that the performance of an agent will generally not be preserved if its context is allowed to vary. Hence, this disposition is not possessed by an agent alone, but is rather possessed by the system consisting of an agent and its context, which we dub an agent-in-context. An agent's context may include an environment, other agents, cultural artifacts (like language, technology), or all of these, as is typically the case for humans and artificial intelligence systems, as well as many non-human animals. In virtue of the thesis of extended intelligence, we contend that intelligence is context-bound, task-particular and incommensurable among agents. Our thesis carries strong implications for how intelligence is analyzed in the context of both psychology and artificial intelligence. △ Less","15 September, 2022",https://arxiv.org/pdf/2209.07449
Chemotaxis of sea urchin sperm cells through deep reinforcement learning,Chaojie Mo;Xin Bian,"By imitating biological microswimmers, microrobots can be designed to accomplish targeted delivery of cargos and biomedical manipulations at microscale. However, it is still a great challenge to enable microrobots to maneuver in a complex environment. Machine learning algorithms offer a tool to boost mobility and flexibility of a synthetic microswimmer, hence could help us design truly smart microrobots. In this work, we investigate how a model of sea urchin sperm cell can self-learn chemotactic motion in a chemoattractant concentration field. We employ an artificial neural network to act as a decision-making agent and facilitate the sperm cell to discover efficient maneuver strategies through a deep reinforcement learning (DRL) algorithm. Our results show that chemotactic behaviours, very similar to the realistic ones, can be achieved by the DRL utilizing only limited environmental information. In most cases, the DRL algorithm discovers more efficient strategies than the human-devised one. Furthermore, the DRL can even utilize an external disturbance to facilitate the chemotactic motion if the extra flow information is also taken into account by the artificial neural network. Our results provide insights to the chemotactic process of sea urchin sperm cells and also prepare guidance for the intelligent maneuver of microrobots. △ Less","2 August, 2022",https://arxiv.org/pdf/2209.07407
Deep Reinforcement Learning for Task Offloading in UAV-Aided Smart Farm Networks,Anne Catherine Nguyen;Turgay Pamuklu;Aisha Syed;W. Sean Kennedy;Melike Erol-Kantarci,"The fifth and sixth generations of wireless communication networks are enabling tools such as internet of things devices, unmanned aerial vehicles (UAVs), and artificial intelligence, to improve the agricultural landscape using a network of devices to automatically monitor farmlands. Surveying a large area requires performing a lot of image classification tasks within a specific period of time in order to prevent damage to the farm in case of an incident, such as fire or flood. UAVs have limited energy and computing power, and may not be able to perform all of the intense image classification tasks locally and within an appropriate amount of time. Hence, it is assumed that the UAVs are able to partially offload their workload to nearby multi-access edge computing devices. The UAVs need a decision-making algorithm that will decide where the tasks will be performed, while also considering the time constraints and energy level of the other UAVs in the network. In this paper, we introduce a Deep Q-Learning (DQL) approach to solve this multi-objective problem. The proposed method is compared with Q-Learning and three heuristic baselines, and the simulation results show that our proposed DQL-based method achieves comparable results when it comes to the UAVs' remaining battery levels and percentage of deadline violations. In addition, our method is able to reach convergence 13 times faster than Q-Learning. △ Less","15 September, 2022",https://arxiv.org/pdf/2209.07367
Socially Enhanced Situation Awareness from Microblogs using Artificial Intelligence: A Survey,Rabindra Lamsal;Aaron Harwood;Maria Rodriguez Read,"The rise of social media platforms provides an unbounded, infinitely rich source of aggregate knowledge of the world around us, both historic and real-time, from a human perspective. The greatest challenge we face is how to process and understand this raw and unstructured data, go beyond individual observations and see the ""big picture""--the domain of Situation Awareness. We provide an extensive survey of Artificial Intelligence research, focusing on microblog social media data with applications to Situation Awareness, that gives the seminal work and state-of-the-art approaches across six thematic areas: Crime, Disasters, Finance, Physical Environment, Politics, and Health and Population. We provide a novel, unified methodological perspective, identify key results and challenges, and present ongoing research directions. △ Less","13 September, 2022",https://arxiv.org/pdf/2209.07272
Responsible AI Implementation: A Human-centered Framework for Accelerating the Innovation Process,Dian Tjondronegoro;Elizabeth Yuwono;Brent Richards;Damian Green;Siiri Hatakka,"There is still a significant gap between expectations and the successful adoption of AI to innovate and improve businesses. Due to the emergence of deep learning, AI adoption is more complex as it often incorporates big data and the internet of things, affecting data privacy. Existing frameworks have identified the need to focus on human-centered design, combining technical and business/organizational perspectives. However, trust remains a critical issue that needs to be designed from the beginning. The proposed framework expands from the human-centered design approach, emphasizing and maintaining the trust that underpins the process. This paper proposes a theoretical framework for responsible artificial intelligence (AI) implementation. The proposed framework emphasizes a synergistic business technology approach for the agile co-creation process. The aim is to streamline the adoption process of AI to innovate and improve business by involving all stakeholders throughout the project so that the AI technology is designed, developed, and deployed in conjunction with people and not in isolation. The framework presents a fresh viewpoint on responsible AI implementation based on analytical literature review, conceptual framework design, and practitioners' mediating expertise. The framework emphasizes establishing and maintaining trust throughout the human-centered design and agile development of AI. This human-centered approach is aligned with and enabled by the privacy by design principle. The creators of the technology and the end-users are working together to tailor the AI solution specifically for the business requirements and human characteristics. An illustrative case study on adopting AI for assisting planning in a hospital will demonstrate that the proposed framework applies to real-life applications. △ Less","15 September, 2022",https://arxiv.org/pdf/2209.07076
A novel illumination condition varied image dataset-Food Vision Dataset (FVD) for fair and reliable consumer acceptability predictions from food,Swarna Sethu;Dongyi Wang,"Recent advances in artificial intelligence promote a wide range of computer vision applications in many different domains. Digital cameras, acting as human eyes, can perceive fundamental object properties, such as shapes and colors, and can be further used for conducting high-level tasks, such as image classification, and object detections. Human perceptions have been widely recognized as the ground truth for training and evaluating computer vision models. However, in some cases, humans can be deceived by what they have seen. Well-functioned human vision relies on stable external lighting while unnatural illumination would influence human perception of essential characteristics of goods. To evaluate the illumination effects on human and computer perceptions, the group presents a novel dataset, the Food Vision Dataset (FVD), to create an evaluation benchmark to quantify illumination effects, and to push forward developments of illumination estimation methods for fair and reliable consumer acceptability prediction from food appearances. FVD consists of 675 images captured under 3 different power and 5 different temperature settings every alternate day for five such days. △ Less","14 September, 2022",https://arxiv.org/pdf/2209.06967
"Out of One, Many: Using Language Models to Simulate Human Samples",Lisa P. Argyle;Ethan C. Busby;Nancy Fulda;Joshua Gubler;Christopher Rytting;David Wingate,"We propose and explore the possibility that language models can be studied as effective proxies for specific human sub-populations in social science research. Practical and research applications of artificial intelligence tools have sometimes been limited by problematic biases (such as racism or sexism), which are often treated as uniform properties of the models. We show that the ""algorithmic bias"" within one such tool -- the GPT-3 language model -- is instead both fine-grained and demographically correlated, meaning that proper conditioning will cause it to accurately emulate response distributions from a wide variety of human subgroups. We term this property ""algorithmic fidelity"" and explore its extent in GPT-3. We create ""silicon samples"" by conditioning the model on thousands of socio-demographic backstories from real human participants in multiple large surveys conducted in the United States. We then compare the silicon and human samples to demonstrate that the information contained in GPT-3 goes far beyond surface similarity. It is nuanced, multifaceted, and reflects the complex interplay between ideas, attitudes, and socio-cultural context that characterize human attitudes. We suggest that language models with sufficient algorithmic fidelity thus constitute a novel and powerful tool to advance understanding of humans and society across a variety of disciplines. △ Less","14 September, 2022",https://arxiv.org/pdf/2209.06899
A Model Drift Detection and Adaptation Framework for 5G Core Networks,Dimitrios Michael Manias;Ali Chouman;Abdallah Shami,"The advent of Fifth Generation (5G) and beyond 5G networks (5G+) has revolutionized the way network operators consider the management and orchestration of their networks. With an increased focus on intelligence and automation through core network functions such as the NWDAF, service providers are tasked with integrating machine learning models and artificial intelligence systems into their existing network operation practices. Due to the dynamic nature of next-generation networks and their supported use cases and applications, model drift is a serious concern, which can deteriorate the performance of intelligent models deployed throughout the network. The work presented in this paper introduces a model drift detection and adaptation module for 5G core networks. Using a functional prototype of a 5G core network, a drift in user behaviour is emulated, and the proposed framework is deployed and tested. The results of this work demonstrate the ability of the drift detection module to accurately characterize a drifted concept as well as the ability of the drift adaptation module to begin the necessary remediation efforts to restore system performance. △ Less","8 August, 2022",https://arxiv.org/pdf/2209.06852
The Embeddings World and Artificial General Intelligence,Mostafa Haghir Chehreghani,"From early days, a key and controversial question inside the artificial intelligence community was whether Artificial General Intelligence (AGI) is achievable. AGI is the ability of machines and computer programs to achieve human-level intelligence and do all tasks that a human being can. While there exist a number of systems in the literature claiming they realize AGI, several other researchers argue that it is impossible to achieve it. In this paper, we take a different view to the problem. First, we discuss that in order to realize AGI, along with building intelligent machines and programs, an intelligent world should also be constructed which is on the one hand, an accurate approximation of our world and on the other hand, a significant part of reasoning of intelligent machines is already embedded in this world. Then we discuss that AGI is not a product or algorithm, rather it is a continuous process which will become more and more mature over time (like human civilization and wisdom). Then, we argue that pre-trained embeddings play a key role in building this intelligent world and as a result, realizing AGI. We discuss how pre-trained embeddings facilitate achieving several characteristics of human-level intelligence, such as embodiment, common sense knowledge, unconscious knowledge and continuality of learning, by machines. △ Less","14 September, 2022",https://arxiv.org/pdf/2209.06569
Collaborative SQL-injections detection system with machine learning,M Lodeiro-Santiago;C Caballero-Gil;P Caballero-Gil,"Data mining and information extraction from data is a field that has gained relevance in recent years thanks to techniques based on artificial intelligence and use of machine and deep learning. The main aim of the present work is the development of a tool based on a previous behaviour study of security audit tools (oriented to SQL pentesting) with the purpose of creating testing sets capable of performing an accurate detection of a SQL attack. The study is based on the information collected through the generated web server logs in a pentesting laboratory environment. Then, making use of the common extracted patterns from the logs, each attack vector has been classified in risk levels (dangerous attack, normal attack, non-attack, etc.). Finally, a training with the generated data was performed in order to obtain a classifier system that has a variable performance between 97 and 99 percent in positive attack detection. The training data is shared to other servers in order to create a distributed network capable of deciding if a query is an attack or is a real petition and inform to connected clients in order to block the petitions from the attacker's IP. △ Less","14 September, 2022",https://arxiv.org/pdf/2209.06553
Explainable AI for clinical and remote health applications: a survey on tabular and time series data,Flavio Di Martino;Franca Delmastro,"Nowadays Artificial Intelligence (AI) has become a fundamental component of healthcare applications, both clinical and remote, but the best performing AI systems are often too complex to be self-explaining. Explainable AI (XAI) techniques are defined to unveil the reasoning behind the system's predictions and decisions, and they become even more critical when dealing with sensitive and personal health data. It is worth noting that XAI has not gathered the same attention across different research areas and data types, especially in healthcare. In particular, many clinical and remote health applications are based on tabular and time series data, respectively, and XAI is not commonly analysed on these data types, while computer vision and Natural Language Processing (NLP) are the reference applications. To provide an overview of XAI methods that are most suitable for tabular and time series data in the healthcare domain, this paper provides a review of the literature in the last 5 years, illustrating the type of generated explanations and the efforts provided to evaluate their relevance and quality. Specifically, we identify clinical validation, consistency assessment, objective and standardised quality evaluation, and human-centered quality assessment as key features to ensure effective explanations for the end users. Finally, we highlight the main research challenges in the field as well as the limitations of existing XAI methods. △ Less","14 September, 2022",https://arxiv.org/pdf/2209.06528
"A Survey on Evolutionary Computation for Computer Vision and Image Analysis: Past, Present, and Future Trends",Ying Bi;Bing Xue;Pablo Mesejo;Stefano Cagnoni;Mengjie Zhang,"Computer vision (CV) is a big and important field in artificial intelligence covering a wide range of applications. Image analysis is a major task in CV aiming to extract, analyse and understand the visual content of images. However, image-related tasks are very challenging due to many factors, e.g., high variations across images, high dimensionality, domain expertise requirement, and image distortions. Evolutionary computation (EC) approaches have been widely used for image analysis with significant achievement. However, there is no comprehensive survey of existing EC approaches to image analysis. To fill this gap, this paper provides a comprehensive survey covering all essential EC approaches to important image analysis tasks including edge detection, image segmentation, image feature analysis, image classification, object detection, and others. This survey aims to provide a better understanding of evolutionary computer vision (ECV) by discussing the contributions of different approaches and exploring how and why EC is used for CV and image analysis. The applications, challenges, issues, and trends associated to this research field are also discussed and summarised to provide further guidelines and opportunities for future research. △ Less","13 September, 2022",https://arxiv.org/pdf/2209.06399
Classical Sequence Match is a Competitive Few-Shot One-Class Learner,Mengting Hu;Hang Gao;Yinhao Bai;Mingming Liu,"Nowadays, transformer-based models gradually become the default choice for artificial intelligence pioneers. The models also show superiority even in the few-shot scenarios. In this paper, we revisit the classical methods and propose a new few-shot alternative. Specifically, we investigate the few-shot one-class problem, which actually takes a known sample as a reference to detect whether an unknown instance belongs to the same class. This problem can be studied from the perspective of sequence match. It is shown that with meta-learning, the classical sequence match method, i.e. Compare-Aggregate, significantly outperforms transformer ones. The classical approach requires much less training cost. Furthermore, we perform an empirical comparison between two kinds of sequence match approaches under simple fine-tuning and meta-learning. Meta-learning causes the transformer models' features to have high-correlation dimensions. The reason is closely related to the number of layers and heads of transformer models. Experimental codes and data are available at https://github.com/hmt2014/FewOne △ Less","5 October, 2022",https://arxiv.org/pdf/2209.06394
The Role of Explanatory Value in Natural Language Processing,Kees van Deemter,"A key aim of science is explanation, yet the idea of explaining language phenomena has taken a backseat in mainstream Natural Language Processing (NLP) and many other areas of Artificial Intelligence. I argue that explanation of linguistic behaviour should be a main goal of NLP, and that this is not the same as making NLP models explainable. To illustrate these ideas, some recent models of human language production are compared with each other. I conclude by asking what it would mean for NLP research and institutional policies if our community took explanatory value seriously, while heeding some possible pitfalls. △ Less","13 September, 2022",https://arxiv.org/pdf/2209.06169
Bit-Line Computing for CNN Accelerators Co-Design in Edge AI Inference,Marco Rios;Flavio Ponzina;Alexandre Levisse;Giovanni Ansaloni;David Atienza,"By supporting the access of multiple memory words at the same time, Bit-line Computing (BC) architectures allow the parallel execution of bit-wise operations in-memory. At the array periphery, arithmetic operations are then derived with little additional overhead. Such a paradigm opens novel opportunities for Artificial Intelligence (AI) at the edge, thanks to the massive parallelism inherent in memory arrays and the extreme energy efficiency of computing in-situ, hence avoiding data transfers. Previous works have shown that BC brings disruptive efficiency gains when targeting AI workloads, a key metric in the context of emerging edge AI scenarios. This manuscript builds on these findings by proposing an end-to-end framework that leverages BC-specific optimizations to enable high parallelism and aggressive compression of AI models. Our approach is supported by a novel hardware module performing real-time decoding, as well as new algorithms to enable BC-friendly model compression. Our hardware/software approach results in a 91% energy savings (for a 1% accuracy degradation constraint) regarding state-of-the-art BC computing approaches. △ Less","12 September, 2022",https://arxiv.org/pdf/2209.06108
Virtual Underwater Datasets for Autonomous Inspections,Ioannis Polymenis;Maryam Haroutunian;Rose Norman;David Trodden,"Underwater Vehicles have become more sophisticated, driven by the off-shore sector and the scientific community's rapid advancements in underwater operations. Notably, many underwater tasks, including the assessment of subsea infrastructure, are performed with the assistance of Autonomous Underwater Vehicles (AUVs). There have been recent breakthroughs in Artificial Intelligence (AI) and, notably, Deep Learning (DL) models and applications, which have widespread usage in a variety of fields, including aerial unmanned vehicles, autonomous car navigation, and other applications. However, they are not as prevalent in underwater applications due to the difficulty of obtaining underwater datasets for a specific application. In this sense, the current study utilises recent advancements in the area of DL to construct a bespoke dataset generated from photographs of items captured in a laboratory environment. Generative Adversarial Networks (GANs) were utilised to translate the laboratory object dataset into the underwater domain by combining the collected images with photographs containing the underwater environment. The findings demonstrated the feasibility of creating such a dataset, since the resulting images closely resembled the real underwater environment when compared with real-world underwater ship hull images. Therefore, the artificial datasets of the underwater environment can overcome the difficulties arising from the limited access to real-world underwater images and are used to enhance underwater operations through underwater object image classification and detection. △ Less","14 September, 2022",https://arxiv.org/pdf/2209.06013
A Survey on Mobile Edge Computing for Video Streaming: Opportunities and Challenges,Muhammad Asif Khan;Emna Baccour;Zina Chkirbene;Aiman Erbad;Ridha Hamila;Mounir Hamdi;Moncef Gabbouj,"5G communication brings substantial improvements in the quality of service provided to various applications by achieving higher throughput and lower latency. However, interactive multimedia applications (e.g., ultra high definition video conferencing, 3D and multiview video streaming, crowd-sourced video streaming, cloud gaming, virtual and augmented reality) are becoming more ambitious with high volume and low latency video streams putting strict demands on the already congested networks. Mobile Edge Computing (MEC) is an emerging paradigm that extends cloud computing capabilities to the edge of the network i.e., at the base station level. To meet the latency requirements and avoid the end-to-end communication with remote cloud data centers, MEC allows to store and process video content (e.g., caching, transcoding, pre-processing) at the base stations. Both video on demand and live video streaming can utilize MEC to improve existing services and develop novel use cases, such as video analytics, and targeted advertisements. MEC is expected to reshape the future of video streaming by providing ultra-reliable and low latency streaming (e.g., in augmented reality, virtual reality, and autonomous vehicles), pervasive computing (e.g., in real-time video analytics), and blockchain-enabled architecture for secure live streaming. This paper presents a comprehensive survey of recent developments in MEC-enabled video streaming bringing unprecedented improvement to enable novel use cases. A detailed review of the state-of-the-art is presented covering novel caching schemes, optimal computation offloading, cooperative caching and offloading and the use of artificial intelligence (i.e., machine learning, deep learning, and reinforcement learning) in MEC-assisted video streaming services. △ Less","13 September, 2022",https://arxiv.org/pdf/2209.05761
A Many-ported and Shared Memory Architecture for High-Performance ADAS SoCs,Hao Luan;Yu Yao;Chang Huang,"Increasing investment in computing technologies and the advancements in silicon technology has fueled rapid growth in advanced driver assistance systems (ADAS) and corresponding SoC developments. An ADAS SoC represents a heterogeneous architecture that consists of CPUs, GPUs and artificial intelligence (AI) accelerators. In order to guarantee its safety and reliability, it must process massive amount of raw data collected from multiple redundant sources such as high-definition video cameras, Radars, and Lidars to recognize objects correctly and to make the right decisions promptly. A domain specific memory architecture is essential to achieve the above goals. We present a shared memory architecture that enables high data throughput among multiple parallel accesses native to the ADAS applications. It also provides deterministic access latency with proper isolation under the stringent real-time QoS constraints. A prototype is built and analyzed. The results validate that the proposed architecture provides close to 100\% throughput for both read and write accesses generated simultaneously by many accessing masters with full injection rate. It can also provide consistent QoS to the domain specific payloads while enabling the scalability and modularity of the design. △ Less","13 September, 2022",https://arxiv.org/pdf/2209.05731
Active Learning and Novel Model Calibration Measurements for Automated Visual Inspection in Manufacturing,Jože M. Rožanec;Luka Bizjak;Elena Trajkova;Patrik Zajec;Jelle Keizer;Blaž Fortuna;Dunja Mladenić,"Quality control is a crucial activity performed by manufacturing enterprises to ensure that their products meet quality standards and avoid potential damage to the brand's reputation. The decreased cost of sensors and connectivity enabled increasing digitalization of manufacturing. In addition, artificial intelligence enables higher degrees of automation, reducing overall costs and time required for defect inspection. This research compares three active learning approaches, having single and multiple oracles, to visual inspection. Six new metrics are proposed to assess the quality of calibration without the need for ground truth. Furthermore, this research explores whether existing calibrators can improve their performance by leveraging an approximate ground truth to enlarge the calibration set. The experiments were performed on real-world data provided by Philips Consumer Lifestyle BV. Our results show that the explored active learning settings can reduce the data labeling effort by between three and four percent without detriment to the overall quality goals, considering a threshold of p=0.95. Furthermore, the results show that the proposed calibration metrics successfully capture relevant information otherwise available to metrics used up to date only through ground truth data. Therefore, the proposed metrics can be used to estimate the quality of models' probability calibration without committing to a labeling effort to obtain ground truth data. △ Less","25 November, 2022",https://arxiv.org/pdf/2209.05486
A Molecular Multimodal Foundation Model Associating Molecule Graphs with Natural Language,Bing Su;Dazhao Du;Zhao Yang;Yujie Zhou;Jiangmeng Li;Anyi Rao;Hao Sun;Zhiwu Lu;Ji-Rong Wen,"Although artificial intelligence (AI) has made significant progress in understanding molecules in a wide range of fields, existing models generally acquire the single cognitive ability from the single molecular modality. Since the hierarchy of molecular knowledge is profound, even humans learn from different modalities including both intuitive diagrams and professional texts to assist their understanding. Inspired by this, we propose a molecular multimodal foundation model which is pretrained from molecular graphs and their semantically related textual data (crawled from published Scientific Citation Index papers) via contrastive learning. This AI model represents a critical attempt that directly bridges molecular graphs and natural language. Importantly, through capturing the specific and complementary information of the two modalities, our proposed model can better grasp molecular expertise. Experimental results show that our model not only exhibits promising performance in cross-modal tasks such as cross-modal retrieval and molecule caption, but also enhances molecular property prediction and possesses capability to generate meaningful molecular graphs from natural language descriptions. We believe that our model would have a broad impact on AI-empowered fields across disciplines such as biology, chemistry, materials, environment, and medicine, among others. △ Less","11 September, 2022",https://arxiv.org/pdf/2209.05481
"Tackling problems, harvesting benefits -- A systematic review of the regulatory debate around AI",Anja Folberth;Jutta Jahnel;Jascha Bareis;Carsten Orwat;Christian Wadephul,"How to integrate an emerging and all-pervasive technology such as AI into the structures and operations of our society is a question of contemporary politics, science and public debate. It has produced a considerable amount of international academic literature from different disciplines. This article analyzes the academic debate around the regulation of artificial intelligence (AI). The systematic review comprises a sample of 73 peer-reviewed journal articles published between January 1st, 2016, and December 31st, 2020. The analysis concentrates on societal risks and harms, questions of regulatory responsibility, and possible adequate policy frameworks, including risk-based and principle-based approaches. The main interests are proposed regulatory approaches and instruments. Various forms of interventions such as bans, approvals, standard-setting, and disclosure are presented. The assessments of the included papers indicate the complexity of the field, which shows its prematurity and the remaining lack of clarity. By presenting a structured analysis of the academic debate, we contribute both empirically and conceptually to a better understanding of the nexus of AI and regulation and the underlying normative decisions. A comparison of the scientific proposals with the proposed European AI regulation illustrates the specific approach of the regulation, its strengths and weaknesses. △ Less","7 September, 2022",https://arxiv.org/pdf/2209.05468
Why Are Some Online Educational Programs Successful? Student Cognition and Success,Marissa Keech;Ashok Goel,"Massive Open Online Courses (MOOCs) once offered the promise of accessibility and affordability. However, MOOCs typically lack expert feedback and social interaction, and have low student engagement and retention. Thus, alternative programs for online education have emerged including an online graduate program in computer science at a major public university in USA. This program is considered a success with over 9000 students now enrolled in the program. We adopt the perspective of cognitive science to answer the question why do only some online educational courses succeed? We measure learner motivation and self-regulation in one course in the program, specifically a course on artificial intelligence (AI). Surveys of students indicate that students self-reported assessments of self-efficacy, cognitive strategy use, and intrinsic value of the course are not only fairly high, but also generally increase over the course of learning. This data suggests that the online AI course might be a success because the students have high self-efficacy and the class fosters self-regulated learning. △ Less","4 September, 2022",https://arxiv.org/pdf/2209.05462
How Do AI Timelines Affect Existential Risk?,Stephen McAleese,"Superhuman artificial general intelligence could be created this century and would likely be a significant source of existential risk. Delaying the creation of superintelligent AI (ASI) could decrease total existential risk by increasing the amount of time humanity has to work on the AI alignment problem. However, since ASI could reduce most risks, delaying the creation of ASI could also increase other existential risks, especially from advanced future technologies such as synthetic biology and molecular nanotechnology. If AI existential risk is high relative to the sum of other existential risk, delaying the creation of ASI will tend to decrease total existential risk and vice-versa. Other factors such as war and a hardware overhang could increase AI risk and cognitive enhancement could decrease AI risk. To reduce total existential risk, humanity should take robustly positive actions such as working on existential risk analysis, AI governance and safety, and reducing all sources of existential risk by promoting differential technological development. △ Less","30 August, 2022",https://arxiv.org/pdf/2209.05459
Reliable and Resilient AI and IoT-based Personalised Healthcare Services: A Survey,Najma Taimoor;Semeen Rehman,"Recent technological and economic developments have transformed the healthcare sector towards more personalized and IoT-based healthcare services. These services are realized through control and monitoring applications that are typically developed using artificial intelligence/machine learning-based algorithms, which play a significant role in highlighting the efficiency of traditional healthcare systems. Current personalized healthcare services are dedicated to a specific environment to support technological personalization. However, they are unable to consider different interrelated health conditions, leading to inappropriate diagnoses and affecting sustainability and the long-term health of patients. To this end, current Healthcare 5.0 technology has evolved that supersede previous healthcare technologies. The goal of healthcare 5.0 is to achieve an autonomous healthcare service, that takes into account the interdependent effect of different health conditions of a patient. This paper conducts a comprehensive survey on personalized healthcare services. In particular, we first present an overview of key requirements of comprehensive personalized healthcare services in modern healthcare Internet of Things (HIoT), including the definition of personalization and an example use case scenario as a representative for modern HIoT. Second, we explored a fundamental three-layer architecture for IoT-based healthcare systems using AI and non-AI-based approaches, considering key requirements for CPHS followed by their strengths and weaknesses in the frame of personalized healthcare services. Third, we highlighted different security threats against each layer of IoT architecture along with the possible AI and non-AI-based solutions. Finally, we propose a methodology to develop reliable, resilient, and personalized healthcare services that address the identified weaknesses of existing approaches. △ Less","29 August, 2022",https://arxiv.org/pdf/2209.05457
Leveraging Artificial Intelligence Techniques for Smart Palm Tree Detection: A Decade Systematic Review,Yosra Hajjaji;Wadii Boulila;Imed Riadh Farah,"Over the past few years, total financial investment in the agricultural sector has increased substantially. Palm tree is important for many countries' economies, particularly in northern Africa and the Middle East. Monitoring in terms of detection and counting palm trees provides useful information for various stakeholders; it helps in yield estimation and examination to ensure better crop quality and prevent pests, diseases, better irrigation, and other potential threats. Despite their importance, this information is still challenging to obtain. This study systematically reviews research articles between 2011 and 2021 on artificial intelligence (AI) technology for smart palm tree detection. A systematic review (SR) was performed using the PRISMA approach based on a four-stage selection process. Twenty-two articles were included for the synthesis activity reached from the search strategy alongside the inclusion criteria in order to answer to two main research questions. The study's findings reveal patterns, relationships, networks, and trends in applying artificial intelligence in palm tree detection over the last decade. Despite the good results in most of the studies, the effective and efficient management of large-scale palm plantations is still a challenge. In addition, countries whose economies strongly related to intelligent palm services, especially in North Africa, should give more attention to this kind of study. The results of this research could benefit both the research community and stakeholders. △ Less","12 September, 2022",https://arxiv.org/pdf/2209.05282
Embracing AI in 5G-Advanced Towards 6G: A Joint 3GPP and O-RAN Perspective,Xingqin Lin;Lopamudra Kundu;Chris Dick;Soma Velayutham,"Artificial intelligence (AI) has emerged as a powerful technology that improves system performance and enables new features in 5G and beyond. Standardization, defining functionality and interfaces, is essential for driving the industry alignment required to deliver the mass adoption of AI in 5G-Advanced and 6G. However, fragmented efforts in different standards bodies, such as the third generation partnership project (3GPP) and the open radio access network (O-RAN) Alliance, can lead to confusion and uncertainty about which standards to follow and which aspects of the standards to embrace. This article provides a joint 3GPP and O-RAN perspective on the state of the art in AI adoption in mobile communication systems, including the fundamentals of 5G architecture and its evolution towards openness and intelligence, AI for 5G-Advanced evolution, and a case study on AI-enabled traffic steering. We also identify several areas for future exploration to accelerate AI adoption on the path towards 6G. △ Less","11 September, 2022",https://arxiv.org/pdf/2209.04987
Resisting Deep Learning Models Against Adversarial Attack Transferability via Feature Randomization,Ehsan Nowroozi;Mohammadreza Mohammadi;Pargol Golmohammadi;Yassine Mekdad;Mauro Conti;Selcuk Uluagac,"In the past decades, the rise of artificial intelligence has given us the capabilities to solve the most challenging problems in our day-to-day lives, such as cancer prediction and autonomous navigation. However, these applications might not be reliable if not secured against adversarial attacks. In addition, recent works demonstrated that some adversarial examples are transferable across different models. Therefore, it is crucial to avoid such transferability via robust models that resist adversarial manipulations. In this paper, we propose a feature randomization-based approach that resists eight adversarial attacks targeting deep learning models in the testing phase. Our novel approach consists of changing the training strategy in the target network classifier and selecting random feature samples. We consider the attacker with a Limited-Knowledge and Semi-Knowledge conditions to undertake the most prevalent types of adversarial attacks. We evaluate the robustness of our approach using the well-known UNSW-NB15 datasets that include realistic and synthetic attacks. Afterward, we demonstrate that our strategy outperforms the existing state-of-the-art approach, such as the Most Powerful Attack, which consists of fine-tuning the network model against specific adversarial attacks. Finally, our experimental results show that our methodology can secure the target network and resists adversarial attack transferability by over 60%. △ Less","11 September, 2022",https://arxiv.org/pdf/2209.04930
People detection and social distancing classification in smart cities for COVID-19 by using thermal images and deep learning algorithms,Abdussalam Elhanashi;Sergio Saponara;Alessio Gagliardi,"COVID-19 is a disease caused by severe respiratory syndrome coronavirus. It was identified in December 2019 in Wuhan, China. It has resulted in an ongoing pandemic that caused infected cases including some deaths. Coronavirus is primarily spread between people during close contact. Motivating to this notion, this research proposes an artificial intelligence system for social distancing classification of persons by using thermal images. By exploiting YOLOv2 (you look at once), a deep learning detection technique is developed for detecting and tracking people in indoor and outdoor scenarios. An algorithm is also implemented for measuring and classifying the distance between persons and automatically check if social distancing rules are respected or not. Hence, this work aims at minimizing the spread of the COVID-19 virus by evaluating if and how persons comply with social distancing rules. The proposed approach is applied to images acquired through thermal cameras, to establish a complete AI system for people tracking, social distancing classification, and body temperature monitoring. The training phase is done with two datasets captured from different thermal cameras. Ground Truth Labeler app is used for labeling the persons in the images. The achieved results show that the proposed method is suitable for the creation of a smart surveillance system in smart cities for people detection, social distancing classification, and body temperature analysis. △ Less","10 September, 2022",https://arxiv.org/pdf/2209.04704
Trustworthy Federated Learning via Blockchain,Zhanpeng Yang;Yuanming Shi;Yong Zhou;Zixin Wang;Kai Yang,"The safety-critical scenarios of artificial intelligence (AI), such as autonomous driving, Internet of Things, smart healthcare, etc., have raised critical requirements of trustworthy AI to guarantee the privacy and security with reliable decisions. As a nascent branch for trustworthy AI, federated learning (FL) has been regarded as a promising privacy preserving framework for training a global AI model over collaborative devices. However, security challenges still exist in the FL framework, e.g., Byzantine attacks from malicious devices, and model tampering attacks from malicious server, which will degrade or destroy the accuracy of trained global AI model. In this paper, we shall propose a decentralized blockchain based FL (B-FL) architecture by using a secure global aggregation algorithm to resist malicious devices, and deploying practical Byzantine fault tolerance consensus protocol with high effectiveness and low energy consumption among multiple edge servers to prevent model tampering from the malicious server. However, to implement B-FL system at the network edge, multiple rounds of cross-validation in blockchain consensus protocol will induce long training latency. We thus formulate a network optimization problem that jointly considers bandwidth and power allocation for the minimization of long-term average training latency consisting of progressive learning rounds. We further propose to transform the network optimization problem as a Markov decision process and leverage the deep reinforcement learning based algorithm to provide high system performance with low computational complexity. Simulation results demonstrate that B-FL can resist malicious attacks from edge devices and servers, and the training latency of B-FL can be significantly reduced by deep reinforcement learning based algorithm compared with baseline algorithms. △ Less","12 August, 2022",https://arxiv.org/pdf/2209.04418
Survey on Deep Fuzzy Systems in regression applications: a view on interpretability,Jorge S. S. Júnior;Jérôme Mendes;Francisco Souza;Cristiano Premebida,"Regression problems have been more and more embraced by deep learning (DL) techniques. The increasing number of papers recently published in this domain, including surveys and reviews, shows that deep regression has captured the attention of the community due to efficiency and good accuracy in systems with high-dimensional data. However, many DL methodologies have complex structures that are not readily transparent to human users. Accessing the interpretability of these models is an essential factor for addressing problems in sensitive areas such as cyber-security systems, medical, financial surveillance, and industrial processes. Fuzzy logic systems (FLS) are inherently interpretable models, well known in the literature, capable of using nonlinear representations for complex systems through linguistic terms with membership degrees mimicking human thought. Within an atmosphere of explainable artificial intelligence, it is necessary to consider a trade-off between accuracy and interpretability for developing intelligent models. This paper aims to investigate the state-of-the-art on existing methodologies that combine DL and FLS, namely deep fuzzy systems, to address regression problems, configuring a topic that is currently not sufficiently explored in the literature and thus deserves a comprehensive survey. △ Less","9 September, 2022",https://arxiv.org/pdf/2209.04230
Selecting Related Knowledge via Efficient Channel Attention for Online Continual Learning,Ya-nan Han;Jian-wei Liu,"Continual learning aims to learn a sequence of tasks by leveraging the knowledge acquired in the past in an online-learning manner while being able to perform well on all previous tasks, this ability is crucial to the artificial intelligence (AI) system, hence continual learning is more suitable for most real-word and complex applicative scenarios compared to the traditional learning pattern. However, the current models usually learn a generic representation base on the class label on each task and an effective strategy is selected to avoid catastrophic forgetting. We postulate that selecting the related and useful parts only from the knowledge obtained to perform each task is more effective than utilizing the whole knowledge. Based on this fact, in this paper we propose a new framework, named Selecting Related Knowledge for Online Continual Learning (SRKOCL), which incorporates an additional efficient channel attention mechanism to pick the particular related knowledge for every task. Our model also combines experience replay and knowledge distillation to circumvent the catastrophic forgetting. Finally, extensive experiments are conducted on different benchmarks and the competitive experimental results demonstrate that our proposed SRKOCL is a promised approach against the state-of-the-art. △ Less","9 September, 2022",https://arxiv.org/pdf/2209.04212
"Metaverse for Healthcare: A Survey on Potential Applications, Challenges and Future Directions",Rajeswari Chengoden;Nancy Victor;Thien Huynh-The;Gokul Yenduri;Rutvij H. Jhaveri;Mamoun Alazab;Sweta Bhattacharya;Pawan Hegde;Praveen Kumar Reddy Maddikunta;Thippa Reddy Gadekallu,"The rapid progress in digitalization and automation have led to an accelerated growth in healthcare, generating novel models that are creating new channels for rendering treatment with reduced cost. The Metaverse is an emerging technology in the digital space which has huge potential in healthcare, enabling realistic experiences to the patients as well as the medical practitioners. The Metaverse is a confluence of multiple enabling technologies such as artificial intelligence, virtual reality, augmented reality, internet of medical devices, robotics, quantum computing, etc. through which new directions for providing quality healthcare treatment and services can be explored. The amalgamation of these technologies ensures immersive, intimate and personalized patient care. It also provides adaptive intelligent solutions that eliminates the barriers between healthcare providers and receivers. This article provides a comprehensive review of the Metaverse for healthcare, emphasizing on the state of the art, the enabling technologies for adopting the Metaverse for healthcare, the potential applications and the related projects. The issues in the adaptation of the Metaverse for healthcare applications are also identified and the plausible solutions are highlighted as part of future research directions. △ Less","9 September, 2022",https://arxiv.org/pdf/2209.04160
"Vision for Bosnia and Herzegovina in Artificial Intelligence Age: Global Trends, Potential Opportunities, Selected Use-cases and Realistic Goals",Zlatan Ajanović;Emina Aličković;Aida Branković;Sead Delalić;Eldar Kurtić;Salem Malikić;Adnan Mehonić;Hamza Merzić;Kenan Šehić;Bahrudin Trbalić,"Artificial Intelligence (AI) is one of the most promising technologies of the 21. century, with an already noticeable impact on society and the economy. With this work, we provide a short overview of global trends, applications in industry and selected use-cases from our international experience and work in industry and academia. The goal is to present global and regional positive practices and provide an informed opinion on the realistic goals and opportunities for positioning B&H on the global AI scene. △ Less","8 September, 2022",https://arxiv.org/pdf/2209.03990
Best of Both Worlds: Agents with Entitlements,Martin Hoefer;Marco Schmalhofer;Giovanna Varricchio,"Fair division of indivisible goods is a central challenge in artificial intelligence. For many prominent fairness criteria including envy-freeness (EF) or proportionality (PROP), no allocations satisfying these criteria might exist. Two popular remedies to this problem are randomization or relaxation of fairness concepts. A timely research direction is to combine the advantages of both, commonly referred to as Best of Both Worlds (BoBW). We consider fair division with entitlements, which allows to adjust notions of fairness to heterogeneous priorities among agents. This is an important generalization to standard fair division models and is not well-understood in terms of BoBW results. Our main result is a lottery for additive valuations and different entitlements that is ex-ante weighted envy-free (WEF), as well as ex-post weighted proportional up to one good (WPROP1) and weighted transfer envy-free up to one good (WEF(1,1)). It can be computed in strongly polynomial time. We show that this result is tight - ex-ante WEF is incompatible with any stronger ex-post WEF relaxation. In addition, we extend BoBW results on group fairness to entitlements and explore generalizations of our results to instances with more expressive valuation functions. △ Less","8 September, 2022",https://arxiv.org/pdf/2209.03908
What and How of Machine Learning Transparency: Building Bespoke Explainability Tools with Interoperable Algorithmic Components,Kacper Sokol;Alexander Hepburn;Raul Santos-Rodriguez;Peter Flach,"Explainability techniques for data-driven predictive models based on artificial intelligence and machine learning algorithms allow us to better understand the operation of such systems and help to hold them accountable. New transparency approaches are developed at breakneck speed, enabling us to peek inside these black boxes and interpret their decisions. Many of these techniques are introduced as monolithic tools, giving the impression of one-size-fits-all and end-to-end algorithms with limited customisability. Nevertheless, such approaches are often composed of multiple interchangeable modules that need to be tuned to the problem at hand to produce meaningful explanations. This paper introduces a collection of hands-on training materials -- slides, video recordings and Jupyter Notebooks -- that provide guidance through the process of building and evaluating bespoke modular surrogate explainers for tabular data. These resources cover the three core building blocks of this technique: interpretable representation composition, data sampling and explanation generation. △ Less","8 September, 2022",https://arxiv.org/pdf/2209.03813
Enabling Connectivity for Automated Mobility: A Novel MQTT-based Interface Evaluated in a 5G Case Study on Edge-Cloud Lidar Object Detection,Lennart Reiher;Bastian Lampe;Timo Woopen;Raphael van Kempen;Till Beemelmanns;Lutz Eckstein,"Enabling secure and reliable high-bandwidth lowlatency connectivity between automated vehicles and external servers, intelligent infrastructure, and other road users is a central step in making fully automated driving possible. The availability of data interfaces, which allow this kind of connectivity, has the potential to distinguish artificial agents' capabilities in connected, cooperative, and automated mobility systems from the capabilities of human operators, who do not possess such interfaces. Connected agents can for example share data to build collective environment models, plan collective behavior, and learn collectively from the shared data that is centrally combined. This paper presents multiple solutions that allow connected entities to exchange data. In particular, we propose a new universal communication interface which uses the Message Queuing Telemetry Transport (MQTT) protocol to connect agents running the Robot Operating System (ROS). Our work integrates methods to assess the connection quality in the form of various key performance indicators in real-time. We compare a variety of approaches that provide the connectivity necessary for the exemplary use case of edge-cloud lidar object detection in a 5G network. We show that the mean latency between the availability of vehicle-based sensor measurements and the reception of a corresponding object list from the edge-cloud is below 87 ms. All implemented solutions are made open-source and free to use. Source code is available at https://github.com/ika-rwth-aachen/ros-v2x-benchmarking-suite. △ Less","8 September, 2022",https://arxiv.org/pdf/2209.03630
TAG: Learning Circuit Spatial Embedding From Layouts,Keren Zhu;Hao Chen;Walker J. Turner;George F. Kokai;Po-Hsuan Wei;David Z. Pan;Haoxing Ren,"Analog and mixed-signal (AMS) circuit designs still rely on human design expertise. Machine learning has been assisting circuit design automation by replacing human experience with artificial intelligence. This paper presents TAG, a new paradigm of learning the circuit representation from layouts leveraging text, self-attention and graph. The embedding network model learns spatial information without manual labeling. We introduce text embedding and a self-attention mechanism to AMS circuit learning. Experimental results demonstrate the ability to predict layout distances between instances with industrial FinFET technology benchmarks. The effectiveness of the circuit representation is verified by showing the transferability to three other learning tasks with limited data in the case studies: layout matching prediction, wirelength estimation, and net parasitic capacitance prediction. △ Less","7 September, 2022",https://arxiv.org/pdf/2209.03465
Responsibility: An Example-based Explainable AI approach via Training Process Inspection,Faraz Khadivpour;Arghasree Banerjee;Matthew Guzdial,"Explainable Artificial Intelligence (XAI) methods are intended to help human users better understand the decision making of an AI agent. However, many modern XAI approaches are unintuitive to end users, particularly those without prior AI or ML knowledge. In this paper, we present a novel XAI approach we call Responsibility that identifies the most responsible training example for a particular decision. This example can then be shown as an explanation: ""this is what I (the AI) learned that led me to do that"". We present experimental results across a number of domains along with the results of an Amazon Mechanical Turk user study, comparing responsibility and existing XAI methods on an image classification task. Our results demonstrate that responsibility can help improve accuracy for both human end users and secondary ML models. △ Less","7 September, 2022",https://arxiv.org/pdf/2209.03433
Explainable Artificial Intelligence to Detect Image Spam Using Convolutional Neural Network,Zhibo Zhang;Ernesto Damiani;Hussam Al Hamadi;Chan Yeob Yeun;Fatma Taher,"Image spam threat detection has continually been a popular area of research with the internet's phenomenal expansion. This research presents an explainable framework for detecting spam images using Convolutional Neural Network(CNN) algorithms and Explainable Artificial Intelligence (XAI) algorithms. In this work, we use CNN model to classify image spam respectively whereas the post-hoc XAI methods including Local Interpretable Model Agnostic Explanation (LIME) and Shapley Additive Explanations (SHAP) were deployed to provide explanations for the decisions that the black-box CNN models made about spam image detection. We train and then evaluate the performance of the proposed approach on a 6636 image dataset including spam images and normal images collected from three different publicly available email corpora. The experimental results show that the proposed framework achieved satisfactory detection results in terms of different performance metrics whereas the model-independent XAI algorithms could provide explanations for the decisions of different models which could be utilized for comparison for the future study. △ Less","7 September, 2022",https://arxiv.org/pdf/2209.03166
The Outcome of the 2022 Landslide4Sense Competition: Advanced Landslide Detection from Multi-Source Satellite Imagery,Omid Ghorbanzadeh;Yonghao Xu;Hengwei Zhao;Junjue Wang;Yanfei Zhong;Dong Zhao;Qi Zang;Shuang Wang;Fahong Zhang;Yilei Shi;Xiao Xiang Zhu;Lin Bai;Weile Li;Weihang Peng;Pedram Ghamisi,"The scientific outcomes of the 2022 Landslide4Sense (L4S) competition organized by the Institute of Advanced Research in Artificial Intelligence (IARAI) are presented here. The objective of the competition is to automatically detect landslides based on large-scale multiple sources of satellite imagery collected globally. The 2022 L4S aims to foster interdisciplinary research on recent developments in deep learning (DL) models for the semantic segmentation task using satellite imagery. In the past few years, DL-based models have achieved performance that meets expectations on image interpretation, due to the development of convolutional neural networks (CNNs). The main objective of this article is to present the details and the best-performing algorithms featured in this competition. The winning solutions are elaborated with state-of-the-art models like the Swin Transformer, SegFormer, and U-Net. Advanced machine learning techniques and strategies such as hard example mining, self-training, and mix-up data augmentation are also considered. Moreover, we describe the L4S benchmark data set in order to facilitate further comparisons, and report the results of the accuracy assessment online. The data is accessible on \textit{Future Development Leaderboard} for future evaluation at \url{https://www.iarai.ac.at/landslide4sense/challenge/}, and researchers are invited to submit more prediction results, evaluate the accuracy of their methods, compare them with those of other users, and, ideally, improve the landslide detection results reported in this article. △ Less","12 September, 2022",https://arxiv.org/pdf/2209.02556
UPAR: Unified Pedestrian Attribute Recognition and Person Retrieval,Andreas Specker;Mickael Cormier;Jürgen Beyerer,"Recognizing soft-biometric pedestrian attributes is essential in video surveillance and fashion retrieval. Recent works show promising results on single datasets. Nevertheless, the generalization ability of these methods under different attribute distributions, viewpoints, varying illumination, and low resolutions remains rarely understood due to strong biases and varying attributes in current datasets. To close this gap and support a systematic investigation, we present UPAR, the Unified Person Attribute Recognition Dataset. It is based on four well-known person attribute recognition datasets: PA100K, PETA, RAPv2, and Market1501. We unify those datasets by providing 3,3M additional annotations to harmonize 40 important binary attributes over 12 attribute categories across the datasets. We thus enable research on generalizable pedestrian attribute recognition as well as attribute-based person retrieval for the first time. Due to the vast variance of the image distribution, pedestrian pose, scale, and occlusion, existing approaches are greatly challenged both in terms of accuracy and efficiency. Furthermore, we develop a strong baseline for PAR and attribute-based person retrieval based on a thorough analysis of regularization methods. Our models achieve state-of-the-art performance in cross-domain and specialization settings on PA100k, PETA, RAPv2, Market1501-Attributes, and UPAR. We believe UPAR and our strong baseline will contribute to the artificial intelligence community and promote research on large-scale, generalizable attribute recognition systems. △ Less","6 September, 2022",https://arxiv.org/pdf/2209.02522
Butterflies: A new source of inspiration for futuristic aerial robotics,Chakravarthi Jada;Lokesh Ch. R. S;Ashok Urlana;Shridi Swamy Yerubandi;Kantha Rao Bora;Gouse Basha Shaik;Pavan Baswani;Balaraju Karri,"Nature is an inhabitant for enormous number of species. All the species do perform complex activities with simple and elegant rules for their survival. The property of emergence of collective behavior is remarkably supporting their activities. One form of the collective behaviour is the swarm intelligence -- all agents poses same rules and capabilities. This equality along with local cooperation in the agents tremendously leads to achieving global results. Some of the swarm behaviours in the nature includes birds formations , fish school maneuverings, ants movement. Recently, one school of research has studied these behaviours and proposed artificial paradigms such as Particle Swarm Optimization (PSO), Ant Colony Optimization (ACO), Glowworm Swarm Optimization (GSO) etc. Another school of research used these models and designed robotic platforms to detect (locate) multiple signal sources such as light, fire, plume, odour etc. Kinbots platform is one such recent experiment. In the same line of thought, this extended abstract presents the recently proposed butterfly inspired metaphor and corresponding simulations, ongoing experiments with outcomes. △ Less","24 August, 2022",https://arxiv.org/pdf/2209.02391
Being Automated or Not? Risk Identification of Occupations with Graph Neural Networks,Dawei Xu;Haoran Yang;Marian-Andrei Rizoiu;Guandong Xu,"The rapid advances in automation technologies, such as artificial intelligence (AI) and robotics, pose an increasing risk of automation for occupations, with a likely significant impact on the labour market. Recent social-economic studies suggest that nearly 50\% of occupations are at high risk of being automated in the next decade. However, the lack of granular data and empirically informed models have limited the accuracy of these studies and made it challenging to predict which jobs will be automated. In this paper, we study the automation risk of occupations by performing a classification task between automated and non-automated occupations. The available information is 910 occupations' task statements, skills and interactions categorised by Standard Occupational Classification (SOC). To fully utilize this information, we propose a graph-based semi-supervised classification method named \textbf{A}utomated \textbf{O}ccupation \textbf{C}lassification based on \textbf{G}raph \textbf{C}onvolutional \textbf{N}etworks (\textbf{AOC-GCN}) to identify the automated risk for occupations. This model integrates a heterogeneous graph to capture occupations' local and global contexts. The results show that our proposed method outperforms the baseline models by considering the information of both internal features of occupations and their external interactions. This study could help policymakers identify potential automated occupations and support individuals' decision-making before entering the job market. △ Less","5 September, 2022",https://arxiv.org/pdf/2209.02182
Applying Machine Learning to Life Insurance: some knowledge sharing to master it,Antoine Chancel;Laura Bradier;Antoine Ly;Razvan Ionescu;Laurene Martin;Marguerite Sauce,"Machine Learning permeates many industries, which brings new source of benefits for companies. However within the life insurance industry, Machine Learning is not widely used in practice as over the past years statistical models have shown their efficiency for risk assessment. Thus insurers may face difficulties to assess the value of the artificial intelligence. Focusing on the modification of the life insurance industry over time highlights the stake of using Machine Learning for insurers and benefits that it can bring by unleashing data value. This paper reviews traditional actuarial methodologies for survival modeling and extends them with Machine Learning techniques. It points out differences with regular machine learning models and emphasizes importance of specific implementations to face censored data with machine learning models family. In complement to this article, a Python library has been developed. Different open-source Machine Learning algorithms have been adjusted to adapt the specificities of life insurance data, namely censoring and truncation. Such models can be easily applied from this SCOR library to accurately model life insurance risks. △ Less","27 September, 2022",https://arxiv.org/pdf/2209.02057
Incremental Permutation Feature Importance (iPFI): Towards Online Explanations on Data Streams,Fabian Fumagalli;Maximilian Muschalik;Eyke Hüllermeier;Barbara Hammer,"Explainable Artificial Intelligence (XAI) has mainly focused on static learning scenarios so far. We are interested in dynamic scenarios where data is sampled progressively, and learning is done in an incremental rather than a batch mode. We seek efficient incremental algorithms for computing feature importance (FI) measures, specifically, an incremental FI measure based on feature marginalization of absent features similar to permutation feature importance (PFI). We propose an efficient, model-agnostic algorithm called iPFI to estimate this measure incrementally and under dynamic modeling conditions including concept drift. We prove theoretical guarantees on the approximation quality in terms of expectation and variance. To validate our theoretical findings and the efficacy of our approaches compared to traditional batch PFI, we conduct multiple experimental studies on benchmark data with and without concept drift. △ Less","7 September, 2022",https://arxiv.org/pdf/2209.01939
Towards Zero Touch Networks: From the Perspective of Hierarchical Language Systems,Guozhi Lin;Jingguo Ge;Yulei Wu,"With ever-increasing complexity and dynamicity of communication networks, intelligent network operation and maintenance has become more important to network operators. With the fast development of artificial intelligence, concepts such as ""Zero Touch"", ""Intent-based"", ""Knowledge-defined"" and ""Self-driving"" networks have become well-known in networking community for a great vision of making networks automatically manageable and responsive to user demands. This article discusses how to achieve Zero Touch Networks from the perspective of language-like systems. We propose a novel hierarchical `language' framework dedicated for networks to enable the Zero Touch Network, which covers from symbolizing network components, a unified framework for understanding network systems, to the logical description with network semantics. A case study based on the proposed language framework is provided. Finally, we discuss the challenges and open issues of intelligence models for zero touch networks. △ Less","5 September, 2022",https://arxiv.org/pdf/2209.01794
Impact of 4ir technology and its impact on the current deployment,Bandar Alsulaimani;Amanul Islam,"The Fourth Industrial Revolution represents a fundamental change in how we live, work, and relate to one another. It is a new chapter in human development with remarkable technological advancements comparable to those of the first, second, and third industrial revolutions. These developments are fusing the physical, digital, and biological worlds in ways that hold great promise as well as the possibility of great danger. The way that modern people live and work is changing as a result of disruptive technologies and trends including the Internet of Things (IoT), robotics, virtual reality (VR), and artificial intelligence (AI). This is known as the fourth industrial revolution. Industry 4.0 refers to the incorporation of these technologies into production processes. In this article, we discussed the history of 4IR technology, its impact of 4IR technology, and its impact on the current deployment. △ Less","5 September, 2022",https://arxiv.org/pdf/2209.01791
When Bioprocess Engineering Meets Machine Learning: A Survey from the Perspective of Automated Bioprocess Development,Nghia Duong-Trung;Stefan Born;Jong Woo Kim;Marie-Therese Schermeyer;Katharina Paulick;Maxim Borisyak;Mariano Nicolas Cruz-Bournazou;Thorben Werner;Randolf Scholz;Lars Schmidt-Thieme;Peter Neubauer;Ernesto Martinez,"Machine learning (ML) is becoming increasingly crucial in many fields of engineering but has not yet played out its full potential in bioprocess engineering. While experimentation has been accelerated by increasing levels of lab automation, experimental planning and data modeling are still largerly depend on human intervention. ML can be seen as a set of tools that contribute to the automation of the whole experimental cycle, including model building and practical planning, thus allowing human experts to focus on the more demanding and overarching cognitive tasks. First, probabilistic programming is used for the autonomous building of predictive models. Second, machine learning automatically assesses alternative decisions by planning experiments to test hypotheses and conducting investigations to gather informative data that focus on model selection based on the uncertainty of model predictions. This review provides a comprehensive overview of ML-based automation in bioprocess development. On the one hand, the biotech and bioengineering community should be aware of the potential and, most importantly, the limitation of existing ML solutions for their application in biotechnology and biopharma. On the other hand, it is essential to identify the missing links to enable the easy implementation of ML and Artificial Intelligence (AI) tools in valuable solutions for the bio-community. △ Less","1 November, 2022",https://arxiv.org/pdf/2209.01083
MIME: Minority Inclusion for Majority Group Enhancement of AI Performance,Pradyumna Chari;Yunhao Ba;Shreeram Athreya;Achuta Kadambi,"Several papers have rightly included minority groups in artificial intelligence (AI) training data to improve test inference for minority groups and/or society-at-large. A society-at-large consists of both minority and majority stakeholders. A common misconception is that minority inclusion does not increase performance for majority groups alone. In this paper, we make the surprising finding that including minority samples can improve test error for the majority group. In other words, minority group inclusion leads to majority group enhancements (MIME) in performance. A theoretical existence proof of the MIME effect is presented and found to be consistent with experimental results on six different datasets. Project webpage: https://visual.ee.ucla.edu/mime.htm/ △ Less","1 September, 2022",https://arxiv.org/pdf/2209.00746
A Technique to Create Weaker Abstract Board Game Agents via Reinforcement Learning,Peter Jamieson;Indrima Upadhyay,"Board games, with the exception of solo games, need at least one other player to play. Because of this, we created Artificial Intelligent (AI) agents to play against us when an opponent is missing. These AI agents are created in a number of ways, but one challenge with these agents is that an agent can have superior ability compared to us. In this work, we describe how to create weaker AI agents that play board games. We use Tic-Tac-Toe, Nine-Men's Morris, and Mancala, and our technique uses a Reinforcement Learning model where an agent uses the Q-learning algorithm to learn these games. We show how these agents can learn to play the board game perfectly, and we then describe our approach to making weaker versions of these agents. Finally, we provide a methodology to compare AI agents. △ Less","1 September, 2022",https://arxiv.org/pdf/2209.00711
Deep reinforcement learning for quantum multiparameter estimation,Valeria Cimini;Mauro Valeri;Emanuele Polino;Simone Piacentini;Francesco Ceccarelli;Giacomo Corrielli;Nicolò Spagnolo;Roberto Osellame;Fabio Sciarrino,"Estimation of physical quantities is at the core of most scientific research and the use of quantum devices promises to enhance its performances. In real scenarios, it is fundamental to consider that the resources are limited and Bayesian adaptive estimation represents a powerful approach to efficiently allocate, during the estimation process, all the available resources. However, this framework relies on the precise knowledge of the system model, retrieved with a fine calibration that often results computationally and experimentally demanding. Here, we introduce a model-free and deep learning-based approach to efficiently implement realistic Bayesian quantum metrology tasks accomplishing all the relevant challenges, without relying on any a-priori knowledge on the system. To overcome this need, a neural network is trained directly on experimental data to learn the multiparameter Bayesian update. Then, the system is set at its optimal working point through feedbacks provided by a reinforcement learning algorithm trained to reconstruct and enhance experiment heuristics of the investigated quantum sensor. Notably, we prove experimentally the achievement of higher estimation performances than standard methods, demonstrating the strength of the combination of these two black-box algorithms on an integrated photonic circuit. This work represents an important step towards fully artificial intelligence-based quantum metrology. △ Less","1 September, 2022",https://arxiv.org/pdf/2209.00671
Generative Personas That Behave and Experience Like Humans,Matthew Barthet;Ahmed Khalifa;Antonios Liapis;Georgios N. Yannakakis,"Using artificial intelligence (AI) to automatically test a game remains a critical challenge for the development of richer and more complex game worlds and for the advancement of AI at large. One of the most promising methods for achieving that long-standing goal is the use of generative AI agents, namely procedural personas, that attempt to imitate particular playing behaviors which are represented as rules, rewards, or human demonstrations. All research efforts for building those generative agents, however, have focused solely on playing behavior which is arguably a narrow perspective of what a player actually does in a game. Motivated by this gap in the existing state of the art, in this paper we extend the notion of behavioral procedural personas to cater for player experience, thus examining generative agents that can both behave and experience their game as humans would. For that purpose, we employ the Go-Explore reinforcement learning paradigm for training human-like procedural personas, and we test our method on behavior and experience demonstrations of more than 100 players of a racing game. Our findings suggest that the generated agents exhibit distinctive play styles and experience responses of the human personas they were designed to imitate. Importantly, it also appears that experience, which is tied to playing behavior, can be a highly informative driver for better behavioral exploration. △ Less","26 August, 2022",https://arxiv.org/pdf/2209.00459
Characterization and modeling of spiking and bursting in experimental NbOx neuron,Marie Drouhin;Shuai Li;Matthieu Grelier;Sophie Collin;Florian Godel;Robert G. Elliman;Bruno Dlubak;Juan Trastoy;Damien Querlioz;Julie Grollier,"Hardware spiking neural networks hold the promise of realizing artificial intelligence with high energy efficiency. In this context, solid-state and scalable memristors can be used to mimic biological neuron characteristics. However, these devices show limited neuronal behaviors and have to be integrated in more complex circuits to implement the rich dynamics of biological neurons. Here we studied a NbOx memristor neuron that is capable of emulating numerous neuronal dynamics, including tonic spiking, stochastic spiking, leaky-integrate-and-fire features, spike latency, temporal integration. The device also exhibits phasic bursting, a property that has scarcely been observed and studied in solid-state nano-neurons. We show that we can reproduce and understand this particular response through simulations using non-linear dynamics. These results show that a single NbOx device is sufficient to emulate a collection of rich neuronal dynamics that paves a path forward for realizing scalable and energy-efficient neuromorphic computing paradigms. △ Less","1 September, 2022",https://arxiv.org/pdf/2209.00413
Secrecy Analysis for IRS-aided Wiretap MIMO Communications: Fundamental Limits and System Design,Xin Zhang;Shenghui Song,"In order to meet the demands of future innovative applications, many efforts have been made to exceed the limits predicted by Shannon's Theory. Besides the investigation of beyond-Shannon metrics such as security, latency, and semantics, another direction is to jointly design the transceiver and the environment by utilizing the intelligent reflecting surface (IRS). In this paper, we consider the analysis and design of IRS-aided multiple-input multiple-output (MIMO) secure communications, which has attracted much research attention but still in its infancy. For example, despite their importance, the fundamental limits of IRS-aided wiretap MIMO communications are not yet available in the literature. In this paper, we will investigate these fundamental limits by determining the ergodic secrecy rate (ESR) and secrecy outage probability (SOP). For that purpose, the central limit theorem (CLT) for the joint distributions of the mutual information (MI) statistics over the IRS-aided MIMO secure communication channel is derived by utilizing the random matrix theory (RMT). The derived CLT is then used to obtain the closed form expressions for the ESR and SOP, which are also extended to the scenario with multiple multi-antenna eavesdroppers. Based on the theoretical results, algorithms for maximizing the artificial noise (AN) aided ESR and minimizing the SOP are proposed. Numerical results validate the accuracy of the theoretical results and effectiveness of the proposed optimization algorithms. △ Less","1 September, 2022",https://arxiv.org/pdf/2209.00392
Attack Tactic Identification by Transfer Learning of Language Model,Ling-Hsuan Lin;Shun-Wen Hsiao,"Cybersecurity has become a primary global concern with the rapid increase in security attacks and data breaches. Artificial intelligence is promising to help humans analyzing and identifying attacks. However, labeling millions of packets for supervised learning is never easy. This study aims to leverage transfer learning technique that stores the knowledge gained from well-defined attack lifecycle documents and applies it to hundred thousands of unlabeled attacks (packets) for identifying their attack tactics. We anticipate the knowledge of an attack is well-described in the documents, and the cutting edge transformer-based language model can embed the knowledge into a high-dimensional latent space. Then, reusing the information from the language model for the learning of attack tactic carried by packets to improve the learning efficiency. We propose a system, PELAT, that fine-tunes BERT model with 1,417 articles from MITRE ATT&CK lifecycle framework to enhance its attack knowledge (including syntax used and semantic meanings embedded). PELAT then transfers its knowledge to perform semi-supervised learning for unlabeled packets to generate their tactic labels. Further, when a new attack packet arrives, the packet payload will be processed by the PELAT language model with a downstream classifier to predict its tactics. In this way, we can effectively reduce the burden of manually labeling big datasets. In a one-week honeypot attack dataset (227 thousand packets per day), PELAT performs 99% of precision, recall, and F1 on testing dataset. PELAT can infer over 99% of tactics on two other testing datasets (while nearly 90% of tactics are identified). △ Less","1 September, 2022",https://arxiv.org/pdf/2209.00263
CPS Attack Detection under Limited Local Information in Cyber Security: A Multi-node Multi-class Classification Ensemble Approach,Junyi Liu;Yifu Tang;Haimeng Zhao;Xieheng Wang;Fangyu Li;Jingyi Zhang,"Cybersecurity breaches are the common anomalies for distributed cyber-physical systems (CPS). However, the cyber security breach classification is still a difficult problem, even using cutting-edge artificial intelligence (AI) approaches. In this paper, we study the multi-class classification problem in cyber security for attack detection. A challenging multi-node data-censoring case is considered. In such a case, data within each data center/node cannot be shared while the local data is incomplete. Particularly, local nodes contain only a part of the multiple classes. In order to train a global multi-class classifier without sharing the raw data across all nodes, the main result of our study is designing a multi-node multi-class classification ensemble approach. By gathering the estimated parameters of the binary classifiers and data densities from each local node, the missing information for each local node is completed to build the global multi-class classifier. Numerical experiments are given to validate the effectiveness of the proposed approach under the multi-node data-censoring case. Under such a case, we even show the out-performance of the proposed approach over the full-data approach. △ Less","31 August, 2022",https://arxiv.org/pdf/2209.00170
"Feynman on Artificial Intelligence and Machine Learning, with Updates",Eric Mjolsness,"I present my recollections of Richard Feynman's mid-1980s interest in artificial intelligence and neural networks, set in the technical context of the physics-related approaches to neural networks of that time. I attempt to evaluate his ideas in the light of the substantial advances in the field since then, and vice versa. There are aspects of Feynman's interests that I think have been largely achieved and others that remain excitingly open, notably in computational science, and potentially including the revival of symbolic methods therein. △ Less","31 August, 2022",https://arxiv.org/pdf/2209.00083
Explainable Artificial Intelligence Applications in Cyber Security: State-of-the-Art in Research,Zhibo Zhang;Hussam Al Hamadi;Ernesto Damiani;Chan Yeob Yeun;Fatma Taher,"This survey presents a comprehensive review of current literature on Explainable Artificial Intelligence (XAI) methods for cyber security applications. Due to the rapid development of Internet-connected systems and Artificial Intelligence in recent years, Artificial Intelligence including Machine Learning (ML) and Deep Learning (DL) has been widely utilized in the fields of cyber security including intrusion detection, malware detection, and spam filtering. However, although Artificial Intelligence-based approaches for the detection and defense of cyber attacks and threats are more advanced and efficient compared to the conventional signature-based and rule-based cyber security strategies, most ML-based techniques and DL-based techniques are deployed in the black-box manner, meaning that security experts and customers are unable to explain how such procedures reach particular conclusions. The deficiencies of transparency and interpretability of existing Artificial Intelligence techniques would decrease human users' confidence in the models utilized for the defense against cyber attacks, especially in current situations where cyber attacks become increasingly diverse and complicated. Therefore, it is essential to apply XAI in the establishment of cyber security models to create more explainable models while maintaining high accuracy and allowing human users to comprehend, trust, and manage the next generation of cyber defense mechanisms. Although there are papers reviewing Artificial Intelligence applications in cyber security areas and the vast literature on applying XAI in many fields including healthcare, financial services, and criminal justice, the surprising fact is that there are currently no survey research articles that concentrate on XAI applications in cyber security. △ Less","31 August, 2022",https://arxiv.org/pdf/2208.14937
Intelligent Closed-loop RAN Control with xApps in OpenRAN Gym,Leonardo Bonati;Michele Polese;Salvatore D'Oro;Stefano Basagni;Tommaso Melodia,"Softwarization, programmable network control and the use of all-encompassing controllers acting at different timescales are heralded as the key drivers for the evolution to next-generation cellular networks. These technologies have fostered newly designed intelligent data-driven solutions for managing large sets of diverse cellular functionalities, basically impossible to implement in traditionally closed cellular architectures. Despite the evident interest of industry on Artificial Intelligence (AI) and Machine Learning (ML) solutions for closed-loop control of the Radio Access Network (RAN), and several research works in the field, their design is far from mainstream, and it is still a sophisticated and often overlooked operation. In this paper, we discuss how to design AI/ML solutions for the intelligent closed-loop control of the Open RAN, providing guidelines and insights based on exemplary solutions with high-performance record. We then show how to embed these solutions into xApps instantiated on the O-RAN near-real-time RAN Intelligent Controller (RIC) through OpenRAN Gym, the first publicly available toolbox for data-driven O-RAN experimentation at scale. We showcase a use case of an xApp developed with OpenRAN Gym and tested on a cellular network with 7 base stations and 42 users deployed on the Colosseum wireless network emulator. Our demonstration shows the high degree of flexibility of the OpenRAN Gym-based xApp development environment, which is independent of deployment scenarios and traffic demand. △ Less","31 August, 2022",https://arxiv.org/pdf/2208.14877
Artificial intelligence-based locoregional markers of brain peritumoral microenvironment,Zahra Riahi Samani;Drew Parker;Hamed Akbari;Spyridon Bakas;Ronald L. Wolf;Steven Brem;Ragini Verma,"In malignant primary brain tumors, cancer cells infiltrate into the peritumoral brain structures which results in inevitable recurrence. Quantitative assessment of infiltrative heterogeneity in the peritumoral region, the area where biopsy or resection can be hazardous, is important for clinical decision making. Previous work on characterizing the infiltrative heterogeneity in the peritumoral region used various imaging modalities, but information of extracellular free water movement restriction has been limitedly explored. Here, we derive a unique set of Artificial Intelligence (AI)-based markers capturing the heterogeneity of tumor infiltration, by characterizing free water movement restriction in the peritumoral region using Diffusion Tensor Imaging (DTI)-based free water volume fraction maps. A novel voxel-wise deep learning-based peritumoral microenvironment index (PMI) is first extracted by leveraging the widely different water diffusivity properties of glioblastomas and brain metastases as regions with and without infiltrations in the peritumoral tissue. Descriptive characteristics of locoregional hubs of uniformly high PMI values are extracted as AI-based markers to capture distinct aspects of infiltrative heterogeneity. The proposed markers are applied to two clinical use cases on an independent population of 275 adult-type diffuse gliomas (CNS WHO grade 4), analyzing the duration of survival among Isocitrate-Dehydrogenase 1 (IDH1)-wildtypes and the differences with IDH1-mutants. Our findings provide a panel of markers as surrogates of infiltration that captures unique insight about underlying biology of peritumoral microstructural heterogeneity, establishing them as biomarkers of prognosis pertaining to survival and molecular stratification, with potential applicability in clinical decision making. △ Less","29 August, 2022",https://arxiv.org/pdf/2208.14445
A Black-Box Attack on Optical Character Recognition Systems,Samet Bayram;Kenneth Barner,"Adversarial machine learning is an emerging area showing the vulnerability of deep learning models. Exploring attack methods to challenge state of the art artificial intelligence (A.I.) models is an area of critical concern. The reliability and robustness of such A.I. models are one of the major concerns with an increasing number of effective adversarial attack methods. Classification tasks are a major vulnerable area for adversarial attacks. The majority of attack strategies are developed for colored or gray-scaled images. Consequently, adversarial attacks on binary image recognition systems have not been sufficiently studied. Binary images are simple two possible pixel-valued signals with a single channel. The simplicity of binary images has a significant advantage compared to colored and gray scaled images, namely computation efficiency. Moreover, most optical character recognition systems (O.C.R.s), such as handwritten character recognition, plate number identification, and bank check recognition systems, use binary images or binarization in their processing steps. In this paper, we propose a simple yet efficient attack method, Efficient Combinatorial Black-box Adversarial Attack, on binary image classifiers. We validate the efficiency of the attack technique on two different data sets and three classification networks, demonstrating its performance. Furthermore, we compare our proposed method with state-of-the-art methods regarding advantages and disadvantages as well as applicability. △ Less","30 August, 2022",https://arxiv.org/pdf/2208.14302
Solving the Capsulation Attack against Backdoor-based Deep Neural Network Watermarks by Reversing Triggers,Fangqi Li;Shilin Wang;Yun Zhu,"Backdoor-based watermarking schemes were proposed to protect the intellectual property of artificial intelligence models, especially deep neural networks, under the black-box setting. Compared with ordinary backdoors, backdoor-based watermarks need to digitally incorporate the owner's identity, which fact adds extra requirements to the trigger generation and verification programs. Moreover, these concerns produce additional security risks after the watermarking scheme has been published for as a forensics tool or the owner's evidence has been eavesdropped on. This paper proposes the capsulation attack, an efficient method that can invalidate most established backdoor-based watermarking schemes without sacrificing the pirated model's functionality. By encapsulating the deep neural network with a rule-based or Bayes filter, an adversary can block ownership probing and reject the ownership verification. We propose a metric, CAScore, to measure a backdoor-based watermarking scheme's security against the capsulation attack. This paper also proposes a new backdoor-based deep neural network watermarking scheme that is secure against the capsulation attack by reversing the encoding process and randomizing the exposure of triggers. △ Less","30 August, 2022",https://arxiv.org/pdf/2208.14127
SB-SSL: Slice-Based Self-Supervised Transformers for Knee Abnormality Classification from MRI,Sara Atito;Syed Muhammad Anwar;Muhammad Awais;Josef Kitler,"The availability of large scale data with high quality ground truth labels is a challenge when developing supervised machine learning solutions for healthcare domain. Although, the amount of digital data in clinical workflows is increasing, most of this data is distributed on clinical sites and protected to ensure patient privacy. Radiological readings and dealing with large-scale clinical data puts a significant burden on the available resources, and this is where machine learning and artificial intelligence play a pivotal role. Magnetic Resonance Imaging (MRI) for musculoskeletal (MSK) diagnosis is one example where the scans have a wealth of information, but require a significant amount of time for reading and labeling. Self-supervised learning (SSL) can be a solution for handling the lack of availability of ground truth labels, but generally requires a large amount of training data during the pretraining stage. Herein, we propose a slice-based self-supervised deep learning framework (SB-SSL), a novel slice-based paradigm for classifying abnormality using knee MRI scans. We show that for a limited number of cases (<1000), our proposed framework is capable to identify anterior cruciate ligament tear with an accuracy of 89.17% and an AUC of 0.954, outperforming state-of-the-art without usage of external data during pretraining. This demonstrates that our proposed framework is suited for SSL in the limited data regime. △ Less","29 August, 2022",https://arxiv.org/pdf/2208.13923
A systematic review of research on the use and impact of technology for learning Chinese,Angelina Maksimova,"In light of technological development enforced by the pandemic, learning Chinese has become more digitalised. Confucius institutes went online and now follow 2021 to 2025 Action Plans for the Construction of Teaching Resources for International Chinese Education and International Chinese Online Education. New ways of learning Chinese emerged, such as educational games and intelligent tutoring systems ITS, some of them based on artificial intelligence. The aim of this systematic review is to examine recent research published in ScienceDirect and Scopus databases on the use and impact of educational games and ITS in Chinese language learning. A total of 29 selected studies were analysed. △ Less","29 August, 2022",https://arxiv.org/pdf/2208.13630
When Internet of Things meets Metaverse: Convergence of Physical and Cyber Worlds,Kai Li;Yingping Cui;Weicai Li;Tiejun Lv;Xin Yuan;Shenghong Li;Wei Ni;Meryem Simsek;Falko Dressler,"In recent years, the Internet of Things (IoT) is studied in the context of the Metaverse to provide users immersive cyber-virtual experiences in mixed reality environments. This survey introduces six typical IoT applications in the Metaverse, including collaborative healthcare, education, smart city, entertainment, real estate, and socialization. In the IoT-inspired Metaverse, we also comprehensively survey four pillar technologies that enable augmented reality (AR) and virtual reality (VR), namely, responsible artificial intelligence (AI), high-speed data communications, cost-effective mobile edge computing (MEC), and digital twins. According to the physical-world demands, we outline the current industrial efforts and seven key requirements for building the IoT-inspired Metaverse: immersion, variety, economy, civility, interactivity, authenticity, and independence. In addition, this survey describes the open issues in the IoT-inspired Metaverse, which need to be addressed to eventually achieve the convergence of physical and cyber worlds. △ Less","29 August, 2022",https://arxiv.org/pdf/2208.13501
Industrial Requirements for Supporting AI-Enhanced Model-Driven Engineering,Johan Bergelin;Per Erik Strandberg,"There is an increasing interest in research on the combination of AI techniques and methods with MDE. However, there is a gap between AI and MDE practices, as well as between researchers and practitioners. This paper tackles this gap by reporting on industrial requirements in this field. In the AIDOaRt research project, practitioners and researchers collaborate on AI-augmented automation supporting modeling, coding, testing, monitoring, and continuous development in cyber-physical systems. The project specifically lies at the intersection of industry and academia collaboration with several industrial use cases. Through a process of elicitation and refinement, 78 high-level requirements were defined, and generalized into 30 generic requirements by the AIDOaRt partners. The main contribution of this paper is the set of generic requirements from the project for enhancing the development of cyber-physical systems with artificial intelligence, DevOps, and model-driven engineering, identifying the hot spots of industry needs in the interactions of MDE and AI. Future work will refine, implement and evaluate solutions toward these requirements in industry contexts. △ Less","29 August, 2022",https://arxiv.org/pdf/2208.13421
Power Delivery for Ultra-Large-Scale Applications on Si-IF,Yousef Safari;Anja Kroon;Boris Vaisband,"In recent years, with the rise of artificial intelligence and big data, there is an even greater demand for scaling out computing and memory capacity. Silicon interconnect fabric (Si-IF), a wafer-scale integration platform, promotes a paradigm shift in packaging features and enables ultra-large-scale systems, while significantly improving communication bandwidth and latency. Such systems are expected to dissipate tens of kilowatts of power. Designing an efficient and robust power delivery methodology for these high power applications is a key challenge in the enablement of the Si-IF platform. Based on several figure-of-merit parameters, an efficient power delivery methodology is matched with each of three candidate applications on the Si-IF, namely, artificial intelligence accelerators, high-performance computing, and neuromorphic computing. The proposed power delivery approaches were simulated and exhibit compatibility with the relevant ultra-large-scale application on Si-IF. The simulation results confirm that the dedicated power delivery topologies can support ultra-large-scale applications on the SI-IF. △ Less","27 August, 2022",https://arxiv.org/pdf/2208.13034
Ammunition Component Classification Using Deep Learning,Hadi Ghahremannezhad;Chengjun Liu;Hang Shi,"Ammunition scrap inspection is an essential step in the process of recycling ammunition metal scrap. Most ammunition is composed of a number of components, including case, primer, powder, and projectile. Ammo scrap containing energetics is considered to be potentially dangerous and should be separated before the recycling process. Manually inspecting each piece of scrap is tedious and time-consuming. We have gathered a dataset of ammunition components with the goal of applying artificial intelligence for classifying safe and unsafe scrap pieces automatically. First, two training datasets are manually created from visual and x-ray images of ammo. Second, the x-ray dataset is augmented using the spatial transforms of histogram equalization, averaging, sharpening, power law, and Gaussian blurring in order to compensate for the lack of sufficient training data. Lastly, the representative YOLOv4 object detection method is applied to detect the ammo components and classify the scrap pieces into safe and unsafe classes, respectively. The trained models are tested against unseen data in order to evaluate the performance of the applied method. The experiments demonstrate the feasibility of ammo component detection and classification using deep learning. The datasets and the pre-trained models are available at https://github.com/hadi-ghnd/Scrap-Classification. △ Less","26 August, 2022",https://arxiv.org/pdf/2208.12863
What Do NLP Researchers Believe? Results of the NLP Community Metasurvey,Julian Michael;Ari Holtzman;Alicia Parrish;Aaron Mueller;Alex Wang;Angelica Chen;Divyam Madaan;Nikita Nangia;Richard Yuanzhe Pang;Jason Phang;Samuel R. Bowman,"We present the results of the NLP Community Metasurvey. Run from May to June 2022, the survey elicited opinions on controversial issues, including industry influence in the field, concerns about AGI, and ethics. Our results put concrete numbers to several controversies: For example, respondents are split almost exactly in half on questions about the importance of artificial general intelligence, whether language models understand language, and the necessity of linguistic structure and inductive bias for solving NLP problems. In addition, the survey posed meta-questions, asking respondents to predict the distribution of survey responses. This allows us not only to gain insight on the spectrum of beliefs held by NLP researchers, but also to uncover false sociological beliefs where the community's predictions don't match reality. We find such mismatches on a wide range of issues. Among other results, the community greatly overestimates its own belief in the usefulness of benchmarks and the potential for scaling to solve real-world problems, while underestimating its own belief in the importance of linguistic structure, inductive bias, and interdisciplinary science. △ Less","26 August, 2022",https://arxiv.org/pdf/2208.12852
Complexity-Driven CNN Compression for Resource-constrained Edge AI,Muhammad Zawish;Steven Davy;Lizy Abraham,"Recent advances in Artificial Intelligence (AI) on the Internet of Things (IoT)-enabled network edge has realized edge intelligence in several applications such as smart agriculture, smart hospitals, and smart factories by enabling low-latency and computational efficiency. However, deploying state-of-the-art Convolutional Neural Networks (CNNs) such as VGG-16 and ResNets on resource-constrained edge devices is practically infeasible due to their large number of parameters and floating-point operations (FLOPs). Thus, the concept of network pruning as a type of model compression is gaining attention for accelerating CNNs on low-power devices. State-of-the-art pruning approaches, either structured or unstructured do not consider the different underlying nature of complexities being exhibited by convolutional layers and follow a training-pruning-retraining pipeline, which results in additional computational overhead. In this work, we propose a novel and computationally efficient pruning pipeline by exploiting the inherent layer-level complexities of CNNs. Unlike typical methods, our proposed complexity-driven algorithm selects a particular layer for filter-pruning based on its contribution to overall network complexity. We follow a procedure that directly trains the pruned model and avoids the computationally complex ranking and fine-tuning steps. Moreover, we define three modes of pruning, namely parameter-aware (PA), FLOPs-aware (FA), and memory-aware (MA), to introduce versatile compression of CNNs. Our results show the competitive performance of our approach in terms of accuracy and acceleration. Lastly, we present a trade-off between different resources and accuracy which can be helpful for developers in making the right decisions in resource-constrained IoT environments. △ Less","26 August, 2022",https://arxiv.org/pdf/2208.12816
Quality Diversity Evolutionary Learning of Decision Trees,Andrea Ferigo;Leonardo Lucio Custode;Giovanni Iacca,"Addressing the need for explainable Machine Learning has emerged as one of the most important research directions in modern Artificial Intelligence (AI). While the current dominant paradigm in the field is based on black-box models, typically in the form of (deep) neural networks, these models lack direct interpretability for human users, i.e., their outcomes (and, even more so, their inner working) are opaque and hard to understand. This is hindering the adoption of AI in safety-critical applications, where high interests are at stake. In these applications, explainable by design models, such as decision trees, may be more suitable, as they provide interpretability. Recent works have proposed the hybridization of decision trees and Reinforcement Learning, to combine the advantages of the two approaches. So far, however, these works have focused on the optimization of those hybrid models. Here, we apply MAP-Elites for diversifying hybrid models over a feature space that captures both the model complexity and its behavioral variability. We apply our method on two well-known control problems from the OpenAI Gym library, on which we discuss the ""illumination"" patterns projected by MAP-Elites, comparing its results against existing similar approaches. △ Less","17 August, 2022",https://arxiv.org/pdf/2208.12758
A Joint Framework to Privacy-Preserving Edge Intelligence in Vehicular Networks,Muhammad Firdaus;Kyung-Hyune Rhee,"The number of internet-connected devices has been exponentially growing with the massive volume of heterogeneous data generated from various devices, resulting in a highly intertwined cyber-physical system. Currently, the Edge Intelligence System (EIS) concept that leverages the merits of edge computing and Artificial Intelligence (AI) is utilized to provide smart cloud services with powerful computational processing and reduce decision-making delays. Thus, EIS offers a possible solution to realizing future Intelligent Transportation Systems (ITS), especially in a vehicular network framework. However, since the central aggregator server supervises the entire system orchestration, the existing EIS framework faces several challenges and is still potentially susceptible to numerous malicious attacks. Hence, to solve the issues mentioned earlier, this paper presents the notion of secure edge intelligence, merging the benefits of Federated Learning (FL), blockchain, and Local Differential Privacy (LDP). The blockchain-assisted FL approach efficiently improves traffic prediction accuracy and enhances user privacy and security by recording transactions in immutable distributed ledger networks and providing a decentralized reward mechanism system. Furthermore, LDP is empowered to strengthen the confidentiality of data sharing transactions, especially in protecting users' private data from various attacks. The proposed framework has been implemented in two scenarios, i.e., blockchain-based FL to efficiently develop the decentralized traffic management for vehicular networks and LDP-based FL to produce randomized privacy protection using the IBM Library for differential privacy. △ Less","18 August, 2022",https://arxiv.org/pdf/2208.12755
Symbolic Explanation of Affinity-Based Reinforcement Learning Agents with Markov Models,Charl Maree;Christian W. Omlin,"The proliferation of artificial intelligence is increasingly dependent on model understanding. Understanding demands both an interpretation - a human reasoning about a model's behavior - and an explanation - a symbolic representation of the functioning of the model. Notwithstanding the imperative of transparency for safety, trust, and acceptance, the opacity of state-of-the-art reinforcement learning algorithms conceals the rudiments of their learned strategies. We have developed a policy regularization method that asserts the global intrinsic affinities of learned strategies. These affinities provide a means of reasoning about a policy's behavior, thus making it inherently interpretable. We have demonstrated our method in personalized prosperity management where individuals' spending behavior in time dictate their investment strategies, i.e. distinct spending personalities may have dissimilar associations with different investment classes. We now explain our model by reproducing the underlying prototypical policies with discretized Markov models. These global surrogates are symbolic representations of the prototypical policies. △ Less","29 August, 2022",https://arxiv.org/pdf/2208.12627
Towards Benchmarking Explainable Artificial Intelligence Methods,Lars Holmberg,"The currently dominating artificial intelligence and machine learning technology, neural networks, builds on inductive statistical learning. Neural networks of today are information processing systems void of understanding and reasoning capabilities, consequently, they cannot explain promoted decisions in a humanly valid form. In this work, we revisit and use fundamental philosophy of science theories as an analytical lens with the goal of revealing, what can be expected, and more importantly, not expected, from methods that aim to explain decisions promoted by a neural network. By conducting a case study we investigate a selection of explainability method's performance over two mundane domains, animals and headgear. Through our study, we lay bare that the usefulness of these methods relies on human domain knowledge and our ability to understand, generalise and reason. The explainability methods can be useful when the goal is to gain further insights into a trained neural network's strengths and weaknesses. If our aim instead is to use these explainability methods to promote actionable decisions or build trust in ML-models they need to be less ambiguous than they are today. In this work, we conclude from our study, that benchmarking explainability methods, is a central quest towards trustworthy artificial intelligence and machine learning. △ Less","25 August, 2022",https://arxiv.org/pdf/2208.12120
A Platform-Free Proof of Federated Learning Consensus Mechanism for Sustainable Blockchains,Yuntao Wang;Haixia Peng;Zhou Su;Tom H Luan;Abderrahim Benslimane;Yuan Wu,"Proof of work (PoW), as the representative consensus protocol for blockchain, consumes enormous amounts of computation and energy to determine bookkeeping rights among miners but does not achieve any practical purposes. To address the drawback of PoW, we propose a novel energy-recycling consensus mechanism named platform-free proof of federated learning (PF-PoFL), which leverages the computing power originally wasted in solving hard but meaningless PoW puzzles to conduct practical federated learning (FL) tasks. Nevertheless, potential security threats and efficiency concerns may occur due to the untrusted environment and miners' self-interested features. In this paper, by devising a novel block structure, new transaction types, and credit-based incentives, PF-PoFL allows efficient artificial intelligence (AI) task outsourcing, federated mining, model evaluation, and reward distribution in a fully decentralized manner, while resisting spoofing and Sybil attacks. Besides, PF-PoFL equips with a user-level differential privacy mechanism for miners to prevent implicit privacy leakage in training FL models. Furthermore, by considering dynamic miner characteristics (e.g., training samples, non-IID degree, and network delay) under diverse FL tasks, a federation formation game-based mechanism is presented to distributively form the optimized disjoint miner partition structure with Nash-stable convergence. Extensive simulations validate the efficiency and effectiveness of PF-PoFL. △ Less","29 October, 2022",https://arxiv.org/pdf/2208.12046
Explainable AI for tailored electricity consumption feedback -- an experimental evaluation of visualizations,Jacqueline Wastensteiner;Tobias M. Weiss;Felix Haag;Konstantin Hopf,"Machine learning (ML) methods can effectively analyse data, recognize patterns in them, and make high-quality predictions. Good predictions usually come along with ""black-box"" models that are unable to present the detected patterns in a human-readable way. Technical developments recently led to eXplainable Artificial Intelligence (XAI) techniques that aim to open such black-boxes and enable humans to gain new insights from detected patterns. We investigated the application of XAI in an area where specific insights can have a significant effect on consumer behaviour, namely electricity use. Knowing that specific feedback on individuals' electricity consumption triggers resource conservation, we created five visualizations with ML and XAI methods from electricity consumption time series for highly personalized feedback, considering existing domain-specific design knowledge. Our experimental evaluation with 152 participants showed that humans can assimilate the pattern displayed by XAI visualizations, but such visualizations should follow known visualization patterns to be well-understood by users. △ Less","24 August, 2022",https://arxiv.org/pdf/2208.11408
Augmented cross-selling through explainable AI -- a case from energy retailing,Felix Haag;Konstantin Hopf;Pedro Menelau Vasconcelos;Thorsten Staake,"The advance of Machine Learning (ML) has led to a strong interest in this technology to support decision making. While complex ML models provide predictions that are often more accurate than those of traditional tools, such models often hide the reasoning behind the prediction from their users, which can lead to lower adoption and lack of insight. Motivated by this tension, research has put forth Explainable Artificial Intelligence (XAI) techniques that uncover patterns discovered by ML. Despite the high hopes in both ML and XAI, there is little empirical evidence of the benefits to traditional businesses. To this end, we analyze data on 220,185 customers of an energy retailer, predict cross-purchases with up to 86% correctness (AUC), and show that the XAI method SHAP provides explanations that hold for actual buyers. We further outline implications for research in information systems, XAI, and relationship marketing. △ Less","24 August, 2022",https://arxiv.org/pdf/2208.11404
Advanced Tools and Methods for Treewidth-Based Problem Solving -- Extended Abstract,Markus Hecher,"Computer programs, so-called solvers, for solving the well-known Boolean satisfiability problem (Sat) have been improving for decades. Among the reasons, why these solvers are so fast, is the implicit usage of the formula's structural properties during solving. One of such structural indicators is the so-called treewidth, which tries to measure how close a formula instance is to being easy (tree-like). This work focuses on logic-based problems and treewidth-based methods and tools for solving them. Many of these problems are also relevant for knowledge representation and reasoning (KR) as well as artificial intelligence (AI) in general. We present a new type of problem reduction, which is referred to by decomposition-guided (DG). This reduction type forms the basis to solve a problem for quantified Boolean formulas (QBFs) of bounded treewidth that has been open since 2004. The solution of this problem then gives rise to a new methodology for proving precise lower bounds for a range of further formalisms in logic, KR, and AI. Despite the established lower bounds, we implement an algorithm for solving extensions of Sat efficiently, by directly using treewidth. Our implementation is based on finding abstractions of instances, which are then incrementally refined in the process. Thereby, our observations confirm that treewidth is an important measure that should be considered in the design of modern solvers. △ Less","24 August, 2022",https://arxiv.org/pdf/2208.11340
Accelerating Monte-Carlo Tree Search on CPU-FPGA Heterogeneous Platform,Yuan Meng;Rajgopal Kannan;Viktor Prasanna,"Monte Carlo Tree Search (MCTS) methods have achieved great success in many Artificial Intelligence (AI) benchmarks. The in-tree operations become a critical performance bottleneck in realizing parallel MCTS on CPUs. In this work, we develop a scalable CPU-FPGA system for Tree-Parallel MCTS. We propose a novel decomposition and mapping of MCTS data structure and computation onto CPU and FPGA to reduce communication and coordination. High scalability of our system is achieved by encapsulating in-tree operations in an SRAM-based FPGA accelerator. To lower the high data access latency and inter-worker synchronization overheads, we develop several hardware optimizations. We show that by using our accelerator, we obtain up to 35\times speedup for in-tree operations, and 3\times higher overall system throughput. Our CPU-FPGA system also achieves superior scalability wrt number of parallel workers than state-of-the-art parallel MCTS implementations on CPU. △ Less","23 August, 2022",https://arxiv.org/pdf/2208.11208
Distance-Aware Occlusion Detection with Focused Attention,Yang Li;Yucheng Tu;Xiaoxue Chen;Hao Zhao;Guyue Zhou,"For humans, understanding the relationships between objects using visual signals is intuitive. For artificial intelligence, however, this task remains challenging. Researchers have made significant progress studying semantic relationship detection, such as human-object interaction detection and visual relationship detection. We take the study of visual relationships a step further from semantic to geometric. In specific, we predict relative occlusion and relative distance relationships. However, detecting these relationships from a single image is challenging. Enforcing focused attention to task-specific regions plays a critical role in successfully detecting these relationships. In this work, (1) we propose a novel three-decoder architecture as the infrastructure for focused attention; 2) we use the generalized intersection box prediction task to effectively guide our model to focus on occlusion-specific regions; 3) our model achieves a new state-of-the-art performance on distance-aware relationship detection. Specifically, our model increases the distance F1-score from 33.8% to 38.6% and boosts the occlusion F1-score from 34.4% to 41.2%. Our code is publicly available. △ Less","23 August, 2022",https://arxiv.org/pdf/2208.11122
Challenges and Complexities in Machine Learning based Credit Card Fraud Detection,Gayan K. Kulatilleke,"Credit cards play an exploding role in modern economies. Its popularity and ubiquity have created a fertile ground for fraud, assisted by the cross boarder reach and instantaneous confirmation. While transactions are growing, the fraud percentages are also on the rise as well as the true cost of a dollar fraud. Volume of transactions, uniqueness of frauds and ingenuity of the fraudster are main challenges in detecting frauds. The advent of machine learning, artificial intelligence and big data has opened up new tools in the fight against frauds. Given past transactions, a machine learning algorithm has the ability to 'learn' infinitely complex characteristics in order to identify frauds in real-time, surpassing the best human investigators. However, the developments in fraud detection algorithms has been challenging and slow due the massively unbalanced nature of fraud data, absence of benchmarks and standard evaluation metrics to identify better performing classifiers, lack of sharing and disclosure of research findings and the difficulties in getting access to confidential transaction data for research. This work investigates the properties of typical massively imbalanced fraud data sets, their availability, suitability for research use while exploring the widely varying nature of fraud distributions. Furthermore, we show how human annotation errors compound with machine classification errors. We also carry out experiments to determine the effect of PCA obfuscation (as a means of disseminating sensitive transaction data for research and machine learning) on algorithmic performance of classifiers and show that while PCA does not significantly degrade performance, care should be taken to use the appropriate principle component size (dimensions) to avoid overfitting. △ Less","20 August, 2022",https://arxiv.org/pdf/2208.10943
Voice Chatbot for Hospitality,Sagina Athikkal;John Jenq,"Chatbot is a machine with the ability to answer automatically through a conversational interface. A chatbot is considered as one of the most exceptional and promising expressions of human computer interaction. Voice-based chatbots or artificial intelligence devices transform human-computer bidirectional interactions that allow users to navigate an interactive voice response system with their voice generally using natural language. In this paper, we focus on voice based chatbots for mediating interactions between hotels and guests from both the hospitality technology providers' and guests' perspectives. We developed a hotel web application with the capability to receive a voice input. The application was developed with Speech recognition and deep synthesis API for voice to text and text to voice conversion, a closed domain question answering NLP solution was used for query the answer. △ Less","13 August, 2022",https://arxiv.org/pdf/2208.10926
"AI and 6G into the Metaverse: Fundamentals, Challenges and Future Research Trends",Muhammad Zawish;Fayaz Ali Dharejo;Sunder Ali Khowaja;Kapal Dev;Steven Davy;Nawab Muhammad Faseeh Qureshi;Paolo Bellavista,"Since Facebook was renamed Meta, a lot of attention, debate, and exploration have intensified about what the Metaverse is, how it works, and the possible ways to exploit it. It is anticipated that Metaverse will be a continuum of rapidly emerging technologies, usecases, capabilities, and experiences that will make it up for the next evolution of the Internet. Several researchers have already surveyed the literature on artificial intelligence (AI) and wireless communications in realizing the Metaverse. However, due to the rapid emergence and continuous evolution of technologies, there is a need for a comprehensive and in-depth survey of the role of AI, 6G, and the nexus of both in realizing the immersive experiences of Metaverse. Therefore, in this survey, we first introduce the background and ongoing progress in augmented reality (AR), virtual reality (VR), mixed reality (MR) and spatial computing, followed by the technical aspects of AI and 6G. Then, we survey the role of AI in the Metaverse by reviewing the state-of-the-art in deep learning, computer vision, and Edge AI to extract the requirements of 6G in Metaverse. Next, we investigate the promising services of B5G/6G towards Metaverse, followed by identifying the role of AI in 6G networks and 6G networks for AI in support of Metaverse applications, and the need for sustainability in Metaverse. Finally, we enlist the existing and potential applications, usecases, and projects to highlight the importance of progress in the Metaverse. Moreover, in order to provide potential research directions to researchers, we underline the challenges, research gaps, and lessons learned identified from the literature review of the aforementioned technologies. △ Less","24 September, 2022",https://arxiv.org/pdf/2208.10921
Application of Causal Inference to Analytical Customer Relationship Management in Banking and Insurance,Satyam Kumar;Vadlamani Ravi,"Of late, in order to have better acceptability among various domain, researchers have argued that machine intelligence algorithms must be able to provide explanations that humans can understand causally. This aspect, also known as causability, achieves a specific level of human-level explainability. A specific class of algorithms known as counterfactuals may be able to provide causability. In statistics, causality has been studied and applied for many years, but not in great detail in artificial intelligence (AI). In a first-of-its-kind study, we employed the principles of causal inference to provide explainability for solving the analytical customer relationship management (ACRM) problems. In the context of banking and insurance, current research on interpretability tries to address causality-related questions like why did this model make such decisions, and was the model's choice influenced by a particular factor? We propose a solution in the form of an intervention, wherein the effect of changing the distribution of features of ACRM datasets is studied on the target feature. Subsequently, a set of counterfactuals is also obtained that may be furnished to any customer who demands an explanation of the decision taken by the bank/insurance company. Except for the credit card churn prediction dataset, good quality counterfactuals were generated for the loan default, insurance fraud detection, and credit card fraud detection datasets, where changes in no more than three features are observed. △ Less","19 August, 2022",https://arxiv.org/pdf/2208.10916
String-based Molecule Generation via Multi-decoder VAE,Kisoo Kwon;Kuhwan Jung;Junghyun Park;Hwidong Na;Jinwoo Shin,"In this paper, we investigate the problem of string-based molecular generation via variational autoencoders (VAEs) that have served a popular generative approach for various tasks in artificial intelligence. We propose a simple, yet effective idea to improve the performance of VAE for the task. Our main idea is to maintain multiple decoders while sharing a single encoder, i.e., it is a type of ensemble techniques. Here, we first found that training each decoder independently may not be effective as the bias of the ensemble decoder increases severely under its auto-regressive inference. To maintain both small bias and variance of the ensemble model, our proposed technique is two-fold: (a) a different latent variable is sampled for each decoder (from estimated mean and variance offered by the shared encoder) to encourage diverse characteristics of decoders and (b) a collaborative loss is used during training to control the aggregated quality of decoders using different latent variables. In our experiments, the proposed VAE model particularly performs well for generating a sample from out-of-domain distribution. △ Less","22 August, 2022",https://arxiv.org/pdf/2208.10718
Predicting microsatellite instability and key biomarkers in colorectal cancer from H&E-stained images: Achieving SOTA predictive performance with fewer data using Swin Transformer,Bangwei Guo;Xingyu Li;Jitendra Jonnagaddala;Hong Zhang;Xu Steven Xu,"Artificial intelligence (AI) models have been developed for predicting clinically relevant biomarkers, including microsatellite instability (MSI), for colorectal cancers (CRC). However, the current deep-learning networks are data-hungry and require large training datasets, which are often lacking in the medical domain. In this study, based on the latest Hierarchical Vision Transformer using Shifted Windows (Swin-T), we developed an efficient workflow for biomarkers in CRC (MSI, hypermutation, chromosomal instability, CpG island methylator phenotype, BRAF, and TP53 mutation) that only required relatively small datasets, but achieved the state-of-the-art (SOTA) predictive performance. Our Swin-T workflow not only substantially outperformed published models in an intra-study cross-validation experiment using TCGA-CRC-DX dataset (N = 462), but also showed excellent generalizability in cross-study external validation and delivered a SOTA AUROC of 0.90 for MSI using the MCO dataset for training (N = 1065) and the same TCGA-CRC-DX for testing. Similar performance (AUROC=0.91) was achieved by Echle and colleagues using approximately 8000 training samples (ResNet18) on the same testing dataset. Swin-T was extremely efficient using small training datasets and exhibits robust predictive performance with only 200-500 training samples. These data indicate that Swin-T may be 5-10 times more efficient than the current state-of-the-art algorithms for MSI based on ResNet18 and ShuffleNet. Furthermore, the Swin-T models showed promise as pre-screening tests for MSI status and BRAF mutation status, which could exclude and reduce the samples before the subsequent standard testing in a cascading diagnostic workflow to allow turnaround time reduction and cost saving. △ Less","11 September, 2022",https://arxiv.org/pdf/2208.10495
Shapelet-Based Counterfactual Explanations for Multivariate Time Series,Omar Bahri;Soukaina Filali Boubrahimi;Shah Muhammad Hamdi,"As machine learning and deep learning models have become highly prevalent in a multitude of domains, the main reservation in their adoption for decision-making processes is their black-box nature. The Explainable Artificial Intelligence (XAI) paradigm has gained a lot of momentum lately due to its ability to reduce models opacity. XAI methods have not only increased stakeholders' trust in the decision process but also helped developers ensure its fairness. Recent efforts have been invested in creating transparent models and post-hoc explanations. However, fewer methods have been developed for time series data, and even less when it comes to multivariate datasets. In this work, we take advantage of the inherent interpretability of shapelets to develop a model agnostic multivariate time series (MTS) counterfactual explanation algorithm. Counterfactuals can have a tremendous impact on making black-box models explainable by indicating what changes have to be performed on the input to change the final decision. We test our approach on a real-life solar flare prediction dataset and prove that our approach produces high-quality counterfactuals. Moreover, a comparison to the only MTS counterfactual generation algorithm shows that, in addition to being visually interpretable, our explanations are superior in terms of proximity, sparsity, and plausibility. △ Less","22 August, 2022",https://arxiv.org/pdf/2208.10462
ProtoPFormer: Concentrating on Prototypical Parts in Vision Transformers for Interpretable Image Recognition,Mengqi Xue;Qihan Huang;Haofei Zhang;Lechao Cheng;Jie Song;Minghui Wu;Mingli Song,"Prototypical part network (ProtoPNet) has drawn wide attention and boosted many follow-up studies due to its self-explanatory property for explainable artificial intelligence (XAI). However, when directly applying ProtoPNet on vision transformer (ViT) backbones, learned prototypes have a ""distraction"" problem: they have a relatively high probability of being activated by the background and pay less attention to the foreground. The powerful capability of modeling long-term dependency makes the transformer-based ProtoPNet hard to focus on prototypical parts, thus severely impairing its inherent interpretability. This paper proposes prototypical part transformer (ProtoPFormer) for appropriately and effectively applying the prototype-based method with ViTs for interpretable image recognition. The proposed method introduces global and local prototypes for capturing and highlighting the representative holistic and partial features of targets according to the architectural characteristics of ViTs. The global prototypes are adopted to provide the global view of objects to guide local prototypes to concentrate on the foreground while eliminating the influence of the background. Afterwards, local prototypes are explicitly supervised to concentrate on their respective prototypical visual parts, increasing the overall interpretability. Extensive experiments demonstrate that our proposed global and local prototypes can mutually correct each other and jointly make final decisions, which faithfully and transparently reason the decision-making processes associatively from the whole and local perspectives, respectively. Moreover, ProtoPFormer consistently achieves superior performance and visualization results over the state-of-the-art (SOTA) prototype-based baselines. Our code has been released at https://github.com/zju-vipa/ProtoPFormer. △ Less","26 September, 2022",https://arxiv.org/pdf/2208.10431
Defensive Distillation based Adversarial Attacks Mitigation Method for Channel Estimation using Deep Learning Models in Next-Generation Wireless Networks,Ferhat Ozgur Catak;Murat Kuzlu;Evren Catak;Umit Cali;Ozgur Guler,"Future wireless networks (5G and beyond) are the vision of forthcoming cellular systems, connecting billions of devices and people together. In the last decades, cellular networks have been dramatically growth with advanced telecommunication technologies for high-speed data transmission, high cell capacity, and low latency. The main goal of those technologies is to support a wide range of new applications, such as virtual reality, metaverse, telehealth, online education, autonomous and flying vehicles, smart cities, smart grids, advanced manufacturing, and many more. The key motivation of NextG networks is to meet the high demand for those applications by improving and optimizing network functions. Artificial Intelligence (AI) has a high potential to achieve these requirements by being integrated in applications throughout all layers of the network. However, the security concerns on network functions of NextG using AI-based models, i.e., model poising, have not been investigated deeply. Therefore, it needs to design efficient mitigation techniques and secure solutions for NextG networks using AI-based methods. This paper proposes a comprehensive vulnerability analysis of deep learning (DL)-based channel estimation models trained with the dataset obtained from MATLAB's 5G toolbox for adversarial attacks and defensive distillation-based mitigation methods. The adversarial attacks produce faulty results by manipulating trained DL-based models for channel estimation in NextG networks, while making models more robust against any attacks through mitigation methods. This paper also presents the performance of the proposed defensive distillation mitigation method for each adversarial attack against the channel estimation model. The results indicated that the proposed mitigation method can defend the DL-based channel estimation models against adversarial attacks in NextG networks. △ Less","12 August, 2022",https://arxiv.org/pdf/2208.10279
A Trust Framework for Government Use of Artificial Intelligence and Automated Decision Making,Pia Andrews;Tim de Sousa;Bruce Haefele;Matt Beard;Marcus Wigan;Abhinav Palia;Kathy Reid;Saket Narayan;Morgan Dumitru;Alex Morrison;Geoff Mason;Aurelie Jacquet,"This paper identifies the current challenges of the mechanisation, digitisation and automation of public sector systems and processes, and proposes a modern and practical framework to ensure and assure ethical and high veracity Artificial Intelligence (AI) or Automated Decision Making (ADM) systems in public institutions. This framework is designed for the specific context of the public sector, in the jurisdictional and constitutional context of Australia, but is extendable to other jurisdictions and private sectors. The goals of the framework are to: 1) earn public trust and grow public confidence in government systems; 2) to ensure the unique responsibilities and accountabilities (including to the public) of public institutions under Administrative Law are met effectively; and 3) to assure a positive human, societal and ethical impact from the adoption of such systems. The framework could be extended to assure positive environmental or other impacts, but this paper focuses on human/societal outcomes and public trust. This paper is meant to complement principles-based frameworks like Australia's Artificial Intelligence Ethics Framework and the EU Assessment List for Trustworthy AI. In many countries, COVID created a bubble of improved trust, a bubble which has arguably already popped, and in an era of unprecedented mistrust of public institutions (but even in times of high trust) it is not enough that a service is faster, or more cost-effective. This paper proposes recommendations for government systems (technology platforms, operations, culture, governance, engagement, etc.) that would help to improve public confidence and trust in public institutions, policies and services, whilst meeting the special obligations and responsibilities of the public sector. △ Less","22 August, 2022",https://arxiv.org/pdf/2208.10087
MetaRF: Differentiable Random Forest for Reaction Yield Prediction with a Few Trails,Kexin Chen;Guangyong Chen;Junyou Li;Yuansheng Huang;Pheng-Ann Heng,"Artificial intelligence has deeply revolutionized the field of medicinal chemistry with many impressive applications, but the success of these applications requires a massive amount of training samples with high-quality annotations, which seriously limits the wide usage of data-driven methods. In this paper, we focus on the reaction yield prediction problem, which assists chemists in selecting high-yield reactions in a new chemical space only with a few experimental trials. To attack this challenge, we first put forth MetaRF, an attention-based differentiable random forest model specially designed for the few-shot yield prediction, where the attention weight of a random forest is automatically optimized by the meta-learning framework and can be quickly adapted to predict the performance of new reagents while given a few additional samples. To improve the few-shot learning performance, we further introduce a dimension-reduction based sampling method to determine valuable samples to be experimentally tested and then learned. Our methodology is evaluated on three different datasets and acquires satisfactory performance on few-shot prediction. In high-throughput experimentation (HTE) datasets, the average yield of our methodology's top 10 high-yield reactions is relatively close to the results of ideal yield selection. △ Less","22 August, 2022",https://arxiv.org/pdf/2208.10083
A Survey on Intelligent Computation Offloading and Pricing Strategy in UAV-Enabled MEC Network: Challenges and Research Directions,Asrar Ahmed Baktayan;Ibrahim Ahmed Al-Baltah,"The Mobile Network Operator (MNO) must select how to delegate Mobile Device (MD) queries to its Mobile Edge Computing (MEC) server in order to maximize the overall benefit of admitted requests with varying latency needs. Unmanned Aerial Vehicles (UAVs) and Artificial Intelligent (AI) can increase MNO performance because of their flexibility in deployment, high mobility of UAV, and efficiency of AI algorithms. There is a trade-off between the cost incurred by the MD and the profit received by the MNO. Intelligent computing offloading to UAV-enabled MEC, on the other hand, is a promising way to bridge the gap between MDs' limited processing resources, as well as the intelligent algorithms that are utilized for computation offloading in the UAV-MEC network and the high computing demands of upcoming applications. This study looks at some of the research on the benefits of computation offloading process in the UAV-MEC network, as well as the intelligent models that are utilized for computation offloading. In addition, this article examines several intelligent pricing techniques in different structures in the UAV-MEC network. Finally, this work highlights some important open research issues and future research directions of Artificial Intelligent (AI) in computation offloading and applying intelligent pricing strategies in the UAV-MEC network. △ Less","22 August, 2022",https://arxiv.org/pdf/2208.10072
Do-AIQ: A Design-of-Experiment Approach to Quality Evaluation of AI Mislabel Detection Algorithm,J. Lian;K. Choi;B. Veeramani;A. Hu;L. Freeman;E. Bowen;X. Deng,"The quality of Artificial Intelligence (AI) algorithms is of significant importance for confidently adopting algorithms in various applications such as cybersecurity, healthcare, and autonomous driving. This work presents a principled framework of using a design-of-experimental approach to systematically evaluate the quality of AI algorithms, named as Do-AIQ. Specifically, we focus on investigating the quality of the AI mislabel data algorithm against data poisoning. The performance of AI algorithms is affected by hyperparameters in the algorithm and data quality, particularly, data mislabeling, class imbalance, and data types. To evaluate the quality of the AI algorithms and obtain a trustworthy assessment on the quality of the algorithms, we establish a design-of-experiment framework to construct an efficient space-filling design in a high-dimensional constraint space and develop an effective surrogate model using additive Gaussian process to enable the emulation of the quality of AI algorithms. Both theoretical and numerical studies are conducted to justify the merits of the proposed framework. The proposed framework can set an exemplar for AI algorithm to enhance the AI assurance of robustness, reproducibility, and transparency. △ Less","21 August, 2022",https://arxiv.org/pdf/2208.09953
SeNMFk-SPLIT: Large Corpora Topic Modeling by Semantic Non-negative Matrix Factorization with Automatic Model Selection,Maksim E. Eren;Nick Solovyev;Manish Bhattarai;Kim Rasmussen;Charles Nicholas;Boian S. Alexandrov,"As the amount of text data continues to grow, topic modeling is serving an important role in understanding the content hidden by the overwhelming quantity of documents. One popular topic modeling approach is non-negative matrix factorization (NMF), an unsupervised machine learning (ML) method. Recently, Semantic NMF with automatic model selection (SeNMFk) has been proposed as a modification to NMF. In addition to heuristically estimating the number of topics, SeNMFk also incorporates the semantic structure of the text. This is performed by jointly factorizing the term frequency-inverse document frequency (TF-IDF) matrix with the co-occurrence/word-context matrix, the values of which represent the number of times two words co-occur in a predetermined window of the text. In this paper, we introduce a novel distributed method, SeNMFk-SPLIT, for semantic topic extraction suitable for large corpora. Contrary to SeNMFk, our method enables the joint factorization of large documents by decomposing the word-context and term-document matrices separately. We demonstrate the capability of SeNMFk-SPLIT by applying it to the entire artificial intelligence (AI) and ML scientific literature uploaded on arXiv. △ Less","21 August, 2022",https://arxiv.org/pdf/2208.09942
"Alexa, Predict My Flight Delay",Sia Gholami;Saba Khashe,"Airlines are critical today for carrying people and commodities on time. Any delay in the schedule of these planes can potentially disrupt the business and trade of thousands of employees at any given time. Therefore, precise flight delay prediction is beneficial for the aviation industry and passenger travel. Recent research has focused on using artificial intelligence algorithms to predict the possibility of flight delays. Earlier prediction algorithms were designed for a specific air route or airfield. Many present flight delay prediction algorithms rely on tiny samples and are challenging to understand, allowing almost no room for machine learning implementation. This research study develops a flight delay prediction system by analyzing data from domestic flights inside the United States of America. The proposed models learn about the factors that cause flight delays and cancellations and the link between departure and arrival delays. △ Less","21 August, 2022",https://arxiv.org/pdf/2208.09921
Rate-Splitting Multiple Access for Intelligent Reflecting Surface-Aided Secure Transmission,Ying Gao;Qingqing Wu;Wen Chen;Derrick Wing Kwan Ng,"In this letter, we study a rate-splitting multiple access (RSMA)-based intelligent reflecting surface (IRS)-aided multi-user multiple-input single-output (MISO) secure communication system with a potential eavesdropper (Eve). Aiming to maximize the minimum secrecy rate (SR) among all the legitimate users (LUs), a design problem for jointly optimizing the transmit beamforming with artificial noise (AN), the IRS beamforming, and the secrecy common rate allocation is formulated. Since the design problem is highly non-convex with coupled optimization variables, we develop a computationally efficient algorithm based on the alternating optimization (AO) technique to solve it suboptimally. Numerical results demonstrate that the proposed design can significantly improve the max-min SR over the benchmark schemes without IRS or adopting other multiple access techniques. In particular, employing the RSMA strategy can substantially reduce the required numbers of IRS elements for achieving a target level of secrecy performance compared with the benchmark schemes. △ Less","22 November, 2022",https://arxiv.org/pdf/2208.09818
Visual Analysis of Neural Architecture Spaces for Summarizing Design Principles,Jun Yuan;Mengchen Liu;Fengyuan Tian;Shixia Liu,"Recent advances in artificial intelligence largely benefit from better neural network architectures. These architectures are a product of a costly process of trial-and-error. To ease this process, we develop ArchExplorer, a visual analysis method for understanding a neural architecture space and summarizing design principles. The key idea behind our method is to make the architecture space explainable by exploiting structural distances between architectures. We formulate the pairwise distance calculation as solving an all-pairs shortest path problem. To improve efficiency, we decompose this problem into a set of single-source shortest path problems. The time complexity is reduced from O(kn^2N) to O(knN). Architectures are hierarchically clustered according to the distances between them. A circle-packing-based architecture visualization has been developed to convey both the global relationships between clusters and local neighborhoods of the architectures in each cluster. Two case studies and a post-analysis are presented to demonstrate the effectiveness of ArchExplorer in summarizing design principles and selecting better-performing architectures. △ Less","20 August, 2022",https://arxiv.org/pdf/2208.09665
HySAGE: A Hybrid Static and Adaptive Graph Embedding Network for Context-Drifting Recommendations,Sichun Luo;Xinyi Zhang;Yuanzhang Xiao;Linqi Song,"The recent popularity of edge devices and Artificial Intelligent of Things (AIoT) has driven a new wave of contextual recommendations, such as location based Point of Interest (PoI) recommendations and computing resource-aware mobile app recommendations. In many such recommendation scenarios, contexts are drifting over time. For example, in a mobile game recommendation, contextual features like locations, battery, and storage levels of mobile devices are frequently drifting over time. However, most existing graph-based collaborative filtering methods are designed under the assumption of static features. Therefore, they would require frequent retraining and/or yield graphical models burgeoning in sizes, impeding their suitability for context-drifting recommendations. In this work, we propose a specifically tailor-made Hybrid Static and Adaptive Graph Embedding (HySAGE) network for context-drifting recommendations. Our key idea is to disentangle the relatively static user-item interaction and rapidly drifting contextual features. Specifically, our proposed HySAGE network learns a relatively static graph embedding from user-item interaction and an adaptive embedding from drifting contextual features. These embeddings are incorporated into an interest network to generate the user interest in some certain context. We adopt an interactive attention module to learn the interactions among static graph embeddings, adaptive contextual embeddings, and user interest, helping to achieve a better final representation. Extensive experiments on real-world datasets demonstrate that HySAGE significantly improves the performance of the existing state-of-the-art recommendation algorithms. △ Less","19 August, 2022",https://arxiv.org/pdf/2208.09586
Carefully choose the baseline: Lessons learned from applying XAI attribution methods for regression tasks in geoscience,Antonios Mamalakis;Elizabeth A. Barnes;Imme Ebert-Uphoff,"Methods of eXplainable Artificial Intelligence (XAI) are used in geoscientific applications to gain insights into the decision-making strategy of Neural Networks (NNs) highlighting which features in the input contribute the most to a NN prediction. Here, we discuss our lesson learned that the task of attributing a prediction to the input does not have a single solution. Instead, the attribution results and their interpretation depend greatly on the considered baseline (sometimes referred to as reference point) that the XAI method utilizes; a fact that has been overlooked so far in the literature. This baseline can be chosen by the user or it is set by construction in the method s algorithm, often without the user being aware of that choice. We highlight that different baselines can lead to different insights for different science questions and, thus, should be chosen accordingly. To illustrate the impact of the baseline, we use a large ensemble of historical and future climate simulations forced with the SSP3-7.0 scenario and train a fully connected NN to predict the ensemble- and global-mean temperature (i.e., the forced global warming signal) given an annual temperature map from an individual ensemble member. We then use various XAI methods and different baselines to attribute the network predictions to the input. We show that attributions differ substantially when considering different baselines, as they correspond to answering different science questions. We conclude by discussing some important implications and considerations about the use of baselines in XAI research. △ Less","19 August, 2022",https://arxiv.org/pdf/2208.09473
Text to Image Generation: Leaving no Language Behind,Pedro Reviriego;Elena Merino-Gómez,"One of the latest applications of Artificial Intelligence (AI) is to generate images from natural language descriptions. These generators are now becoming available and achieve impressive results that have been used for example in the front cover of magazines. As the input to the generators is in the form of a natural language text, a question that arises immediately is how these models behave when the input is written in different languages. In this paper we perform an initial exploration of how the performance of three popular text-to-image generators depends on the language. The results show that there is a significant performance degradation when using languages other than English, especially for languages that are not widely used. This observation leads us to discuss different alternatives on how text-to-image generators can be improved so that performance is consistent across different languages. This is fundamental to ensure that this new technology can be used by non-native English speakers and to preserve linguistic diversity. △ Less","17 November, 2022",https://arxiv.org/pdf/2208.09333
Blockchain-based traffic management for Advanced Air Mobility,I. Romani de Oliveira;T. Matsumoto;E. C. Pinto Neto,"The large public interest in Advanced Air Mobility (AAM) will soon lead to congested skies overhead cities, analogously to what happened with other transportation means, including commercial aviation. In the latter case, the combination of large distances and demanded number flights is such that a system with centralized control, with most of the decisions made by human operators, is safe. However, for AAM, it is expected a much higher demand, because it will be used for people's daily commutes. Thus, higher automation levels will become a requirement for coordinating this traffic, which might not be effectively managed by humans. The establishment of fixed air routes can abate complexity, however at the cost of limiting capacity and decreasing efficiency. Another alternative is the use of a powerful central system based on Artificial Intelligence (AI), which would allow flexible trajectories and higher efficiency. However, such system would require concentrated investment, could contain Single-Points-of-Failure (SPoFs), would be a highly sought target of malicious attacks, and would be subject to periods of unavailability. This work proposes a new technology that solves the problem of managing the high complexity of the AAM traffic with a secure distributed approach, without the need for a proprietary centralized automation system. This technology enables distributed airspace allocation management and conflict resolution by means of trusted shared data structures and associated smart contracts running on a blockchain ecosystem. This way, it greatly reduces the risk of system outages due to SPoFs, by allowing peer-to-peer conflict resolution, and being more resilient to failures in the ground communication infrastructure. Furthermore, it provides priority-based balancing mechanisms that help to regulate fairness among participants in the utilization of the airspace. △ Less","19 August, 2022",https://arxiv.org/pdf/2208.09312
IoT based Smart Water Quality Prediction for Biofloc Aquaculture,Md. Mamunur Rashid;Al-Akhir Nayan;Md. Obaidur Rahman;Sabrina Afrin Simi;Joyeta Saha;Muhammad Golam Kibria,"Traditional fish farming faces several challenges, including water pollution, temperature imbalance, feed, space, cost, etc. Biofloc technology in aquaculture transforms the manual into an advanced system that allows the reuse of unused feed by converting them into microbial protein. The objective of the research is to propose an IoT-based solution to aquaculture that increases efficiency and productivity. The article presented a system that collects data using sensors, analyzes them using a machine learning model, generates decisions with the help of Artificial Intelligence (AI), and sends notifications to the user. The proposed system has been implemented and tested to validate and achieve a satisfactory result. △ Less","26 July, 2022",https://arxiv.org/pdf/2208.08866
Explainable Reinforcement Learning on Financial Stock Trading using SHAP,Satyam Kumar;Mendhikar Vishal;Vadlamani Ravi,"Explainable Artificial Intelligence (XAI) research gained prominence in recent years in response to the demand for greater transparency and trust in AI from the user communities. This is especially critical because AI is adopted in sensitive fields such as finance, medicine etc., where implications for society, ethics, and safety are immense. Following thorough systematic evaluations, work in XAI has primarily focused on Machine Learning (ML) for categorization, decision, or action. To the best of our knowledge, no work is reported that offers an Explainable Reinforcement Learning (XRL) method for trading financial stocks. In this paper, we proposed to employ SHapley Additive exPlanation (SHAP) on a popular deep reinforcement learning architecture viz., deep Q network (DQN) to explain an action of an agent at a given instance in financial stock trading. To demonstrate the effectiveness of our method, we tested it on two popular datasets namely, SENSEX and DJIA, and reported the results. △ Less","18 August, 2022",https://arxiv.org/pdf/2208.08790
Error Parity Fairness: Testing for Group Fairness in Regression Tasks,Furkan Gursoy;Ioannis A. Kakadiaris,"The applications of Artificial Intelligence (AI) surround decisions on increasingly many aspects of human lives. Society responds by imposing legal and social expectations for the accountability of such automated decision systems (ADSs). Fairness, a fundamental constituent of AI accountability, is concerned with just treatment of individuals and sensitive groups (e.g., based on sex, race). While many studies focus on fair learning and fairness testing for the classification tasks, the literature is rather limited on how to examine fairness in regression tasks. This work presents error parity as a regression fairness notion and introduces a testing methodology to assess group fairness based on a statistical hypothesis testing procedure. The error parity test checks whether prediction errors are distributed similarly across sensitive groups to determine if an ADS is fair. It is followed by a suitable permutation test to compare groups on several statistics to explore disparities and identify impacted groups. The usefulness and applicability of the proposed methodology are demonstrated via a case study on COVID-19 projections in the US at the county level, which revealed race-based differences in forecast errors. Overall, the proposed regression fairness testing methodology fills a gap in the fair machine learning literature and may serve as a part of larger accountability assessments and algorithm audits. △ Less","16 August, 2022",https://arxiv.org/pdf/2208.08279
Multimodal foundation models are better simulators of the human brain,Haoyu Lu;Qiongyi Zhou;Nanyi Fei;Zhiwu Lu;Mingyu Ding;Jingyuan Wen;Changde Du;Xin Zhao;Hao Sun;Huiguang He;Ji-Rong Wen,"Multimodal learning, especially large-scale multimodal pre-training, has developed rapidly over the past few years and led to the greatest advances in artificial intelligence (AI). Despite its effectiveness, understanding the underlying mechanism of multimodal pre-training models still remains a grand challenge. Revealing the explainability of such models is likely to enable breakthroughs of novel learning paradigms in the AI field. To this end, given the multimodal nature of the human brain, we propose to explore the explainability of multimodal learning models with the aid of non-invasive brain imaging technologies such as functional magnetic resonance imaging (fMRI). Concretely, we first present a newly-designed multimodal foundation model pre-trained on 15 million image-text pairs, which has shown strong multimodal understanding and generalization abilities in a variety of cognitive downstream tasks. Further, from the perspective of neural encoding (based on our foundation model), we find that both visual and lingual encoders trained multimodally are more brain-like compared with unimodal ones. Particularly, we identify a number of brain regions where multimodally-trained encoders demonstrate better neural encoding performance. This is consistent with the findings in existing studies on exploring brain multi-sensory integration. Therefore, we believe that multimodal foundation models are more suitable tools for neuroscientists to study the multimodal signal processing mechanisms in the human brain. Our findings also demonstrate the potential of multimodal foundation models as ideal computational simulators to promote both AI-for-brain and brain-for-AI research. △ Less","17 August, 2022",https://arxiv.org/pdf/2208.08263
Assurance Cases as Foundation Stone for Auditing AI-enabled and Autonomous Systems: Workshop Results and Political Recommendations for Action from the ExamAI Project,Rasmus Adler;Michael Klaes,"The European Machinery Directive and related harmonized standards do consider that software is used to generate safety-relevant behavior of the machinery but do not consider all kinds of software. In particular, software based on machine learning (ML) are not considered for the realization of safety-relevant behavior. This limits the introduction of suitable safety concepts for autonomous mobile robots and other autonomous machinery, which commonly depend on ML-based functions. We investigated this issue and the way safety standards define safety measures to be implemented against software faults. Functional safety standards use Safety Integrity Levels (SILs) to define which safety measures shall be implemented. They provide rules for determining the SIL and rules for selecting safety measures depending on the SIL. In this paper, we argue that this approach can hardly be adopted with respect to ML and other kinds of Artificial Intelligence (AI). Instead of simple rules for determining an SIL and applying related measures against faults, we propose the use of assurance cases to argue that the individually selected and applied measures are sufficient in the given case. To get a first rating regarding the feasibility and usefulness of our proposal, we presented and discussed it in a workshop with experts from industry, German statutory accident insurance companies, work safety and standardization commissions, and representatives from various national, European, and international working groups dealing with safety and AI. In this paper, we summarize the proposal and the workshop discussion. Moreover, we check to which extent our proposal is in line with the European AI Act proposal and current safety standardization initiatives addressing AI and Autonomous Systems △ Less","17 August, 2022",https://arxiv.org/pdf/2208.08198
A Concept and Argumentation based Interpretable Model in High Risk Domains,Haixiao Chi;Dawei Wang;Gaojie Cui;Feng Mao;Beishui Liao,"Interpretability has become an essential topic for artificial intelligence in some high-risk domains such as healthcare, bank and security. For commonly-used tabular data, traditional methods trained end-to-end machine learning models with numerical and categorical data only, and did not leverage human understandable knowledge such as data descriptions. Yet mining human-level knowledge from tabular data and using it for prediction remain a challenge. Therefore, we propose a concept and argumentation based model (CAM) that includes the following two components: a novel concept mining method to obtain human understandable concepts and their relations from both descriptions of features and the underlying data, and a quantitative argumentation-based method to do knowledge representation and reasoning. As a result of it, CAM provides decisions that are based on human-level knowledge and the reasoning process is intrinsically interpretable. Finally, to visualize the purposed interpretable model, we provide a dialogical explanation that contain dominated reasoning path within CAM. Experimental results on both open source benchmark dataset and real-word business dataset show that (1) CAM is transparent and interpretable, and the knowledge inside the CAM is coherent with human understanding; (2) Our interpretable approach can reach competitive results comparing with other state-of-art models. △ Less","17 August, 2022",https://arxiv.org/pdf/2208.08149
Artificial Intelligence Empowered Multiple Access for Ultra Reliable and Low Latency THz Wireless Networks,Alexandros-Apostolos A. Boulogeorgos;Edwin Yaqub;Rachana Desai;Tachporn Sanguanpuak;Nikos Katzouris;Fotis Lazarakis;Angeliki Alexiou;Marco Di Renzo,"Terahertz (THz) wireless networks are expected to catalyze the beyond fifth generation (B5G) era. However, due to the directional nature and the line-of-sight demand of THz links, as well as the ultra-dense deployment of THz networks, a number of challenges that the medium access control (MAC) layer needs to face are created. In more detail, the need of rethinking user association and resource allocation strategies by incorporating artificial intelligence (AI) capable of providing ""real-time"" solutions in complex and frequently changing environments becomes evident. Moreover, to satisfy the ultra-reliability and low-latency demands of several B5G applications, novel mobility management approaches are required. Motivated by this, this article presents a holistic MAC layer approach that enables intelligent user association and resource allocation, as well as flexible and adaptive mobility management, while maximizing systems' reliability through blockage minimization. In more detail, a fast and centralized joint user association, radio resource allocation, and blockage avoidance by means of a novel metaheuristic-machine learning framework is documented, that maximizes the THz networks performance, while minimizing the association latency by approximately three orders of magnitude. To support, within the access point (AP) coverage area, mobility management and blockage avoidance, a deep reinforcement learning (DRL) approach for beam-selection is discussed. Finally, to support user mobility between coverage areas of neighbor APs, a proactive hand-over mechanism based on AI-assisted fast channel prediction is~reported. △ Less","16 August, 2022",https://arxiv.org/pdf/2208.08039
FALSE: Fake News Automatic and Lightweight Solution,Fatema Al Mukhaini;Shaikhah Al Abdoulie;Aisha Al Kharuosi;Amal El Ahmad;Monther Aldwairi,"Fake news existed ever since there was news, from rumors to printed media then radio and television. Recently, the information age, with its communications and Internet breakthroughs, exacerbated the spread of fake news. Additionally, aside from e-Commerce, the current Internet economy is dependent on advertisements, views and clicks, which prompted many developers to bait the end users to click links or ads. Consequently, the wild spread of fake news through social media networks has impacted real world issues from elections to 5G adoption and the handling of the Covid- 19 pandemic. Efforts to detect and thwart fake news has been there since the advent of fake news, from fact checkers to artificial intelligence-based detectors. Solutions are still evolving as more sophisticated techniques are employed by fake news propagators. In this paper, R code have been used to study and visualize a modern fake news dataset. We use clustering, classification, correlation and various plots to analyze and present the data. The experiments show high efficiency of classifiers in telling apart real from fake news. △ Less","16 August, 2022",https://arxiv.org/pdf/2208.07686
A Review of the Convergence of 5G/6G Architecture and Deep Learning,Olusola T. Odeyomi;Olubiyi O. Akintade;Temitayo O. Olowu;Gergely Zaruba,"The convergence of 5G architecture and deep learning has gained a lot of research interests in both the fields of wireless communication and artificial intelligence. This is because deep learning technologies have been identified to be the potential driver of the 5G technologies, that make up the 5G architecture. Hence, there have been extensive surveys on the convergence of 5G architecture and deep learning. However, most of the existing survey papers mainly focused on how deep learning can converge with a specific 5G technology, thus, not covering the full spectrum of the 5G architecture. Although there is a recent survey paper that appears to be robust, a review of that paper shows that it is not well structured to specifically cover the convergence of deep learning and the 5G technologies. Hence, this paper provides a robust overview of the convergence of the key 5G technologies and deep learning. The challenges faced by such convergence are discussed. In addition, a brief overview of the future 6G architecture, and how it can converge with deep learning is also discussed. △ Less","16 August, 2022",https://arxiv.org/pdf/2208.07643
Traffic Analytics Development Kits (TADK): Enable Real-Time AI Inference in Networking Apps,Kun Qiu;Harry Chang;Ying Wang;Xiahui Yu;Wenjun Zhu;Yingqi Liu;Jianwei Ma;Weigang Li;Xiaobo Liu;Shuo Dai,"Sophisticated traffic analytics, such as the encrypted traffic analytics and unknown malware detection, emphasizes the need for advanced methods to analyze the network traffic. Traditional methods of using fixed patterns, signature matching, and rules to detect known patterns in network traffic are being replaced with AI (Artificial Intelligence) driven algorithms. However, the absence of a high-performance AI networking-specific framework makes deploying real-time AI-based processing within networking workloads impossible. In this paper, we describe the design of Traffic Analytics Development Kits (TADK), an industry-standard framework specific for AI-based networking workloads processing. TADK can provide real-time AI-based networking workload processing in networking equipment from the data center out to the edge without the need for specialized hardware (e.g., GPUs, Neural Processing Unit, and so on). We have deployed TADK in commodity WAF and 5G UPF, and the evaluation result shows that TADK can achieve a throughput up to 35.3Gbps per core on traffic feature extraction, 6.5Gbps per core on traffic classification, and can decrease SQLi/XSS detection down to 4.5us per request with higher accuracy than fixed pattern solution. △ Less","16 August, 2022",https://arxiv.org/pdf/2208.07558
CTI4AI: Threat Intelligence Generation and Sharing after Red Teaming AI Models,Chuyen Nguyen;Caleb Morgan;Sudip Mittal,"As the practicality of Artificial Intelligence (AI) and Machine Learning (ML) based techniques grow, there is an ever increasing threat of adversarial attacks. There is a need to red team this ecosystem to identify system vulnerabilities, potential threats, characterize properties that will enhance system robustness, and encourage the creation of effective defenses. A secondary need is to share this AI security threat intelligence between different stakeholders like, model developers, users, and AI/ML security professionals. In this paper, we create and describe a prototype system CTI4AI, to overcome the need to methodically identify and share AI/ML specific vulnerabilities and threat intelligence. △ Less","15 August, 2022",https://arxiv.org/pdf/2208.07476
Learn2Trust: A video and streamlit-based educational programme for AI-based medical image analysis targeted towards medical students,Hanna Siebert;Marian Himstedt;Mattias Heinrich,"In order to be able to use artificial intelligence (AI) in medicine without scepticism and to recognise and assess its growing potential, a basic understanding of this topic is necessary among current and future medical staff. Under the premise of ""trust through understanding"", we developed an innovative online course as a learning opportunity within the framework of the German KI Campus (AI campus) project, which is a self-guided course that teaches the basics of AI for the analysis of medical image data. The main goal is to provide a learning environment for a sufficient understanding of AI in medical image analysis so that further interest in this topic is stimulated and inhibitions towards its use can be overcome by means of positive application experience. The focus was on medical applications and the fundamentals of machine learning. The online course was divided into consecutive lessons, which include theory in the form of explanatory videos, practical exercises in the form of Streamlit and practical exercises and/or quizzes to check learning progress. A survey among the participating medical students in the first run of the course was used to analyse our research hypotheses quantitatively. △ Less","15 August, 2022",https://arxiv.org/pdf/2208.07314
Introducing RISK,Christopher D. Wallbridge;Qiyuan Zhang,"This extended abstract introduces the initial steps taken to develop a system for Rapid Internal Simulation of Knowledge (RISK). RISK aims to enable more transparency in artificial intelligence systems, especially those created by deep learning networks by allowing real-time simulation of what the system knows. By looking at hypothetical situations based on these simulations a system may make more informed decisions, and produce them for non-expert observers to understand the reasoning behind a given action. △ Less","17 July, 2022",https://arxiv.org/pdf/2208.07306
Explainable Artificial Intelligence for Assault Sentence Prediction in New Zealand,Harry Rodger;Andrew Lensen;Marcin Betkier,"The judiciary has historically been conservative in its use of Artificial Intelligence, but recent advances in machine learning have prompted scholars to reconsider such use in tasks like sentence prediction. This paper investigates by experimentation the potential use of explainable artificial intelligence for predicting imprisonment sentences in assault cases in New Zealand's courts. We propose a proof-of-concept explainable model and verify in practice that it is fit for purpose, with predicted sentences accurate to within one year. We further analyse the model to understand the most influential phrases in sentence length prediction. We conclude the paper with an evaluative discussion of the future benefits and risks of different ways of using such an AI model in New Zealand's courts. △ Less","14 August, 2022",https://arxiv.org/pdf/2208.06981
Simply Logical -- Intelligent Reasoning by Example (Fully Interactive Online Edition),Peter Flach;Kacper Sokol,"""Simply Logical -- Intelligent Reasoning by Example"" by Peter Flach was first published by John Wiley in 1994. It could be purchased as book-only or with a 3.5 inch diskette containing the SWI-Prolog programmes printed in the book (for various operating systems). In 2007 the copyright reverted back to the author at which point the book and programmes were made freely available online; the print version is no longer distributed through John Wiley publishers. In 2015, as a pilot, we ported most of the original book into an online, interactive website using SWI-Prolog's SWISH platform. Since then, we launched the Simply Logical open source organisation committed to maintaining a suite of freely available interactive online educational resources about Artificial Intelligence and Logic Programming with Prolog. With the advent of new educational technologies we were inspired to rebuild the book from the ground up using the Jupyter Book platform enhanced with a collection of bespoke plugins that implement, among other things, interactive SWI-Prolog code blocks that can be executed directly in a web browser. This new version is more modular, easier to maintain, and can be split into custom teaching modules, in addition to being modern-looking, visually appealing, and compatible with a range of (mobile) devices of varying screen sizes. △ Less","14 August, 2022",https://arxiv.org/pdf/2208.06823
An Empirical Comparison of Explainable Artificial Intelligence Methods for Clinical Data: A Case Study on Traumatic Brain Injury,Amin Nayebi;Sindhu Tipirneni;Brandon Foreman;Chandan K. Reddy;Vignesh Subbian,"A longstanding challenge surrounding deep learning algorithms is unpacking and understanding how they make their decisions. Explainable Artificial Intelligence (XAI) offers methods to provide explanations of internal functions of algorithms and reasons behind their decisions in ways that are interpretable and understandable to human users. . Numerous XAI approaches have been developed thus far, and a comparative analysis of these strategies seems necessary to discern their relevance to clinical prediction models. To this end, we first implemented two prediction models for short- and long-term outcomes of traumatic brain injury (TBI) utilizing structured tabular as well as time-series physiologic data, respectively. Six different interpretation techniques were used to describe both prediction models at the local and global levels. We then performed a critical analysis of merits and drawbacks of each strategy, highlighting the implications for researchers who are interested in applying these methodologies. The implemented methods were compared to one another in terms of several XAI characteristics such as understandability, fidelity, and stability. Our findings show that SHAP is the most stable with the highest fidelity but falls short of understandability. Anchors, on the other hand, is the most understandable approach, but it is only applicable to tabular data and not time series data. △ Less","13 August, 2022",https://arxiv.org/pdf/2208.06717
Recognition of All Categories of Entities by AI,Hiroshi Yamakawa;Yutaka Matsuo,"Human-level AI will have significant impacts on human society. However, estimates for the realization time are debatable. To arrive at human-level AI, artificial general intelligence (AGI), as opposed to AI systems that are specialized for a specific task, was set as a technically meaningful long-term goal. But now, propelled by advances in deep learning, that achievement is getting much closer. Considering the recent technological developments, it would be meaningful to discuss the completion date of human-level AI through the ""comprehensive technology map approach,"" wherein we map human-level capabilities at a reasonable granularity, identify the current range of technology, and discuss the technical challenges in traversing unexplored areas and predict when all of them will be overcome. This paper presents a new argumentative option to view the ontological sextet, which encompasses entities in a way that is consistent with our everyday intuition and scientific practice, as a comprehensive technological map. Because most of the modeling of the world, in terms of how to interpret it, by an intelligent subject is the recognition of distal entities and the prediction of their temporal evolution, being able to handle all distal entities is a reasonable goal. Based on the findings of philosophy and engineering cognitive technology, we predict that in the relatively near future, AI will be able to recognize various entities to the same degree as humans. △ Less","16 August, 2022",https://arxiv.org/pdf/2208.06590
Developing moral AI to support antimicrobial decision making,William J Bolton;Cosmin Badea;Pantelis Georgiou;Alison Holmes;Timothy M Rawson,"Artificial intelligence (AI) assisting with antimicrobial prescribing raises significant moral questions. Utilising ethical frameworks alongside AI-driven systems, while considering infection specific complexities, can support moral decision making to tackle antimicrobial resistance.","12 August, 2022",https://arxiv.org/pdf/2208.06327
"Safety and Performance, Why not Both? Bi-Objective Optimized Model Compression toward AI Software Deployment",Jie Zhu;Leye Wang;Xiao Han,"The size of deep learning models in artificial intelligence (AI) software is increasing rapidly, which hinders the large-scale deployment on resource-restricted devices (e.g., smartphones). To mitigate this issue, AI software compression plays a crucial role, which aims to compress model size while keeping high performance. However, the intrinsic defects in the big model may be inherited by the compressed one. Such defects may be easily leveraged by attackers, since the compressed models are usually deployed in a large number of devices without adequate protection. In this paper, we try to address the safe model compression problem from a safety-performance co-optimization perspective. Specifically, inspired by the test-driven development (TDD) paradigm in software engineering, we propose a test-driven sparse training framework called SafeCompress. By simulating the attack mechanism as the safety test, SafeCompress can automatically compress a big model to a small one following the dynamic sparse training paradigm. Further, considering a representative attack, i.e., membership inference attack (MIA), we develop a concrete safe model compression mechanism, called MIA-SafeCompress. Extensive experiments are conducted to evaluate MIA-SafeCompress on five datasets for both computer vision and natural language processing tasks. The results verify the effectiveness and generalization of our method. We also discuss how to adapt SafeCompress to other attacks besides MIA, demonstrating the flexibility of SafeCompress. △ Less","9 September, 2022",https://arxiv.org/pdf/2208.05969
Optimizing Irregular-Shaped Matrix-Matrix Multiplication on Multi-Core DSPs,Shangfei Yin;Qinglin Wang;Ruochen Hao;Tianyang Zhou;Songzhu Mei;Jie Liu,"General Matrix Multiplication (GEMM) has a wide range of applications in scientific simulation and artificial intelligence. Although traditional libraries can achieve high performance on large regular-shaped GEMMs, they often behave not well on irregular-shaped GEMMs, which are often found in new algorithms and applications of high-performance computing (HPC). Due to energy efficiency constraints, low-power multi-core digital signal processors (DSPs) have become an alternative architecture in HPC systems. Targeting multi-core DSPs in FT-m7032, a prototype CPU-DSPs heterogeneous processor for HPC, an efficient implementation - ftIMM - for three types of irregular-shaped GEMMs is proposed. FtIMM supports automatic generation of assembly micro-kernels, two parallelization strategies, and auto-tuning of block sizes and parallelization strategies. The experiments show that ftIMM can get better performance than the traditional GEMM implementations on multi-core DSPs in FT-m7032, yielding on up to 7.2x performance improvement, when performing on irregular-shaped GEMMs. And ftIMM on multi-core DSPs can also far outperform the open source library on multi-core CPUs in FT-m7032, delivering up to 3.1x higher efficiency. △ Less","11 August, 2022",https://arxiv.org/pdf/2208.05872
An Overview on Over-the-Air Federated Edge Learning,Xiaowen Cao;Zhonghao Lyu;Guangxu Zhu;Jie Xu;Lexi Xu;Shuguang Cui,"Over-the-air federated edge learning (Air-FEEL) has emerged as a promising solution to support edge artificial intelligence (AI) in future beyond 5G (B5G) and 6G networks. In Air-FEEL, distributed edge devices use their local data to collaboratively train AI models while preserving data privacy, in which the over-the-air model/gradient aggregation is exploited for enhancing the learning efficiency. This article provides an overview on the state of the art of Air-FEEL. First, we present the basic principle of Air-FEEL, and introduce the technical challenges for Air-FEEL design due to the over-the-air aggregation errors, as well as the resource and data heterogeneities at edge devices. Next, we present the fundamental performance metrics for Air-FEEL, and review resource management solutions and design considerations for enhancing the Air-FEEL performance. Finally, several interesting research directions are pointed out to motivate future work. △ Less","11 August, 2022",https://arxiv.org/pdf/2208.05643
OpenMedIA: Open-Source Medical Image Analysis Toolbox and Benchmark under Heterogeneous AI Computing Platforms,Jia-Xin Zhuang;Xiansong Huang;Yang Yang;Jiancong Chen;Yue Yu;Wei Gao;Ge Li;Jie Chen;Tong Zhang,"In this paper, we present OpenMedIA, an open-source toolbox library containing a rich set of deep learning methods for medical image analysis under heterogeneous Artificial Intelligence (AI) computing platforms. Various medical image analysis methods, including 2D/3D medical image classification, segmentation, localisation, and detection, have been included in the toolbox with PyTorch and/or MindSpore implementations under heterogeneous NVIDIA and Huawei Ascend computing systems. To our best knowledge, OpenMedIA is the first open-source algorithm library providing compared PyTorch and MindSpore implementations and results on several benchmark datasets. The source codes and models are available at https://git.openi.org.cn/OpenMedIA. △ Less","7 September, 2022",https://arxiv.org/pdf/2208.05616
Trustworthy Visual Analytics in Clinical Gait Analysis: A Case Study for Patients with Cerebral Palsy,Alexander Rind;Djordje Slijepčević;Matthias Zeppelzauer;Fabian Unglaube;Andreas Kranzl;Brian Horsak,"Three-dimensional clinical gait analysis is essential for selecting optimal treatment interventions for patients with cerebral palsy (CP), but generates a large amount of time series data. For the automated analysis of these data, machine learning approaches yield promising results. However, due to their black-box nature, such approaches are often mistrusted by clinicians. We propose gaitXplorer, a visual analytics approach for the classification of CP-related gait patterns that integrates Grad-CAM, a well-established explainable artificial intelligence algorithm, for explanations of machine learning classifications. Regions of high relevance for classification are highlighted in the interactive visual interface. The approach is evaluated in a case study with two clinical gait experts. They inspected the explanations for a sample of eight patients using the visual interface and expressed which relevance scores they found trustworthy and which they found suspicious. Overall, the clinicians gave positive feedback on the approach as it allowed them a better understanding of which regions in the data were relevant for the classification. △ Less","19 December, 2022",https://arxiv.org/pdf/2208.05232
Reflections on the Evolution of Computer Science Education,Sreekrishnan Venkateswaran,"Computer Science education has been evolving over the years to reflect applied realities. Until about a decade ago, theory of computation, algorithm design and system software dominated the curricula. Most courses were considered core and were hence mandatory; the programme structure did not allow much of a choice or variety. This column analyses why this changed Circa 2010 when elective subjects across scores of topics become part of mainstream education to reflect the on-going lateral acceleration of Computer Science. Fundamental discoveries in artificial intelligence, machine learning, virtualization and cloud computing are several decades old. Many core theories in data science are centuries old. Yet their leverage exploded only after Circa 2010, when the stage got set for people-centric problem solving in massive scale. This was due in part to the rush of innovative real-world applications that reached the common man through the ubiquitous smart phone. AI/ML modules arrived in popular programming languages; they could be used to build and train models on powerful - yet affordable - compute on public clouds reachable through high-speed Internet connectivity. Academia responded by adapting Computer Science curricula to align it with the changing technology landscape. The goal of this experiential piece is to trigger a lively discussion on the past and future of Computer Science education. △ Less","9 July, 2022",https://arxiv.org/pdf/2208.04713
The Transform-o-meter: A method to forecast the transformative impact of innovation,Hector G. T. Torres,"With the advent of Transformative Artificial Intelligence, it is now more important than ever to be able to both measure and forecast the transformative impact/potential of innovation. However, current methods fall short when faced with this task. This paper introduces the Transform-o-meter; a methodology that can be used to achieve the aforementioned goal, and be applied to any innovation, both material and immaterial. While this method can effectively be used for the mentioned purpose, it should be taken as a first approach; to be iterated, researched, and expanded further upon. △ Less","15 July, 2022",https://arxiv.org/pdf/2208.04711
AI Approaches in Processing and Using Data in Personalized Medicine,Mirjana Ivanovic;Serge Autexier;Miltiadis Kokkonidis,"In modern dynamic constantly developing society, more and more people suffer from chronic and serious diseases and doctors and patients need special and sophisticated medical and health support. Accordingly, prominent health stakeholders have recognized the importance of development of such services to make patients life easier. Such support requires the collection of huge amount of patients complex data like clinical, environmental, nutritional, daily activities, variety of data from smart wearable devices, data from clothing equipped with sensors etc. Holistic patients data must be properly aggregated, processed, analyzed, and presented to the doctors and caregivers to recommend adequate treatment and actions to improve patients health related parameters and general wellbeing. Advanced artificial intelligence techniques offer the opportunity to analyze such big data, consume them, and derive new knowledge to support personalized medical decisions. New approaches like those based on advanced machine learning, federated learning, transfer learning, explainable artificial intelligence open new paths for more quality use of health and medical data in future. In this paper, we will present some crucial aspects and characteristic examples in the area of application of a range of artificial intelligence approaches in personalized medical decisions. △ Less","26 July, 2022",https://arxiv.org/pdf/2208.04698
Let it RAIN for Social Good,Mattias Brännström;Andreas Theodorou;Virginia Dignum,"Artificial Intelligence (AI) as a highly transformative technology take on a special role as both an enabler and a threat to UN Sustainable Development Goals (SDGs). AI Ethics and emerging high-level policy efforts stand at the pivot point between these outcomes but is barred from effect due the abstraction gap between high-level values and responsible action. In this paper the Responsible Norms (RAIN) framework is presented, bridging this gap thereby enabling effective high-level control of AI impact. With effective and operationalized AI Ethics, AI technologies can be directed towards global sustainable development. △ Less","26 July, 2022",https://arxiv.org/pdf/2208.04697
AI in Telemedicine: An Appraisal on Deep Learning-Based Approaches to Virtual Diagnostic Solutions (VDS),Ozioma Collins Oguine;Kanyifeechukwu Jane Oguine,"Advancements in Telemedicine as an approach to healthcare delivery have heralded a new dawn in modern Medicine. Its fast-paced development in our contemporary society is credence to the advances in Artificial Intelligence and Information Technology. This paper carries out a descriptive study to broadly explore AI's implementations in healthcare delivery with a more holistic view of the usability of various Telemedical Innovations in enhancing Virtual Diagnostic Solutions (VDS). This research further explores notable developments in Deep Learning model optimizations for Virtual Diagnostic Solutions. A further research review on the prospects of Virtual Diagnostic Solutions (VDS) and foreseeable challenges was also highlighted. Conclusively, this research gives a general overview of Artificial Intelligence in Telemedicine with a central focus on Deep Learning-based approaches to Virtual Diagnostic Solutions. △ Less","31 July, 2022",https://arxiv.org/pdf/2208.04690
"Applying data technologies to combat AMR: current status, challenges, and opportunities on the way forward",Leonid Chindelevitch;Elita Jauneikaite;Nicole E. Wheeler;Kasim Allel;Bede Yaw Ansiri-Asafoakaa;Wireko A. Awuah;Denis C. Bauer;Stephan Beisken;Kara Fan;Gary Grant;Michael Graz;Yara Khalaf;Veranja Liyanapathirana;Carlos Montefusco-Pereira;Lawrence Mugisha;Atharv Naik;Sylvia Nanono;Anthony Nguyen;Timothy Rawson;Kessendri Reddy;Juliana M. Ruzante;Anneke Schmider;Roman Stocker;Leonhardt Unruh;Daniel Waruingi,"Antimicrobial resistance (AMR) is a growing public health threat, estimated to cause over 10 million deaths per year and cost the global economy 100 trillion USD by 2050 under status quo projections. These losses would mainly result from an increase in the morbidity and mortality from treatment failure, AMR infections during medical procedures, and a loss of quality of life attributed to AMR. Numerous interventions have been proposed to control the development of AMR and mitigate the risks posed by its spread. This paper reviews key aspects of bacterial AMR management and control which make essential use of data technologies such as artificial intelligence, machine learning, and mathematical and statistical modelling, fields that have seen rapid developments in this century. Although data technologies have become an integral part of biomedical research, their impact on AMR management has remained modest. We outline the use of data technologies to combat AMR, detailing recent advancements in four complementary categories: surveillance, prevention, diagnosis, and treatment. We provide an overview on current AMR control approaches using data technologies within biomedical research, clinical practice, and in the ""One Health"" context. We discuss the potential impact and challenges wider implementation of data technologies is facing in high-income as well as in low- and middle-income countries, and recommend concrete actions needed to allow these technologies to be more readily integrated within the healthcare and public health sectors. △ Less","11 August, 2022",https://arxiv.org/pdf/2208.04683
A Means-End Account of Explainable Artificial Intelligence,Oliver Buchholz,"Explainable artificial intelligence (XAI) seeks to produce explanations for those machine learning methods which are deemed opaque. However, there is considerable disagreement about what this means and how to achieve it. Authors disagree on what should be explained (topic), to whom something should be explained (stakeholder), how something should be explained (instrument), and why something should be explained (goal). In this paper, I employ insights from means-end epistemology to structure the field. According to means-end epistemology, different means ought to be rationally adopted to achieve different epistemic ends. Applied to XAI, different topics, stakeholders, and goals thus require different instruments. I call this the means-end account of XAI. The means-end account has a descriptive and a normative component: on the one hand, I show how the specific means-end relations give rise to a taxonomy of existing contributions to the field of XAI; on the other hand, I argue that the suitability of XAI methods can be assessed by analyzing whether they are prescribed by a given topic, stakeholder, and goal. △ Less","9 August, 2022",https://arxiv.org/pdf/2208.04638
Using Sentence Embeddings and Semantic Similarity for Seeking Consensus when Assessing Trustworthy AI,Dennis Vetter;Jesmin Jahan Tithi;Magnus Westerlund;Roberto V. Zicari;Gemma Roig,"Assessing the trustworthiness of artificial intelligence systems requires knowledge from many different disciplines. These disciplines do not necessarily share concepts between them and might use words with different meanings, or even use the same words differently. Additionally, experts from different disciplines might not be aware of specialized terms readily used in other disciplines. Therefore, a core challenge of the assessment process is to identify when experts from different disciplines talk about the same problem but use different terminologies. In other words, the problem is to group problem descriptions (a.k.a. issues) with the same semantic meaning but described using slightly different terminologies. In this work, we show how we employed recent advances in natural language processing, namely sentence embeddings and semantic textual similarity, to support this identification process and to bridge communication gaps in interdisciplinary teams of experts assessing the trustworthiness of an artificial intelligence system used in healthcare. △ Less","9 August, 2022",https://arxiv.org/pdf/2208.04608
Soft Sensors and Process Control using AI and Dynamic Simulation,Shumpei Kubosawa;Takashi Onishi;Yoshimasa Tsuruoka,"During the operation of a chemical plant, product quality must be consistently maintained, and the production of off-specification products should be minimized. Accordingly, process variables related to the product quality, such as the temperature and composition of materials at various parts of the plant must be measured, and appropriate operations (that is, control) must be performed based on the measurements. Some process variables, such as temperature and flow rate, can be measured continuously and instantaneously. However, other variables, such as composition and viscosity, can only be obtained through time-consuming analysis after sampling substances from the plant. Soft sensors have been proposed for estimating process variables that cannot be obtained in real time from easily measurable variables. However, the estimation accuracy of conventional statistical soft sensors, which are constructed from recorded measurements, can be very poor in unrecorded situations (extrapolation). In this study, we estimate the internal state variables of a plant by using a dynamic simulator that can estimate and predict even unrecorded situations on the basis of chemical engineering knowledge and an artificial intelligence (AI) technology called reinforcement learning, and propose to use the estimated internal state variables of a plant as soft sensors. In addition, we describe the prospects for plant operation and control using such soft sensors and the methodology to obtain the necessary prediction models (i.e., simulators) for the proposed system. △ Less","8 August, 2022",https://arxiv.org/pdf/2208.04373
Improving performance in multi-objective decision-making in Bottles environments with soft maximin approaches,Benjamin J Smith;Robert Klassert;Roland Pihlakas,"Balancing multiple competing and conflicting objectives is an essential task for any artificial intelligence tasked with satisfying human values or preferences. Conflict arises both from misalignment between individuals with competing values, but also between conflicting value systems held by a single human. Starting with principle of loss-aversion, we designed a set of soft maximin function approaches to multi-objective decision-making. Bench-marking these functions in a set of previously-developed environments, we found that one new approach in particular, 'split-function exp-log loss aversion' (SFELLA), learns faster than the state of the art thresholded alignment objective method (Vamplew et al, 2021) on three of four tasks it was tested on, and achieved the same optimal performance after learning. SFELLA also showed relative robustness improvements against changes in objective scale, which may highlight an advantage dealing with distribution shifts in the environment dynamics. Due to publishing rules, further work could not be presented in the preprint, but in the final published version, we will further compare SFELLA to the multi-objective reward exponentials (MORE) approach (Rolf, 2020), demonstrating that SFELLA performs similarly to MORE in a simple previously-described foraging task, but in a modified foraging environment with a new resource that was not depleted as the agent worked, SFELLA collected more of the new resource with very little cost incurred in terms of the old resource. Overall, we found SFELLA useful for avoiding problems that sometimes occur with a thresholded approach, and more reward-responsive than MORE while retaining its conservative, loss-averse incentive structure. △ Less","11 August, 2022",https://arxiv.org/pdf/2208.04273
A Historical Interaction between Artificial Intelligence and Philosophy,Youheng Zhang,"This paper reviews the historical development of AI and representative philosophical thinking from the perspective of the research paradigm. Additionally, it considers the methodology and applications of AI from a philosophical perspective and anticipates its continued advancement. In the history of AI, Symbolism and connectionism are the two main paradigms in AI research. Symbolism holds that the world can be explained by symbols and dealt with through precise, logical processes, but connectionism believes this process should be implemented through artificial neural networks. Regardless of how intelligent machines or programs should achieve their smart goals, the historical development of AI demonstrates the best answer at this time. Still, it is not the final answer of AI research. △ Less","23 July, 2022",https://arxiv.org/pdf/2208.04148
Deep Machine Learning Reconstructing Lattice Topology with Strong Thermal Fluctuations,Xiao-Han Wang;Pei Shi;Bin Xi;Jie Hu;Shi-Ju Ran,"Applying artificial intelligence to scientific problems (namely AI for science) is currently under hot debate. However, the scientific problems differ much from the conventional ones with images, texts, and etc., where new challenges emerges with the unbalanced scientific data and complicated effects from the physical setups. In this work, we demonstrate the validity of the deep convolutional neural network (CNN) on reconstructing the lattice topology (i.e., spin connectivities) in the presence of strong thermal fluctuations and unbalanced data. Taking the kinetic Ising model with Glauber dynamics as an example, the CNN maps the time-dependent local magnetic momenta (a single-node feature) evolved from a specific initial configuration (dubbed as an evolution instance) to the probabilities of the presences of the possible couplings. Our scheme distinguishes from the previous ones that might require the knowledge on the node dynamics, the responses from perturbations, or the evaluations of statistic quantities such as correlations or transfer entropy from many evolution instances. The fine tuning avoids the ""barren plateau"" caused by the strong thermal fluctuations at high temperatures. Accurate reconstructions can be made where the thermal fluctuations dominate over the correlations and consequently the statistic methods in general fail. Meanwhile, we unveil the generalization of CNN on dealing with the instances evolved from the unlearnt initial spin configurations and those with the unlearnt lattices. We raise an open question on the learning with unbalanced data in the nearly ""double-exponentially"" large sample space. △ Less","8 August, 2022",https://arxiv.org/pdf/2208.04119
Constructing Large-Scale Real-World Benchmark Datasets for AIOps,Zeyan Li;Nengwen Zhao;Shenglin Zhang;Yongqian Sun;Pengfei Chen;Xidao Wen;Minghua Ma;Dan Pei,"Recently, AIOps (Artificial Intelligence for IT Operations) has been well studied in academia and industry to enable automated and effective software service management. Plenty of efforts have been dedicated to AIOps, including anomaly detection, root cause localization, incident management, etc. However, most existing works are evaluated on private datasets, so their generality and real performance cannot be guaranteed. The lack of public large-scale real-world datasets has prevented researchers and engineers from enhancing the development of AIOps. To tackle this dilemma, in this work, we introduce three public real-world, large-scale datasets about AIOps, mainly aiming at KPI anomaly detection, root cause localization on multi-dimensional data, and failure discovery and diagnosis. More importantly, we held three competitions in 2018/2019/2020 based on these datasets, attracting thousands of teams to participate. In the future, we will continue to publish more datasets and hold competitions to promote the development of AIOps further. △ Less","8 August, 2022",https://arxiv.org/pdf/2208.03938
Artificial Intelligence and Machine Learning for Quantum Technologies,Mario Krenn;Jonas Landgraf;Thomas Foesel;Florian Marquardt,"In recent years, the dramatic progress in machine learning has begun to impact many areas of science and technology significantly. In the present perspective article, we explore how quantum technologies are benefiting from this revolution. We showcase in illustrative examples how scientists in the past few years have started to use machine learning and more broadly methods of artificial intelligence to analyze quantum measurements, estimate the parameters of quantum devices, discover new quantum experimental setups, protocols, and feedback strategies, and generally improve aspects of quantum computing, quantum communication, and quantum simulation. We highlight open challenges and future possibilities and conclude with some speculative visions for the next decade. △ Less","7 August, 2022",https://arxiv.org/pdf/2208.03836
An Accurate and Explainable Deep Learning System Improves Interobserver Agreement in the Interpretation of Chest Radiograph,Hieu H. Pham;Ha Q. Nguyen;Hieu T. Nguyen;Linh T. Le;Lam Khanh,"Recent artificial intelligence (AI) algorithms have achieved radiologist-level performance on various medical classification tasks. However, only a few studies addressed the localization of abnormal findings from CXR scans, which is essential in explaining the image-level classification to radiologists. We introduce in this paper an explainable deep learning system called VinDr-CXR that can classify a CXR scan into multiple thoracic diseases and, at the same time, localize most types of critical findings on the image. VinDr-CXR was trained on 51,485 CXR scans with radiologist-provided bounding box annotations. It demonstrated a comparable performance to experienced radiologists in classifying 6 common thoracic diseases on a retrospective validation set of 3,000 CXR scans, with a mean area under the receiver operating characteristic curve (AUROC) of 0.967 (95% confidence interval [CI]: 0.958-0.975). The VinDr-CXR was also externally validated in independent patient cohorts and showed its robustness. For the localization task with 14 types of lesions, our free-response receiver operating characteristic (FROC) analysis showed that the VinDr-CXR achieved a sensitivity of 80.2% at the rate of 1.0 false-positive lesion identified per scan. A prospective study was also conducted to measure the clinical impact of the VinDr-CXR in assisting six experienced radiologists. The results indicated that the proposed system, when used as a diagnosis supporting tool, significantly improved the agreement between radiologists themselves with an increase of 1.5% in mean Fleiss' Kappa. We also observed that, after the radiologists consulted VinDr-CXR's suggestions, the agreement between each of them and the system was remarkably increased by 3.3% in mean Cohen's Kappa. △ Less","6 August, 2022",https://arxiv.org/pdf/2208.03545
Semiconductor Defect Detection by Hybrid Classical-Quantum Deep Learning,YuanFu Yang;Min Sun,"With the rapid development of artificial intelligence and autonomous driving technology, the demand for semiconductors is projected to rise substantially. However, the massive expansion of semiconductor manufacturing and the development of new technology will bring many defect wafers. If these defect wafers have not been correctly inspected, the ineffective semiconductor processing on these defect wafers will cause additional impact to our environment, such as excessive carbon dioxide emission and energy consumption. In this paper, we utilize the information processing advantages of quantum computing to promote the defect learning defect review (DLDR). We propose a classical-quantum hybrid algorithm for deep learning on near-term quantum processors. By tuning parameters implemented on it, quantum circuit driven by our framework learns a given DLDR task, include of wafer defect map classification, defect pattern classification, and hotspot detection. In addition, we explore parametrized quantum circuits with different expressibility and entangling capacities. These results can be used to build a future roadmap to develop circuit-based quantum deep learning for semiconductor defect detection. △ Less","6 August, 2022",https://arxiv.org/pdf/2208.03514
Multi-view deep learning for reliable post-disaster damage classification,Asim Bashir Khajwal;Chih-Shen Cheng;Arash Noshadravan,"This study aims to enable more reliable automated post-disaster building damage classification using artificial intelligence (AI) and multi-view imagery. The current practices and research efforts in adopting AI for post-disaster damage assessment are generally (a) qualitative, lacking refined classification of building damage levels based on standard damage scales, and (b) trained based on aerial or satellite imagery with limited views, which, although indicative, are not completely descriptive of the damage scale. To enable more accurate and reliable automated quantification of damage levels, the present study proposes the use of more comprehensive visual data in the form of multiple ground and aerial views of the buildings. To have such a spatially-aware damage prediction model, a Multi-view Convolution Neural Network (MV-CNN) architecture is used that combines the information from different views of a damaged building. This spatial 3D context damage information will result in more accurate identification of damages and reliable quantification of damage levels. The proposed model is trained and validated on reconnaissance visual dataset containing expert-labeled, geotagged images of the inspected buildings following hurricane Harvey. The developed model demonstrates reasonably good accuracy in predicting the damage levels and can be used to support more informed and reliable AI-assisted disaster management practices. △ Less","5 August, 2022",https://arxiv.org/pdf/2208.03419
Interpretable Uncertainty Quantification in AI for HEP,Thomas Y. Chen;Biprateep Dey;Aishik Ghosh;Michael Kagan;Brian Nord;Nesar Ramachandra,"Estimating uncertainty is at the core of performing scientific measurements in HEP: a measurement is not useful without an estimate of its uncertainty. The goal of uncertainty quantification (UQ) is inextricably linked to the question, ""how do we physically and statistically interpret these uncertainties?"" The answer to this question depends not only on the computational task we aim to undertake, but also on the methods we use for that task. For artificial intelligence (AI) applications in HEP, there are several areas where interpretable methods for UQ are essential, including inference, simulation, and control/decision-making. There exist some methods for each of these areas, but they have not yet been demonstrated to be as trustworthy as more traditional approaches currently employed in physics (e.g., non-AI frequentist and Bayesian methods). Shedding light on the questions above requires additional understanding of the interplay of AI systems and uncertainty quantification. We briefly discuss the existing methods in each area and relate them to tasks across HEP. We then discuss recommendations for avenues to pursue to develop the necessary techniques for reliable widespread usage of AI with UQ over the next decade. △ Less","6 September, 2022",https://arxiv.org/pdf/2208.03284
Tools and Methodologies for Verifying Answer Set Programs,Zach Hansen,"Answer Set Programming (ASP) is a powerful declarative programming paradigm commonly used for solving challenging search and optimization problems. The modeling languages of ASP are supported by sophisticated solving algorithms (solvers) that make the solution search efficient while enabling the programmer to model the problem at a high level of abstraction. As an approach to Knowledge Representation and Reasoning, ASP benefits from its simplicity, conciseness and rigorously defined semantics. These characteristics make ASP a straightforward way to develop formally verifiable programs. In the context of artificial intelligence (AI), the clarity of ASP programs lends itself to the construction of explainable, trustworthy AI. In support of these goals, my research is concerned with extending the theory and tools supporting the verification of ASP progams. △ Less","5 August, 2022",https://arxiv.org/pdf/2208.03096
"A 35-Year Longitudinal Analysis of Dermatology Patient Behavior across Economic & Cultural Manifestations in Tunisia, and the Impact of Digital Tools",Mohamed Akrout;Hayet Amdouni;Amal Feriani;Monia Kourda;Latif Abid,"The evolution of behavior of dermatology patients has seen significantly accelerated change over the past decade, driven by surging availability and adoption of digital tools and platforms. Through our longitudinal analysis of this behavior within Tunisia over a 35-year time frame, we identify behavioral patterns across economic and cultural dimensions and how digital tools have impacted those patterns in preceding years. Throughout this work, we highlight the witnessed effects of available digital tools as experienced by patients, and conclude by presenting a vision for how future tools can help address the issues identified across economic and cultural manifestations. Our analysis is further framed around three types of digital tools: ""Dr. Google"", social media, and artificial intelligence (AI) tools, and across three stages of clinical care: pre-visit, in-visit, and post-visit. △ Less","4 August, 2022",https://arxiv.org/pdf/2208.02852
Core and Periphery as Closed-System Precepts for Engineering General Intelligence,Tyler Cody;Niloofar Shadab;Alejandro Salado;Peter Beling,"Engineering methods are centered around traditional notions of decomposition and recomposition that rely on partitioning the inputs and outputs of components to allow for component-level properties to hold after their composition. In artificial intelligence (AI), however, systems are often expected to influence their environments, and, by way of their environments, to influence themselves. Thus, it is unclear if an AI system's inputs will be independent of its outputs, and, therefore, if AI systems can be treated as traditional components. This paper posits that engineering general intelligence requires new general systems precepts, termed the core and periphery, and explores their theoretical uses. The new precepts are elaborated using abstract systems theory and the Law of Requisite Variety. By using the presented material, engineers can better understand the general character of regulating the outcomes of AI to achieve stakeholder needs and how the general systems nature of embodiment challenges traditional engineering practice. △ Less","4 August, 2022",https://arxiv.org/pdf/2208.02837
On the use of Artificial Neural Networks in Topology Optimisation,Rebekka V. Woldseth;Niels Aage;J. Andreas Bærentzen;Ole Sigmund,"The question of how methods from the field of artificial intelligence can help improve the conventional frameworks for topology optimisation has received increasing attention over the last few years. Motivated by the capabilities of neural networks in image analysis, different model-variations aimed at obtaining iteration-free topology optimisation have been proposed with varying success. Other works focused on speed-up through replacing expensive optimisers and state solvers, or reducing the design-space have been attempted, but have not yet received the same attention. The portfolio of articles presenting different applications has as such become extensive, but few real breakthroughs have yet been celebrated. An overall trend in the literature is the strong faith in the ""magic"" of artificial intelligence and thus misunderstandings about the capabilities of such methods. The aim of this article is therefore to present a critical review of the current state of research in this field. To this end, an overview of the different model-applications is presented, and efforts are made to identify reasons for the overall lack of convincing success. A thorough analysis identifies and differentiates between problematic and promising aspects of existing models. The resulting findings are used to detail recommendations believed to encourage avenues of potential scientific progress for further research within the field. △ Less","4 August, 2022",https://arxiv.org/pdf/2208.02563
Multi-modal volumetric concept activation to explain detection and classification of metastatic prostate cancer on PSMA-PET/CT,Rosa C. J. Kraaijveld;Marielle E. P. Philippens;Wietse S. C. Eppinga;Ina M. Jürgenliemk-Schulz;Kenneth G. A. Gilhuijs;Petra S. Kroon;Bas H. M. van der Velden,"Explainable artificial intelligence (XAI) is increasingly used to analyze the behavior of neural networks. Concept activation uses human-interpretable concepts to explain neural network behavior. This study aimed at assessing the feasibility of regression concept activation to explain detection and classification of multi-modal volumetric data. Proof-of-concept was demonstrated in metastatic prostate cancer patients imaged with positron emission tomography/computed tomography (PET/CT). Multi-modal volumetric concept activation was used to provide global and local explanations. Sensitivity was 80% at 1.78 false positive per patient. Global explanations showed that detection focused on CT for anatomical location and on PET for its confidence in the detection. Local explanations showed promise to aid in distinguishing true positives from false positives. Hence, this study demonstrated feasibility to explain detection and classification of multi-modal volumetric data using regression concept activation. △ Less","4 August, 2022",https://arxiv.org/pdf/2208.02555
"Smartphone Apps for Tracking Food Consumption and Recommendations: Evaluating Artificial Intelligence-based Functionalities, Features and Quality of Current Apps",Sabiha Samad;Fahmida Ahmed;Samsun Naher;Muhammad Ashad Kabir;Anik Das;Sumaiya Amin;Sheikh Mohammed Shariful Islam,"The advancement of artificial intelligence (AI) and the significant growth in the use of food consumption tracking and recommendation-related apps in the app stores have created a need for an evaluation system, as minimal information is available about the evidence-based quality and technological advancement of these apps. Electronic searches were conducted across three major app stores and the selected apps were evaluated by three independent raters. A total of 473 apps were found and 80 of them were selected for review based on inclusion and exclusion criteria. An app rating tool is devised to evaluate the selected apps. Our rating tool assesses the apps' essential features, AI-based advanced functionalities, and software quality characteristics required for food consumption tracking and recommendations, as well as their usefulness to general users. Users' comments from the app stores are collected and evaluated to better understand their expectations and perspectives. Following an evaluation of the assessed applications, design considerations that emphasize automation-based approaches using artificial intelligence are proposed. According to our assessment, most mobile apps in the app stores do not satisfy the overall requirements for tracking food consumption and recommendations. ""Foodvisor"" is the only app that can automatically recognize food items, and compute the recommended volume and nutritional information of that food item. However, these features need to be improvised in the food consumption tracking and recommendation apps. This study provides both researchers and developers with an insight into current state-of-the-art apps and design guidelines with necessary information on essential features and software quality characteristics for designing and developing a better app. △ Less","4 August, 2022",https://arxiv.org/pdf/2208.02490
Evaluating Plant Disease Detection Mobile Applications: Quality and Limitations,Ayesha Siddiqua;Muhammad Ashad Kabir;Tanzina Ferdous;Israt Bintea Ali;Leslie A. Weston,"In this technologically advanced era, with the proliferation of artificial intelligence, many mobile apps are available for plant disease detection, diagnosis, and treatment, each with a variety of features. These apps need to be categorized and reviewed following a proper framework that ensures their quality. This study aims to present an approach to evaluating plant disease detection mobile apps, this includes providing ratings of distinct features of the apps and insights into the exploitation of artificial intelligence used in plant disease detection. For this purpose, plant disease detection apps were searched in three prominent app stores using a set of keywords. A total of 606 apps were found and from them 17 relevant apps were identified based on inclusion and exclusion criteria. The selected apps were reviewed by three raters using our devised app rating scale. User comments from the app stores are collected and analyzed to understand their expectations and views. Following the rating procedure, most apps earned acceptable ratings in software quality characteristics such as aesthetics, usability, and performance, but gained poor ratings in AI-based advanced functionality, which is the key aspect of this study. However, most of the apps cannot be used as a complete solution to plant disease detection, diagnosis, and treatment. Only one app, Plantix - your crop doctor, could successfully identify plants from images, detect diseases, maintain a rich plant database, and suggest potential treatments for the disease presented. It also provides a community where plant lovers can communicate with each other to gain additional benefits. In general, all existing apps need to improve functionalities, user experience, and software quality. Therefore, a set of design considerations has been proposed for future app improvements. △ Less","4 August, 2022",https://arxiv.org/pdf/2208.02446
Credal Valuation Networks for Machine Reasoning Under Uncertainty,Branko Ristic;Alessio Benavoli;Sanjeev Arulampalam,"Contemporary undertakings provide limitless opportunities for widespread application of machine reasoning and artificial intelligence in situations characterised by uncertainty, hostility and sheer volume of data. The paper develops a valuation network as a graphical system for higher-level fusion and reasoning under uncertainty in support of the human operators. Valuations, which are mathematical representation of (uncertain) knowledge and collected data, are expressed as credal sets, defined as coherent interval probabilities in the framework of imprecise probability theory. The basic operations with such credal sets, combination and marginalisation, are defined to satisfy the axioms of a valuation algebra. A practical implementation of the credal valuation network is discussed and its utility demonstrated on a small scale example. △ Less","4 August, 2022",https://arxiv.org/pdf/2208.02443
On the independence between phenomenal consciousness and computational intelligence,Eduardo C. Garrido Merchán;Sara Lumbreras,"Consciousness and intelligence are properties commonly understood as dependent by folk psychology and society in general. The term artificial intelligence and the kind of problems that it managed to solve in the recent years has been shown as an argument to establish that machines experience some sort of consciousness. Following the analogy of Russell, if a machine is able to do what a conscious human being does, the likelihood that the machine is conscious increases. However, the social implications of this analogy are catastrophic. Concretely, if rights are given to entities that can solve the kind of problems that a neurotypical person can, does the machine have potentially more rights that a person that has a disability? For example, the autistic syndrome disorder spectrum can make a person unable to solve the kind of problems that a machine solves. We believe that the obvious answer is no, as problem solving does not imply consciousness. Consequently, we will argue in this paper how phenomenal consciousness and, at least, computational intelligence are independent and why machines do not possess phenomenal consciousness, although they can potentially develop a higher computational intelligence that human beings. In order to do so, we try to formulate an objective measure of computational intelligence and study how it presents in human beings, animals and machines. Analogously, we study phenomenal consciousness as a dichotomous variable and how it is distributed in humans, animals and machines. As phenomenal consciousness and computational intelligence are independent, this fact has critical implications for society that we also analyze in this work. △ Less","3 August, 2022",https://arxiv.org/pdf/2208.02187
Spectrum Focused Frequency Adversarial Attacks for Automatic Modulation Classification,Sicheng Zhang;Jiarun Yu;Zhida Bao;Shiwen Mao;Yun Lin,"Artificial intelligence (AI) technology has provided a potential solution for automatic modulation recognition (AMC). Unfortunately, AI-based AMC models are vulnerable to adversarial examples, which seriously threatens the efficient, secure and trusted application of AI in AMC. This issue has attracted the attention of researchers. Various studies on adversarial attacks and defenses evolve in a spiral. However, the existing adversarial attack methods are all designed in the time domain. They introduce more high-frequency components in the frequency domain, due to abrupt updates in the time domain. For this issue, from the perspective of frequency domain, we propose a spectrum focused frequency adversarial attacks (SFFAA) for AMC model, and further draw on the idea of meta-learning, propose a Meta-SFFAA algorithm to improve the transferability in the black-box attacks. Extensive experiments, qualitative and quantitative metrics demonstrate that the proposed algorithm can concentrate the adversarial energy on the spectrum where the signal is located, significantly improve the adversarial attack performance while maintaining the concealment in the frequency domain. △ Less","3 August, 2022",https://arxiv.org/pdf/2208.01919
Joint Optimization of DNN Inference Delay and Energy under Accuracy Constraints for AR Applications,Guangjin Pan;Heng Zhang;Shugong Xu;Shunqing Zhang;Xiaojing Chen,"The high computational complexity and high energy consumption of artificial intelligence (AI) algorithms hinder their application in augmented reality (AR) systems. This paper considers the scene of completing video-based AI inference tasks in the mobile edge computing (MEC) system. We use multiply-and-accumulate operations (MACs) for problem analysis and optimize delay and energy consumption under accuracy constraints. To solve this problem, we first assume that offloading policy is known and decouple the problem into two subproblems. After solving these two subproblems, we propose an iterative-based scheduling algorithm to obtain the optimal offloading policy. We also experimentally discuss the relationship between delay, energy consumption, and inference accuracy. △ Less","3 August, 2022",https://arxiv.org/pdf/2208.01860
Effidit: Your AI Writing Assistant,Shuming Shi;Enbo Zhao;Duyu Tang;Yan Wang;Piji Li;Wei Bi;Haiyun Jiang;Guoping Huang;Leyang Cui;Xinting Huang;Cong Zhou;Yong Dai;Dongyang Ma,"In this technical report, we introduce Effidit (Efficient and Intelligent Editing), a digital writing assistant that facilitates users to write higher-quality text more efficiently by using artificial intelligence (AI) technologies. Previous writing assistants typically provide the function of error checking (to detect and correct spelling and grammatical errors) and limited text-rewriting functionality. With the emergence of large-scale neural language models, some systems support automatically completing a sentence or a paragraph. In Effidit, we significantly expand the capacities of a writing assistant by providing functions in five categories: text completion, error checking, text polishing, keywords to sentences (K2S), and cloud input methods (cloud IME). In the text completion category, Effidit supports generation-based sentence completion, retrieval-based sentence completion, and phrase completion. In contrast, many other writing assistants so far only provide one or two of the three functions. For text polishing, we have three functions: (context-aware) phrase polishing, sentence paraphrasing, and sentence expansion, whereas many other writing assistants often support one or two functions in this category. The main contents of this report include major modules of Effidit, methods for implementing these modules, and evaluation results of some key methods. △ Less","4 August, 2022",https://arxiv.org/pdf/2208.01815
Deep Reinforcement Learning for Multi-Agent Interaction,Ibrahim H. Ahmed;Cillian Brewitt;Ignacio Carlucho;Filippos Christianos;Mhairi Dunion;Elliot Fosong;Samuel Garcin;Shangmin Guo;Balint Gyevnar;Trevor McInroe;Georgios Papoudakis;Arrasy Rahman;Lukas Schäfer;Massimiliano Tamborski;Giuseppe Vecchio;Cheng Wang;Stefano V. Albrecht,"The development of autonomous agents which can interact with other agents to accomplish a given task is a core area of research in artificial intelligence and machine learning. Towards this goal, the Autonomous Agents Research Group develops novel machine learning algorithms for autonomous systems control, with a specific focus on deep reinforcement learning and multi-agent reinforcement learning. Research problems include scalable learning of coordinated agent policies and inter-agent communication; reasoning about the behaviours, goals, and composition of other agents from limited observations; and sample-efficient learning based on intrinsic motivation, curriculum learning, causal inference, and representation learning. This article provides a broad overview of the ongoing research portfolio of the group and discusses open problems for future directions. △ Less","2 August, 2022",https://arxiv.org/pdf/2208.01769
Heterogeneous-Agent Mirror Learning: A Continuum of Solutions to Cooperative MARL,Jakub Grudzien Kuba;Xidong Feng;Shiyao Ding;Hao Dong;Jun Wang;Yaodong Yang,"The necessity for cooperation among intelligent machines has popularised cooperative multi-agent reinforcement learning (MARL) in the artificial intelligence (AI) research community. However, many research endeavors have been focused on developing practical MARL algorithms whose effectiveness has been studied only empirically, thereby lacking theoretical guarantees. As recent studies have revealed, MARL methods often achieve performance that is unstable in terms of reward monotonicity or suboptimal at convergence. To resolve these issues, in this paper, we introduce a novel framework named Heterogeneous-Agent Mirror Learning (HAML) that provides a general template for MARL algorithmic designs. We prove that algorithms derived from the HAML template satisfy the desired properties of the monotonic improvement of the joint reward and the convergence to Nash equilibrium. We verify the practicality of HAML by proving that the current state-of-the-art cooperative MARL algorithms, HATRPO and HAPPO, are in fact HAML instances. Next, as a natural outcome of our theory, we propose HAML extensions of two well-known RL algorithms, HAA2C (for A2C) and HADDPG (for DDPG), and demonstrate their effectiveness against strong baselines on StarCraftII and Multi-Agent MuJoCo tasks. △ Less","2 August, 2022",https://arxiv.org/pdf/2208.01682
Diagnosis of Paratuberculosis in Histopathological Images Based on Explainable Artificial Intelligence and Deep Learning,Tuncay Yiğit;Nilgün Şengöz;Özlem Özmen;Jude Hemanth;Ali Hakan Işık,"Artificial intelligence holds great promise in medical imaging, especially histopathological imaging. However, artificial intelligence algorithms cannot fully explain the thought processes during decision-making. This situation has brought the problem of explainability, i.e., the black box problem, of artificial intelligence applications to the agenda: an algorithm simply responds without stating the reasons for the given images. To overcome the problem and improve the explainability, explainable artificial intelligence (XAI) has come to the fore, and piqued the interest of many researchers. Against this backdrop, this study examines a new and original dataset using the deep learning algorithm, and visualizes the output with gradient-weighted class activation mapping (Grad-CAM), one of the XAI applications. Afterwards, a detailed questionnaire survey was conducted with the pathologists on these images. Both the decision-making processes and the explanations were verified, and the accuracy of the output was tested. The research results greatly help pathologists in the diagnosis of paratuberculosis. △ Less","2 August, 2022",https://arxiv.org/pdf/2208.01674
Single chip photonic deep neural network with accelerated training,Saumil Bandyopadhyay;Alexander Sludds;Stefan Krastanov;Ryan Hamerly;Nicholas Harris;Darius Bunandar;Matthew Streshinsky;Michael Hochberg;Dirk Englund,"As deep neural networks (DNNs) revolutionize machine learning, energy consumption and throughput are emerging as fundamental limitations of CMOS electronics. This has motivated a search for new hardware architectures optimized for artificial intelligence, such as electronic systolic arrays, memristor crossbar arrays, and optical accelerators. Optical systems can perform linear matrix operations at exceptionally high rate and efficiency, motivating recent demonstrations of low latency linear algebra and optical energy consumption below a photon per multiply-accumulate operation. However, demonstrating systems that co-integrate both linear and nonlinear processing units in a single chip remains a central challenge. Here we introduce such a system in a scalable photonic integrated circuit (PIC), enabled by several key advances: (i) high-bandwidth and low-power programmable nonlinear optical function units (NOFUs); (ii) coherent matrix multiplication units (CMXUs); and (iii) in situ training with optical acceleration. We experimentally demonstrate this fully-integrated coherent optical neural network (FICONN) architecture for a 3-layer DNN comprising 12 NOFUs and three CMXUs operating in the telecom C-band. Using in situ training on a vowel classification task, the FICONN achieves 92.7% accuracy on a test set, which is identical to the accuracy obtained on a digital computer with the same number of weights. This work lends experimental evidence to theoretical proposals for in situ training, unlocking orders of magnitude improvements in the throughput of training data. Moreover, the FICONN opens the path to inference at nanosecond latency and femtojoule per operation energy efficiency. △ Less","2 August, 2022",https://arxiv.org/pdf/2208.01623
Optimal Friendly Jamming and Transmit Power Allocation in RIS-assisted Secure Communication,Burhan Wafai;Chinmoy Kundu;Ankit Dubey;Mark F. Flanagan,This paper analyzes the secrecy performance of a reconfigurable intelligent surface (RIS) assisted wireless communication system with a friendly jammer in the presence of an eavesdropper. The friendly jammer enhances the secrecy by introducing artificial noise towards the eavesdropper without degrading the reception at the destination. Approximate secrecy outage probability (SOP) is derived in closed form. We also provide a simpler approximate closed-form expression for the SOP in order to understand the effect of system parameters on the performance and to find the optimal power allocation for the transmitter and jammer. The optimal transmit and jamming power allocation factor is derived by minimizing the SOP assuming a total power constraint. It is shown that the SOP performance is significantly improved by the introduction of the jammer and a gain of approximately 3 dB is achieved at an SOP of 10^{-4} by optimally allocating power compared to the case of equal power allocation. △ Less,"2 August, 2022",https://arxiv.org/pdf/2208.01537
A Secure Dynamic Edge Resource Federation Architecture for Cross-Domain IoT Systems,Ronghua Xu;Yu Chen;Xiaohua Li;Erik Blasch,"The fast integration of 5G communication, Artificial Intelligence (AI), and Internet-of-Things (IoT) technologies is envisioned to enable Next Generation Networks (NGNs) for diverse smart services and user-defined applications for Smart Cities. However, it is still challenging to build a scalable and efficient infrastructure that satisfies the various performance, security, and management demands by heterogeneous IoT applications across multiple administrative domains. This paper presents a dynamic edge resource federation architecture, which integrates the concept of network slicing (NS) and blockchain to improve scalability, dynamicity, and security for multi-domain IoT applications. A NS-enabled dynamic edge resource federation framework adopts intelligent mechanisms to support efficient multi-domain service coordination that satisfies diverse Quality of Service (QoS) and security requirements. We propose a Hierarchical Integrated Federated Ledger (HIFL), which aims to guarantee decentralized security and privacy-preserving properties in multi-domain resource orchestration and service re-adjustment. As a secure-by-design solution, HIFL is promising to support efficient, trust and secured end-to-end IoT services. A preliminary proof-of-concept prototype has been implemented for comparing intra- and inter-domain performance expectations. △ Less","2 August, 2022",https://arxiv.org/pdf/2208.01466
Interplay between Distributed AI Workflow and URLLC,Milad Ganjalizadeh;Hossein S. Ghadikolaei;Johan Haraldson;Marina Petrova,"Distributed artificial intelligence (AI) has recently accomplished tremendous breakthroughs in various communication services, ranging from fault-tolerant factory automation to smart cities. When distributed learning is run over a set of wireless connected devices, random channel fluctuations, and the incumbent services simultaneously running on the same network affect the performance of distributed learning. In this paper, we investigate the interplay between distributed AI workflow and ultra-reliable low latency communication (URLLC) services running concurrently over a network. Using 3GPP compliant simulations in a factory automation use case, we show the impact of various distributed AI settings (e.g., model size and the number of participating devices) on the convergence time of distributed AI and the application layer performance of URLLC. Unless we leverage the existing 5G-NR quality of service handling mechanisms to separate the traffic from the two services, our simulation results show that the impact of distributed AI on the availability of the URLLC devices is significant. Moreover, with proper setting of distributed AI (e.g., proper user selection), we can substantially reduce network resource utilization, leading to lower latency for distributed AI and higher availability for the URLLC users. Our results provide important insights for future 6G and AI standardization. △ Less","2 August, 2022",https://arxiv.org/pdf/2208.01352
Generative Adversarial Learning for Intelligent Trust Management in 6G Wireless Networks,Liu Yang;Yun Li;Simon X. Yang;Yinzhi Lu;Tan Guo;Keping Yu,"Emerging six generation (6G) is the integration of heterogeneous wireless networks, which can seamlessly support anywhere and anytime networking. But high Quality-of-Trust should be offered by 6G to meet mobile user expectations. Artificial intelligence (AI) is considered as one of the most important components in 6G. Then AI-based trust management is a promising paradigm to provide trusted and reliable services. In this article, a generative adversarial learning-enabled trust management method is presented for 6G wireless networks. Some typical AI-based trust management schemes are first reviewed, and then a potential heterogeneous and intelligent 6G architecture is introduced. Next, the integration of AI and trust management is developed to optimize the intelligence and security. Finally, the presented AI-based trust management method is applied to secure clustering to achieve reliable and real-time communications. Simulation results have demonstrated its excellent performance in guaranteeing network security and service quality. △ Less","1 August, 2022",https://arxiv.org/pdf/2208.01221
Making a Spiking Net Work: Robust brain-like unsupervised machine learning,Peter G. Stratton;Andrew Wabnitz;Chip Essam;Allen Cheung;Tara J. Hamilton,"The surge in interest in Artificial Intelligence (AI) over the past decade has been driven almost exclusively by advances in Artificial Neural Networks (ANNs). While ANNs set state-of-the-art performance for many previously intractable problems, the use of global gradient descent necessitates large datasets and computational resources for training, potentially limiting their scalability for real-world domains. Spiking Neural Networks (SNNs) are an alternative to ANNs that use more brain-like artificial neurons and can use local unsupervised learning to rapidly discover sparse recognizable features in the input data. SNNs, however, struggle with dynamical stability and have failed to match the accuracy of ANNs. Here we show how an SNN can overcome many of the shortcomings that have been identified in the literature, including offering a principled solution to the dynamical ""vanishing spike problem"", to outperform all existing shallow SNNs and equal the performance of an ANN. It accomplishes this while using unsupervised learning with unlabeled data and only 1/50th of the training epochs (labeled data is used only for a simple linear readout layer). This result makes SNNs a viable new method for fast, accurate, efficient, explainable, and re-deployable machine learning with unlabeled data. △ Less","31 August, 2022",https://arxiv.org/pdf/2208.01204
A Feasibility Study on Image Inpainting for Non-cleft Lip Generation from Patients with Cleft Lip,Shuang Chen;Amir Atapour-Abarghouei;Jane Kerby;Edmond S. L. Ho;David C. G. Sainsbury;Sophie Butterworth;Hubert P. H. Shum,"A Cleft lip is a congenital abnormality requiring surgical repair by a specialist. The surgeon must have extensive experience and theoretical knowledge to perform surgery, and Artificial Intelligence (AI) method has been proposed to guide surgeons in improving surgical outcomes. If AI can be used to predict what a repaired cleft lip would look like, surgeons could use it as an adjunct to adjust their surgical technique and improve results. To explore the feasibility of this idea while protecting patient privacy, we propose a deep learning-based image inpainting method that is capable of covering a cleft lip and generating a lip and nose without a cleft. Our experiments are conducted on two real-world cleft lip datasets and are assessed by expert cleft lip surgeons to demonstrate the feasibility of the proposed method. △ Less","1 August, 2022",https://arxiv.org/pdf/2208.01149
Face-to-Face Contrastive Learning for Social Intelligence Question-Answering,Alex Wilf;Martin Q. Ma;Paul Pu Liang;Amir Zadeh;Louis-Philippe Morency,"Creating artificial social intelligence - algorithms that can understand the nuances of multi-person interactions - is an exciting and emerging challenge in processing facial expressions and gestures from multimodal videos. Recent multimodal methods have set the state of the art on many tasks, but have difficulty modeling the complex face-to-face conversational dynamics across speaking turns in social interaction, particularly in a self-supervised setup. In this paper, we propose Face-to-Face Contrastive Learning (F2F-CL), a graph neural network designed to model social interactions using factorization nodes to contextualize the multimodal face-to-face interaction along the boundaries of the speaking turn. With the F2F-CL model, we propose to perform contrastive learning between the factorization nodes of different speaking turns within the same video. We experimentally evaluated the challenging Social-IQ dataset and show state-of-the-art results. △ Less","27 October, 2022",https://arxiv.org/pdf/2208.01036
Effective Gesture Based Framework for Capturing User Input,Pabbathi Sri Charan;Saksham Gupta;Satvik Agrawal;Gadupudi Sahithi Sindhu,"Computers today aren't just confined to laptops and desktops. Mobile gadgets like mobile phones and laptops also make use of it. However, one input device that hasn't changed in the last 50 years is the QWERTY keyboard. Users of virtual keyboards can type on any surface as if it were a keyboard thanks to sensor technology and artificial intelligence. In this research, we use the idea of image processing to create an application for seeing a computer keyboard using a novel framework which can detect hand gestures with precise accuracy while also being sustainable and financially viable. A camera is used to capture keyboard images and finger movements which subsequently acts as a virtual keyboard. In addition, a visible virtual mouse that accepts finger coordinates as input is also described in this study. This system has a direct benefit of reducing peripheral cost, reducing electronics waste generated due to external devices and providing accessibility to people who cannot use the traditional keyboard and mouse. △ Less","1 August, 2022",https://arxiv.org/pdf/2208.00913
Real Time Object Detection System with YOLO and CNN Models: A Review,Viswanatha V;Chandana R K;Ramachandra A. C.,"The field of artificial intelligence is built on object detection techniques. YOU ONLY LOOK ONCE (YOLO) algorithm and it's more evolved versions are briefly described in this research survey. This survey is all about YOLO and convolution neural networks (CNN)in the direction of real time object detection.YOLO does generalized object representation more effectively without precision losses than other object detection models.CNN architecture models have the ability to eliminate highlights and identify objects in any given image. When implemented appropriately, CNN models can address issues like deformity diagnosis, creating educational or instructive application, etc. This article reached atnumber of observations and perspective findings through the analysis.Also it provides support for the focused visual information and feature extraction in the financial and other industries, highlights the method of target detection and feature selection, and briefly describe the development process of YOLO algorithm. △ Less","23 July, 2022",https://arxiv.org/pdf/2208.00773
Assessing the robustness of critical behavior in stochastic cellular automata,Sidney Pontes-Filho;Pedro Lind;Stefano Nichele,"There is evidence that biological systems, such as the brain, work at a critical regime robust to noise, and are therefore able to remain in it under perturbations. In this work, we address the question of robustness of critical systems to noise. In particular, we investigate the robustness of stochastic cellular automata (CAs) at criticality. A stochastic CA is one of the simplest stochastic models showing criticality. The transition state of stochastic CA is defined through a set of probabilities. We systematically perturb the probabilities of an optimal stochastic CA known to produce critical behavior, and we report that such a CA is able to remain in a critical regime up to a certain degree of noise. We present the results using error metrics of the resulting power-law fitting, such as Kolmogorov-Smirnov statistic and Kullback-Leibler divergence. We discuss the implication of our results in regards to future realization of brain-inspired artificial intelligence systems. △ Less","1 August, 2022",https://arxiv.org/pdf/2208.00746
Distributed Intelligence in Wireless Networks,Xiaolan Liu;Jiadong Yu;Yuanwei Liu;Yue Gao;Toktam Mahmoodi;Sangarapillai Lambotharan;Danny H. K. Tsang,"The cloud-based solutions are becoming inefficient due to considerably large time delays, high power consumption, security and privacy concerns caused by billions of connected wireless devices and typically zillions bytes of data they produce at the network edge. A blend of edge computing and Artificial Intelligence (AI) techniques could optimally shift the resourceful computation servers closer to the network edge, which provides the support for advanced AI applications (e.g., video/audio surveillance and personal recommendation system) by enabling intelligent decision making on computing at the point of data generation as and when it is needed, and distributed Machine Learning (ML) with its potential to avoid the transmission of large dataset and possible compromise of privacy that may exist in cloud-based centralized learning. Therefore, AI is envisioned to become native and ubiquitous in future communication and networking systems. In this paper, we conduct a comprehensive overview of recent advances in distributed intelligence in wireless networks under the umbrella of native-AI wireless networks, with a focus on the basic concepts of native-AI wireless networks, on the AI-enabled edge computing, on the design of distributed learning architectures for heterogeneous networks, on the communication-efficient technologies to support distributed learning, and on the AI-empowered end-to-end communications. We highlight the advantages of hybrid distributed learning architectures compared to the state-of-art distributed learning techniques. We summarize the challenges of existing research contributions in distributed intelligence in wireless networks and identify the potential future opportunities. △ Less","31 July, 2022",https://arxiv.org/pdf/2208.00545
Eco2AI: carbon emissions tracking of machine learning models as the first step towards sustainable AI,Semen Budennyy;Vladimir Lazarev;Nikita Zakharenko;Alexey Korovin;Olga Plosskaya;Denis Dimitrov;Vladimir Arkhipkin;Ivan Oseledets;Ivan Barsola;Ilya Egorov;Aleksandra Kosterina;Leonid Zhukov,"The size and complexity of deep neural networks continue to grow exponentially, significantly increasing energy consumption for training and inference by these models. We introduce an open-source package eco2AI to help data scientists and researchers to track energy consumption and equivalent CO2 emissions of their models in a straightforward way. In eco2AI we put emphasis on accuracy of energy consumption tracking and correct regional CO2 emissions accounting. We encourage research community to search for new optimal Artificial Intelligence (AI) architectures with a lower computational cost. The motivation also comes from the concept of AI-based green house gases sequestrating cycle with both Sustainable AI and Green AI pathways. △ Less","3 August, 2022",https://arxiv.org/pdf/2208.00406
Multi-Agent Path Finding Based on Subdimensional Expansion with Bypass,Qingzhou Liu;Feng Wu,"Multi-agent path finding (MAPF) is an active area in artificial intelligence, which has many real-world applications such as warehouse management, traffic control, robotics, etc. Recently, M* and its variants have greatly improved the ability to solve the MAPF problem. Although subdimensional expansion used in those approaches significantly decreases the dimensionality of the joint search space and reduces the branching factor, they do not make full use of the possible non-uniqueness of the optimal path of each agent. As a result, the updating of the collision sets may bring a large number of redundant computation. In this paper, the idea of bypass is introduced into subdimensional expansion to reduce the redundant computation. Specifically, we propose the BPM* algorithm, which is an implementation of subdimensional expansion with bypass in M*. In the experiments, we show that BPM* outperforms the state-of-the-art in solving several MAPF benchmark problems. △ Less","29 July, 2022",https://arxiv.org/pdf/2207.14657
Active Distribution System Coordinated Control Method via Artificial Intelligence,Matthew Lau;Kayla Thames;Sakis Meliopoulos,"The increasing deployment of end use power resources in distribution systems created active distribution systems. Uncontrolled active distribution systems exhibit wide variations of voltage and loading throughout the day as some of these resources operate under max power tracking control of highly variable wind and solar irradiation while others exhibit random variations and/or dependency on weather conditions. It is necessary to control the system to provide power reliably and securely under normal voltages and frequency. Classical optimization approaches to control the system towards this goal suffer from the dimensionality of the problem and the need for a global optimization approach to coordinate a huge number of small resources. Artificial Intelligence (AI) methods offer an alternative that can provide a practical approach to this problem. We suggest that neural networks with self-attention mechanisms have the potential to aid in the optimization of the system. In this paper, we present this approach and provide promising preliminary results. △ Less","12 July, 2022",https://arxiv.org/pdf/2207.14642
Large Language Models and the Reverse Turing Test,Terrence Sejnowski,"Large Language Models (LLMs) have been transformative. They are pre-trained foundational models that are self-supervised and can be adapted with fine tuning to a wide range of natural language tasks, each of which previously would have required a separate network model. This is one step closer to the extraordinary versatility of human language. GPT-3 and more recently LaMDA can carry on dialogs with humans on many topics after minimal priming with a few examples. However, there has been a wide range of reactions and debate on whether these LLMs understand what they are saying or exhibit signs of intelligence. This high variance is exhibited in three interviews with LLMs reaching wildly different conclusions. A new possibility was uncovered that could explain this divergence. What appears to be intelligence in LLMs may in fact be a mirror that reflects the intelligence of the interviewer, a remarkable twist that could be considered a Reverse Turing Test. If so, then by studying interviews we may be learning more about the intelligence and beliefs of the interviewer than the intelligence of the LLMs. As LLMs become more capable they may transform the way we interact with machines and how they interact with each other. Increasingly, LLMs are being coupled with sensorimotor devices. LLMs can talk the talk, but can they walk the walk? A road map for achieving artificial general autonomy is outlined with seven major improvements inspired by brain systems. LLMs could be used to uncover new insights into brain function by downloading brain data during natural behaviors. △ Less","15 November, 2022",https://arxiv.org/pdf/2207.14382
Latent Properties of Lifelong Learning Systems,Corban Rivera;Chace Ashcraft;Alexander New;James Schmidt;Gautam Vallabha,"Creating artificial intelligence (AI) systems capable of demonstrating lifelong learning is a fundamental challenge, and many approaches and metrics have been proposed to analyze algorithmic properties. However, for existing lifelong learning metrics, algorithmic contributions are confounded by task and scenario structure. To mitigate this issue, we introduce an algorithm-agnostic explainable surrogate-modeling approach to estimate latent properties of lifelong learning algorithms. We validate the approach for estimating these properties via experiments on synthetic data. To validate the structure of the surrogate model, we analyze real performance data from a collection of popular lifelong learning approaches and baselines adapted for lifelong classification and lifelong reinforcement learning. △ Less","28 July, 2022",https://arxiv.org/pdf/2207.14378
Towards the Neuroevolution of Low-level Artificial General Intelligence,Sidney Pontes-Filho;Kristoffer Olsen;Anis Yazidi;Michael A. Riegler;Pål Halvorsen;Stefano Nichele,"In this work, we argue that the search for Artificial General Intelligence (AGI) should start from a much lower level than human-level intelligence. The circumstances of intelligent behavior in nature resulted from an organism interacting with its surrounding environment, which could change over time and exert pressure on the organism to allow for learning of new behaviors or environment models. Our hypothesis is that learning occurs through interpreting sensory feedback when an agent acts in an environment. For that to happen, a body and a reactive environment are needed. We evaluate a method to evolve a biologically-inspired artificial neural network that learns from environment reactions named Neuroevolution of Artificial General Intelligence (NAGI), a framework for low-level AGI. This method allows the evolutionary complexification of a randomly-initialized spiking neural network with adaptive synapses, which controls agents instantiated in mutable environments. Such a configuration allows us to benchmark the adaptivity and generality of the controllers. The chosen tasks in the mutable environments are food foraging, emulation of logic gates, and cart-pole balancing. The three tasks are successfully solved with rather small network topologies and therefore it opens up the possibility of experimenting with more complex tasks and scenarios where curriculum learning is beneficial. △ Less","27 July, 2022",https://arxiv.org/pdf/2207.13583
A Proper Orthogonal Decomposition approach for parameters reduction of Single Shot Detector networks,Laura Meneghetti;Nicola Demo;Gianluigi Rozza,"As a major breakthrough in artificial intelligence and deep learning, Convolutional Neural Networks have achieved an impressive success in solving many problems in several fields including computer vision and image processing. Real-time performance, robustness of algorithms and fast training processes remain open problems in these contexts. In addition object recognition and detection are challenging tasks for resource-constrained embedded systems, commonly used in the industrial sector. To overcome these issues, we propose a dimensionality reduction framework based on Proper Orthogonal Decomposition, a classical model order reduction technique, in order to gain a reduction in the number of hyperparameters of the net. We have applied such framework to SSD300 architecture using PASCAL VOC dataset, demonstrating a reduction of the network dimension and a remarkable speedup in the fine-tuning of the network in a transfer learning context. △ Less","27 July, 2022",https://arxiv.org/pdf/2207.13551
Lecture Notes on Neural Information Retrieval,Nicola Tonellotto,"These lecture notes focus on the recent advancements in neural information retrieval, with particular emphasis on the systems and models exploiting transformer networks. These networks, originally proposed by Google in 2017, have seen a large success in many natural language processing and information retrieval tasks. While there are many fantastic textbook on information retrieval and natural language processing as well as specialised books for a more advanced audience, these lecture notes target people aiming at developing a basic understanding of the main information retrieval techniques and approaches based on deep learning. These notes have been prepared for a IR graduate course of the MSc program in Artificial Intelligence and Data Engineering at the University of Pisa, Italy. △ Less","12 September, 2022",https://arxiv.org/pdf/2207.13443
Applied Computer Vision on 2-Dimensional Lung X-Ray Images for Assisted Medical Diagnosis of Pneumonia,Ralph Joseph S. D. Ligueran;Manuel Luis C. Delos Santos;Ronaldo S. Tinio;Emmanuel H. Valencia,"This study focuses on the application of a specific subfield of artificial intelligence referred to as computer vision in the analysis of 2-dimensional lung x-ray images for the assisted medical diagnosis of ordinary pneumonia. A convolutional neural network algorithm was implemented in a Python-coded, Flask-based web application that can analyze x-ray images for the detection of ordinary pneumonia. Since convolutional neural network algorithms rely on machine learning for the identification and detection of patterns, a technique referred to as transfer learning was implemented to train the neural network in the identification and detection of patterns within the dataset. Open-source lung x-ray images were used as training data to create a knowledge base that served as the core element of the web application and the experimental design employed a 5-Trial Confirmatory Test for the validation of the web application. The results of the 5-Trial Confirmatory Test show the calculation of Diagnostic Precision Percentage per Trial, General Diagnostic Precision Percentage, and General Diagnostic Error Percentage while the Confusion Matrix further shows the relationship between the label and the corresponding diagnosis result of the web application on each test images. The developed web application can be used by medical practitioners in A.I.-assisted diagnosis of ordinary pneumonia, and by researchers in the fields of computer science and bioinformatics. △ Less","27 July, 2022",https://arxiv.org/pdf/2207.13295
Evaluation of key impression of resilient supply chain based on artificial intelligence of things (AIoT),Alireza Aliahmadi;Hamed Nozari;Javid Ghahremani-Nahr;Agnieszka Szmelter-Jarosz,"In recent years, the high complexity of the business environment, dynamism and environmental change, uncertainty and concepts such as globalization and increasing competition of organizations in the national and international arena have caused many changes in the equations governing the supply chain. In this case, supply chain organizations must always be prepared for a variety of challenges and dynamic environmental changes. One of the effective solutions to face these challenges is to create a resilient supply chain. Resilient supply chain is able to overcome uncertainties and disruptions in the business environment. The competitive advantage of this supply chain does not depend only on low costs, high quality, reduced latency and high level of service. Rather, it has the ability of the chain to avoid catastrophes and overcome critical situations, and this is the resilience of the supply chain. AI and IoT technologies and their combination, called AIoT, have played a key role in improving supply chain performance in recent years and can therefore increase supply chain resilience. For this reason, in this study, an attempt was made to better understand the impact of these technologies on equity by examining the dimensions and components of the Artificial Intelligence of Things (AIoT)-based supply chain. Finally, using nonlinear fuzzy decision making method, the most important components of the impact on the resilient smart supply chain are determined. Understanding this assessment can help empower the smart supply chain. △ Less","18 July, 2022",https://arxiv.org/pdf/2207.13174
Video Manipulations Beyond Faces: A Dataset with Human-Machine Analysis,Trisha Mittal;Ritwik Sinha;Viswanathan Swaminathan;John Collomosse;Dinesh Manocha,"As tools for content editing mature, and artificial intelligence (AI) based algorithms for synthesizing media grow, the presence of manipulated content across online media is increasing. This phenomenon causes the spread of misinformation, creating a greater need to distinguish between ``real'' and ``manipulated'' content. To this end, we present VideoSham, a dataset consisting of 826 videos (413 real and 413 manipulated). Many of the existing deepfake datasets focus exclusively on two types of facial manipulations -- swapping with a different subject's face or altering the existing face. VideoSham, on the other hand, contains more diverse, context-rich, and human-centric, high-resolution videos manipulated using a combination of 6 different spatial and temporal attacks. Our analysis shows that state-of-the-art manipulation detection algorithms only work for a few specific attacks and do not scale well on VideoSham. We performed a user study on Amazon Mechanical Turk with 1200 participants to understand if they can differentiate between the real and manipulated videos in VideoSham. Finally, we dig deeper into the strengths and weaknesses of performances by humans and SOTA-algorithms to identify gaps that need to be filled with better AI algorithms. We present the dataset at https://github.com/adobe-research/VideoSham-dataset. △ Less","7 December, 2022",https://arxiv.org/pdf/2207.13064
From Interpretable Filters to Predictions of Convolutional Neural Networks with Explainable Artificial Intelligence,Shagufta Henna;Juan Miguel Lopez Alcaraz,"Convolutional neural networks (CNN) are known for their excellent feature extraction capabilities to enable the learning of models from data, yet are used as black boxes. An interpretation of the convolutional filtres and associated features can help to establish an understanding of CNN to distinguish various classes. In this work, we focus on the explainability of a CNN model called as cnnexplain that is used for Covid-19 and non-Covid-19 classification with a focus on the interpretability of features by the convolutional filters, and how these features contribute to classification. Specifically, we have used various explainable artificial intelligence (XAI) methods, such as visualizations, SmoothGrad, Grad-CAM, and LIME to provide interpretation of convolutional filtres, and relevant features, and their role in classification. We have analyzed the explanation of these methods for Covid-19 detection using dry cough spectrograms. Explanation results obtained from the LIME, SmoothGrad, and Grad-CAM highlight important features of different spectrograms and their relevance to classification. △ Less","26 July, 2022",https://arxiv.org/pdf/2207.12958
Bodily Behaviors in Social Interaction: Novel Annotations and State-of-the-Art Evaluation,Michal Balazia;Philipp Müller;Ákos Levente Tánczos;August von Liechtenstein;François Brémond,"Body language is an eye-catching social signal and its automatic analysis can significantly advance artificial intelligence systems to understand and actively participate in social interactions. While computer vision has made impressive progress in low-level tasks like head and body pose estimation, the detection of more subtle behaviors such as gesturing, grooming, or fumbling is not well explored. In this paper we present BBSI, the first set of annotations of complex Bodily Behaviors embedded in continuous Social Interactions in a group setting. Based on previous work in psychology, we manually annotated 26 hours of spontaneous human behavior in the MPIIGroupInteraction dataset with 15 distinct body language classes. We present comprehensive descriptive statistics on the resulting dataset as well as results of annotation quality evaluations. For automatic detection of these behaviors, we adapt the Pyramid Dilated Attention Network (PDAN), a state-of-the-art approach for human action detection. We perform experiments using four variants of spatial-temporal features as input to PDAN: Two-Stream Inflated 3D CNN, Temporal Segment Networks, Temporal Shift Module and Swin Transformer. Results are promising and indicate a great room for improvement in this difficult task. Representing a key piece in the puzzle towards automatic understanding of social behavior, BBSI is fully available to the research community. △ Less","7 December, 2022",https://arxiv.org/pdf/2207.12817
SPAIC: A Spike-based Artificial Intelligence Computing Framework,Chaofei Hong;Mengwen Yuan;Mengxiao Zhang;Xiao Wang;Chegnjun Zhang;Jiaxin Wang;Gang Pan;Zhaohui Wu;Huajin Tang,"Neuromorphic computing is an emerging research field that aims to develop new intelligent systems by integrating theories and technologies from multi-disciplines such as neuroscience and deep learning. Currently, there have been various software frameworks developed for the related fields, but there is a lack of an efficient framework dedicated for spike-based computing models and algorithms. In this work, we present a Python based spiking neural network (SNN) simulation and training framework, aka SPAIC that aims to support brain-inspired model and algorithm researches integrated with features from both deep learning and neuroscience. To integrate different methodologies from the two overwhelming disciplines, and balance between flexibility and efficiency, SPAIC is designed with neuroscience-style frontend and deep learning backend structure. We provide a wide range of examples including neural circuits Simulation, deep SNN learning and neuromorphic applications, demonstrating the concise coding style and wide usability of our framework. The SPAIC is a dedicated spike-based artificial intelligence computing platform, which will significantly facilitate the design, prototype and validation of new models, theories and applications. Being user-friendly, flexible and high-performance, it will help accelerate the rapid growth and wide applicability of neuromorphic computing research. △ Less","26 July, 2022",https://arxiv.org/pdf/2207.12750
"OpenRAN Gym: AI/ML Development, Data Collection, and Testing for O-RAN on PAWR Platforms",Leonardo Bonati;Michele Polese;Salvatore D'Oro;Stefano Basagni;Tommaso Melodia,"Open Radio Access Network (RAN) architectures will enable interoperability, openness and programmable data-driven control in next generation cellular networks. However, developing and testing efficient solutions that generalize across heterogeneous cellular deployments and scales, and that optimize network performance in such diverse environments is a complex task that is still largely unexplored. In this paper we present OpenRAN Gym, a unified, open, and O-RAN-compliant experimental toolbox for data collection, design, prototyping and testing of end-to-end data-driven control solutions for next generation Open RAN systems. OpenRAN Gym extends and combines into a unique solution several software frameworks for data collection of RAN statistics and RAN control, and a lightweight O-RAN near-real-time RAN Intelligent Controller (RIC) tailored to run on experimental wireless platforms. We first provide an overview of the various architectural components of OpenRAN Gym and describe how it is used to collect data and design, train and test artificial intelligence and machine learning O-RAN-compliant applications (xApps) at scale. We then describe in detail how to test the developed xApps on softwarized RANs and provide an example of two xApps developed with OpenRAN Gym that are used to control a network with 7 base stations and 42 users deployed on the Colosseum testbed. Finally, we show how solutions developed with OpenRAN Gym on Colosseum can be exported to real-world, heterogeneous wireless platforms, such as the Arena testbed and the POWDER and COSMOS platforms of the PAWR program. OpenRAN Gym and its software components are open-source and publicly-available to the research community. By guiding the readers through running experiments with OpenRAN Gym, we aim at providing a key reference for researchers and practitioners working on experimental Open RAN systems. △ Less","17 December, 2022",https://arxiv.org/pdf/2207.12362
Unsupervised Hebbian Learning on Point Sets in StarCraft II,Beomseok Kang;Harshit Kumar;Saurabh Dash;Saibal Mukhopadhyay,"Learning the evolution of real-time strategy (RTS) game is a challenging problem in artificial intelligent (AI) system. In this paper, we present a novel Hebbian learning method to extract the global feature of point sets in StarCraft II game units, and its application to predict the movement of the points. Our model includes encoder, LSTM, and decoder, and we train the encoder with the unsupervised learning method. We introduce the concept of neuron activity aware learning combined with k-Winner-Takes-All. The optimal value of neuron activity is mathematically derived, and experiments support the effectiveness of the concept over the downstream task. Our Hebbian learning rule benefits the prediction with lower loss compared to self-supervised learning. Also, our model significantly saves the computational cost such as activations and FLOPs compared to a frame-based approach. △ Less","13 July, 2022",https://arxiv.org/pdf/2207.12323
Estimación de áreas de cultivo mediante Deep Learning y programación convencional,Javier Caicedo;Pamela Acosta;Romel Pozo;Henry Guilcapi;Christian Mejia-Escobar,"Artificial Intelligence has enabled the implementation of more accurate and efficient solutions to problems in various areas. In the agricultural sector, one of the main needs is to know at all times the extent of land occupied or not by crops in order to improve production and profitability. The traditional methods of calculation demand the collection of data manually and in person in the field, causing high labor costs, execution times, and inaccuracy in the results. The present work proposes a new method based on Deep Learning techniques complemented with conventional programming for the determination of the area of populated and unpopulated crop areas. We have considered as a case study one of the most recognized companies in the planting and harvesting of sugar cane in Ecuador. The strategy combines a Generative Adversarial Neural Network (GAN) that is trained on a dataset of aerial photographs of natural and urban landscapes to improve image resolution; a Convolutional Neural Network (CNN) trained on a dataset of aerial photographs of sugar cane plots to distinguish populated or unpopulated crop areas; and a standard image processing module for the calculation of areas in a percentage manner. The experiments performed demonstrate a significant improvement in the quality of the aerial photographs as well as a remarkable differentiation between populated and unpopulated crop areas, consequently, a more accurate result of cultivated and uncultivated areas. The proposed method can be extended to the detection of possible pests, areas of weed vegetation, dynamic crop development, and both qualitative and quantitative quality control. △ Less","25 July, 2022",https://arxiv.org/pdf/2207.12310
Lifelong Machine Learning of Functionally Compositional Structures,Jorge A. Mendez,"A hallmark of human intelligence is the ability to construct self-contained chunks of knowledge and reuse them in novel combinations for solving different problems. Learning such compositional structures has been a challenge for artificial systems, due to the underlying combinatorial search. To date, research into compositional learning has largely proceeded separately from work on lifelong or continual learning. This dissertation integrated these two lines of work to present a general-purpose framework for lifelong learning of functionally compositional structures. The framework separates the learning into two stages: learning how to combine existing components to assimilate a novel problem, and learning how to adapt the existing components to accommodate the new problem. This separation explicitly handles the trade-off between stability and flexibility. This dissertation instantiated the framework into various supervised and reinforcement learning (RL) algorithms. Supervised learning evaluations found that 1) compositional models improve lifelong learning of diverse tasks, 2) the multi-stage process permits lifelong learning of compositional knowledge, and 3) the components learned by the framework represent self-contained and reusable functions. Similar RL evaluations demonstrated that 1) algorithms under the framework accelerate the discovery of high-performing policies, and 2) these algorithms retain or improve performance on previously learned tasks. The dissertation extended one lifelong compositional RL algorithm to the nonstationary setting, where the task distribution varies over time, and found that modularity permits individually tracking changes to different elements in the environment. The final contribution of this dissertation was a new benchmark for compositional RL, which exposed that existing methods struggle to discover the compositional properties of the environment. △ Less","25 July, 2022",https://arxiv.org/pdf/2207.12256
Towards Systems Education for Artificial Intelligence: A Course Practice in Intelligent Computing Architectures,Jianlei Yang;Xiaopeng Gao;Weisheng Zhao,"With the rapid development of artificial intelligence (AI) community, education in AI is receiving more and more attentions. There have been many AI related courses in the respects of algorithms and applications, while not many courses in system level are seriously taken into considerations. In order to bridge the gap between AI and computing systems, we are trying to explore how to conduct AI education from the perspective of computing systems. In this paper, a course practice in intelligent computing architectures are provided to demonstrate the system education in AI era. The motivation for this course practice is first introduced as well as the learning orientations. The main goal of this course aims to teach students for designing AI accelerators on FPGA platforms. The elaborated course contents include lecture notes and related technical materials. Especially several practical labs and projects are detailed illustrated. Finally, some teaching experiences and effects are discussed as well as some potential improvements in the future. △ Less","22 June, 2022",https://arxiv.org/pdf/2207.12229
PirouNet: Creating Dance through Artist-Centric Deep Learning,Mathilde Papillon;Mariel Pettee;Nina Miolane,"Using Artificial Intelligence (AI) to create dance choreography with intention is still at an early stage. Methods that conditionally generate dance sequences remain limited in their ability to follow choreographer-specific creative direction, often relying on external prompts or supervised learning. In the same vein, fully annotated dance datasets are rare and labor intensive. To fill this gap and help leverage deep learning as a meaningful tool for choreographers, we propose ""PirouNet"", a semi-supervised conditional recurrent variational autoencoder together with a dance labeling web application. PirouNet allows dance professionals to annotate data with their own subjective creative labels and subsequently generate new bouts of choreography based on their aesthetic criteria. Thanks to the proposed semi-supervised approach, PirouNet only requires a small portion of the dataset to be labeled, typically on the order of 1%. We demonstrate PirouNet's capabilities as it generates original choreography based on the ""Laban Time Effort"", an established dance notion describing intention for a movement's time dynamics. We extensively evaluate PirouNet's dance creations through a series of qualitative and quantitative metrics, validating its applicability as a tool for choreographers. △ Less","14 October, 2022",https://arxiv.org/pdf/2207.12126
Multi-Scale Asset Distribution Model for Dynamic Environments,Payam Zahadat;Ada Diaconescu,"In many self-organising systems the ability to extract necessary resources from the external environment is essential to the system's growth and survival. Examples include the extraction of sunlight and nutrients in organic plants, of monetary income in business organisations and of mobile robots in swarm intelligence actions. When operating within competitive, ever-changing environments, such systems must distribute their internal assets wisely so as to improve and adapt their ability to extract available resources. As the system size increases, the asset-distribution process often gets organised around a multi-scale control topology. This topology may be static (fixed) or dynamic (enabling growth and structural adaptation) depending on the system's internal constraints and adaptive mechanisms. In this paper, we expand on a plant-inspired asset-distribution model and introduce a more general multi-scale model applicable across a wider range of natural and artificial system domains. We study the impact that the topology of the multi-scale control process has upon the system's ability to self-adapt asset distribution when resource availability changes within the environment. Results show how different topological characteristics and different competition levels between system branches impact overall system profitability, adaptation delays and disturbances when environmental changes occur. These findings provide a basis for system designers to select the most suitable topology and configuration for their particular application and execution environment. △ Less","25 July, 2022",https://arxiv.org/pdf/2207.12063
Flowsheet synthesis through hierarchical reinforcement learning and graph neural networks,Laura Stops;Roel Leenhouts;Qinghe Gao;Artur M. Schweidtmann,"Process synthesis experiences a disruptive transformation accelerated by digitization and artificial intelligence. We propose a reinforcement learning algorithm for chemical process design based on a state-of-the-art actor-critic logic. Our proposed algorithm represents chemical processes as graphs and uses graph convolutional neural networks to learn from process graphs. In particular, the graph neural networks are implemented within the agent architecture to process the states and make decisions. Moreover, we implement a hierarchical and hybrid decision-making process to generate flowsheets, where unit operations are placed iteratively as discrete decisions and corresponding design variables are selected as continuous decisions. We demonstrate the potential of our method to design economically viable flowsheets in an illustrative case study comprising equilibrium reactions, azeotropic separation, and recycles. The results show quick learning in discrete, continuous, and hybrid action spaces. Due to the flexible architecture of the proposed reinforcement learning agent, the method is predestined to include large action-state spaces and an interface to process simulators in future research. △ Less","25 July, 2022",https://arxiv.org/pdf/2207.12051
Collaboration in Participant-Centric Federated Learning: A Game-Theoretical Perspective,Guangjing Huang;Xu Chen;Tao Ouyang;Qian Ma;Lin Chen;Junshan Zhang,"Federated learning (FL) is a promising distributed framework for collaborative artificial intelligence model training while protecting user privacy. A bootstrapping component that has attracted significant research attention is the design of incentive mechanism to stimulate user collaboration in FL. The majority of works adopt a broker-centric approach to help the central operator to attract participants and further obtain a well-trained model. Few works consider forging participant-centric collaboration among participants to pursue an FL model for their common interests, which induces dramatic differences in incentive mechanism design from the broker-centric FL. To coordinate the selfish and heterogeneous participants, we propose a novel analytic framework for incentivizing effective and efficient collaborations for participant-centric FL. Specifically, we respectively propose two novel game models for contribution-oblivious FL (COFL) and contribution-aware FL (CAFL), where the latter one implements a minimum contribution threshold mechanism. We further analyze the uniqueness and existence for Nash equilibrium of both COFL and CAFL games and design efficient algorithms to achieve equilibrium solutions. Extensive performance evaluations show that there exists free-riding phenomenon in COFL, which can be greatly alleviated through the adoption of CAFL model with the optimized minimum threshold. △ Less","25 July, 2022",https://arxiv.org/pdf/2207.12030
AI Powered Anti-Cyber Bullying System using Machine Learning Algorithm of Multinomial Naive Bayes and Optimized Linear Support Vector Machine,Tosin Ige;Sikiru Adewale,"""Unless and until our society recognizes cyber bullying for what it is, the suffering of thousands of silent victims will continue."" ~ Anna Maria Chavez. There had been series of research on cyber bullying which are unable to provide reliable solution to cyber bullying. In this research work, we were able to provide a permanent solution to this by developing a model capable of detecting and intercepting bullying incoming and outgoing messages with 92% accuracy. We also developed a chatbot automation messaging system to test our model leading to the development of Artificial Intelligence powered anti-cyber bullying system using machine learning algorithm of Multinomial Naive Bayes (MNB) and optimized linear Support Vector Machine (SVM). Our model is able to detect and intercept bullying outgoing and incoming bullying messages and take immediate action. △ Less","25 July, 2022",https://arxiv.org/pdf/2207.11897
RGB-D Robotic Pose Estimation For a Servicing Robotic Arm,Jared Herron;Daniel Lopez;Jarred Jordan;Jillian Rudy;Aryslan Malik;Daniel Posada;Mehran Andalibi;Troy Henderson,"A large number of robotic and human-assisted missions to the Moon and Mars are forecast. NASA's efforts to learn about the geology and makeup of these celestial bodies rely heavily on the use of robotic arms. The safety and redundancy aspects will be crucial when humans will be working alongside the robotic explorers. Additionally, robotic arms are crucial to satellite servicing and planned orbit debris mitigation missions. The goal of this work is to create a custom Computer Vision (CV) based Artificial Neural Network (ANN) that would be able to rapidly identify the posture of a 7 Degree of Freedom (DoF) robotic arm from a single (RGB-D) image - just like humans can easily identify if an arm is pointing in some general direction. The Sawyer robotic arm is used for developing and training this intelligent algorithm. Since Sawyer's joint space spans 7 dimensions, it is an insurmountable task to cover the entire joint configuration space. In this work, orthogonal arrays are used, similar to the Taguchi method, to efficiently span the joint space with the minimal number of training images. This ``optimally'' generated database is used to train the custom ANN and its degree of accuracy is on average equal to twice the smallest joint displacement step used for database generation. A pre-trained ANN will be useful for estimating the postures of robotic manipulators used on space stations, spacecraft, and rovers as an auxiliary tool or for contingency plans. △ Less","23 July, 2022",https://arxiv.org/pdf/2207.11537
Towards Smart Fake News Detection Through Explainable AI,Athira A B;S D Madhu Kumar;Anu Mary Chacko,"People now see social media sites as their sole source of information due to their popularity. The Majority of people get their news through social media. At the same time, fake news has grown exponentially on social media platforms in recent years. Several artificial intelligence-based solutions for detecting fake news have shown promising results. On the other hand, these detection systems lack explanation capabilities, i.e., the ability to explain why they made a prediction. This paper highlights the current state of the art in explainable fake news detection. We discuss the pitfalls in the current explainable AI-based fake news detection models and present our ongoing research on multi-modal explainable fake news detection model. △ Less","23 July, 2022",https://arxiv.org/pdf/2207.11490
Machine Learning Modeling to Evaluate the Value of Football Players,Chenyao Li;Stylianos Kampakis;Philip Treleaven,"In most sports, especially football, most coaches and analysts search for key performance indicators using notational analysis. This method utilizes a statistical summary of events based on video footage and numerical records of goal scores. Unfortunately, this approach is now obsolete owing to the continuous evolutionary increase in technology that simplifies the analysis of more complex process variables through machine learning (ML). Machine learning, a form of artificial intelligence (AI), uses algorithms to detect meaningful patterns and define a structure based on positional data. This research investigates a new method to evaluate the value of current football players, based on establishing the machine learning models to investigate the relations among the various features of players, the salary of players, and the market value of players. The data of the football players used for this project is from several football websites. The data on the salary of football players will be the proxy for evaluating the value of players, and other features will be used to establish and train the ML model for predicting the suitable salary for the players. The motivation is to explore what are the relations between different features of football players and their salaries - how each feature affects their salaries, or which are the most important features to affect the salary? Although many standards can reflect the value of football players, the salary of the players is one of the most intuitive and crucial indexes, so this study will use the salary of players as the proxy to evaluate their value. Moreover, many features of players can affect the valuation of the football players, but the value of players is mainly decided by three types of factors: basic characteristics, performance on the court, and achievements at the club. △ Less","22 July, 2022",https://arxiv.org/pdf/2207.11361
VTrackIt: A Synthetic Self-Driving Dataset with Infrastructure and Pooled Vehicle Information,Mayuresh Savargaonkar;Abdallah Chehade,"Artificial intelligence solutions for Autonomous Vehicles (AVs) have been developed using publicly available datasets such as Argoverse, ApolloScape, Level5, and NuScenes. One major limitation of these datasets is the absence of infrastructure and/or pooled vehicle information like lane line type, vehicle speed, traffic signs, and intersections. Such information is necessary and not complementary to eliminating high-risk edge cases. The rapid advancements in Vehicle-to-Infrastructure and Vehicle-to-Vehicle technologies show promise that infrastructure and pooled vehicle information will soon be accessible in near real-time. Taking a leap in the future, we introduce the first comprehensive synthetic dataset with intelligent infrastructure and pooled vehicle information for advancing the next generation of AVs, named VTrackIt. We also introduce the first deep learning model (InfraGAN) for trajectory predictions that considers such information. Our experiments with InfraGAN show that the comprehensive information offered by VTrackIt reduces the number of high-risk edge cases. The VTrackIt dataset is available upon request under the Creative Commons CC BY-NC-SA 4.0 license at http://vtrackit.irda.club. △ Less","15 July, 2022",https://arxiv.org/pdf/2207.11146
Near Real-Time Distributed State Estimation via AI/ML-Empowered 5G Networks,Ognjen Kundacina;Miodrag Forcan;Mirsad Cosovic;Darijo Raca;Merim Dzaferagic;Dragisa Miskovic;Mirjana Maksimovic;Dejan Vukobratovic,"Fifth-Generation (5G) networks have a potential to accelerate power system transition to a flexible, softwarized, data-driven, and intelligent grid. With their evolving support for Machine Learning (ML)/Artificial Intelligence (AI) functions, 5G networks are expected to enable novel data-centric Smart Grid (SG) services. In this paper, we explore how data-driven SG services could be integrated with ML/AI-enabled 5G networks in a symbiotic relationship. We focus on the State Estimation (SE) function as a key element of the energy management system and focus on two main questions. Firstly, in a tutorial fashion, we present an overview on how distributed SE can be integrated with the elements of the 5G core network and radio access network architecture. Secondly, we present and compare two powerful distributed SE methods based on: i) graphical models and belief propagation, and ii) graph neural networks. We discuss their performance and capability to support a near real-time distributed SE via 5G network, taking into account communication delays. △ Less","22 July, 2022",https://arxiv.org/pdf/2207.11117
Rapid Lung Ultrasound COVID-19 Severity Scoring with Resource-Efficient Deep Feature Extraction,Pierre Raillard;Lorenzo Cristoni;Andrew Walden;Roberto Lazzari;Thomas Pulimood;Louis Grandjean;Claudia AM Gandini Wheeler-Kingshott;Yipeng Hu;Zachary MC Baum,"Artificial intelligence-based analysis of lung ultrasound imaging has been demonstrated as an effective technique for rapid diagnostic decision support throughout the COVID-19 pandemic. However, such techniques can require days- or weeks-long training processes and hyper-parameter tuning to develop intelligent deep learning image analysis models. This work focuses on leveraging 'off-the-shelf' pre-trained models as deep feature extractors for scoring disease severity with minimal training time. We propose using pre-trained initializations of existing methods ahead of simple and compact neural networks to reduce reliance on computational capacity. This reduction of computational capacity is of critical importance in time-limited or resource-constrained circumstances, such as the early stages of a pandemic. On a dataset of 49 patients, comprising over 20,000 images, we demonstrate that the use of existing methods as feature extractors results in the effective classification of COVID-19-related pneumonia severity while requiring only minutes of training time. Our methods can achieve an accuracy of over 0.93 on a 4-level severity score scale and provides comparable per-patient region and global scores compared to expert annotated ground truths. These results demonstrate the capability for rapid deployment and use of such minimally-adapted methods for progress monitoring, patient stratification and management in clinical practice for COVID-19 patients, and potentially in other respiratory diseases. △ Less","22 July, 2022",https://arxiv.org/pdf/2207.10998
A Reinforcement Learning-based Offensive semantics Censorship System for Chatbots,Shaokang Cai;Dezhi Han;Zibin Zheng;Dun Li;NoelCrespi,"The rapid development of artificial intelligence (AI) technology has enabled large-scale AI applications to land in the market and practice. However, while AI technology has brought many conveniences to people in the productization process, it has also exposed many security issues. Especially, attacks against online learning vulnerabilities of chatbots occur frequently. Therefore, this paper proposes a semantics censorship chatbot system based on reinforcement learning, which is mainly composed of two parts: the Offensive semantics censorship model and the semantics purification model. Offensive semantics review can combine the context of user input sentences to detect the rapid evolution of Offensive semantics and respond to Offensive semantics responses. The semantics purification model For the case of chatting robot models, it has been contaminated by large numbers of offensive semantics, by strengthening the offensive reply learned by the learning algorithm, rather than rolling back to the early versions. In addition, by integrating a once-through learning approach, the speed of semantics purification is accelerated while reducing the impact on the quality of replies. The experimental results show that our proposed approach reduces the probability of the chat model generating offensive replies and that the integration of the few-shot learning algorithm improves the training speed rapidly while effectively slowing down the decline in BLEU values. △ Less","13 July, 2022",https://arxiv.org/pdf/2207.10569
LPYOLO: Low Precision YOLO for Face Detection on FPGA,Bestami Günay;Sefa Burak Okcu;Hasan Şakir Bilge,"In recent years, number of edge computing devices and artificial intelligence applications on them have advanced excessively. In edge computing, decision making processes and computations are moved from servers to edge devices. Hence, cheap and low power devices are required. FPGAs are very low power, inclined to do parallel operations and deeply suitable devices for running Convolutional Neural Networks (CNN) which are the fundamental unit of an artificial intelligence application. Face detection on surveillance systems is the most expected application on the security market. In this work, TinyYolov3 architecture is redesigned and deployed for face detection. It is a CNN based object detection method and developed for embedded systems. PYNQ-Z2 is selected as a target board which has low-end Xilinx Zynq 7020 System-on-Chip (SoC) on it. Redesigned TinyYolov3 model is defined in numerous bit width precisions with Brevitas library which brings fundamental CNN layers and activations in integer quantized form. Then, the model is trained in a quantized structure with WiderFace dataset. In order to decrease latency and power consumption, onchip memory of the FPGA is configured as a storage of whole network parameters and the last activation function is modified as rescaled HardTanh instead of Sigmoid. Also, high degree of parallelism is applied to logical resources of the FPGA. The model is converted to an HLS based application with using FINN framework and FINN-HLS library which includes the layer definitions in C++. Later, the model is synthesized and deployed. CPU of the SoC is employed with multithreading mechanism and responsible for preprocessing, postprocessing and TCP/IP streaming operations. Consequently, 2.4 Watt total board power consumption, 18 Frames-Per-Second (FPS) throughput and 0.757 mAP accuracy rate on Easy category of the WiderFace are achieved with 4 bits precision model. △ Less","21 July, 2022",https://arxiv.org/pdf/2207.10482
NSNet: Non-saliency Suppression Sampler for Efficient Video Recognition,Boyang Xia;Wenhao Wu;Haoran Wang;Rui Su;Dongliang He;Haosen Yang;Xiaoran Fan;Wanli Ouyang,"It is challenging for artificial intelligence systems to achieve accurate video recognition under the scenario of low computation costs. Adaptive inference based efficient video recognition methods typically preview videos and focus on salient parts to reduce computation costs. Most existing works focus on complex networks learning with video classification based objectives. Taking all frames as positive samples, few of them pay attention to the discrimination between positive samples (salient frames) and negative samples (non-salient frames) in supervisions. To fill this gap, in this paper, we propose a novel Non-saliency Suppression Network (NSNet), which effectively suppresses the responses of non-salient frames. Specifically, on the frame level, effective pseudo labels that can distinguish between salient and non-salient frames are generated to guide the frame saliency learning. On the video level, a temporal attention module is learned under dual video-level supervisions on both the salient and the non-salient representations. Saliency measurements from both two levels are combined for exploitation of multi-granularity complementary information. Extensive experiments conducted on four well-known benchmarks verify our NSNet not only achieves the state-of-the-art accuracy-efficiency trade-off but also present a significantly faster (2.4~4.3x) practical inference speed than state-of-the-art methods. Our project page is at https://lawrencexia2008.github.io/projects/nsnet . △ Less","21 July, 2022",https://arxiv.org/pdf/2207.10388
Reinforcement learning for Energies of the future and carbon neutrality: a Challenge Design,Gaëtan Serré;Eva Boguslawski;Benjamin Donnot;Adrien Pavão;Isabelle Guyon;Antoine Marot,"Current rapid changes in climate increase the urgency to change energy production and consumption management, to reduce carbon and other green-house gas production. In this context, the French electricity network management company RTE (R{é}seau de Transport d'{É}lectricit{é}) has recently published the results of an extensive study outlining various scenarios for tomorrow's French power management. We propose a challenge that will test the viability of such a scenario. The goal is to control electricity transportation in power networks, while pursuing multiple objectives: balancing production and consumption, minimizing energetic losses, and keeping people and equipment safe and particularly avoiding catastrophic failures. While the importance of the application provides a goal in itself, this challenge also aims to push the state-of-the-art in a branch of Artificial Intelligence (AI) called Reinforcement Learning (RL), which offers new possibilities to tackle control problems. In particular, various aspects of the combination of Deep Learning and RL called Deep Reinforcement Learning remain to be harnessed in this application domain. This challenge belongs to a series started in 2019 under the name ""Learning to run a power network"" (L2RPN). In this new edition, we introduce new more realistic scenarios proposed by RTE to reach carbon neutrality by 2050, retiring fossil fuel electricity production, increasing proportions of renewable and nuclear energy and introducing batteries. Furthermore, we provide a baseline using state-of-the-art reinforcement learning algorithm to stimulate the future participants. △ Less","21 July, 2022",https://arxiv.org/pdf/2207.10330
Unsupervised Legendre-Galerkin Neural Network for Singularly Perturbed Partial Differential Equations,Junho Choi;Namjung Kim;Youngjoon Hong,"Machine learning methods have been lately used to solve partial differential equations (PDEs) and dynamical systems. These approaches have been developed into a novel research field known as scientific machine learning in which techniques such as deep neural networks and statistical learning are applied to classical problems of applied mathematics. In this paper, we develop a novel numerical algorithm that incorporates machine learning and artificial intelligence to solve PDEs. Based on the Legendre-Galerkin framework, we propose the {\it unsupervised machine learning} algorithm to learn {\it multiple instances} of the solutions for different types of PDEs. Our approach overcomes the limitations of data-driven and physics-based methods. The proposed neural network is applied to general 1D and 2D PDEs with various boundary conditions as well as convection-dominated {\it singularly perturbed PDEs} that exhibit strong boundary layer behavior. △ Less","8 December, 2022",https://arxiv.org/pdf/2207.10241
Automated Kantian Ethics: A Faithful Implementation,Lavanya Singh,"As we grant artificial intelligence increasing power and independence in contexts like healthcare, policing, and driving, AI faces moral dilemmas but lacks the tools to solve them. Warnings from regulators, philosophers, and computer scientists about the dangers of unethical artificial intelligence have spurred interest in automated ethics-i.e., the development of machines that can perform ethical reasoning. However, prior work in automated ethics rarely engages with philosophical literature. Philosophers have spent centuries debating moral dilemmas so automated ethics will be most nuanced, consistent, and reliable when it draws on philosophical literature. In this paper, I present an implementation of automated Kantian ethics that is faithful to the Kantian philosophical tradition. I formalize Kant's categorical imperative in Dyadic Deontic Logic, implement this formalization in the Isabelle theorem prover, and develop a testing framework to evaluate how well my implementation coheres with expected properties of Kantian ethic. My system is an early step towards philosophically mature ethical AI agents and it can make nuanced judgements in complex ethical dilemmas because it is grounded in philosophical literature. Because I use an interactive theorem prover, my system's judgements are explainable. △ Less","20 July, 2022",https://arxiv.org/pdf/2207.10152
DDPG based on multi-scale strokes for financial time series trading strategy,Jun-Cheng Chen;Cong-Xiao Chen;Li-Juan Duan;Zhi Cai,"With the development of artificial intelligence,more and more financial practitioners apply deep reinforcement learning to financial trading strategies.However,It is difficult to extract accurate features due to the characteristics of considerable noise,highly non-stationary,and non-linearity of single-scale time series,which makes it hard to obtain high returns.In this paper,we extract a multi-scale feature matrix on multiple time scales of financial time series,according to the classic financial theory-Chan Theory,and put forward to an approach of multi-scale stroke deep deterministic policy gradient reinforcement learning model(MSSDDPG)to search for the optimal trading strategy.We carried out experiments on the datasets of the Dow Jones,S&P 500 of U.S. stocks, and China's CSI 300,SSE Composite,evaluate the performance of our approach compared with turtle trading strategy, Deep Q-learning(DQN)reinforcement learning strategy,and deep deterministic policy gradient (DDPG) reinforcement learning strategy.The result shows that our approach gets the best performance in China CSI 300,SSE Composite,and get an outstanding result in Dow Jones,S&P 500 of U.S. △ Less","5 June, 2022",https://arxiv.org/pdf/2207.10071
AI Fairness: from Principles to Practice,Arash Bateni;Matthew C. Chan;Ray Eitel-Porter,"This paper summarizes and evaluates various approaches, methods, and techniques for pursuing fairness in artificial intelligence (AI) systems. It examines the merits and shortcomings of these measures and proposes practical guidelines for defining, measuring, and preventing bias in AI. In particular, it cautions against some of the simplistic, yet common, methods for evaluating bias in AI systems, and offers more sophisticated and effective alternatives. The paper also addresses widespread controversies and confusions in the field by providing a common language among different stakeholders of high-impact AI systems. It describes various trade-offs involving AI fairness, and provides practical recommendations for balancing them. It offers techniques for evaluating the costs and benefits of fairness targets, and defines the role of human judgment in setting these targets. This paper provides discussions and guidelines for AI practitioners, organization leaders, and policymakers, as well as various links to additional materials for a more technical audience. Numerous real-world examples are provided to clarify the concepts, challenges, and recommendations from a practical perspective. △ Less","20 July, 2022",https://arxiv.org/pdf/2207.09833
Task Allocation using a Team of Robots,Haris Aziz;Arindam Pal;Ali Pourmiri;Fahimeh Ramezani;Brendan Sims,"Task allocation using a team or coalition of robots is one of the most important problems in robotics, computer science, operational research, and artificial intelligence. In recent work, research has focused on handling complex objectives and feasibility constraints amongst other variations of the multi-robot task allocation problem. There are many examples of important research progress in these directions. We present a general formulation of the task allocation problem that generalizes several versions that are well-studied. Our formulation includes the states of robots, tasks, and the surrounding environment in which they operate. We describe how the problem can vary depending on the feasibility constraints, objective functions, and the level of dynamically changing information. In addition, we discuss existing solution approaches for the problem including optimization-based approaches, and market-based approaches. △ Less","20 July, 2022",https://arxiv.org/pdf/2207.09650
Roadmap Towards Responsible AI in Crisis Resilience Management,Cheng-Chun Lee;Tina Comes;Megan Finn;Ali Mostafavi,"Novel data sensing and AI technologies are finding practical use in the analysis of crisis resilience, revealing the need to consider how responsible artificial intelligence (AI) practices can mitigate harmful outcomes and protect vulnerable populations. In this paper, we present a responsible AI roadmap that is embedded in the Crisis Information Management Circle. This roadmap includes six propositions to highlight and address important challenges and considerations specifically related to responsible AI for crisis resilience management. We cover a wide spectrum of interwoven challenges and considerations pertaining to the responsible collection, analysis, sharing, and use of information such as equity, fairness, biases, explainability and transparency, accountability, privacy and security, inter-organizational coordination, and public engagement. Through examining issues around AI systems for crisis resilience management, we dissect the inherent complexities of information management and decision-making in crises and highlight the urgency of responsible AI research and practice. The ideas laid out in this paper are the first attempt in establishing a roadmap for researchers, practitioners, developers, emergency managers, humanitarian organizations, and public officials to address important considerations for responsible AI pertaining to crisis resilience management. △ Less","8 September, 2022",https://arxiv.org/pdf/2207.09648
Mimetic Models: Ethical Implications of AI that Acts Like You,Reid McIlroy-Young;Jon Kleinberg;Siddhartha Sen;Solon Barocas;Ashton Anderson,"An emerging theme in artificial intelligence research is the creation of models to simulate the decisions and behavior of specific people, in domains including game-playing, text generation, and artistic expression. These models go beyond earlier approaches in the way they are tailored to individuals, and the way they are designed for interaction rather than simply the reproduction of fixed, pre-computed behaviors. We refer to these as mimetic models, and in this paper we develop a framework for characterizing the ethical and social issues raised by their growing availability. Our framework includes a number of distinct scenarios for the use of such models, and considers the impacts on a range of different participants, including the target being modeled, the operator who deploys the model, and the entities that interact with it. △ Less","19 July, 2022",https://arxiv.org/pdf/2207.09394
Alterfactual Explanations -- The Relevance of Irrelevance for Explaining AI Systems,Silvan Mertes;Christina Karle;Tobias Huber;Katharina Weitz;Ruben Schlagowski;Elisabeth André,"Explanation mechanisms from the field of Counterfactual Thinking are a widely-used paradigm for Explainable Artificial Intelligence (XAI), as they follow a natural way of reasoning that humans are familiar with. However, all common approaches from this field are based on communicating information about features or characteristics that are especially important for an AI's decision. We argue that in order to fully understand a decision, not only knowledge about relevant features is needed, but that the awareness of irrelevant information also highly contributes to the creation of a user's mental model of an AI system. Therefore, we introduce a new way of explaining AI systems. Our approach, which we call Alterfactual Explanations, is based on showing an alternative reality where irrelevant features of an AI's input are altered. By doing so, the user directly sees which characteristics of the input data can change arbitrarily without influencing the AI's decision. We evaluate our approach in an extensive user study, revealing that it is able to significantly contribute to the participants' understanding of an AI. We show that alterfactual explanations are suited to convey an understanding of different aspects of the AI's reasoning than established counterfactual explanation methods. △ Less","19 July, 2022",https://arxiv.org/pdf/2207.09374
Recognizing Hand Use and Hand Role at Home After Stroke from Egocentric Video,Meng-Fen Tsai;Rosalie H. Wang;Jośe Zariffa,"Introduction: Hand function is a central determinant of independence after stroke. Measuring hand use in the home environment is necessary to evaluate the impact of new interventions, and calls for novel wearable technologies. Egocentric video can capture hand-object interactions in context, as well as show how more-affected hands are used during bilateral tasks (for stabilization or manipulation). Automated methods are required to extract this information. Objective: To use artificial intelligence-based computer vision to classify hand use and hand role from egocentric videos recorded at home after stroke. Methods: Twenty-one stroke survivors participated in the study. A random forest classifier, a SlowFast neural network, and the Hand Object Detector neural network were applied to identify hand use and hand role at home. Leave-One-Subject-Out-Cross-Validation (LOSOCV) was used to evaluate the performance of the three models. Between-group differences of the models were calculated based on the Mathews correlation coefficient (MCC). Results: For hand use detection, the Hand Object Detector had significantly higher performance than the other models. The macro average MCCs using this model in the LOSOCV were 0.50 +- 0.23 for the more-affected hands and 0.58 +- 0.18 for the less-affected hands. Hand role classification had macro average MCCs in the LOSOCV that were close to zero for all models. Conclusion: Using egocentric video to capture the hand use of stroke survivors at home is feasible. Pose estimation to track finger movements may be beneficial to classifying hand roles in the future. △ Less","21 July, 2022",https://arxiv.org/pdf/2207.08920
A Survey of Decision Making in Adversarial Games,Xiuxian Li;Min Meng;Yiguang Hong;Jie Chen,"Game theory has by now found numerous applications in various fields, including economics, industry, jurisprudence, and artificial intelligence, where each player only cares about its own interest in a noncooperative or cooperative manner, but without obvious malice to other players. However, in many practical applications, such as poker, chess, evader pursuing, drug interdiction, coast guard, cyber-security, and national defense, players often have apparently adversarial stances, that is, selfish actions of each player inevitably or intentionally inflict loss or wreak havoc on other players. Along this line, this paper provides a systematic survey on three main game models widely employed in adversarial games, i.e., zero-sum normal-form and extensive-form games, Stackelberg (security) games, zero-sum differential games, from an array of perspectives, including basic knowledge of game models, (approximate) equilibrium concepts, problem classifications, research frontiers, (approximate) optimal strategy seeking techniques, prevailing algorithms, and practical applications. Finally, promising future research directions are also discussed for relevant adversarial games. △ Less","16 July, 2022",https://arxiv.org/pdf/2207.07971
Robust AI Driving Strategy for Autonomous Vehicles,Subramanya Nageshrao;Yousaf Rahman;Vladimir Ivanovic;Mrdjan Jankovic;Eric Tseng;Michael Hafner;Dimitar Filev,"There has been significant progress in sensing, perception, and localization for automated driving, However, due to the wide spectrum of traffic/road structure scenarios and the long tail distribution of human driver behavior, it has remained an open challenge for an intelligent vehicle to always know how to make and execute the best decision on road given available sensing / perception / localization information. In this chapter, we talk about how artificial intelligence and more specifically, reinforcement learning, can take advantage of operational knowledge and safety reflex to make strategical and tactical decisions. We discuss some challenging problems related to the robustness of reinforcement learning solutions and their implications to the practical design of driving strategies for autonomous vehicles. We focus on automated driving on highway and the integration of reinforcement learning, vehicle motion control, and control barrier function, leading to a robust AI driving strategy that can learn and adapt safely. △ Less","16 July, 2022",https://arxiv.org/pdf/2207.07829
On Scheduling Ring-All-Reduce Learning Jobs in Multi-Tenant GPU Clusters with Communication Contention,Menglu Yu;Bo Ji;Hridesh Rajan;Jia Liu,"Powered by advances in deep learning (DL) techniques, machine learning and artificial intelligence have achieved astonishing successes. However, the rapidly growing needs for DL also led to communication- and resource-intensive distributed training jobs for large-scale DL training, which are typically deployed over GPU clusters. To sustain the ever-increasing demand for DL training, the so-called ""ring-all-reduce"" (RAR) technologies have recently emerged as a favorable computing architecture to efficiently process network communication and computation load in GPU clusters. The most salient feature of RAR is that it removes the need for dedicated parameter servers, thus alleviating the potential communication bottleneck. However, when multiple RAR-based DL training jobs are deployed over GPU clusters, communication bottlenecks could still occur due to contentions between DL training jobs. So far, there remains a lack of theoretical understanding on how to design contention-aware resource scheduling algorithms for RAR-based DL training jobs, which motivates us to fill this gap in this work. Our main contributions are three-fold: i) We develop a new analytical model that characterizes both communication overhead related to the worker distribution of the job and communication contention related to the co-location of different jobs; ii) Based on the proposed analytical model, we formulate the problem as a non-convex integer program to minimize the makespan of all RAR-based DL training jobs. To address the unique structure in this problem that is not amenable for optimization algorithm design, we reformulate the problem into an integer linear program that enables provable approximation algorithm design called SJF-BCO (Smallest Job First with Balanced Contention and Overhead); and iii) We conduct extensive experiments to show the superiority of SJF-BCO over existing schedulers. △ Less","14 August, 2022",https://arxiv.org/pdf/2207.07817
A Survey on Collaborative DNN Inference for Edge Intelligence,Weiqing Ren;Yuben Qu;Chao Dong;Yuqian Jing;Hao Sun;Qihui Wu;Song Guo,"With the vigorous development of artificial intelligence (AI), the intelligent applications based on deep neural network (DNN) change people's lifestyles and the production efficiency. However, the huge amount of computation and data generated from the network edge becomes the major bottleneck, and traditional cloud-based computing mode has been unable to meet the requirements of real-time processing tasks. To solve the above problems, by embedding AI model training and inference capabilities into the network edge, edge intelligence (EI) becomes a cutting-edge direction in the field of AI. Furthermore, collaborative DNN inference among the cloud, edge, and end device provides a promising way to boost the EI. Nevertheless, at present, EI oriented collaborative DNN inference is still in its early stage, lacking a systematic classification and discussion of existing research efforts. Thus motivated, we have made a comprehensive investigation on the recent studies about EI oriented collaborative DNN inference. In this paper, we firstly review the background and motivation of EI. Then, we classify four typical collaborative DNN inference paradigms for EI, and analyze the characteristics and key technologies of them. Finally, we summarize the current challenges of collaborative DNN inference, discuss the future development trend and provide the future research direction. △ Less","15 July, 2022",https://arxiv.org/pdf/2207.07812
Value-based Engineering with IEEE 7000TM,Sarah Spiekermann;Till Winkler,"Digital ethics is being discussed worldwide as a necessity to create more reliable IT systems. This discussion, fueled by the fear of uncontrollable artificial intelligence (AI) has moved many institutions and scientists to demand a value-based system engineering. This article presents how organizations can build responsible and ethically founded systems with the 'Value-based Engineering' (VBE) approach that was standardized in the IEEE 7000TM standard. VBE is a transparent, clearly-structured, step-by-step methodology combining innovation management, risk management, system and software engineering in one process framework. It embeds a robust value ontology and terminology. It has been tested in various case studies. This article introduces readers to the most important steps and contributions of the approach. △ Less","21 June, 2022",https://arxiv.org/pdf/2207.07599
Creating an Explainable Intrusion Detection System Using Self Organizing Maps,Jesse Ables;Thomas Kirby;William Anderson;Sudip Mittal;Shahram Rahimi;Ioana Banicescu;Maria Seale,"Modern Artificial Intelligence (AI) enabled Intrusion Detection Systems (IDS) are complex black boxes. This means that a security analyst will have little to no explanation or clarification on why an IDS model made a particular prediction. A potential solution to this problem is to research and develop Explainable Intrusion Detection Systems (X-IDS) based on current capabilities in Explainable Artificial Intelligence (XAI). In this paper, we create a Self Organizing Maps (SOMs) based X-IDS system that is capable of producing explanatory visualizations. We leverage SOM's explainability to create both global and local explanations. An analyst can use global explanations to get a general idea of how a particular IDS model computes predictions. Local explanations are generated for individual datapoints to explain why a certain prediction value was computed. Furthermore, our SOM based X-IDS was evaluated on both explanation generation and traditional accuracy tests using the NSL-KDD and the CIC-IDS-2017 datasets. △ Less","15 July, 2022",https://arxiv.org/pdf/2207.07465
Plex: Towards Reliability using Pretrained Large Model Extensions,Dustin Tran;Jeremiah Liu;Michael W. Dusenberry;Du Phan;Mark Collier;Jie Ren;Kehang Han;Zi Wang;Zelda Mariet;Huiyi Hu;Neil Band;Tim G. J. Rudner;Karan Singhal;Zachary Nado;Joost van Amersfoort;Andreas Kirsch;Rodolphe Jenatton;Nithum Thain;Honglin Yuan;Kelly Buchanan;Kevin Murphy;D. Sculley;Yarin Gal;Zoubin Ghahramani;Jasper Snoek,"A recent trend in artificial intelligence is the use of pretrained models for language and vision tasks, which have achieved extraordinary performance but also puzzling failures. Probing these models' abilities in diverse ways is therefore critical to the field. In this paper, we explore the reliability of models, where we define a reliable model as one that not only achieves strong predictive performance but also performs well consistently over many decision-making tasks involving uncertainty (e.g., selective prediction, open set recognition), robust generalization (e.g., accuracy and proper scoring rules such as log-likelihood on in- and out-of-distribution datasets), and adaptation (e.g., active learning, few-shot uncertainty). We devise 10 types of tasks over 40 datasets in order to evaluate different aspects of reliability on both vision and language domains. To improve reliability, we developed ViT-Plex and T5-Plex, pretrained large model extensions for vision and language modalities, respectively. Plex greatly improves the state-of-the-art across reliability tasks, and simplifies the traditional protocol as it improves the out-of-the-box performance and does not require designing scores or tuning the model for each task. We demonstrate scaling effects over model sizes up to 1B parameters and pretraining dataset sizes up to 4B examples. We also demonstrate Plex's capabilities on challenging tasks including zero-shot open set recognition, active learning, and uncertainty in conversational language understanding. △ Less","15 July, 2022",https://arxiv.org/pdf/2207.07411
A Robustly Optimized Long Text to Math Models for Numerical Reasoning On FinQA,Renhui Zhang;Youwei Zhang;Yao Yu,"Numerical reasoning is required when solving most problems in our life, but it has been neglected in previous artificial intelligence researches. FinQA challenge has been organized to strengthen the study on numerical reasoning where the participants are asked to predict the numerical reasoning program to solve financial question. The result of FinQA will be evaluated by both execution accuracy and program accuracy. In this paper, we present our approach to tackle the task objective by developing models with different specialized capabilities and fusing their strength. Overall, our approach achieves the 1st place in FinQA challenge, with 71.93% execution accuracy and 67.03% program accuracy. △ Less","29 June, 2022",https://arxiv.org/pdf/2207.06490
A Data-Efficient Deep Learning Framework for Segmentation and Classification of Histopathology Images,Pranav Singh;Jacopo Cirrone,"The current study of cell architecture of inflammation in histopathology images commonly performed for diagnosis and research purposes excludes a lot of information available on the biopsy slide. In autoimmune diseases, major outstanding research questions remain regarding which cell types participate in inflammation at the tissue level, and how they interact with each other. While these questions can be partially answered using traditional methods, artificial intelligence approaches for segmentation and classification provide a much more efficient method to understand the architecture of inflammation in autoimmune disease, holding great promise for novel insights. In this paper, we empirically develop deep learning approaches that use dermatomyositis biopsies of human tissue to detect and identify inflammatory cells. Our approach improves classification performance by 26% and segmentation performance by 5%. We also propose a novel post-processing autoencoder architecture that improves segmentation performance by an additional 3%. △ Less","22 October, 2022",https://arxiv.org/pdf/2207.06489
Quantum Metropolis Solver: A Quantum Walks Approach to Optimization Problems,Roberto Campos;Pablo A M Casares;M A Martin-Delgado,"The efficient resolution of optimization problems is one of the key issues in today's industry. This task relies mainly on classical algorithms that present scalability problems and processing limitations. Quantum computing has emerged to challenge these types of problems. In this paper, we focus on the Metropolis-Hastings quantum algorithm that is based on quantum walks. We use this algorithm to build a quantum software tool called Quantum Metropolis Solver (QMS). We validate QMS with the N-Queen problem to show a potential quantum advantage in an example that can be easily extrapolated to an Artificial Intelligence domain. We carry out different simulations to validate the performance of QMS and its configuration. △ Less","13 July, 2022",https://arxiv.org/pdf/2207.06462
Deep Learning Discovery of Demographic Biomarkers in Echocardiography,Grant Duffy;Shoa L. Clarke;Matthew Christensen;Bryan He;Neal Yuan;Susan Cheng;David Ouyang,"Deep learning has been shown to accurately assess 'hidden' phenotypes and predict biomarkers from medical imaging beyond traditional clinician interpretation of medical imaging. Given the black box nature of artificial intelligence (AI) models, caution should be exercised in applying models to healthcare as prediction tasks might be short-cut by differences in demographics across disease and patient populations. Using large echocardiography datasets from two healthcare systems, we test whether it is possible to predict age, race, and sex from cardiac ultrasound images using deep learning algorithms and assess the impact of varying confounding variables. We trained video-based convolutional neural networks to predict age, sex, and race. We found that deep learning models were able to identify age and sex, while unable to reliably predict race. Without considering confounding differences between categories, the AI model predicted sex with an AUC of 0.85 (95% CI 0.84 - 0.86), age with a mean absolute error of 9.12 years (95% CI 9.00 - 9.25), and race with AUCs ranging from 0.63 - 0.71. When predicting race, we show that tuning the proportion of a confounding variable (sex) in the training data significantly impacts model AUC (ranging from 0.57 to 0.84), while in training a sex prediction model, tuning a confounder (race) did not substantially change AUC (0.81 - 0.83). This suggests a significant proportion of the model's performance on predicting race could come from confounding features being detected by AI. Further work remains to identify the particular imaging features that associate with demographic information and to better understand the risks of demographic identification in medical AI as it pertains to potentially perpetuating bias and disparities. △ Less","13 July, 2022",https://arxiv.org/pdf/2207.06421
DiverGet: A Search-Based Software Testing Approach for Deep Neural Network Quantization Assessment,Ahmed Haj Yahmed;Houssem Ben Braiek;Foutse Khomh;Sonia Bouzidi;Rania Zaatour,"Quantization is one of the most applied Deep Neural Network (DNN) compression strategies, when deploying a trained DNN model on an embedded system or a cell phone. This is owing to its simplicity and adaptability to a wide range of applications and circumstances, as opposed to specific Artificial Intelligence (AI) accelerators and compilers that are often designed only for certain specific hardware (e.g., Google Coral Edge TPU). With the growing demand for quantization, ensuring the reliability of this strategy is becoming a critical challenge. Traditional testing methods, which gather more and more genuine data for better assessment, are often not practical because of the large size of the input space and the high similarity between the original DNN and its quantized counterpart. As a result, advanced assessment strategies have become of paramount importance. In this paper, we present DiverGet, a search-based testing framework for quantization assessment. DiverGet defines a space of metamorphic relations that simulate naturally-occurring distortions on the inputs. Then, it optimally explores these relations to reveal the disagreements among DNNs of different arithmetic precision. We evaluate the performance of DiverGet on state-of-the-art DNNs applied to hyperspectral remote sensing images. We chose the remote sensing DNNs as they're being increasingly deployed at the edge (e.g., high-lift drones) in critical domains like climate change research and astronomy. Our results show that DiverGet successfully challenges the robustness of established quantization techniques against naturally-occurring shifted data, and outperforms its most recent concurrent, DiffChaser, with a success rate that is (on average) four times higher. △ Less","13 July, 2022",https://arxiv.org/pdf/2207.06282
"Explainable Intrusion Detection Systems (X-IDS): A Survey of Current Methods, Challenges, and Opportunities",Subash Neupane;Jesse Ables;William Anderson;Sudip Mittal;Shahram Rahimi;Ioana Banicescu;Maria Seale,"The application of Artificial Intelligence (AI) and Machine Learning (ML) to cybersecurity challenges has gained traction in industry and academia, partially as a result of widespread malware attacks on critical systems such as cloud infrastructures and government institutions. Intrusion Detection Systems (IDS), using some forms of AI, have received widespread adoption due to their ability to handle vast amounts of data with a high prediction accuracy. These systems are hosted in the organizational Cyber Security Operation Center (CSoC) as a defense tool to monitor and detect malicious network flow that would otherwise impact the Confidentiality, Integrity, and Availability (CIA). CSoC analysts rely on these systems to make decisions about the detected threats. However, IDSs designed using Deep Learning (DL) techniques are often treated as black box models and do not provide a justification for their predictions. This creates a barrier for CSoC analysts, as they are unable to improve their decisions based on the model's predictions. One solution to this problem is to design explainable IDS (X-IDS). This survey reviews the state-of-the-art in explainable AI (XAI) for IDS, its current challenges, and discusses how these challenges span to the design of an X-IDS. In particular, we discuss black box and white box approaches comprehensively. We also present the tradeoff between these approaches in terms of their performance and ability to produce explanations. Furthermore, we propose a generic architecture that considers human-in-the-loop which can be used as a guideline when designing an X-IDS. Research recommendations are given from three critical viewpoints: the need to define explainability for IDS, the need to create explanations tailored to various stakeholders, and the need to design metrics to evaluate explanations. △ Less","13 July, 2022",https://arxiv.org/pdf/2207.06236
Continual Learning with Deep Learning Methods in an Application-Oriented Context,Benedikt Pfülb,"Abstract knowledge is deeply grounded in many computer-based applications. An important research area of Artificial Intelligence (AI) deals with the automatic derivation of knowledge from data. Machine learning offers the according algorithms. One area of research focuses on the development of biologically inspired learning algorithms. The respective machine learning methods are based on neurological concepts so that they can systematically derive knowledge from data and store it. One type of machine learning algorithms that can be categorized as ""deep learning"" model is referred to as Deep Neural Networks (DNNs). DNNs consist of multiple artificial neurons arranged in layers that are trained by using the backpropagation algorithm. These deep learning methods exhibit amazing capabilities for inferring and storing complex knowledge from high-dimensional data. However, DNNs are affected by a problem that prevents new knowledge from being added to an existing base. The ability to continuously accumulate knowledge is an important factor that contributed to evolution and is therefore a prerequisite for the development of strong AIs. The so-called ""catastrophic forgetting"" (CF) effect causes DNNs to immediately loose already derived knowledge after a few training iterations on a new data distribution. Only an energetically expensive retraining with the joint data distribution of past and new data enables the abstraction of the entire new set of knowledge. In order to counteract the effect, various techniques have been and are still being developed with the goal to mitigate or even solve the CF problem. These published CF avoidance studies usually imply the effectiveness of their approaches for various continual learning tasks. This dissertation is set in the context of continual machine learning with deep learning methods. The first part deals with the development of an ... △ Less","12 July, 2022",https://arxiv.org/pdf/2207.06233
Improving Wikipedia Verifiability with AI,Fabio Petroni;Samuel Broscheit;Aleksandra Piktus;Patrick Lewis;Gautier Izacard;Lucas Hosseini;Jane Dwivedi-Yu;Maria Lomeli;Timo Schick;Pierre-Emmanuel Mazaré;Armand Joulin;Edouard Grave;Sebastian Riedel,"Verifiability is a core content policy of Wikipedia: claims that are likely to be challenged need to be backed by citations. There are millions of articles available online and thousands of new articles are released each month. For this reason, finding relevant sources is a difficult task: many claims do not have any references that support them. Furthermore, even existing citations might not support a given claim or become obsolete once the original source is updated or deleted. Hence, maintaining and improving the quality of Wikipedia references is an important challenge and there is a pressing need for better tools to assist humans in this effort. Here, we show that the process of improving references can be tackled with the help of artificial intelligence (AI). We develop a neural network based system, called Side, to identify Wikipedia citations that are unlikely to support their claims, and subsequently recommend better ones from the web. We train this model on existing Wikipedia references, therefore learning from the contributions and combined wisdom of thousands of Wikipedia editors. Using crowd-sourcing, we observe that for the top 10% most likely citations to be tagged as unverifiable by our system, humans prefer our system's suggested alternatives compared to the originally cited reference 70% of the time. To validate the applicability of our system, we built a demo to engage with the English-speaking Wikipedia community and find that Side's first citation recommendation collects over 60% more preferences than existing Wikipedia citations for the same top 10% most likely unverifiable claims according to Side. Our results indicate that an AI-based system could be used, in tandem with humans, to improve the verifiability of Wikipedia. More generally, we hope that our work can be used to assist fact checking efforts and increase the general trustworthiness of information online. △ Less","8 July, 2022",https://arxiv.org/pdf/2207.06220
A Novel DeBERTa-based Model for Financial Question Answering Task,Yanbo J. Wang;Yuming Li;Hui Qin;Yuhang Guan;Sheng Chen,"As a rising star in the field of natural language processing, question answering systems (Q&A Systems) are widely used in all walks of life. Compared with other scenarios, the applicationin financial scenario has strong requirements in the traceability and interpretability of the Q&A systems. In addition, since the demand for artificial intelligence technology has gradually shifted from the initial computational intelligence to cognitive intelligence, this research mainly focuses on the financial numerical reasoning dataset - FinQA. In the shared task, the objective is to generate the reasoning program and the final answer according to the given financial report containing text and tables. We use the method based on DeBERTa pre-trained language model, with additional optimization methods including multi-model fusion, training set combination on this basis. We finally obtain an execution accuracy of 68.99 and a program accuracy of 64.53, ranking No. 4 in the 2022 FinQA Challenge. △ Less","12 July, 2022",https://arxiv.org/pdf/2207.05875
BASED-XAI: Breaking Ablation Studies Down for Explainable Artificial Intelligence,Isha Hameed;Samuel Sharpe;Daniel Barcklow;Justin Au-Yeung;Sahil Verma;Jocelyn Huang;Brian Barr;C. Bayan Bruss,"Explainable artificial intelligence (XAI) methods lack ground truth. In its place, method developers have relied on axioms to determine desirable properties for their explanations' behavior. For high stakes uses of machine learning that require explainability, it is not sufficient to rely on axioms as the implementation, or its usage, can fail to live up to the ideal. As a result, there exists active research on validating the performance of XAI methods. The need for validation is especially magnified in domains with a reliance on XAI. A procedure frequently used to assess their utility, and to some extent their fidelity, is an ablation study. By perturbing the input variables in rank order of importance, the goal is to assess the sensitivity of the model's performance. Perturbing important variables should correlate with larger decreases in measures of model capability than perturbing less important features. While the intent is clear, the actual implementation details have not been studied rigorously for tabular data. Using five datasets, three XAI methods, four baselines, and three perturbations, we aim to show 1) how varying perturbations and adding simple guardrails can help to avoid potentially flawed conclusions, 2) how treatment of categorical variables is an important consideration in both post-hoc explainability and ablation studies, and 3) how to identify useful baselines for XAI methods and viable perturbations for ablation studies. △ Less","1 September, 2022",https://arxiv.org/pdf/2207.05566
Brain-inspired Graph Spiking Neural Networks for Commonsense Knowledge Representation and Reasoning,Hongjian Fang;Yi Zeng;Jianbo Tang;Yuwei Wang;Yao Liang;Xin Liu,"How neural networks in the human brain represent commonsense knowledge, and complete related reasoning tasks is an important research topic in neuroscience, cognitive science, psychology, and artificial intelligence. Although the traditional artificial neural network using fixed-length vectors to represent symbols has gained good performance in some specific tasks, it is still a black box that lacks interpretability, far from how humans perceive the world. Inspired by the grandmother-cell hypothesis in neuroscience, this work investigates how population encoding and spiking timing-dependent plasticity (STDP) mechanisms can be integrated into the learning of spiking neural networks, and how a population of neurons can represent a symbol via guiding the completion of sequential firing between different neuron populations. The neuron populations of different communities together constitute the entire commonsense knowledge graph, forming a giant graph spiking neural network. Moreover, we introduced the Reward-modulated spiking timing-dependent plasticity (R-STDP) mechanism to simulate the biological reinforcement learning process and completed the related reasoning tasks accordingly, achieving comparable accuracy and faster convergence speed than the graph convolutional artificial neural networks. For the fields of neuroscience and cognitive science, the work in this paper provided the foundation of computational modeling for further exploration of the way the human brain represents commonsense knowledge. For the field of artificial intelligence, this paper indicated the exploration direction for realizing a more robust and interpretable neural network by constructing a commonsense knowledge representation and reasoning spiking neural networks with solid biological plausibility. △ Less","11 July, 2022",https://arxiv.org/pdf/2207.05561
Western Mediterranean wetlands bird species classification: evaluating small-footprint deep learning approaches on a new annotated dataset,Juan Gómez-Gómez;Ester Vidaña-Vila;Xavier Sevillano,"The deployment of an expert system running over a wireless acoustic sensors network made up of bioacoustic monitoring devices that recognise bird species from their sounds would enable the automation of many tasks of ecological value, including the analysis of bird population composition or the detection of endangered species in areas of environmental interest. Endowing these devices with accurate audio classification capabilities is possible thanks to the latest advances in artificial intelligence, among which deep learning techniques excel. However, a key issue to make bioacoustic devices affordable is the use of small footprint deep neural networks that can be embedded in resource and battery constrained hardware platforms. For this reason, this work presents a critical comparative analysis between two heavy and large footprint deep neural networks (VGG16 and ResNet50) and a lightweight alternative, MobileNetV2. Our experimental results reveal that MobileNetV2 achieves an average F1-score less than a 5\% lower than ResNet50 (0.789 vs. 0.834), performing better than VGG16 with a footprint size nearly 40 times smaller. Moreover, to compare the models, we have created and made public the Western Mediterranean Wetland Birds dataset, consisting of 201.6 minutes and 5,795 audio excerpts of 20 endemic bird species of the Aiguamolls de l'Empordà Natural Park. △ Less","12 July, 2022",https://arxiv.org/pdf/2207.05393
Recent Developments in AI and USPTO Open Data,Scott Beliveau;Jerry Ma,"The USPTO disseminates one of the largest publicly accessible repositories of scientific, technical, and commercial data worldwide. USPTO data has historically seen frequent use in fields such as patent analytics, economics, and prosecution & litigation tools. This article highlights an emerging class of usecases directed to the research, development, and application of artificial intelligence technology. Such usecases contemplate both the delivery of artificial intelligence capabilities for practical IP applications and the enablement of future state-of-the-art artificial intelligence research via USPTO data products. Examples from both within and beyond the USPTO are offered as case studies. △ Less","11 July, 2022",https://arxiv.org/pdf/2207.05239
Horizontal Federated Learning and Secure Distributed Training for Recommendation System with Intel SGX,Siyuan Hui;Yuqiu Zhang;Albert Hu;Edmund Song,"With the advent of big data era and the development of artificial intelligence and other technologies, data security and privacy protection have become more important. Recommendation systems have many applications in our society, but the model construction of recommendation systems is often inseparable from users' data. Especially for deep learning-based recommendation systems, due to the complexity of the model and the characteristics of deep learning itself, its training process not only requires long training time and abundant computational resources but also needs to use a large amount of user data, which poses a considerable challenge in terms of data security and privacy protection. How to train a distributed recommendation system while ensuring data security has become an urgent problem to be solved. In this paper, we implement two schemes, Horizontal Federated Learning and Secure Distributed Training, based on Intel SGX(Software Guard Extensions), an implementation of a trusted execution environment, and TensorFlow framework, to achieve secure, distributed recommendation system-based learning schemes in different scenarios. We experiment on the classical Deep Learning Recommendation Model (DLRM), which is a neural network-based machine learning model designed for personalization and recommendation, and the results show that our implementation introduces approximately no loss in model performance. The training speed is within acceptable limits. △ Less","11 July, 2022",https://arxiv.org/pdf/2207.05079
What AI can do for horse-racing ?,Pierre Colle,"Since the 1980s, machine learning has been widely used for horse-racing predictions, gradually expanding to where algorithms are now playing a huge role in the betting market. Machine learning has changed the horse-racing betting market over the last ten years, but main changes are still to come. The paradigm shift of neural networks (deep learning) may not only improve our ability to simply predict the outcome of a race, but it will also certainly shake our entire way of thinking about horse-racing - and maybe more generally about horses. Since 2012, deep learning provided more and more state-of-the-art results in computer vision and now statistical learning or game theory. We describe how the convergence of the three machine learning fields (computer vision, statistical learning, and game theory) will be game-changers in the next decade in our ability to predict and understand horse-racing. We consider that horse-racing is a real world laboratory where we can work on the animal-human interaction and build a non-anthropocentric Artificial Intelligence. We believe that this will lead us to understand the horses better and the interactions between animals and humans in general. △ Less","11 July, 2022",https://arxiv.org/pdf/2207.04981
A Late Fusion Framework with Multiple Optimization Methods for Media Interestingness,Maria Shoukat;Khubaib Ahmad;Naina Said;Nasir Ahmad;Mohammed Hassanuzaman;Kashif Ahmad,"The recent advancement in Multimedia Analytical, Computer Vision (CV), and Artificial Intelligence (AI) algorithms resulted in several interesting tools allowing an automatic analysis and retrieval of multimedia content of users' interests. However, retrieving the content of interest generally involves analysis and extraction of semantic features, such as emotions and interestingness-level. The extraction of such meaningful information is a complex task and generally, the performance of individual algorithms is very low. One way to enhance the performance of the individual algorithms is to combine the predictive capabilities of multiple algorithms using fusion schemes. This allows the individual algorithms to complement each other, leading to improved performance. This paper proposes several fusion methods for the media interestingness score prediction task introduced in CLEF Fusion 2022. The proposed methods include both a naive fusion scheme, where all the inducers are treated equally and a merit-based fusion scheme where multiple weight optimization methods are employed to assign weights to the individual inducers. In total, we used six optimization methods including a Particle Swarm Optimization (PSO), a Genetic Algorithm (GA), Nelder Mead, Trust Region Constrained (TRC), and Limited-memory Broyden Fletcher Goldfarb Shanno Algorithm (LBFGSA), and Truncated Newton Algorithm (TNA). Overall better results are obtained with PSO and TNA achieving 0.109 mean average precision at 10. The task is complex and generally, scores are low. We believe the presented analysis will provide a baseline for future research in the domain. △ Less","11 July, 2022",https://arxiv.org/pdf/2207.04762
No Language Left Behind: Scaling Human-Centered Machine Translation,NLLB Team;Marta R. Costa-jussà;James Cross;Onur Çelebi;Maha Elbayad;Kenneth Heafield;Kevin Heffernan;Elahe Kalbassi;Janice Lam;Daniel Licht;Jean Maillard;Anna Sun;Skyler Wang;Guillaume Wenzek;Al Youngblood;Bapi Akula;Loic Barrault;Gabriel Mejia Gonzalez;Prangthip Hansanti;John Hoffman;Semarley Jarrett;Kaushik Ram Sadagopan;Dirk Rowe;Shannon Spruit;Chau Tran,"Driven by the goal of eradicating language barriers on a global scale, machine translation has solidified itself as a key focus of artificial intelligence research today. However, such efforts have coalesced around a small subset of languages, leaving behind the vast majority of mostly low-resource languages. What does it take to break the 200 language barrier while ensuring safe, high quality results, all while keeping ethical considerations in mind? In No Language Left Behind, we took on this challenge by first contextualizing the need for low-resource language translation support through exploratory interviews with native speakers. Then, we created datasets and models aimed at narrowing the performance gap between low and high-resource languages. More specifically, we developed a conditional compute model based on Sparsely Gated Mixture of Experts that is trained on data obtained with novel and effective data mining techniques tailored for low-resource languages. We propose multiple architectural and training improvements to counteract overfitting while training on thousands of tasks. Critically, we evaluated the performance of over 40,000 different translation directions using a human-translated benchmark, Flores-200, and combined human evaluation with a novel toxicity benchmark covering all languages in Flores-200 to assess translation safety. Our model achieves an improvement of 44% BLEU relative to the previous state-of-the-art, laying important groundwork towards realizing a universal translation system. Finally, we open source all contributions described in this work, accessible at https://github.com/facebookresearch/fairseq/tree/nllb. △ Less","25 August, 2022",https://arxiv.org/pdf/2207.04672
On the Principles of Parsimony and Self-Consistency for the Emergence of Intelligence,Yi Ma;Doris Tsao;Heung-Yeung Shum,"Ten years into the revival of deep networks and artificial intelligence, we propose a theoretical framework that sheds light on understanding deep networks within a bigger picture of Intelligence in general. We introduce two fundamental principles, Parsimony and Self-consistency, that address two fundamental questions regarding Intelligence: what to learn and how to learn, respectively. We believe the two principles are the cornerstones for the emergence of Intelligence, artificial or natural. While these two principles have rich classical roots, we argue that they can be stated anew in entirely measurable and computable ways. More specifically, the two principles lead to an effective and efficient computational framework, compressive closed-loop transcription, that unifies and explains the evolution of modern deep networks and many artificial intelligence practices. While we mainly use modeling of visual data as an example, we believe the two principles will unify understanding of broad families of autonomous intelligent systems and provide a framework for understanding the brain. △ Less","27 July, 2022",https://arxiv.org/pdf/2207.04630
Explainable AI (XAI) in Biomedical Signal and Image Processing: Promises and Challenges,Guang Yang;Arvind Rao;Christine Fernandez-Maloigne;Vince Calhoun;Gloria Menegaz,"Artificial intelligence has become pervasive across disciplines and fields, and biomedical image and signal processing is no exception. The growing and widespread interest on the topic has triggered a vast research activity that is reflected in an exponential research effort. Through study of massive and diverse biomedical data, machine and deep learning models have revolutionized various tasks such as modeling, segmentation, registration, classification and synthesis, outperforming traditional techniques. However, the difficulty in translating the results into biologically/clinically interpretable information is preventing their full exploitation in the field. Explainable AI (XAI) attempts to fill this translational gap by providing means to make the models interpretable and providing explanations. Different solutions have been proposed so far and are gaining increasing interest from the community. This paper aims at providing an overview on XAI in biomedical data processing and points to an upcoming Special Issue on Deep Learning in Biomedical Image and Signal Processing of the IEEE Signal Processing Magazine that is going to appear in March 2022. △ Less","9 July, 2022",https://arxiv.org/pdf/2207.04295
An Outlook on the Future Marine Traffic Management System for Autonomous Ships,Michele Martelli;Antonio Virdis;Alberto Gotta;Pietro CassarÀ;Maria Di Summa,"In the shipping digitalisation process, the peak will be reached with the advent of a wholly autonomous and at the same time safe and reliable ship. Full autonomy could be obtained by two linked Artificial-Intelligence systems representing the ship navigator and the ship engineer that possess sensing and analysis skills, situational awareness, planning, and control capabilities. Many efforts have been made in developing onboard systems; however, the shore facilities are not ready yet to deal with these new technologies. The paper aims to present the innovative technologies and methodologies needed to develop a futuristic Vessel Traffic System. The proposed systems will aim at faultless data acquisition and processing, provide input to decision-making systems, and suggest evasive manoeuvre; to deal with hazards and systems failure without human intervention onboard. The system is composed of three different and interacting layers. The first is an artificially intelligent tool to detect and control autonomous ships, thanks to situation recognition and obstacle avoidance strategies. The second is an orchestration and management platform designed to coordinate the sensing-actuation infrastructure and the AI algorithms results made available by multiple ships, mustering edge, and distributed computing techniques to fulfil the specific harsh requirements of the sea environment. The final part is a holistic guidance-navigation-control framework to manage autonomous ships navigation in a crowded area. Eventually, a cyber-physical scenario, using both a ship digital-twin and a real model-scale ship, is suggested to test and validate the innovative system without the availability of a full-scale scenario. △ Less","8 July, 2022",https://arxiv.org/pdf/2207.04140
Pick the Right Co-Worker: Online Assessment of Cognitive Ergonomics in Human-Robot Collaborative Assembly,Marta Lagomarsino;Marta Lorenzini;Pietro Balatti;Elena De Momi;Arash Ajoudani,"Human-robot collaborative assembly systems enhance the efficiency and productivity of the workplace but may increase the workers' cognitive demand. This paper proposes an online and quantitative framework to assess the cognitive workload induced by the interaction with a co-worker, either a human operator or an industrial collaborative robot with different control strategies. The approach monitors the operator's attention distribution and upper-body kinematics benefiting from the input images of a low-cost stereo camera and cutting-edge artificial intelligence algorithms (i.e. head pose estimation and skeleton tracking). Three experimental scenarios with variations in workstation features and interaction modalities were designed to test the performance of our online method against state-of-the-art offline measurements. Results proved that our vision-based cognitive load assessment has the potential to be integrated into the new generation of collaborative robotic technologies. The latter would enable human cognitive state monitoring and robot control strategy adaptation for improving human comfort, ergonomics, and trust in automation. △ Less","8 July, 2022",https://arxiv.org/pdf/2207.03779
Enabling and Assessing Trust when Cooperating with Robots in Disaster Response (EASIER),Laurent Frering;Matthias Eder;Bettina Kubicek;Dietrich Albert;Denis Kalkofen;Thomas Gschwandtner;Heimo Krajnz;Gerald Steinbauer-Wagner,"This paper presents a conceptual overview of the EASIER project and its scope. EASIER focuses on supporting emergency forces in disaster response scenarios with a semi-autonomous mobile manipulator. Specifically, we examine the operator's trust in the system and his/her cognitive load generated by its use. We plan to address different research topics, exploring how shared autonomy, interaction design, and transparency relate to trust and cognitive load. Another goal is to develop non-invasive methods to continuously measure trust and cognitive load in the context of disaster response using a multilevel approach. This project is conducted by multiple academic partners specializing in artificial intelligence, interaction design, and psychology, as well as an industrial partner for disaster response equipment and end-users for framing the project and the experiments in real use-cases. △ Less","8 July, 2022",https://arxiv.org/pdf/2207.03763
AVDDPG: Federated reinforcement learning applied to autonomous platoon control,Christian Boin;Lei Lei;Simon X. Yang,"Since 2016 federated learning (FL) has been an evolving topic of discussion in the artificial intelligence (AI) research community. Applications of FL led to the development and study of federated reinforcement learning (FRL). Few works exist on the topic of FRL applied to autonomous vehicle (AV) platoons. In addition, most FRL works choose a single aggregation method (usually weight or gradient aggregation). We explore FRL's effectiveness as a means to improve AV platooning by designing and implementing an FRL framework atop a custom AV platoon environment. The application of FRL in AV platooning is studied under two scenarios: (1) Inter-platoon FRL (Inter-FRL) where FRL is applied to AVs across different platoons; (2) Intra-platoon FRL (Intra-FRL) where FRL is applied to AVs within a single platoon. Both Inter-FRL and Intra-FRL are applied to a custom AV platooning environment using both gradient and weight aggregation to observe the performance effects FRL can have on AV platoons relative to an AV platooning environment trained without FRL. It is concluded that Intra-FRL using weight aggregation (Intra-FRLWA) provides the best performance for controlling an AV platoon. In addition, we found that weight aggregation in FRL for AV platooning provides increases in performance relative to gradient aggregation. Finally, a performance analysis is conducted for Intra-FRLWA versus a platooning environment without FRL for platoons of length 3, 4 and 5 vehicles. It is concluded that Intra-FRLWA largely out-performs the platooning environment that is trained without FRL. △ Less","5 July, 2022",https://arxiv.org/pdf/2207.03484
SC2EGSet: StarCraft II Esport Replay and Game-state Dataset,Andrzej Białecki;Natalia Jakubowska;Paweł Dobrowolski;Piotr Białecki;Leszek Krupiński;Andrzej Szczap;Robert Białecki;Jan Gajewski,"As a relatively new form of sport, esports offers unparalleled data availability. Despite the vast amounts of data that are generated by game engines, it can be challenging to extract them and verify their integrity for the purposes of practical and scientific use. Our work aims to open esports to a broader scientific community by supplying raw and pre-processed files from StarCraft II esports tournaments. These files can be used in statistical and machine learning modeling tasks and related to various laboratory-based measurements (e.g., behavioral tests, brain imaging). We have gathered publicly available game-engine generated ""replays"" of tournament matches and performed data extraction and cleanup using a low-level application programming interface (API) parser library. Additionally, we open-sourced and published all the custom tools that were developed in the process of creating our dataset. These tools include PyTorch and PyTorch Lightning API abstractions to load and model the data. Our dataset contains replays from major and premiere StarCraft II tournaments since 2016. To prepare the dataset, we processed 55 tournament ""replaypacks"" that contained 17930 files with game-state information. Based on initial investigation of available StarCraft II datasets, we observed that our dataset is the largest publicly available source of StarCraft II esports data upon its publication. Analysis of the extracted data holds promise for further Artificial Intelligence (AI), Machine Learning (ML), psychological, Human-Computer Interaction (HCI), and sports-related studies in a variety of supervised and self-supervised tasks. △ Less","20 September, 2022",https://arxiv.org/pdf/2207.03428
Market Making with Scaled Beta Policies,Joseph Jerome;Gregory Palmer;Rahul Savani,"This paper introduces a new representation for the actions of a market maker in an order-driven market. This representation uses scaled beta distributions, and generalises three approaches taken in the artificial intelligence for market making literature: single price-level selection, ladder strategies and ""market making at the touch"". Ladder strategies place uniform volume across an interval of contiguous prices. Scaled beta distribution based policies generalise these, allowing volume to be skewed across the price interval. We demonstrate that this flexibility is useful for inventory management, one of the key challenges faced by a market maker. In this paper, we conduct three main experiments: first, we compare our more flexible beta-based actions with the special case of ladder strategies; then, we investigate the performance of simple fixed distributions; and finally, we devise and evaluate a simple and intuitive dynamic control policy that adjusts actions in a continuous manner depending on the signed inventory that the market maker has acquired. All empirical evaluations use a high-fidelity limit order book simulator based on historical data with 50 levels on each side. △ Less","27 September, 2022",https://arxiv.org/pdf/2207.03352
Experimental Demonstration of High-Performance Physical Reservoir Computing with Nonlinear Interfered Spin Wave Multi-Detection,Wataru Namiki;Daiki Nishioka;Yu Yamaguchi;Takashi Tsuchiya;Tohru Higuchi;Kazuya Terabe,"Physical reservoir computing, which is a promising method for the implementation of highly efficient artificial intelligence devices, requires a physical system with nonlinearity, fading memory, and the ability to map in high dimensions. Although it is expected that spin wave interference can perform as highly efficient reservoir computing in some micromagnetic simulations, there has been no experimental verification to date. Herein, we demonstrate reservoir computing that utilizes multidetected nonlinear spin wave interference in an yttrium iron garnet single crystal. The subject computing system achieved excellent performance when used for hand-written digit recognition, second-order nonlinear dynamical tasks, and nonlinear autoregressive moving average (NARMA). It is of particular note that normalized mean square errors (NMSEs) for NARMA2 and second-order nonlinear dynamical tasks were 1.81x10-2 and 8.37x10-5, respectively, which are the lowest figures for any experimental physical reservoir so far reported. Said high performance was achieved with higher nonlinearity and the large memory capacity of interfered spin wave multi-detection. △ Less","7 July, 2022",https://arxiv.org/pdf/2207.03216
Evaluating Human-like Explanations for Robot Actions in Reinforcement Learning Scenarios,Francisco Cruz;Charlotte Young;Richard Dazeley;Peter Vamplew,"Explainable artificial intelligence is a research field that tries to provide more transparency for autonomous intelligent systems. Explainability has been used, particularly in reinforcement learning and robotic scenarios, to better understand the robot decision-making process. Previous work, however, has been widely focused on providing technical explanations that can be better understood by AI practitioners than non-expert end-users. In this work, we make use of human-like explanations built from the probability of success to complete the goal that an autonomous robot shows after performing an action. These explanations are intended to be understood by people who have no or very little experience with artificial intelligence methods. This paper presents a user trial to study whether these explanations that focus on the probability an action has of succeeding in its goal constitute a suitable explanation for non-expert end-users. The results obtained show that non-expert participants rate robot explanations that focus on the probability of success higher and with less variance than technical explanations generated from Q-values, and also favor counterfactual explanations over standalone explanations. △ Less","7 July, 2022",https://arxiv.org/pdf/2207.03214
Leveraging Log Instructions in Log-based Anomaly Detection,Jasmin Bogatinovski;Gjorgji Madjarov;Sasho Nedelkoski;Jorge Cardoso;Odej Kao,"Artificial Intelligence for IT Operations (AIOps) describes the process of maintaining and operating large IT systems using diverse AI-enabled methods and tools for, e.g., anomaly detection and root cause analysis, to support the remediation, optimization, and automatic initiation of self-stabilizing IT activities. The core step of any AIOps workflow is anomaly detection, typically performed on high-volume heterogeneous data such as log messages (logs), metrics (e.g., CPU utilization), and distributed traces. In this paper, we propose a method for reliable and practical anomaly detection from system logs. It overcomes the common disadvantage of related works, i.e., the need for a large amount of manually labeled training data, by building an anomaly detection model with log instructions from the source code of 1000+ GitHub projects. The instructions from diverse systems contain rich and heterogenous information about many different normal and abnormal IT events and serve as a foundation for anomaly detection. The proposed method, named ADLILog, combines the log instructions and the data from the system of interest (target system) to learn a deep neural network model through a two-phase learning procedure. The experimental results show that ADLILog outperforms the related approaches by up to 60% on the F1 score while satisfying core non-functional requirements for industrial deployments such as unsupervised design, efficient model updates, and small model sizes. △ Less","7 July, 2022",https://arxiv.org/pdf/2207.03206
"Towards Transparency in Dermatology Image Datasets with Skin Tone Annotations by Experts, Crowds, and an Algorithm",Matthew Groh;Caleb Harris;Roxana Daneshjou;Omar Badri;Arash Koochek,"While artificial intelligence (AI) holds promise for supporting healthcare providers and improving the accuracy of medical diagnoses, a lack of transparency in the composition of datasets exposes AI models to the possibility of unintentional and avoidable mistakes. In particular, public and private image datasets of dermatological conditions rarely include information on skin color. As a start towards increasing transparency, AI researchers have appropriated the use of the Fitzpatrick skin type (FST) from a measure of patient photosensitivity to a measure for estimating skin tone in algorithmic audits of computer vision applications including facial recognition and dermatology diagnosis. In order to understand the variability of estimated FST annotations on images, we compare several FST annotation methods on a diverse set of 460 images of skin conditions from both textbooks and online dermatology atlases. We find the inter-rater reliability between three board-certified dermatologists is comparable to the inter-rater reliability between the board-certified dermatologists and two crowdsourcing methods. In contrast, we find that the Individual Typology Angle converted to FST (ITA-FST) method produces annotations that are significantly less correlated with the experts' annotations than the experts' annotations are correlated with each other. These results demonstrate that algorithms based on ITA-FST are not reliable for annotating large-scale image datasets, but human-centered, crowd-based protocols can reliably add skin type transparency to dermatology datasets. Furthermore, we introduce the concept of dynamic consensus protocols with tunable parameters including expert review that increase the visibility of crowdwork and provide guidance for future crowdsourced annotations of large image datasets. △ Less","6 July, 2022",https://arxiv.org/pdf/2207.02942
Towards the Use of Saliency Maps for Explaining Low-Quality Electrocardiograms to End Users,Ana Lucic;Sheeraz Ahmad;Amanda Furtado Brinhosa;Vera Liao;Himani Agrawal;Umang Bhatt;Krishnaram Kenthapadi;Alice Xiang;Maarten de Rijke;Nicholas Drabowski,"When using medical images for diagnosis, either by clinicians or artificial intelligence (AI) systems, it is important that the images are of high quality. When an image is of low quality, the medical exam that produced the image often needs to be redone. In telemedicine, a common problem is that the quality issue is only flagged once the patient has left the clinic, meaning they must return in order to have the exam redone. This can be especially difficult for people living in remote regions, who make up a substantial portion of the patients at Portal Telemedicina, a digital healthcare organization based in Brazil. In this paper, we report on ongoing work regarding (i) the development of an AI system for flagging and explaining low-quality medical images in real-time, (ii) an interview study to understand the explanation needs of stakeholders using the AI system at OurCompany, and, (iii) a longitudinal user study design to examine the effect of including explanations on the workflow of the technicians in our clinics. To the best of our knowledge, this would be the first longitudinal study on evaluating the effects of XAI methods on end-users -- stakeholders that use AI systems but do not have AI-specific expertise. We welcome feedback and suggestions on our experimental setup. △ Less","6 July, 2022",https://arxiv.org/pdf/2207.02726
Deep Learning approach for Classifying Trusses and Runners of Strawberries,Jakub Pomykala;Francisco de Lemos;Isibor Kennedy Ihianle;David Ada Adama;Pedro Machado,"The use of artificial intelligence in the agricultural sector has been growing at a rapid rate to automate farming activities. Emergent farming technologies focus on mapping and classification of plants, fruits, diseases, and soil types. Although, assisted harvesting and pruning applications using deep learning algorithms are in the early development stages, there is a demand for solutions to automate such processes. This paper proposes the use of Deep Learning for the classification of trusses and runners of strawberry plants using semantic segmentation and dataset augmentation. The proposed approach is based on the use of noises (i.e. Gaussian, Speckle, Poisson and Salt-and-Pepper) to artificially augment the dataset and compensate the low number of data samples and increase the overall classification performance. The results are evaluated using mean average of precision, recall and F1 score. The proposed approach achieved 91%, 95% and 92% on precision, recall and F1 score, respectively, for truss detection using the ResNet101 with dataset augmentation utilising Salt-and-Pepper noise; and 83%, 53% and 65% on precision, recall and F1 score, respectively, for truss detection using the ResNet50 with dataset augmentation utilising Poisson noise. △ Less","21 July, 2022",https://arxiv.org/pdf/2207.02721
Transformers discover an elementary calculation system exploiting local attention and grid-like problem representation,Samuel Cognolato;Alberto Testolin,"Mathematical reasoning is one of the most impressive achievements of human intellect but remains a formidable challenge for artificial intelligence systems. In this work we explore whether modern deep learning architectures can learn to solve a symbolic addition task by discovering effective arithmetic procedures. Although the problem might seem trivial at first glance, generalizing arithmetic knowledge to operations involving a higher number of terms, possibly composed by longer sequences of digits, has proven extremely challenging for neural networks. Here we show that universal transformers equipped with local attention and adaptive halting mechanisms can learn to exploit an external, grid-like memory to carry out multi-digit addition. The proposed model achieves remarkable accuracy even when tested with problems requiring extrapolation outside the training distribution; most notably, it does so by discovering human-like calculation strategies such as place value alignment. △ Less","6 July, 2022",https://arxiv.org/pdf/2207.02536
Distillation to Enhance the Portability of Risk Models Across Institutions with Large Patient Claims Database,Steve Nyemba;Chao Yan;Ziqi Zhang;Amol Rajmane;Pablo Meyer;Prithwish Chakraborty;Bradley Malin,"Artificial intelligence, and particularly machine learning (ML), is increasingly developed and deployed to support healthcare in a variety of settings. However, clinical decision support (CDS) technologies based on ML need to be portable if they are to be adopted on a broad scale. In this respect, models developed at one institution should be reusable at another. Yet there are numerous examples of portability failure, particularly due to naive application of ML models. Portability failure can lead to suboptimal care and medical errors, which ultimately could prevent the adoption of ML-based CDS in practice. One specific healthcare challenge that could benefit from enhanced portability is the prediction of 30-day readmission risk. Research to date has shown that deep learning models can be effective at modeling such risk. In this work, we investigate the practicality of model portability through a cross-site evaluation of readmission prediction models. To do so, we apply a recurrent neural network, augmented with self-attention and blended with expert features, to build readmission prediction models for two independent large scale claims datasets. We further present a novel transfer learning technique that adapts the well-known method of born-again network (BAN) training. Our experiments show that direct application of ML models trained at one institution and tested at another institution perform worse than models trained and tested at the same institution. We further show that the transfer learning approach based on the BAN produces models that are better than those trained on just a single institution's data. Notably, this improvement is consistent across both sites and occurs after a single retraining, which illustrates the potential for a cheap and general model transfer mechanism of readmission risk prediction. △ Less","6 July, 2022",https://arxiv.org/pdf/2207.02445
Guiding Machine Perception with Psychophysics,Justin Dulay;Sonia Poltoratski;Till S. Hartmann;Samuel E. Anthony;Walter J. Scheirer,"{G}{ustav} Fechner's 1860 delineation of psychophysics, the measurement of sensation in relation to its stimulus, is widely considered to be the advent of modern psychological science. In psychophysics, a researcher parametrically varies some aspects of a stimulus, and measures the resulting changes in a human subject's experience of that stimulus; doing so gives insight to the determining relationship between a sensation and the physical input that evoked it. This approach is used heavily in perceptual domains, including signal detection, threshold measurement, and ideal observer analysis. Scientific fields like vision science have always leaned heavily on the methods and procedures of psychophysics, but there is now growing appreciation of them by machine learning researchers, sparked by widening overlap between biological and artificial perception \cite{rojas2011automatic, scheirer2014perceptual,escalera2014chalearn,zhang2018agil, grieggs2021measuring}. Machine perception that is guided by behavioral measurements, as opposed to guidance restricted to arbitrarily assigned human labels, has significant potential to fuel further progress in artificial intelligence. △ Less","5 July, 2022",https://arxiv.org/pdf/2207.02241
Combining Topic Modeling with Grounded Theory: Case Studies of Project Collaboration,Eyyub Can Odacioglu;Lihong Zhang;Richard Allmendinger,"This paper proposes an Artificial Intelligence (AI) Grounded Theory for management studies. We argue that this novel and rigorous approach that embeds topic modelling will lead to the latent knowledge to be found. We illustrate this abductive method using 51 case studies of collaborative innovation published by Project Management Institute (PMI). Initial results are presented and discussed that include 40 topics, 6 categories, 4 of which are core categories, and two new theories of project collaboration. △ Less","28 June, 2022",https://arxiv.org/pdf/2207.02212
Disentangling private classes through regularization,Enzo Tartaglione;Francesca Gennari;Marco Grangetto,"Deep learning models are nowadays broadly deployed to solve an incredibly large variety of tasks. However, little attention has been devoted to connected legal aspects. In 2016, the European Union approved the General Data Protection Regulation which entered into force in 2018. Its main rationale was to protect the privacy and data protection of its citizens by the way of operating of the so-called ""Data Economy"". As data is the fuel of modern Artificial Intelligence, it is argued that the GDPR can be partly applicable to a series of algorithmic decision making tasks before a more structured AI Regulation enters into force. In the meantime, AI should not allow undesired information leakage deviating from the purpose for which is created. In this work we propose DisP, an approach for deep learning models disentangling the information related to some classes we desire to keep private, from the data processed by AI. In particular, DisP is a regularization strategy de-correlating the features belonging to the same private class at training time, hiding the information of private classes membership. Our experiments on state-of-the-art deep learning models show the effectiveness of DisP, minimizing the risk of extraction for the classes we desire to keep private. △ Less","5 July, 2022",https://arxiv.org/pdf/2207.02000
A Framework for Auditing Multilevel Models using Explainability Methods,Debarati Bhaumik;Diptish Dey;Subhradeep Kayal,"Applications of multilevel models usually result in binary classification within groups or hierarchies based on a set of input features. For transparent and ethical applications of such models, sound audit frameworks need to be developed. In this paper, an audit framework for technical assessment of regression MLMs is proposed. The focus is on three aspects, model, discrimination, and transparency and explainability. These aspects are subsequently divided into sub aspects. Contributors, such as inter MLM group fairness, feature contribution order, and aggregated feature contribution, are identified for each of these sub aspects. To measure the performance of the contributors, the framework proposes a shortlist of KPIs. A traffic light risk assessment method is furthermore coupled to these KPIs. For assessing transparency and explainability, different explainability methods (SHAP and LIME) are used, which are compared with a model intrinsic method using quantitative methods and machine learning modelling. Using an open source dataset, a model is trained and tested and the KPIs are computed. It is demonstrated that popular explainability methods, such as SHAP and LIME, underperform in accuracy when interpreting these models. They fail to predict the order of feature importance, the magnitudes, and occasionally even the nature of the feature contribution. For other contributors, such as group fairness and their associated KPIs, similar analysis and calculations have been performed with the aim of adding profundity to the proposed audit framework. The framework is expected to assist regulatory bodies in performing conformity assessments of AI systems using multilevel binomial classification models at businesses. It will also benefit businesses deploying MLMs to be future proof and aligned with the European Commission proposed Regulation on Artificial Intelligence. △ Less","15 July, 2022",https://arxiv.org/pdf/2207.01611
The Dichotomy of Cloud and IoT: Cloud-Assisted IoT From a Security Perspective,Behrouz Zolfaghari;Abbas Yazdinejad;Ali Dehghantanha;Jacob Krzciok;Khodakhast Bibak,"In recent years, the existence of a significant cross-impact between Cloud computing and Internet of Things (IoT) has lead to a dichotomy that gives raise to Cloud-Assisted IoT (CAIoT) and IoT-Based Cloud (IoTBC). Although it is pertinent to study both technologies, this paper focuses on CAIoT, and especially its security issues, which are inherited from both Cloud computing and IoT. This study starts with reviewing existing relevant surveys, noting their shortcomings, which motivate a comprehensive survey in this area. We proceed to highlight existing approaches towards the design of Secure CAIoT (SCAIoT) along with related security challenges and controls. We develop a layered architecture for SCAIoT. Furthermore, we take a look at what the future may hold for SCAIoT with a focus on the role of Artificial Intelligence(AI). △ Less","25 October, 2022",https://arxiv.org/pdf/2207.01590
Efficient Lung Cancer Image Classification and Segmentation Algorithm Based on Improved Swin Transformer,Ruina Sun;Yuexin Pang,"With the development of computer technology, various models have emerged in artificial intelligence. The transformer model has been applied to the field of computer vision (CV) after its success in natural language processing (NLP). Radiologists continue to face multiple challenges in today's rapidly evolving medical field, such as increased workload and increased diagnostic demands. Although there are some conventional methods for lung cancer detection before, their accuracy still needs to be improved, especially in realistic diagnostic scenarios. This paper creatively proposes a segmentation method based on efficient transformer and applies it to medical image analysis. The algorithm completes the task of lung cancer classification and segmentation by analyzing lung cancer data, and aims to provide efficient technical support for medical staff. In addition, we evaluated and compared the results in various aspects. For the classification mission, the max accuracy of Swin-T by regular training and Swin-B in two resolutions by pre-training can be up to 82.3%. For the segmentation mission, we use pre-training to help the model improve the accuracy of our experiments. The accuracy of the three models reaches over 95%. The experiments demonstrate that the algorithm can be well applied to lung cancer classification and segmentation missions. △ Less","4 July, 2022",https://arxiv.org/pdf/2207.01527
Improving Medical Systems in the United States using Knowledge-Based Systems,Seongwoo Choi,"America has one of the best medical systems in the world. The medical treatment care options offered by the medical system make it sophisticated. However, many American patients are not receiving health care on a regular basis, and at the same time, they cannot afford it. Also, the current medical system has many flaws such as high medical treatment costs and lack of doctors to accommodate many patients. This paper presents the principles of medical artificial intelligence called the knowledge based system. Doctors can remotely check and monitor their patients health data, medical history, how and what medical tests were done, and the lab results. The patients have access to detailed health information online and do not need to make an appointment with doctors to check their health on a daily basis. One doctor can check many patients simultaneously online (when medical centers are understaffed) and do not need to spend a lot of time with patients. Thus, doctors save more money for patients, because patients will no longer be transporting to medical centers to receive routine health check-ups. Patients do not need to overpay for their insurance because they will have access to the knowledge-based system, and the system will save the patients money to have their health checked and reduce the number of unnecessary medical exams. This paper undertakes a brief overview of research work done in a knowledge based system rule based expert systems in the field of medical practices. △ Less","7 June, 2022",https://arxiv.org/pdf/2207.01514
Fairness in Agreement With European Values: An Interdisciplinary Perspective on AI Regulation,Alejandra Bringas Colmenarejo;Luca Nannini;Alisa Rieger;Kristen M. Scott;Xuan Zhao;Gourab K. Patro;Gjergji Kasneci;Katharina Kinder-Kurlanda,"With increasing digitalization, Artificial Intelligence (AI) is becoming ubiquitous. AI-based systems to identify, optimize, automate, and scale solutions to complex economic and societal problems are being proposed and implemented. This has motivated regulation efforts, including the Proposal of an EU AI Act. This interdisciplinary position paper considers various concerns surrounding fairness and discrimination in AI, and discusses how AI regulations address them, focusing on (but not limited to) the Proposal. We first look at AI and fairness through the lenses of law, (AI) industry, sociotechnology, and (moral) philosophy, and present various perspectives. Then, we map these perspectives along three axes of interests: (i) Standardization vs. Localization, (ii) Utilitarianism vs. Egalitarianism, and (iii) Consequential vs. Deontological ethics which leads us to identify a pattern of common arguments and tensions between these axes. Positioning the discussion within the axes of interest and with a focus on reconciling the key tensions, we identify and propose the roles AI Regulation should take to make the endeavor of the AI Act a success in terms of AI fairness concerns. △ Less","8 June, 2022",https://arxiv.org/pdf/2207.01510
Can Population-based Engagement Improve Personalisation? A Novel Dataset and Experiments,Sahan Bulathwela;Meghana Verma;Maria Perez-Ortiz;Emine Yilmaz;John Shawe-Taylor,"This work explores how population-based engagement prediction can address cold-start at scale in large learning resource collections. The paper introduces i) VLE, a novel dataset that consists of content and video based features extracted from publicly available scientific video lectures coupled with implicit and explicit signals related to learner engagement, ii) two standard tasks related to predicting and ranking context-agnostic engagement in video lectures with preliminary baselines and iii) a set of experiments that validate the usefulness of the proposed dataset. Our experimental results indicate that the newly proposed VLE dataset leads to building context-agnostic engagement prediction models that are significantly performant than ones based on previous datasets, mainly attributing to the increase of training examples. VLE dataset's suitability in building models towards Computer Science/ Artificial Intelligence education focused on e-learning/ MOOC use-cases is also evidenced. Further experiments in combining the built model with a personalising algorithm show promising improvements in addressing the cold-start problem encountered in educational recommenders. This is the largest and most diverse publicly available dataset to our knowledge that deals with learner engagement prediction tasks. The dataset, helper tools, descriptive statistics and example code snippets are available publicly. △ Less","22 June, 2022",https://arxiv.org/pdf/2207.01504
Aligning Artificial Intelligence with Humans through Public Policy,John Nay;James Daily,"Given that Artificial Intelligence (AI) increasingly permeates our lives, it is critical that we systematically align AI objectives with the goals and values of humans. The human-AI alignment problem stems from the impracticality of explicitly specifying the rewards that AI models should receive for all the actions they could take in all relevant states of the world. One possible solution, then, is to leverage the capabilities of AI models to learn those rewards implicitly from a rich source of data describing human values in a wide range of contexts. The democratic policy-making process produces just such data by developing specific rules, flexible standards, interpretable guidelines, and generalizable precedents that synthesize citizens' preferences over potential actions taken in many states of the world. Therefore, computationally encoding public policies to make them legible to AI systems should be an important part of a socio-technical approach to the broader human-AI alignment puzzle. This Essay outlines research on AI that learn structures in policy data that can be leveraged for downstream tasks. As a demonstration of the ability of AI to comprehend policy, we provide a case study of an AI system that predicts the relevance of proposed legislation to any given publicly traded company and its likely effect on that company. We believe this represents the ""comprehension"" phase of AI and policy, but leveraging policy as a key source of human values to align AI requires ""understanding"" policy. Solving the alignment problem is crucial to ensuring that AI is beneficial both individually (to the person or group deploying the AI) and socially. As AI systems are given increasing responsibility in high-stakes contexts, integrating democratically-determined policy into those systems could align their behavior with human goals in a way that is responsive to a constantly evolving society. △ Less","25 June, 2022",https://arxiv.org/pdf/2207.01497
AI Ethics: An Empirical Study on the Views of Practitioners and Lawmakers,Arif Ali Khan;Muhammad Azeem Akbar;Mahdi Fahmideh;Peng Liang;Muhammad Waseem;Aakash Ahmad;Mahmood Niazi;Pekka Abrahamsson,"Artificial Intelligence (AI) solutions and technologies are being increasingly adopted in smart systems context, however, such technologies are continuously concerned with ethical uncertainties. Various guidelines, principles, and regulatory frameworks are designed to ensure that AI technologies bring ethical well-being. However, the implications of AI ethics principles and guidelines are still being debated. To further explore the significance of AI ethics principles and relevant challenges, we conducted a survey of 99 representative AI practitioners and lawmakers (e.g., AI engineers, lawyers) from twenty countries across five continents. To the best of our knowledge, this is the first empirical study that encapsulates the perceptions of two different types of population (AI practitioners and lawmakers) and the study findings confirm that transparency, accountability, and privacy are the most critical AI ethics principles. On the other hand, lack of ethical knowledge, no legal frameworks, and lacking monitoring bodies are found the most common AI ethics challenges. The impact analysis of the challenges across AI ethics principles reveals that conflict in practice is a highly severe challenge. Moreover, the perceptions of practitioners and lawmakers are statistically correlated with significant differences for particular principles (e.g. fairness, freedom) and challenges (e.g. lacking monitoring bodies, machine distortion). Our findings stimulate further research, especially empowering existing capability maturity models to support the development and quality assessment of ethics-aware AI systems. △ Less","15 November, 2022",https://arxiv.org/pdf/2207.01493
Experts' View on Challenges and Needs for Fairness in Artificial Intelligence for Education,Gianni Fenu;Roberta Galici;Mirko Marras,"In recent years, there has been a stimulating discussion on how artificial intelligence (AI) can support the science and engineering of intelligent educational applications. Many studies in the field are proposing actionable data mining pipelines and machine-learning models driven by learning-related data. The potential of these pipelines and models to amplify unfairness for certain categories of students is however receiving increasing attention. If AI applications are to have a positive impact on education, it is crucial that their design considers fairness at every step. Through anonymous surveys and interviews with experts (researchers and practitioners) who have published their research at top-tier educational conferences in the last year, we conducted the first expert-driven systematic investigation on the challenges and needs for addressing fairness throughout the development of educational systems based on AI. We identified common and diverging views about the challenges and the needs faced by educational technologies experts in practice, that lead the community to have a clear understanding on the main questions raising doubts in this topic. Based on these findings, we highlighted directions that will facilitate the ongoing research towards fairer AI for education. △ Less","23 June, 2022",https://arxiv.org/pdf/2207.01490
Think About the Stakeholders First! Towards an Algorithmic Transparency Playbook for Regulatory Compliance,Andrew Bell;Oded Nov;Julia Stoyanovich,"Increasingly, laws are being proposed and passed by governments around the world to regulate Artificial Intelligence (AI) systems implemented into the public and private sectors. Many of these regulations address the transparency of AI systems, and related citizen-aware issues like allowing individuals to have the right to an explanation about how an AI system makes a decision that impacts them. Yet, almost all AI governance documents to date have a significant drawback: they have focused on what to do (or what not to do) with respect to making AI systems transparent, but have left the brunt of the work to technologists to figure out how to build transparent systems. We fill this gap by proposing a novel stakeholder-first approach that assists technologists in designing transparent, regulatory compliant systems. We also describe a real-world case-study that illustrates how this approach can be used in practice. △ Less","10 June, 2022",https://arxiv.org/pdf/2207.01482
Breaking Bad News in the Era of Artificial Intelligence and Algorithmic Medicine: An Exploration of Disclosure and its Ethical Justification using the Hedonic Calculus,Benjamin Post;Cosmin Badea;Aldo Faisal;Stephen J. Brett,"An appropriate ethical framework around the use of Artificial Intelligence (AI) in healthcare has become a key desirable with the increasingly widespread deployment of this technology. Advances in AI hold the promise of improving the precision of outcome prediction at the level of the individual. However, the addition of these technologies to patient-clinician interactions, as with any complex human interaction, has potential pitfalls. While physicians have always had to carefully consider the ethical background and implications of their actions, detailed deliberations around fast-moving technological progress may not have kept up. We use a common but key challenge in healthcare interactions, the disclosure of bad news (likely imminent death), to illustrate how the philosophical framework of the 'Felicific Calculus' developed in the 18th century by Jeremy Bentham, may have a timely quasi-quantitative application in the age of AI. We show how this ethical algorithm can be used to assess, across seven mutually exclusive and exhaustive domains, whether an AI-supported action can be morally justified. △ Less","28 September, 2022",https://arxiv.org/pdf/2207.01431
Game State Learning via Game Scene Augmentation,Chintan Trivedi;Konstantinos Makantasis;Antonios Liapis;Georgios N. Yannakakis,"Having access to accurate game state information is of utmost importance for any artificial intelligence task including game-playing, testing, player modeling, and procedural content generation. Self-Supervised Learning (SSL) techniques have shown to be capable of inferring accurate game state information from the high-dimensional pixel input of game footage into compressed latent representations. Contrastive Learning is a popular SSL paradigm where the visual understanding of the game's images comes from contrasting dissimilar and similar game states defined by simple image augmentation methods. In this study, we introduce a new game scene augmentation technique -- named GameCLR -- that takes advantage of the game-engine to define and synthesize specific, highly-controlled renderings of different game states, thereby, boosting contrastive learning performance. We test our GameCLR technique on images of the CARLA driving simulator environment and compare it against the popular SimCLR baseline SSL method. Our results suggest that GameCLR can infer the game's state information from game footage more accurately compared to the baseline. Our proposed approach allows us to conduct game artificial intelligence research by directly utilizing screen pixels as input. △ Less","8 July, 2022",https://arxiv.org/pdf/2207.01289
Mental Illness Classification on Social Media Texts using Deep Learning and Transfer Learning,Iqra Ameer;Muhammad Arif;Grigori Sidorov;Helena Gòmez-Adorno;Alexander Gelbukh,"Given the current social distance restrictions across the world, most individuals now use social media as their major medium of communication. Millions of people suffering from mental diseases have been isolated due to this, and they are unable to get help in person. They have become more reliant on online venues to express themselves and seek advice on dealing with their mental disorders. According to the World health organization (WHO), approximately 450 million people are affected. Mental illnesses, such as depression, anxiety, etc., are immensely common and have affected an individuals' physical health. Recently Artificial Intelligence (AI) methods have been presented to help mental health providers, including psychiatrists and psychologists, in decision making based on patients' authentic information (e.g., medical records, behavioral data, social media utilization, etc.). AI innovations have demonstrated predominant execution in numerous real-world applications broadening from computer vision to healthcare. This study analyzes unstructured user data on the Reddit platform and classifies five common mental illnesses: depression, anxiety, bipolar disorder, ADHD, and PTSD. We trained traditional machine learning, deep learning, and transfer learning multi-class models to detect mental disorders of individuals. This effort will benefit the public health system by automating the detection process and informing appropriate authorities about people who require emergency assistance. △ Less","3 July, 2022",https://arxiv.org/pdf/2207.01012
"Task-Oriented Sensing, Computation, and Communication Integration for Multi-Device Edge AI",Dingzhu Wen;Peixi Liu;Guangxu Zhu;Yuanming Shi;Jie Xu;Yonina C. Eldar;Shuguang Cui,"This paper studies a new multi-device edge artificial-intelligent (AI) system, which jointly exploits the AI model split inference and integrated sensing and communication (ISAC) to enable low-latency intelligent services at the network edge. In this system, multiple ISAC devices perform radar sensing to obtain multi-view data, and then offload the quantized version of extracted features to a centralized edge server, which conducts model inference based on the cascaded feature vectors. Under this setup and by considering classification tasks, we measure the inference accuracy by adopting an approximate but tractable metric, namely discriminant gain, which is defined as the distance of two classes in the Euclidean feature space under normalized covariance. To maximize the discriminant gain, we first quantify the influence of the sensing, computation, and communication processes on it with a derived closed-form expression. Then, an end-to-end task-oriented resource management approach is developed by integrating the three processes into a joint design. This integrated sensing, computation, and communication (ISCC) design approach, however, leads to a challenging non-convex optimization problem, due to the complicated form of discriminant gain and the device heterogeneity in terms of channel gain, quantization level, and generated feature subsets. Remarkably, the considered non-convex problem can be optimally solved based on the sum-of-ratios method. This gives the optimal ISCC scheme, that jointly determines the transmit power and time allocation at multiple devices for sensing and communication, as well as their quantization bits allocation for computation distortion control. By using human motions recognition as a concrete AI inference task, extensive experiments are conducted to verify the performance of our derived optimal ISCC scheme. △ Less","3 July, 2022",https://arxiv.org/pdf/2207.00969
WaferSegClassNet -- A Light-weight Network for Classification and Segmentation of Semiconductor Wafer Defects,Subhrajit Nag;Dhruv Makwana;Sai Chandra Teja R;Sparsh Mittal;C Krishna Mohan,"As the integration density and design intricacy of semiconductor wafers increase, the magnitude and complexity of defects in them are also on the rise. Since the manual inspection of wafer defects is costly, an automated artificial intelligence (AI) based computer-vision approach is highly desired. The previous works on defect analysis have several limitations, such as low accuracy and the need for separate models for classification and segmentation. For analyzing mixed-type defects, some previous works require separately training one model for each defect type, which is non-scalable. In this paper, we present WaferSegClassNet (WSCN), a novel network based on encoder-decoder architecture. WSCN performs simultaneous classification and segmentation of both single and mixed-type wafer defects. WSCN uses a ""shared encoder"" for classification, and segmentation, which allows training WSCN end-to-end. We use N-pair contrastive loss to first pretrain the encoder and then use BCE-Dice loss for segmentation, and categorical cross-entropy loss for classification. Use of N-pair contrastive loss helps in better embedding representation in the latent dimension of wafer maps. WSCN has a model size of only 0.51MB and performs only 0.2M FLOPS. Thus, it is much lighter than other state-of-the-art models. Also, it requires only 150 epochs for convergence, compared to 4,000 epochs needed by a previous work. We evaluate our model on the MixedWM38 dataset, which has 38,015 images. WSCN achieves an average classification accuracy of 98.2% and a dice coefficient of 0.9999. We are the first to show segmentation results on the MixedWM38 dataset. The source code can be obtained from https://github.com/ckmvigil/WaferSegClassNet. △ Less","3 July, 2022",https://arxiv.org/pdf/2207.00960
Complementary artificial intelligence designed to augment human discovery,Jamshid Sourati;James Evans,"Neither artificial intelligence designed to play Turing's imitation game, nor augmented intelligence built to maximize the human manipulation of information are tuned to accelerate innovation and improve humanity's collective advance against its greatest challenges. We reconceptualize and pilot beneficial AI to radically augment human understanding by complementing rather than competing with human cognitive capacity. Our approach to complementary intelligence builds on insights underlying the wisdom of crowds, which hinges on the independence and diversity of crowd members' information and approach. By programmatically incorporating information on the evolving distribution of scientific expertise from research papers, our approach follows the distribution of content in the literature while avoiding the scientific crowd and the hypotheses cognitively available to it. We use this approach to generate valuable predictions for what materials possess valuable energy-related properties (e.g., thermoelectricity), and what compounds possess valuable medical properties (e.g., asthma) that complement the human scientific crowd. We demonstrate that our complementary predictions, if identified by human scientists and inventors at all, are only discovered years further into the future. When we evaluate the promise of our predictions with first-principles equations, we demonstrate that increased complementarity of our predictions does not decrease and in some cases increases the probability that the predictions possess the targeted properties. In summary, by tuning AI to avoid the crowd, we can generate hypotheses unlikely to be imagined or pursued until the distant future and promise to punctuate scientific advance. By identifying and correcting for collective human bias, these models also suggest opportunities to improve human prediction by reformulating science education for discovery. △ Less","2 July, 2022",https://arxiv.org/pdf/2207.00902
"The Linguistic Blind Spot of Value-Aligned Agency, Natural and Artificial",Travis LaCroix,"The value-alignment problem for artificial intelligence (AI) asks how we can ensure that the 'values' (i.e., objective functions) of artificial systems are aligned with the values of humanity. In this paper, I argue that linguistic communication (natural language) is a necessary condition for robust value alignment. I discuss the consequences that the truth of this claim would have for research programmes that attempt to ensure value alignment for AI systems; or, more loftily, designing robustly beneficial or ethical artificial agents. △ Less","2 July, 2022",https://arxiv.org/pdf/2207.00868
Enabling Harmonious Human-Machine Interaction with Visual-Context Augmented Dialogue System: A Review,Hao Wang;Bin Guo;Yating Zeng;Yasan Ding;Chen Qiu;Ying Zhang;Lina Yao;Zhiwen Yu,"The intelligent dialogue system, aiming at communicating with humans harmoniously with natural language, is brilliant for promoting the advancement of human-machine interaction in the era of artificial intelligence. With the gradually complex human-computer interaction requirements (e.g., multimodal inputs, time sensitivity), it is difficult for traditional text-based dialogue system to meet the demands for more vivid and convenient interaction. Consequently, Visual Context Augmented Dialogue System (VAD), which has the potential to communicate with humans by perceiving and understanding multimodal information (i.e., visual context in images or videos, textual dialogue history), has become a predominant research paradigm. Benefiting from the consistency and complementarity between visual and textual context, VAD possesses the potential to generate engaging and context-aware responses. For depicting the development of VAD, we first characterize the concepts and unique features of VAD, and then present its generic system architecture to illustrate the system workflow. Subsequently, several research challenges and representative works are detailed investigated, followed by the summary of authoritative benchmarks. We conclude this paper by putting forward some open issues and promising research trends for VAD, e.g., the cognitive mechanisms of human-machine dialogue under cross-modal dialogue context, and knowledge-enhanced cross-modal semantic interaction. △ Less","2 July, 2022",https://arxiv.org/pdf/2207.00782
FAIR principles for AI models with a practical application for accelerated high energy diffraction microscopy,Nikil Ravi;Pranshu Chaturvedi;E. A. Huerta;Zhengchun Liu;Ryan Chard;Aristana Scourtas;K. J. Schmidt;Kyle Chard;Ben Blaiszik;Ian Foster,"A concise and measurable set of FAIR (Findable, Accessible, Interoperable and Reusable) principles for scientific data is transforming the state-of-practice for data management and stewardship, supporting and enabling discovery and innovation. Learning from this initiative, and acknowledging the impact of artificial intelligence (AI) in the practice of science and engineering, we introduce a set of practical, concise, and measurable FAIR principles for AI models. We showcase how to create and share FAIR data and AI models within a unified computational framework combining the following elements: the Advanced Photon Source at Argonne National Laboratory, the Materials Data Facility, the Data and Learning Hub for Science, and funcX, and the Argonne Leadership Computing Facility (ALCF), in particular the ThetaGPU supercomputer and the SambaNova DataScale system at the ALCF AI Testbed. We describe how this domain-agnostic computational framework may be harnessed to enable autonomous AI-driven discovery. △ Less","21 December, 2022",https://arxiv.org/pdf/2207.00611
AI in 6G: Energy-Efficient Distributed Machine Learning for Multilayer Heterogeneous Networks,Mohammad Arif Hossain;Abdullah Ridwan Hossain;Nirwan Ansari,"Adept network management is key for supporting extremely heterogeneous applications with stringent quality of service (QoS) requirements; this is more so when envisioning the complex and ultra-dense 6G mobile heterogeneous network (HetNet). From both the environmental and economical perspectives, non-homogeneous QoS demands obstruct the minimization of the energy footprints and operational costs of the envisioned robust networks. As such, network intelligentization is expected to play an essential role in the realization of such sophisticated aims. The fusion of artificial intelligence (AI) and mobile networks will allow for the dynamic and automatic configuration of network functionalities. Machine learning (ML), one of the backbones of AI, will be instrumental in forecasting changes in network loads and resource utilization, estimating channel conditions, optimizing network slicing, and enhancing security and encryption. However, it is well known that ML tasks themselves incur massive computational burdens and energy costs. To overcome such obstacles, we propose a novel layer-based HetNet architecture which optimally distributes tasks associated with different ML approaches across network layers and entities; such a HetNet boasts multiple access schemes as well as device-to-device (D2D) communications to enhance energy efficiency via collaborative learning and communications. △ Less","4 June, 2022",https://arxiv.org/pdf/2207.00415
A Functional Architecture for 6G Special Purpose Industrial IoT Networks,{Nurul Huda Mahmood;Gilberto Berardinelli;Emil J. Khatib;Ramin Hashemi;Carlos de Lima;Matti Latva-aho,"Future industrial applications will encompass compelling new use cases requiring stringent performance guarantees over multiple key performance indicators (KPI) such as reliability, dependability, latency, time synchronization, security, etc. Achieving such stringent and diverse service requirements necessitates the design of a special-purpose Industrial Internet of Things (IIoT) network comprising a multitude of specialized functionalities and technological enablers. This article proposes an innovative architecture for such a special-purpose 6G IIoT network incorporating seven functional building blocks categorized into: special-purpose functionalities and enabling technologies. The former consists of Wireless Environment Control, Traffic/Channel Prediction, Proactive Resource Management and End-to-End Optimization functions; whereas the latter includes Synchronization and Coordination, Machine Learning and Artificial Intelligence Algorithms, and Auxiliary Functions. The proposed architecture aims at providing a resource-efficient and holistic solution for the complex and dynamically challenging requirements imposed by future 6G industrial use cases. Selected test scenarios are provided and assessed to illustrate cross-functional collaboration and demonstrate the applicability of the proposed architecture in a wireless IIoT network. △ Less","1 July, 2022",https://arxiv.org/pdf/2207.00264
Threat Assessment in Machine Learning based Systems,Lionel Nganyewou Tidjon;Foutse Khomh,"Machine learning is a field of artificial intelligence (AI) that is becoming essential for several critical systems, making it a good target for threat actors. Threat actors exploit different Tactics, Techniques, and Procedures (TTPs) against the confidentiality, integrity, and availability of Machine Learning (ML) systems. During the ML cycle, they exploit adversarial TTPs to poison data and fool ML-based systems. In recent years, multiple security practices have been proposed for traditional systems but they are not enough to cope with the nature of ML-based systems. In this paper, we conduct an empirical study of threats reported against ML-based systems with the aim to understand and characterize the nature of ML threats and identify common mitigation strategies. The study is based on 89 real-world ML attack scenarios from the MITRE's ATLAS database, the AI Incident Database, and the literature; 854 ML repositories from the GitHub search and the Python Packaging Advisory database, selected based on their reputation. Attacks from the AI Incident Database and the literature are used to identify vulnerabilities and new types of threats that were not documented in ATLAS. Results show that convolutional neural networks were one of the most targeted models among the attack scenarios. ML repositories with the largest vulnerability prominence include TensorFlow, OpenCV, and Notebook. In this paper, we also report the most frequent vulnerabilities in the studied ML repositories, the most targeted ML phases and models, the most used TTPs in ML phases and attack scenarios. This information is particularly important for red/blue teams to better conduct attacks/defenses, for practitioners to prevent threats during ML development, and for researchers to develop efficient defense mechanisms. △ Less","30 June, 2022",https://arxiv.org/pdf/2207.00091
"""Explanation"" is Not a Technical Term: The Problem of Ambiguity in XAI",Leilani H. Gilpin;Andrew R. Paley;Mohammed A. Alam;Sarah Spurlock;Kristian J. Hammond,"There is broad agreement that Artificial Intelligence (AI) systems, particularly those using Machine Learning (ML), should be able to ""explain"" their behavior. Unfortunately, there is little agreement as to what constitutes an ""explanation."" This has caused a disconnect between the explanations that systems produce in service of explainable Artificial Intelligence (XAI) and those explanations that users and other audiences actually need, which should be defined by the full spectrum of functional roles, audiences, and capabilities for explanation. In this paper, we explore the features of explanations and how to use those features in evaluating their utility. We focus on the requirements for explanations defined by their functional role, the knowledge states of users who are trying to understand them, and the availability of the information needed to generate them. Further, we discuss the risk of XAI enabling trust in systems without establishing their trustworthiness and define a critical next step for the field of XAI to establish metrics to guide and ground the utility of system-generated explanations. △ Less","27 June, 2022",https://arxiv.org/pdf/2207.00007
Ensemble CNN models for Covid-19 Recognition and Severity Perdition From 3D CT-scan,Fares Bougourzi;Cosimo Distante;Fadi Dornaika;Abdelmalik Taleb-Ahmed,"Since the appearance of Covid-19 in late 2019, Covid-19 has become an active research topic for the artificial intelligence (AI) community. One of the most interesting AI topics is Covid-19 analysis of medical imaging. CT-scan imaging is the most informative tool about this disease. This work is part of the 2nd COV19D competition, where two challenges are set: Covid-19 Detection and Covid-19 Severity Detection from the CT-scans. For Covid-19 detection from CT-scans, we proposed an ensemble of 2D Convolution blocks with Densenet-161 models. Here, each 2D convolutional block with Densenet-161 architecture is trained separately and in testing phase, the ensemble model is based on the average of their probabilities. On the other hand, we proposed an ensemble of Convolutional Layers with Inception models for Covid-19 severity detection. In addition to the Convolutional Layers, three Inception variants were used, namely Inception-v3, Inception-v4 and Inception-Resnet. Our proposed approaches outperformed the baseline approach in the validation data of the 2nd COV19D competition by 11% and 16% for Covid-19 detection and Covid-19 severity detection, respectively. △ Less","29 June, 2022",https://arxiv.org/pdf/2206.15431
Mastering the Game of Stratego with Model-Free Multiagent Reinforcement Learning,Julien Perolat;Bart de Vylder;Daniel Hennes;Eugene Tarassov;Florian Strub;Vincent de Boer;Paul Muller;Jerome T. Connor;Neil Burch;Thomas Anthony;Stephen McAleer;Romuald Elie;Sarah H. Cen;Zhe Wang;Audrunas Gruslys;Aleksandra Malysheva;Mina Khan;Sherjil Ozair;Finbarr Timbers;Toby Pohlen;Tom Eccles;Mark Rowland;Marc Lanctot;Jean-Baptiste Lespiau;Bilal Piot,"We introduce DeepNash, an autonomous agent capable of learning to play the imperfect information game Stratego from scratch, up to a human expert level. Stratego is one of the few iconic board games that Artificial Intelligence (AI) has not yet mastered. This popular game has an enormous game tree on the order of 10^{535} nodes, i.e., 10^{175} times larger than that of Go. It has the additional complexity of requiring decision-making under imperfect information, similar to Texas hold'em poker, which has a significantly smaller game tree (on the order of 10^{164} nodes). Decisions in Stratego are made over a large number of discrete actions with no obvious link between action and outcome. Episodes are long, with often hundreds of moves before a player wins, and situations in Stratego can not easily be broken down into manageably-sized sub-problems as in poker. For these reasons, Stratego has been a grand challenge for the field of AI for decades, and existing AI methods barely reach an amateur level of play. DeepNash uses a game-theoretic, model-free deep reinforcement learning method, without search, that learns to master Stratego via self-play. The Regularised Nash Dynamics (R-NaD) algorithm, a key component of DeepNash, converges to an approximate Nash equilibrium, instead of 'cycling' around it, by directly modifying the underlying multi-agent learning dynamics. DeepNash beats existing state-of-the-art AI methods in Stratego and achieved a yearly (2022) and all-time top-3 rank on the Gravon games platform, competing with human expert players. △ Less","30 June, 2022",https://arxiv.org/pdf/2206.15378
Why we do need Explainable AI for Healthcare,Giovanni Cinà;Tabea Röber;Rob Goedhart;Ilker Birbil,"The recent spike in certified Artificial Intelligence (AI) tools for healthcare has renewed the debate around adoption of this technology. One thread of such debate concerns Explainable AI and its promise to render AI devices more transparent and trustworthy. A few voices active in the medical AI space have expressed concerns on the reliability of Explainable AI techniques, questioning their use and inclusion in guidelines and standards. Revisiting such criticisms, this article offers a balanced and comprehensive perspective on the utility of Explainable AI, focusing on the specificity of clinical applications of AI and placing them in the context of healthcare interventions. Against its detractors and despite valid concerns, we argue that the Explainable AI research program is still central to human-machine interaction and ultimately our main tool against loss of control, a danger that cannot be prevented by rigorous clinical validation alone. △ Less","30 June, 2022",https://arxiv.org/pdf/2206.15363
AI for CSI Feedback Enhancement in 5G-Advanced,Jiajia Guo;Chao-Kai Wen;Shi Jin;Xiao Li,"The 3rd Generation Partnership Project started the study of Release 18 in 2021. Artificial intelligence (AI)-native air interface is one of the key features of Release 18, where AI for channel state information (CSI) feedback enhancement is selected as the representative use case. This article provides an overview of AI for CSI feedback enhancement in 5G-Advanced. Several representative non-AI and AI-enabled CSI feedback frameworks are first introduced and compared. Then, the standardization of AI for CSI feedback enhancement in 5G-advanced is presented in detail. First, the scope of the AI for CSI feedback enhancement in 5G-Advanced is presented and discussed. Then, the main challenges and open problems in the standardization of AI for CSI feedback enhancement, especially focusing on performance evaluation and the design of new protocols for AI-enabled CSI feedback, are identified and discussed. This article provides a guideline for the standardization study of AI-based CSI feedback enhancement. △ Less","17 September, 2022",https://arxiv.org/pdf/2206.15132
Interpretable Melody Generation from Lyrics with Discrete-Valued Adversarial Training,Wei Duan;Zhe Zhang;Yi Yu;Keizo Oyama,"Generating melody from lyrics is an interesting yet challenging task in the area of artificial intelligence and music. However, the difficulty of keeping the consistency between input lyrics and generated melody limits the generation quality of previous works. In our proposal, we demonstrate our proposed interpretable lyrics-to-melody generation system which can interact with users to understand the generation process and recreate the desired songs. To improve the reliability of melody generation that matches lyrics, mutual information is exploited to strengthen the consistency between lyrics and generated melodies. Gumbel-Softmax is exploited to solve the non-differentiability problem of generating discrete music attributes by Generative Adversarial Networks (GANs). Moreover, the predicted probabilities output by the generator is utilized to recommend music attributes. Interacting with our lyrics-to-melody generation system, users can listen to the generated AI song as well as recreate a new song by selecting from recommended music attributes. △ Less","30 June, 2022",https://arxiv.org/pdf/2206.15027
A Data Science Pipeline for Algorithmic Trading: A Comparative Study of Applications for Finance and Cryptoeconomics,Luyao Zhang;Tianyu Wu;Saad Lahrichi;Carlos-Gustavo Salas-Flores;Jiayi Li,"Recent advances in Artificial Intelligence (AI) have made algorithmic trading play a central role in finance. However, current research and applications are disconnected information islands. We propose a generally applicable pipeline for designing, programming, and evaluating the algorithmic trading of stock and crypto assets. Moreover, we demonstrate how our data science pipeline works with respect to four conventional algorithms: the moving average crossover, volume-weighted average price, sentiment analysis, and statistical arbitrage algorithms. Our study offers a systematic way to program, evaluate, and compare different trading strategies. Furthermore, we implement our algorithms through object-oriented programming in Python3, which serves as open-source software for future academic research and applications. △ Less","29 June, 2022",https://arxiv.org/pdf/2206.14932
System-level Simulation of Reconfigurable Intelligent Surface assisted Wireless Communications System,Qi Gu;Dan Wu;Xin Su;Hanning Wang;Jingyuan Cui;Yifei Yuan,"Reconfigurable intelligent surface (RIS) is an emerging technique employing metasurface to reflect the signal from the source node to the destination node. By smartly reconfiguring the electromagnetic (EM) properties of the metasurface and adjusting the EM parameters of the reflected radio waves, RIS can turn the uncontrollable propagation environment into an artificially reconfigurable space, and thus, can significantly increase the communications capacity and improve the coverage of the system. In this paper, we investigate the far field channel in which the line-of-sight (LOS) propagation is dominant. We propose an antenna model that can characterize the radiation patterns of realistic RIS elements, and consider the signal power received from the two-hop path through RIS. System-level simulations of network performance under various scenarios and parameter. △ Less","29 June, 2022",https://arxiv.org/pdf/2206.14777
Not Cheating on the Turing Test: Towards Grounded Language Learning in Artificial Intelligence,Lize Alberts,"Recent hype surrounding the increasing sophistication of language processing models has renewed optimism regarding machines achieving a human-like command of natural language. Research in the area of natural language understanding (NLU) in artificial intelligence claims to have been making great strides in this area, however, the lack of conceptual clarity/consistency in how 'understanding' is used in this and other disciplines makes it difficult to discern how close we actually are. In this interdisciplinary research thesis, I integrate insights from cognitive science/psychology, philosophy of mind, and cognitive linguistics, and evaluate it against a critical review of current approaches in NLU to explore the basic requirements--and remaining challenges--for developing artificially intelligent systems with human-like capacities for language use and comprehension. △ Less","17 November, 2022",https://arxiv.org/pdf/2206.14672
Quantification of Deep Neural Network Prediction Uncertainties for VVUQ of Machine Learning Models,Mahmoud Yaseen;Xu Wu,"Recent performance breakthroughs in Artificial intelligence (AI) and Machine learning (ML), especially advances in Deep learning (DL), the availability of powerful, easy-to-use ML libraries (e.g., scikit-learn, TensorFlow, PyTorch.), and increasing computational power have led to unprecedented interest in AI/ML among nuclear engineers. For physics-based computational models, Verification, Validation and Uncertainty Quantification (VVUQ) have been very widely investigated and a lot of methodologies have been developed. However, VVUQ of ML models has been relatively less studied, especially in nuclear engineering. In this work, we focus on UQ of ML models as a preliminary step of ML VVUQ, more specifically, Deep Neural Networks (DNNs) because they are the most widely used supervised ML algorithm for both regression and classification tasks. This work aims at quantifying the prediction, or approximation uncertainties of DNNs when they are used as surrogate models for expensive physical models. Three techniques for UQ of DNNs are compared, namely Monte Carlo Dropout (MCD), Deep Ensembles (DE) and Bayesian Neural Networks (BNNs). Two nuclear engineering examples are used to benchmark these methods, (1) time-dependent fission gas release data using the Bison code, and (2) void fraction simulation based on the BFBT benchmark using the TRACE code. It was found that the three methods typically require different DNN architectures and hyperparameters to optimize their performance. The UQ results also depend on the amount of training data available and the nature of the data. Overall, all these three methods can provide reasonable estimations of the approximation uncertainties. The uncertainties are generally smaller when the mean predictions are close to the test data, while the BNN methods usually produce larger uncertainties than MCD and DE. △ Less","27 June, 2022",https://arxiv.org/pdf/2206.14615
On-device Synaptic Memory Consolidation using Fowler-Nordheim Quantum-tunneling,Mustafizur Rahman;Subhankar Bose;Shantanu Chakrabartty,"Synaptic memory consolidation has been heralded as one of the key mechanisms for supporting continual learning in neuromorphic Artificial Intelligence (AI) systems. Here we report that a Fowler-Nordheim (FN) quantum-tunneling device can implement synaptic memory consolidation similar to what can be achieved by algorithmic consolidation models like the cascade and the elastic weight consolidation (EWC) models. The proposed FN-synapse not only stores the synaptic weight but also stores the synapse's historical usage statistic on the device itself. We also show that the operation of the FN-synapse is near-optimal in terms of the synaptic lifetime and we demonstrate that a network comprising FN-synapses outperforms a comparable EWC network for a small benchmark continual learning task. With an energy footprint of femtojoules per synaptic update, we believe that the proposed FN-synapse provides an ultra-energy-efficient approach for implementing both synaptic memory consolidation and persistent learning. △ Less","27 June, 2022",https://arxiv.org/pdf/2206.14581
Convolutional Neural Network Based Partial Face Detection,Md. Towfiqul Islam;Tanzim Ahmed;A. B. M. Raihanur Rashid;Taminul Islam;Md. Sadekur Rahman;Md. Tarek Habib,"Due to the massive explanation of artificial intelligence, machine learning technology is being used in various areas of our day-to-day life. In the world, there are a lot of scenarios where a simple crime can be prevented before it may even happen or find the person responsible for it. A face is one distinctive feature that we have and can differentiate easily among many other species. But not just different species, it also plays a significant role in determining someone from the same species as us, humans. Regarding this critical feature, a single problem occurs most often nowadays. When the camera is pointed, it cannot detect a person's face, and it becomes a poor image. On the other hand, where there was a robbery and a security camera installed, the robber's identity is almost indistinguishable due to the low-quality camera. But just making an excellent algorithm to work and detecting a face reduces the cost of hardware, and it doesn't cost that much to focus on that area. Facial recognition, widget control, and such can be done by detecting the face correctly. This study aims to create and enhance a machine learning model that correctly recognizes faces. Total 627 Data have been collected from different Bangladeshi people's faces on four angels. In this work, CNN, Harr Cascade, Cascaded CNN, Deep CNN & MTCNN are these five machine learning approaches implemented to get the best accuracy of our dataset. After creating and running the model, Multi-Task Convolutional Neural Network (MTCNN) achieved 96.2% best model accuracy with training data rather than other machine learning models. △ Less","28 June, 2022",https://arxiv.org/pdf/2206.14350
ECG Heartbeat classification using deep transfer learning with Convolutional Neural Network and STFT technique,Minh Cao;Tianqi Zhao;Yanxun Li;Wenhao Zhang;Peyman Benharash;Ramin Ramezani,"Electrocardiogram (ECG) is a simple non-invasive measure to identify heart-related issues such as irregular heartbeats known as arrhythmias. While artificial intelligence and machine learning is being utilized in a wide range of healthcare related applications and datasets, many arrhythmia classifiers using deep learning methods have been proposed in recent years. However, sizes of the available datasets from which to build and assess machine learning models is often very small and the lack of well-annotated public ECG datasets is evident. In this paper, we propose a deep transfer learning framework that is aimed to perform classification on a small size training dataset. The proposed method is to fine-tune a general-purpose image classifier ResNet-18 with MIT-BIH arrhythmia dataset in accordance with the AAMI EC57 standard. This paper further investigates many existing deep learning models that have failed to avoid data leakage against AAMI recommendations. We compare how different data split methods impact the model performance. This comparison study implies that future work in arrhythmia classification should follow the AAMI EC57 standard when using any including MIT-BIH arrhythmia dataset. △ Less","7 July, 2022",https://arxiv.org/pdf/2206.14200
Multi-Fault Diagnosis Of Industrial Rotating Machines Using Data-Driven Approach: A Review Of Two Decades Of Research,Shreyas Gawde;Shruti Patil;Satish Kumar;Pooja Kamat;Ketan Kotecha;Ajith Abraham,"Industry 4.0 is an era of smart manufacturing. Manufacturing is impossible without the use of machinery. Majority of these machines comprise rotating components and are called rotating machines. The engineers' top priority is to maintain these critical machines to reduce the unplanned shutdown and increase the useful life of machinery. Predictive maintenance (PDM) is the current trend of smart maintenance. The challenging task in PDM is to diagnose the type of fault. With Artificial Intelligence (AI) advancement, data-driven approach for predictive maintenance is taking a new flight towards smart manufacturing. Several researchers have published work related to fault diagnosis in rotating machines, mainly exploring a single type of fault. However, a consolidated review of literature that focuses more on multi-fault diagnosis of rotating machines is lacking. There is a need to systematically cover all the aspects right from sensor selection, data acquisition, feature extraction, multi-sensor data fusion to the systematic review of AI techniques employed in multi-fault diagnosis. In this regard, this paper attempts to achieve the same by implementing a systematic literature review on a Data-driven approach for multi-fault diagnosis of Industrial Rotating Machines using Preferred Reporting Items for Systematic Reviews and Meta-Analysis (PRISMA) method. The PRISMA method is a collection of guidelines for the composition and structure of systematic reviews and other meta-analyses. This paper identifies the foundational work done in the field and gives a comparative study of different aspects related to multi-fault diagnosis of industrial rotating machines. The paper also identifies the major challenges, research gap. It gives solutions using recent advancements in AI in implementing multi-fault diagnosis, giving a strong base for future research in this field. △ Less","30 May, 2022",https://arxiv.org/pdf/2206.14153
Comparing and extending the use of defeasible argumentation with quantitative data in real-world contexts,Lucas Rizzo;Luca Longo,"Dealing with uncertain, contradicting, and ambiguous information is still a central issue in Artificial Intelligence (AI). As a result, many formalisms have been proposed or adapted so as to consider non-monotonicity, with only a limited number of works and researchers performing any sort of comparison among them. A non-monotonic formalism is one that allows the retraction of previous conclusions or claims, from premises, in light of new evidence, offering some desirable flexibility when dealing with uncertainty. This research article focuses on evaluating the inferential capacity of defeasible argumentation, a formalism particularly envisioned for modelling non-monotonic reasoning. In addition to this, fuzzy reasoning and expert systems, extended for handling non-monotonicity of reasoning, are selected and employed as baselines, due to their vast and accepted use within the AI community. Computational trust was selected as the domain of application of such models. Trust is an ill-defined construct, hence, reasoning applied to the inference of trust can be seen as non-monotonic. Inference models were designed to assign trust scalars to editors of the Wikipedia project. In particular, argument-based models demonstrated more robustness than those built upon the baselines despite the knowledge bases or datasets employed. This study contributes to the body of knowledge through the exploitation of defeasible argumentation and its comparison to similar approaches. The practical use of such approaches coupled with a modular design that facilitates similar experiments was exemplified and their respective implementations made publicly available on GitHub [120, 121]. This work adds to previous works, empirically enhancing the generalisability of defeasible argumentation as a compelling approach to reason with quantitative data and uncertain knowledge. △ Less","28 June, 2022",https://arxiv.org/pdf/2206.13959
Explaining Any ML Model? -- On Goals and Capabilities of XAI,Moritz Renftle;Holger Trittenbach;Michael Poznic;Reinhard Heil,"An increasing ubiquity of machine learning (ML) motivates research on algorithms to explain ML models and their predictions -- so-called eXplainable Artificial Intelligence (XAI). Despite many survey papers and discussions, the goals and capabilities of XAI algorithms are far from being well understood. We argue that this is because of a problematic reasoning scheme in XAI literature: XAI algorithms are said to complement ML models with desired properties, such as ""interpretability"", or ""explainability"". These properties are in turn assumed to contribute to a goal, like ""trust"" in an ML system. But most properties lack precise definitions and their relationship to such goals is far from obvious. The result is a reasoning scheme that obfuscates research results and leaves an important question unanswered: What can one expect from XAI algorithms? In this article, we clarify the goals and capabilities of XAI algorithms from a concrete perspective: that of their users. Explaining ML models is only necessary if users have questions about them. We show that users can ask diverse questions, but that only one of them can be answered by current XAI algorithms. Answering this core question can be trivial, difficult or even impossible, depending on the ML application. Based on these insights, we outline which capabilities policymakers, researchers and society can reasonably expect from XAI algorithms. △ Less","28 June, 2022",https://arxiv.org/pdf/2206.13888
ProGen2: Exploring the Boundaries of Protein Language Models,Erik Nijkamp;Jeffrey Ruffolo;Eli N. Weinstein;Nikhil Naik;Ali Madani,"Attention-based models trained on protein sequences have demonstrated incredible success at classification and generation tasks relevant for artificial intelligence-driven protein design. However, we lack a sufficient understanding of how very large-scale models and data play a role in effective protein model development. We introduce a suite of protein language models, named ProGen2, that are scaled up to 6.4B parameters and trained on different sequence datasets drawn from over a billion proteins from genomic, metagenomic, and immune repertoire databases. ProGen2 models show state-of-the-art performance in capturing the distribution of observed evolutionary sequences, generating novel viable sequences, and predicting protein fitness without additional finetuning. As large model sizes and raw numbers of protein sequences continue to become more widely accessible, our results suggest that a growing emphasis needs to be placed on the data distribution provided to a protein sequence model. We release the ProGen2 models and code at https://github.com/salesforce/progen. △ Less","27 June, 2022",https://arxiv.org/pdf/2206.13517
AI-based computer-aided diagnostic system of chest digital tomography synthesis: Demonstrating comparative advantage with X-ray-based AI systems,Kyung-Su Kim;Ju Hwan Lee;Seong Je Oh;Myung Jin Chung,"Compared with chest X-ray (CXR) imaging, which is a single image projected from the front of the patient, chest digital tomosynthesis (CDTS) imaging can be more advantageous for lung lesion detection because it acquires multiple images projected from multiple angles of the patient. Various clinical comparative analysis and verification studies have been reported to demonstrate this, but there were no artificial intelligence (AI)-based comparative analysis studies. Existing AI-based computer-aided detection (CAD) systems for lung lesion diagnosis have been developed mainly based on CXR images; however, CAD-based on CDTS, which uses multi-angle images of patients in various directions, has not been proposed and verified for its usefulness compared to CXR-based counterparts. This study develops/tests a CDTS-based AI CAD system to detect lung lesions to demonstrate performance improvements compared to CXR-based AI CAD. We used multiple projection images as input for the CDTS-based AI model and a single-projection image as input for the CXR-based AI model to fairly compare and evaluate the performance between models. The proposed CDTS-based AI CAD system yielded sensitivities of 0.782 and 0.785 and accuracies of 0.895 and 0.837 for the performance of detecting tuberculosis and pneumonia, respectively, against normal subjects. These results show higher performance than sensitivities of 0.728 and 0.698 and accuracies of 0.874 and 0.826 for detecting tuberculosis and pneumonia through the CXR-based AI CAD, which only uses a single projection image in the frontal direction. We found that CDTS-based AI CAD improved the sensitivity of tuberculosis and pneumonia by 5.4% and 8.7% respectively, compared to CXR-based AI CAD without loss of accuracy. Therefore, we comparatively prove that CDTS-based AI CAD technology can improve performance more than CXR, enhancing the clinical applicability of CDTS. △ Less","18 June, 2022",https://arxiv.org/pdf/2206.13504
An Efficient Industrial Federated Learning Framework for AIoT: A Face Recognition Application,Youlong Ding;Xueyang Wu;Zhitao Li;Zeheng Wu;Shengqi Tan;Qian Xu;Weike Pan;Qiang Yang,"Recently, the artificial intelligence of things (AIoT) has been gaining increasing attention, with an intriguing vision of providing highly intelligent services through the network connection of things, leading to an advanced AI-driven ecology. However, recent regulatory restrictions on data privacy preclude uploading sensitive local data to data centers and utilizing them in a centralized approach. Directly applying federated learning algorithms in this scenario could hardly meet the industrial requirements of both efficiency and accuracy. Therefore, we propose an efficient industrial federated learning framework for AIoT in terms of a face recognition application. Specifically, we propose to utilize the concept of transfer learning to speed up federated training on devices and further present a novel design of a private projector that helps protect shared gradients without incurring additional memory consumption or computational cost. Empirical studies on a private Asian face dataset show that our approach can achieve high recognition accuracy in only 20 communication rounds, demonstrating its effectiveness in prediction and its efficiency in training. △ Less","30 June, 2022",https://arxiv.org/pdf/2206.13398
Multi-Agent Car Parking using Reinforcement Learning,Omar Tanner,"As the industry of autonomous driving grows, so does the potential interaction of groups of autonomous cars. Combined with the advancement of Artificial Intelligence and simulation, such groups can be simulated, and safety-critical models can be learned controlling the cars within. This study applies reinforcement learning to the problem of multi-agent car parking, where groups of cars aim to efficiently park themselves, while remaining safe and rational. Utilising robust tools and machine learning frameworks, we design and implement a flexible car parking environment in the form of a Markov decision process with independent learners, exploiting multi-agent communication. We implement a suite of tools to perform experiments at scale, obtaining models parking up to 7 cars with over a 98.1% success rate, significantly beating existing single-agent models. We also obtain several results relating to competitive and collaborative behaviours exhibited by the cars in our environment, with varying densities and levels of communication. Notably, we discover a form of collaboration that cannot arise without competition, and a 'leaky' form of collaboration whereby agents collaborate without sufficient state. Such work has numerous potential applications in the autonomous driving and fleet management industries, and provides several useful techniques and benchmarks for the application of reinforcement learning to multi-agent car parking. △ Less","22 June, 2022",https://arxiv.org/pdf/2206.13338
Automated Systems For Diagnosis of Dysgraphia in Children: A Survey and Novel Framework,Jayakanth Kunhoth;Somaya Al-Maadeed;Suchithra Kunhoth;Younus Akbari,"Learning disabilities, which primarily interfere with the basic learning skills such as reading, writing and math, are known to affect around 10% of children in the world. The poor motor skills and motor coordination as part of the neurodevelopmental disorder can become a causative factor for the difficulty in learning to write (dysgraphia), hindering the academic track of an individual. The signs and symptoms of dysgraphia include but are not limited to irregular handwriting, improper handling of writing medium, slow or labored writing, unusual hand position, etc. The widely accepted assessment criterion for all the types of learning disabilities is the examination performed by medical experts. The few available artificial intelligence-powered screening systems for dysgraphia relies on the distinctive features of handwriting from the corresponding images.This work presents a review of the existing automated dysgraphia diagnosis systems for children in the literature. The main focus of the work is to review artificial intelligence-based systems for dysgraphia diagnosis in children. This work discusses the data collection method, important handwriting features, machine learning algorithms employed in the literature for the diagnosis of dysgraphia. Apart from that, this article discusses some of the non-artificial intelligence-based automated systems also. Furthermore, this article discusses the drawbacks of existing systems and proposes a novel framework for dysgraphia diagnosis. △ Less","27 June, 2022",https://arxiv.org/pdf/2206.13043
Szloca: towards a framework for full 3D tracking through a single camera in context of interactive arts,Sahaj Garg,"Realtime virtual data of objects and human presence in a large area holds a valuable key in enabling many experiences and applications in various industries and with exponential rise in the technological development of artificial intelligence, computer vision has expanded the possibilities of tracking and classifying things through just video inputs, which is also surpassing the limitations of most popular and common hardware setups known traditionally to detect human pose and position, such as low field of view and limited tracking capacity. The benefits of using computer vision in application development is large as it augments traditional input sources (like video streams) and can be integrated in many environments and platforms. In the context of new media interactive arts, based on physical movements and expanding over large areas or gallaries, this research presents a novel way and a framework towards obtaining data and virtual representation of objects/people - such as three-dimensional positions, skeltons/pose and masks from a single rgb camera. Looking at the state of art through some recent developments and building on prior research in the field of computer vision, the paper also proposes an original method to obtain three dimensional position data from monocular images, the model does not rely on complex training of computer vision systems but combines prior computer vision research and adds a capacity to represent z depth, ieto represent a world position in 3 axis from a 2d input source. △ Less","26 June, 2022",https://arxiv.org/pdf/2206.12958
Malware Detection and Prevention using Artificial Intelligence Techniques,Md Jobair Hossain Faruk;Hossain Shahriar;Maria Valero;Farhat Lamia Barsha;Shahriar Sobhan;Md Abdullah Khan;Michael Whitman;Alfredo Cuzzocreak;Dan Lo;Akond Rahman;Fan Wu,"With the rapid technological advancement, security has become a major issue due to the increase in malware activity that poses a serious threat to the security and safety of both computer systems and stakeholders. To maintain stakeholders, particularly, end users security, protecting the data from fraudulent efforts is one of the most pressing concerns. A set of malicious programming code, scripts, active content, or intrusive software that is designed to destroy intended computer systems and programs or mobile and web applications is referred to as malware. According to a study, naive users are unable to distinguish between malicious and benign applications. Thus, computer systems and mobile applications should be designed to detect malicious activities towards protecting the stakeholders. A number of algorithms are available to detect malware activities by utilizing novel concepts including Artificial Intelligence, Machine Learning, and Deep Learning. In this study, we emphasize Artificial Intelligence (AI) based techniques for detecting and preventing malware activity. We present a detailed review of current malware detection technologies, their shortcomings, and ways to improve efficiency. Our study shows that adopting futuristic approaches for the development of malware detection applications shall provide significant advantages. The comprehension of this synthesis shall help researchers for further research on malware detection and prevention using AI. △ Less","25 June, 2022",https://arxiv.org/pdf/2206.12770
Learning to Infer 3D Shape Programs with Differentiable Renderer,Yichao Liang,"Given everyday artifacts, such as tables and chairs, humans recognize high-level regularities within them, such as the symmetries of a table, the repetition of its legs, while possessing low-level priors of their geometries, e.g., surfaces are smooth and edges are sharp. This kind of knowledge constitutes an important part of human perceptual understanding and reasoning. Representations of and how to reason in such knowledge, and the acquisition thereof, are still open questions in artificial intelligence (AI) and cognitive science. Building on the previous proposal of the \emph{3D shape programs} representation alone with the accompanying neural generator and executor from \citet{tian2019learning}, we propose an analytical yet differentiable executor that is more faithful and controllable in interpreting shape programs (particularly in extrapolation) and more sample efficient (requires no training). These facilitate the generator's learning when ground truth programs are not available, and should be especially useful when new shape-program components are enrolled either by human designers or -- in the context of library learning -- algorithms themselves. Preliminary experiments on using it for adaptation illustrate the aforesaid advantages of the proposed module, encouraging similar methods being explored in building machines that learn to reason with the kind of knowledge described above, and even learn this knowledge itself. △ Less","25 June, 2022",https://arxiv.org/pdf/2206.12675
Crypto Makes AI Evolve,Behrouz Zolfaghari;Elnaz Rabieinejad;Abbas Yazdinejad;Reza M. Parizi;Ali Dehghantanha,"Adopting cryptography has given rise to a significant evolution in Artificial Intelligence (AI). This paper studies the path and stages of this evolution. We start with reviewing existing relevant surveys, noting their shortcomings, especially the lack of a close look at the evolution process and solid future roadmap. These shortcomings justify the work of this paper. Next, we identify, define and discuss five consequent stages in the evolution path, including Crypto-Sensitive AI, Crypto-Adapted AI, Crypto-Friendly AI, Crypto-Enabled AI, Crypto-Protected AI. Then, we establish a future roadmap for further research in this area, focusing on the role of quantum-inspired and bio-inspired AI. △ Less","25 June, 2022",https://arxiv.org/pdf/2206.12669
Unified BERT for Few-shot Natural Language Understanding,Junyu Lu;Ping Yang;Ruyi Gan;Jing Yang;Jiaxing Zhang,"Even as pre-trained language models share a semantic encoder, natural language understanding suffers from a diversity of output schemas. In this paper, we propose UBERT, a unified bidirectional language understanding model based on BERT framework, which can universally model the training objects of different NLU tasks through a biaffine network. Specifically, UBERT encodes prior knowledge from various aspects, uniformly constructing learning representations across multiple NLU tasks, which is conducive to enhancing the ability to capture common semantic understanding. By using the biaffine to model scores pair of the start and end position of the original text, various classification and extraction structures can be converted into a universal, span-decoding approach. Experiments show that UBERT wins the first price in the 2022 AIWIN - World Artificial Intelligence Innovation Competition, Chinese insurance few-shot multi-task track, and realizes the unification of extensive information extraction and linguistic reasoning tasks. △ Less","13 August, 2022",https://arxiv.org/pdf/2206.12094
"Never trust, always verify : a roadmap for Trustworthy AI?",Lionel Nganyewou Tidjon;Foutse Khomh,"Artificial Intelligence (AI) is becoming the corner stone of many systems used in our daily lives such as autonomous vehicles, healthcare systems, and unmanned aircraft systems. Machine Learning is a field of AI that enables systems to learn from data and make decisions on new data based on models to achieve a given goal. The stochastic nature of AI models makes verification and validation tasks challenging. Moreover, there are intrinsic biaises in AI models such as reproductibility bias, selection bias (e.g., races, genders, color), and reporting bias (i.e., results that do not reflect the reality). Increasingly, there is also a particular attention to the ethical, legal, and societal impacts of AI. AI systems are difficult to audit and certify because of their black-box nature. They also appear to be vulnerable to threats; AI systems can misbehave when untrusted data are given, making them insecure and unsafe. Governments, national and international organizations have proposed several principles to overcome these challenges but their applications in practice are limited and there are different interpretations in the principles that can bias implementations. In this paper, we examine trust in the context of AI-based systems to understand what it means for an AI system to be trustworthy and identify actions that need to be undertaken to ensure that AI systems are trustworthy. To achieve this goal, we first review existing approaches proposed for ensuring the trustworthiness of AI systems, in order to identify potential conceptual gaps in understanding what trustworthy AI is. Then, we suggest a trust (resp. zero-trust) model for AI and suggest a set of properties that should be satisfied to ensure the trustworthiness of AI systems. △ Less","23 June, 2022",https://arxiv.org/pdf/2206.11981
"Navigating Incommensurability Between Ethnomethodology, Conversation Analysis, and Artificial Intelligence",Stuart Reeves,"Like many research communities, ethnomethodologists and conversation analysts have begun to get caught up -- yet again -- in the pervasive spectacle of surging interests in Artificial Intelligence (AI). Inspired by discussions amongst a growing network of researchers in ethnomethodology (EM) and conversation analysis (CA) traditions who nurse such interests, I started thinking about what things EM and the more EM end of conversation analysis might be doing about, for, or even with, fields of AI research. So, this piece is about the disciplinary and conceptual questions that might be encountered, and -- in my view -- may need addressing for engagements with AI research and its affiliates. Although I'm mostly concerned with things to be aware of as well as outright dangers, later on we can think about some opportunities. And throughout I will keep using 'we' to talk about EM&CA researchers; but this really is for convenience only -- I don't wish to ventriloquise for our complex research communities. All of the following should be read as emanating from my particular research history, standpoint etc., and treated (hopefully) as an invitation for further discussion amongst EM and CA researchers turning to technology and AI specifically. △ Less","27 June, 2022",https://arxiv.org/pdf/2206.11899
"Modeling the System-Level Reliability towards a Convergence of Communication, Computing and Control",Bin Han;Hans D. Schotten,"Enabled and driven by modern advances in wireless telecommunication and artificial intelligence, the convergence of communication, computing, and control is becoming inevitable in future industrial applications. Analytical and optimizing frameworks, however, are not yet readily developed for this new technical trend. In this work we discuss the necessity and typical scenarios of this convergence, and propose a new approach to model the system-level reliability across all involved domainss △ Less","23 June, 2022",https://arxiv.org/pdf/2206.11522
ICOS Protein Expression Segmentation: Can Transformer Networks Give Better Results?,Vivek Kumar Singh;Paul O Reilly;Jacqueline James;Manuel Salto Tellez;Perry Maxwell,"Biomarkers identify a patients response to treatment. With the recent advances in artificial intelligence based on the Transformer networks, there is only limited research has been done to measure the performance on challenging histopathology images. In this paper, we investigate the efficacy of the numerous state-of-the-art Transformer networks for immune-checkpoint biomarker, Inducible Tcell COStimulator (ICOS) protein cell segmentation in colon cancer from immunohistochemistry (IHC) slides. Extensive and comprehensive experimental results confirm that MiSSFormer achieved the highest Dice score of 74.85% than the rest evaluated Transformer and Efficient U-Net methods. △ Less","23 June, 2022",https://arxiv.org/pdf/2206.11520
A Review of Published Machine Learning Natural Language Processing Applications for Protocolling Radiology Imaging,Nihal Raju;Michael Woodburn;Stefan Kachel;Jack O'Shaughnessy;Laurence Sorace;Natalie Yang;Ruth P Lim,"Machine learning (ML) is a subfield of Artificial intelligence (AI), and its applications in radiology are growing at an ever-accelerating rate. The most studied ML application is the automated interpretation of images. However, natural language processing (NLP), which can be combined with ML for text interpretation tasks, also has many potential applications in radiology. One such application is automation of radiology protocolling, which involves interpreting a clinical radiology referral and selecting the appropriate imaging technique. It is an essential task which ensures that the correct imaging is performed. However, the time that a radiologist must dedicate to protocolling could otherwise be spent reporting, communicating with referrers, or teaching. To date, there have been few publications in which ML models were developed that use clinical text to automate protocol selection. This article reviews the existing literature in this field. A systematic assessment of the published models is performed with reference to best practices suggested by machine learning convention. Progress towards implementing automated protocolling in a clinical setting is discussed. △ Less","23 June, 2022",https://arxiv.org/pdf/2206.11502
Efficient Adaptive Federated Optimization of Federated Learning for IoT,Zunming Chen;Hongyan Cui;Ensen Wu;Yu Xi,"The proliferation of the Internet of Things (IoT) and widespread use of devices with sensing, computing, and communication capabilities have motivated intelligent applications empowered by artificial intelligence. The classical artificial intelligence algorithms require centralized data collection and processing which are challenging in realistic intelligent IoT applications due to growing data privacy concerns and distributed datasets. Federated Learning (FL) has emerged as a distributed privacy-preserving learning framework that enables IoT devices to train global model through sharing model parameters. However, inefficiency due to frequent parameters transmissions significantly reduce FL performance. Existing acceleration algorithms consist of two main type including local update considering trade-offs between communication and computation and parameter compression considering trade-offs between communication and precision. Jointly considering these two trade-offs and adaptively balancing their impacts on convergence have remained unresolved. To solve the problem, this paper proposes a novel efficient adaptive federated optimization (EAFO) algorithm to improve efficiency of FL, which minimizes the learning error via jointly considering two variables including local update and parameter compression and enables FL to adaptively adjust the two variables and balance trade-offs among computation, communication and precision. The experiment results illustrate that comparing with state-of-the-art algorithms, the proposed EAFO can achieve higher accuracies faster. △ Less","22 June, 2022",https://arxiv.org/pdf/2206.11448
Automatic autism spectrum disorder detection using artificial intelligence methods with MRI neuroimaging: A review,Parisa Moridian;Navid Ghassemi;Mahboobeh Jafari;Salam Salloum-Asfar;Delaram Sadeghi;Marjane Khodatars;Afshin Shoeibi;Abbas Khosravi;Sai Ho Ling;Abdulhamit Subasi;Roohallah Alizadehsani;Juan M. Gorriz;Sara A Abdulla;U. Rajendra Acharya,"Autism spectrum disorder (ASD) is a brain condition characterized by diverse signs and symptoms that appear in early childhood. ASD is also associated with communication deficits and repetitive behavior in affected individuals. Various ASD detection methods have been developed, including neuroimaging modalities and psychological tests. Among these methods, magnetic resonance imaging (MRI) imaging modalities are of paramount importance to physicians. Clinicians rely on MRI modalities to diagnose ASD accurately. The MRI modalities are non-invasive methods that include functional (fMRI) and structural (sMRI) neuroimaging methods. However, diagnosing ASD with fMRI and sMRI for specialists is often laborious and time-consuming; therefore, several computer-aided design systems (CADS) based on artificial intelligence (AI) have been developed to assist specialist physicians. Conventional machine learning (ML) and deep learning (DL) are the most popular schemes of AI used for diagnosing ASD. This study aims to review the automated detection of ASD using AI. We review several CADS that have been developed using ML techniques for the automated diagnosis of ASD using MRI modalities. There has been very limited work on the use of DL techniques to develop automated diagnostic models for ASD. A summary of the studies developed using DL is provided in the Supplementary Appendix. Then, the challenges encountered during the automated diagnosis of ASD using MRI and AI techniques are described in detail. Additionally, a graphical comparison of studies using ML and DL to diagnose ASD automatically is discussed. We suggest future approaches to detecting ASDs using AI techniques and MRI neuroimaging. △ Less","6 October, 2022",https://arxiv.org/pdf/2206.11233
Automated Compliance Blueprint Optimization with Artificial Intelligence,Abdulhamid Adebayo;Daby Sow;Muhammed Fatih Bulut,"For highly regulated industries such as banking and healthcare, one of the major hindrances to the adoption of cloud computing is compliance with regulatory standards. This is a complex problem due to many regulatory and technical specification (techspec) documents that the companies need to comply with. The critical problem is to establish the mapping between techspecs and regulation controls so that from day one, companies can comply with regulations with minimal effort. We demonstrate the practicality of an approach to automatically analyze regulatory standards using Artificial Intelligence (AI) techniques. We present early results to identify the mapping between techspecs and regulation controls, and discuss challenges that must be overcome for this solution to be fully practical. △ Less","22 June, 2022",https://arxiv.org/pdf/2206.11187
Explanation-based Counterfactual Retraining(XCR): A Calibration Method for Black-box Models,Liu Zhendong;Wenyu Jiang;Yi Zhang;Chongjun Wang,"With the rapid development of eXplainable Artificial Intelligence (XAI), a long line of past work has shown concerns about the Out-of-Distribution (OOD) problem in perturbation-based post-hoc XAI models and explanations are socially misaligned. We explore the limitations of post-hoc explanation methods that use approximators to mimic the behavior of black-box models. Then we propose eXplanation-based Counterfactual Retraining (XCR), which extracts feature importance fastly. XCR applies the explanations generated by the XAI model as counterfactual input to retrain the black-box model to address OOD and social misalignment problems. Evaluation of popular image datasets shows that XCR can improve model performance when only retaining 12.5% of the most crucial features without changing the black-box model structure. Furthermore, the evaluation of the benchmark of corruption datasets shows that the XCR is very helpful for improving model robustness and positively impacts the calibration of OOD problems. Even though not calibrated in the validation set like some OOD calibration methods, the corrupted data metric outperforms existing methods. Our method also beats current OOD calibration methods on the OOD calibration metric if calibration on the validation set is applied. △ Less","22 June, 2022",https://arxiv.org/pdf/2206.11126
A Survey of the Potential Long-term Impacts of AI,Sam Clarke;Jess Whittlestone,"It is increasingly recognised that advances in artificial intelligence could have large and long-lasting impacts on society. However, what form those impacts will take, just how large and long-lasting they will be, and whether they will ultimately be positive or negative for humanity, is far from clear. Based on surveying literature on the societal impacts of AI, we identify and discuss five potential long-term impacts of AI: how AI could lead to long-term changes in science, cooperation, power, epistemics, and values. We review the state of existing research in each of these areas and highlight priority questions for future research. △ Less","22 June, 2022",https://arxiv.org/pdf/2206.11076
AI Challenges for Society and Ethics,Jess Whittlestone;Sam Clarke,"Artificial intelligence is already being applied in and impacting many important sectors in society, including healthcare, finance, and policing. These applications will increase as AI capabilities continue to progress, which has the potential to be highly beneficial for society, or to cause serious harm. The role of AI governance is ultimately to take practical steps to mitigate this risk of harm while enabling the benefits of innovation in AI. This requires answering challenging empirical questions about current and potential risks and benefits of AI: assessing impacts that are often widely distributed and indirect, and making predictions about a highly uncertain future. It also requires thinking through the normative question of what beneficial use of AI in society looks like, which is equally challenging. Though different groups may agree on high-level principles that uses of AI should respect (e.g., privacy, fairness, and autonomy), challenges arise when putting these principles into practice. For example, it is straightforward to say that AI systems must protect individual privacy, but there is presumably some amount or type of privacy that most people would be willing to give up to develop life-saving medical treatments. Despite these challenges, research can and has made progress on these questions. The aim of this chapter will be to give readers an understanding of this progress, and of the challenges that remain. △ Less","22 June, 2022",https://arxiv.org/pdf/2206.11068
Toward An Optimal Selection of Dialogue Strategies: A Target-Driven Approach for Intelligent Outbound Robots,Ruifeng Qian;Shijie Li;Mengjiao Bao;Huan Chen;Yu Che,"With the growth of the economy and society, enterprises, especially in the FinTech industry, have increasing demands of outbound calls for customers such as debt collection, marketing, anti-fraud calls, and so on. But a large amount of repetitive and mechanical work occupies most of the time of human agents, so the cost of equipment and labor for enterprises is increasing accordingly. At the same time, with the development of artificial intelligence technology in the past few decades, it has become quite common for companies to use new technologies such as Big Data and artificial intelligence to empower outbound call businesses. The intelligent outbound robot is a typical application of the artificial intelligence technology in the field of outbound call businesses. It is mainly used to communicate with customers in order to accomplish a certain target. It has the characteristics of low cost, high reuse, and easy compliance, which has attracted more attention from the industry. At present, there are two kinds of intelligent outbound robots in the industry but both of them still leave large room for improvement. One kind of them is based on a finite state machine relying on the configuration of jump conditions and corresponding nodes based on manual experience. This kind of intelligent outbound robot is also called a flow-based robot. For example, the schematic diagram of the working model of a flow-based robot for debt collection is shown in Fig.\ref{fig:label}. In each round, the robot will reply to the user with the words corresponding to each node. △ Less","22 June, 2022",https://arxiv.org/pdf/2206.10953
AI-based software for lung nodule detection in chest X-rays -- Time for a second reader approach?,Susanne Ohlmann-Knafo;Naglis Ramanauskas;Sebastian Huettinger;Emil Johnson Jeyakumar;Darius Barušauskas;Neringa Bielskienė;Vytautas Naujalis;Jonas Bialopetravičius;Jonas Ražanskas;Artūras Samuilis;Jūratė Dementavičienė;Dirk Pickuth,"Objectives: To compare artificial intelligence (AI) as a second reader in detecting lung nodules on chest X-rays (CXR) versus radiologists of two binational institutions, and to evaluate AI performance when using two different modes: automated versus assisted (additional remote radiologist review). Methods: The CXR public database (n = 247) of the Japanese Society of Radiological Technology with various types and sizes of lung nodules was analyzed. Eight radiologists evaluated the CXR images with regard to the presence of lung nodules and nodule conspicuity. After radiologist review, the AI software processed and flagged the CXR with the highest probability of missed nodules. The calculated accuracy metrics were the area under the curve (AUC), sensitivity, specificity, F1 score, false negative case number (FN), and the effect of different AI modes (automated/assisted) on the accuracy of nodule detection. Results: For radiologists, the average AUC value was 0.77 \pm 0.07, while the average FN was 52.63 \pm 17.53 (all studies) and 32 \pm 11.59 (studies containing a nodule of malignant etiology = 32% rate of missed malignant nodules). Both AI modes -- automated and assisted -- produced an average increase in sensitivity (by 14% and 12%) and of F1-score (5% and 6%) and a decrease in specificity (by 10% and 3%, respectively). Conclusions: Both AI modes flagged the pulmonary nodules missed by radiologists in a significant number of cases. AI as a second reader has a high potential to improve diagnostic accuracy and radiology workflow. AI might detect certain pulmonary nodules earlier than radiologists, with a potentially significant impact on patient outcomes. △ Less","22 June, 2022",https://arxiv.org/pdf/2206.10912
Exploring user needs in relation to algorithmically constructed classifications of publications,Peter Sjögårde,"Algorithmic classification of research publications has been created to study different aspects of research. Such classifications can be used to support information needs in universities for decision making. However, the classifications have foremost been evaluated quantitatively regarding their content, but not qualitatively regarding their feasibility in a specific context. The aim of this study was to explore and evaluate the usefulness of such classifications to users in the context of exploring an emerging research area. I conducted four interviews with managers of a project aimed to support research and application of artificial intelligence at the Swedish medical university Karolinska Institutet. The interviews focused on the information need of the managers. To support the project, a classification was created by clustering of publications in a citation network. A cluster map based on this classification was provided to the project leader and one interview focused on the use of the classification in the project in relation to the stated information needs. The interviews showed that the aim of the project was to improve competence, enhance communication between researchers and develop support structures. Getting an overview of artificial intelligence at the university and information about who is doing what was important to fulfill this aim. The cluster map was used to support activities conducted by the project leader, such as interviews and information gathering. It was also used to get overview and display of AI research at KI. Interpretation was found to be challenging in some cases. The interactivity of the map facilitated interpretation. This study was small in scope, but it provides one piece of knowledge about the information needs related to algorithmic classifications. △ Less","22 June, 2022",https://arxiv.org/pdf/2206.10888
A Feature Memory Rearrangement Network for Visual Inspection of Textured Surface Defects Toward Edge Intelligent Manufacturing,Haiming Yao;Wenyong Yu;Xue Wang,"Recent advances in the industrial inspection of textured surfaces-in the form of visual inspection-have made such inspections possible for efficient, flexible manufacturing systems. We propose an unsupervised feature memory rearrangement network (FMR-Net) to accurately detect various textural defects simultaneously. Consistent with mainstream methods, we adopt the idea of background reconstruction; however, we innovatively utilize artificial synthetic defects to enable the model to recognize anomalies, while traditional wisdom relies only on defect-free samples. First, we employ an encoding module to obtain multiscale features of the textured surface. Subsequently, a contrastive-learning-based memory feature module (CMFM) is proposed to obtain discriminative representations and construct a normal feature memory bank in the latent space, which can be employed as a substitute for defects and fast anomaly scores at the patch level. Next, a novel global feature rearrangement module (GFRM) is proposed to further suppress the reconstruction of residual defects. Finally, a decoding module utilizes the restored features to reconstruct the normal texture background. In addition, to improve inspection performance, a two-phase training strategy is utilized for accurate defect restoration refinement, and we exploit a multimodal inspection method to achieve noise-robust defect localization. We verify our method through extensive experiments and test its practical deployment in collaborative edge--cloud intelligent manufacturing scenarios by means of a multilevel detection method, demonstrating that FMR-Net exhibits state-of-the-art inspection accuracy and shows great potential for use in edge-computing-enabled smart industries. △ Less","22 June, 2022",https://arxiv.org/pdf/2206.10830
Coupling Visual Semantics of Artificial Neural Networks and Human Brain Function via Synchronized Activations,Lin Zhao;Haixing Dai;Zihao Wu;Zhenxiang Xiao;Lu Zhang;David Weizhong Liu;Xintao Hu;Xi Jiang;Sheng Li;Dajiang Zhu;Tianming Liu,"Artificial neural networks (ANNs), originally inspired by biological neural networks (BNNs), have achieved remarkable successes in many tasks such as visual representation learning. However, whether there exists semantic correlations/connections between the visual representations in ANNs and those in BNNs remains largely unexplored due to both the lack of an effective tool to link and couple two different domains, and the lack of a general and effective framework of representing the visual semantics in BNNs such as human functional brain networks (FBNs). To answer this question, we propose a novel computational framework, Synchronized Activations (Sync-ACT), to couple the visual representation spaces and semantics between ANNs and BNNs in human brain based on naturalistic functional magnetic resonance imaging (nfMRI) data. With this approach, we are able to semantically annotate the neurons in ANNs with biologically meaningful description derived from human brain imaging for the first time. We evaluated the Sync-ACT framework on two publicly available movie-watching nfMRI datasets. The experiments demonstrate a) the significant correlation and similarity of the semantics between the visual representations in FBNs and those in a variety of convolutional neural networks (CNNs) models; b) the close relationship between CNN's visual representation similarity to BNNs and its performance in image classification tasks. Overall, our study introduces a general and effective paradigm to couple the ANNs and BNNs and provides novel insights for future studies such as brain-inspired artificial intelligence. △ Less","21 June, 2022",https://arxiv.org/pdf/2206.10821
BOSS: A Benchmark for Human Belief Prediction in Object-context Scenarios,Jiafei Duan;Samson Yu;Nicholas Tan;Li Yi;Cheston Tan,"Humans with an average level of social cognition can infer the beliefs of others based solely on the nonverbal communication signals (e.g. gaze, gesture, pose and contextual information) exhibited during social interactions. This social cognitive ability to predict human beliefs and intentions is more important than ever for ensuring safe human-robot interaction and collaboration. This paper uses the combined knowledge of Theory of Mind (ToM) and Object-Context Relations to investigate methods for enhancing collaboration between humans and autonomous systems in environments where verbal communication is prohibited. We propose a novel and challenging multimodal video dataset for assessing the capability of artificial intelligence (AI) systems in predicting human belief states in an object-context scenario. The proposed dataset consists of precise labelling of human belief state ground-truth and multimodal inputs replicating all nonverbal communication inputs captured by human perception. We further evaluate our dataset with existing deep learning models and provide new insights into the effects of the various input modalities and object-context relations on the performance of the baseline models. △ Less","21 June, 2022",https://arxiv.org/pdf/2206.10665
Stop ordering machine learning algorithms by their explainability! A user-centered investigation of performance and explainability,Lukas-Valentin Herm;Kai Heinrich;Jonas Wanner;Christian Janiesch,"Machine learning algorithms enable advanced decision making in contemporary intelligent systems. Research indicates that there is a tradeoff between their model performance and explainability. Machine learning models with higher performance are often based on more complex algorithms and therefore lack explainability and vice versa. However, there is little to no empirical evidence of this tradeoff from an end user perspective. We aim to provide empirical evidence by conducting two user experiments. Using two distinct datasets, we first measure the tradeoff for five common classes of machine learning algorithms. Second, we address the problem of end user perceptions of explainable artificial intelligence augmentations aimed at increasing the understanding of the decision logic of high-performing complex models. Our results diverge from the widespread assumption of a tradeoff curve and indicate that the tradeoff between model performance and explainability is much less gradual in the end user's perception. This is a stark contrast to assumed inherent model interpretability. Further, we found the tradeoff to be situational for example due to data complexity. Results of our second experiment show that while explainable artificial intelligence augmentations can be used to increase explainability, the type of explanation plays an essential role in end user perception. △ Less","20 June, 2022",https://arxiv.org/pdf/2206.10610
Artificial intelligence system based on multi-value classification of fully connected neural network for construction management,Tetyana Honcharenko;Roman Akselrod;Andrii Shpakov;Oleksandr Khomenko,"This study is devoted to solving the problem to determine the professional adaptive capabilities of construction management staff using artificial intelligence systems.It is proposed Fully Connected Feed-Forward Neural Network architecture and performed empirical modeling to create a Data Set. Model of artificial intelligence system allows evaluating the processes in an Fully Connected Feed-Forward Neural Network during the execution of multi-value classification of professional areas. A method has been developed for the training process of a machine learning model, which reflects the internal connections between the components of an artificial intelligence system that allow it to learn from training data. To train the neural network, a data set of 35 input parameters and 29 output parameters was used; the amount of data in the set is 936 data lines. Neural network training occurred in the proportion of 10% and 90%, respectively. Results of this study research can be used to further improve the knowledge and skills necessary for successful professional realization. △ Less","19 June, 2022",https://arxiv.org/pdf/2206.10604
Neural Transformers for Intraductal Papillary Mucosal Neoplasms (IPMN) Classification in MRI images,Federica Proietto Salanitri;Giovanni Bellitto;Simone Palazzo;Ismail Irmakci;Michael B. Wallace;Candice W. Bolan;Megan Engels;Sanne Hoogenboom;Marco Aldinucci;Ulas Bagci;Daniela Giordano;Concetto Spampinato,"Early detection of precancerous cysts or neoplasms, i.e., Intraductal Papillary Mucosal Neoplasms (IPMN), in pancreas is a challenging and complex task, and it may lead to a more favourable outcome. Once detected, grading IPMNs accurately is also necessary, since low-risk IPMNs can be under surveillance program, while high-risk IPMNs have to be surgically resected before they turn into cancer. Current standards (Fukuoka and others) for IPMN classification show significant intra- and inter-operator variability, beside being error-prone, making a proper diagnosis unreliable. The established progress in artificial intelligence, through the deep learning paradigm, may provide a key tool for an effective support to medical decision for pancreatic cancer. In this work, we follow this trend, by proposing a novel AI-based IPMN classifier that leverages the recent success of transformer networks in generalizing across a wide variety of tasks, including vision ones. We specifically show that our transformer-based model exploits pre-training better than standard convolutional neural networks, thus supporting the sought architectural universalism of transformers in vision, including the medical image domain and it allows for a better interpretation of the obtained results. △ Less","21 June, 2022",https://arxiv.org/pdf/2206.10531
An Energy and Carbon Footprint Analysis of Distributed and Federated Learning,Stefano Savazzi;Vittorio Rampa;Sanaz Kianoush;Mehdi Bennis,"Classical and centralized Artificial Intelligence (AI) methods require moving data from producers (sensors, machines) to energy hungry data centers, raising environmental concerns due to computational and communication resource demands, while violating privacy. Emerging alternatives to mitigate such high energy costs propose to efficiently distribute, or federate, the learning tasks across devices, which are typically low-power. This paper proposes a novel framework for the analysis of energy and carbon footprints in distributed and federated learning (FL). The proposed framework quantifies both the energy footprints and the carbon equivalent emissions for vanilla FL methods and consensus-based fully decentralized approaches. We discuss optimal bounds and operational points that support green FL designs and underpin their sustainability assessment. Two case studies from emerging 5G industry verticals are analyzed: these quantify the environmental footprints of continual and reinforcement learning setups, where the training process is repeated periodically for continuous improvements. For all cases, sustainability of distributed learning relies on the fulfillment of specific requirements on communication efficiency and learner population size. Energy and test accuracy should be also traded off considering the model and the data footprints for the targeted industrial applications. △ Less","21 June, 2022",https://arxiv.org/pdf/2206.10380
The Makerere Radio Speech Corpus: A Luganda Radio Corpus for Automatic Speech Recognition,Jonathan Mukiibi;Andrew Katumba;Joyce Nakatumba-Nabende;Ali Hussein;Josh Meyer,"Building a usable radio monitoring automatic speech recognition (ASR) system is a challenging task for under-resourced languages and yet this is paramount in societies where radio is the main medium of public communication and discussions. Initial efforts by the United Nations in Uganda have proved how understanding the perceptions of rural people who are excluded from social media is important in national planning. However, these efforts are being challenged by the absence of transcribed speech datasets. In this paper, The Makerere Artificial Intelligence research lab releases a Luganda radio speech corpus of 155 hours. To our knowledge, this is the first publicly available radio dataset in sub-Saharan Africa. The paper describes the development of the voice corpus and presents baseline Luganda ASR performance results using Coqui STT toolkit, an open source speech recognition toolkit. △ Less","20 June, 2022",https://arxiv.org/pdf/2206.09790
"Eliminating The Impossible, Whatever Remains Must Be True",Jinqiang Yu;Alexey Ignatiev;Peter J. Stuckey;Nina Narodytska;Joao Marques-Silva,"The rise of AI methods to make predictions and decisions has led to a pressing need for more explainable artificial intelligence (XAI) methods. One common approach for XAI is to produce a post-hoc explanation, explaining why a black box ML model made a certain prediction. Formal approaches to post-hoc explanations provide succinct reasons for why a prediction was made, as well as why not another prediction was made. But these approaches assume that features are independent and uniformly distributed. While this means that ""why"" explanations are correct, they may be longer than required. It also means the ""why not"" explanations may be suspect as the counterexamples they rely on may not be meaningful. In this paper, we show how one can apply background knowledge to give more succinct ""why"" formal explanations, that are presumably easier to interpret by humans, and give more accurate ""why not"" explanations. In addition, we show how to use existing rule induction techniques to efficiently extract background information from a dataset, and also how to report which background information was used to make an explanation, allowing a human to examine it if they doubt the correctness of the explanation. △ Less","30 November, 2022",https://arxiv.org/pdf/2206.09551
Hybrid Facial Expression Recognition (FER2013) Model for Real-Time Emotion Classification and Prediction,Ozioma Collins Oguine;Kanyifeechukwu Jane Oguine;Hashim Ibrahim Bisallah;Daniel Ofuani,"Facial Expression Recognition is a vital research topic in most fields ranging from artificial intelligence and gaming to Human-Computer Interaction (HCI) and Psychology. This paper proposes a hybrid model for Facial Expression recognition, which comprises a Deep Convolutional Neural Network (DCNN) and Haar Cascade deep learning architectures. The objective is to classify real-time and digital facial images into one of the seven facial emotion categories considered. The DCNN employed in this research has more convolutional layers, ReLU Activation functions, and multiple kernels to enhance filtering depth and facial feature extraction. In addition, a haar cascade model was also mutually used to detect facial features in real-time images and video frames. Grayscale images from the Kaggle repository (FER-2013) and then exploited Graphics Processing Unit (GPU) computation to expedite the training and validation process. Pre-processing and data augmentation techniques are applied to improve training efficiency and classification performance. The experimental results show a significantly improved classification performance compared to state-of-the-art (SoTA) experiments and research. Also, compared to other conventional models, this paper validates that the proposed architecture is superior in classification performance with an improvement of up to 6%, totaling up to 70% accuracy, and with less execution time of 2098.8s. △ Less","1 August, 2022",https://arxiv.org/pdf/2206.09509
Modeling Transformative AI Risks (MTAIR) Project -- Summary Report,Sam Clarke;Ben Cottier;Aryeh Englander;Daniel Eth;David Manheim;Samuel Dylan Martin;Issa Rice,"This report outlines work by the Modeling Transformative AI Risk (MTAIR) project, an attempt to map out the key hypotheses, uncertainties, and disagreements in debates about catastrophic risks from advanced AI, and the relationships between them. This builds on an earlier diagram by Ben Cottier and Rohin Shah which laid out some of the crucial disagreements (""cruxes"") visually, with some explanation. Based on an extensive literature review and engagement with experts, the report explains a model of the issues involved, and the initial software-based implementation that can incorporate probability estimates or other quantitative factors to enable exploration, planning, and/or decision support. By gathering information from various debates and discussions into a single more coherent presentation, we hope to enable better discussions and debates about the issues involved. The model starts with a discussion of reasoning via analogies and general prior beliefs about artificial intelligence. Following this, it lays out a model of different paths and enabling technologies for high-level machine intelligence, and a model of how advances in the capabilities of these systems might proceed, including debates about self-improvement, discontinuous improvements, and the possibility of distributed, non-agentic high-level intelligence or slower improvements. The model also looks specifically at the question of learned optimization, and whether machine learning systems will create mesa-optimizers. The impact of different safety research on the previous sets of questions is then examined, to understand whether and how research could be useful in enabling safer systems. Finally, we discuss a model of different failure modes and loss of control or takeover scenarios. △ Less","19 June, 2022",https://arxiv.org/pdf/2206.09360
Productive Reproducible Workflows for DNNs: A Case Study for Industrial Defect Detection,Perry Gibson;José Cano,"As Deep Neural Networks (DNNs) have become an increasingly ubiquitous workload, the range of libraries and tooling available to aid in their development and deployment has grown significantly. Scalable, production quality tools are freely available under permissive licenses, and are accessible enough to enable even small teams to be very productive. However within the research community, awareness and usage of said tools is not necessarily widespread, and researchers may be missing out on potential productivity gains from exploiting the latest tools and workflows. This paper presents a case study where we discuss our recent experience producing an end-to-end artificial intelligence application for industrial defect detection. We detail the high level deep learning libraries, containerized workflows, continuous integration/deployment pipelines, and open source code templates we leveraged to produce a competitive result, matching the performance of other ranked solutions to our three target datasets. We highlight the value that exploiting such systems can bring, even for research, and detail our solution and present our best results in terms of accuracy and inference time on a server class GPU, as well as inference times on a server class CPU, and a Raspberry Pi 4. △ Less","19 June, 2022",https://arxiv.org/pdf/2206.09359
Machine Learning in Sports: A Case Study on Using Explainable Models for Predicting Outcomes of Volleyball Matches,Abhinav Lalwani;Aman Saraiya;Apoorv Singh;Aditya Jain;Tirtharaj Dash,"Machine Learning has become an integral part of engineering design and decision making in several domains, including sports. Deep Neural Networks (DNNs) have been the state-of-the-art methods for predicting outcomes of professional sports events. However, apart from getting highly accurate predictions on these sports events outcomes, it is necessary to answer questions such as ""Why did the model predict that Team A would win Match X against Team B?"" DNNs are inherently black-box in nature. Therefore, it is required to provide high-quality interpretable, and understandable explanations for a model's prediction in sports. This paper explores a two-phased Explainable Artificial Intelligence(XAI) approach to predict outcomes of matches in the Brazilian volleyball League (SuperLiga). In the first phase, we directly use the interpretable rule-based ML models that provide a global understanding of the model's behaviors based on Boolean Rule Column Generation (BRCG; extracts simple AND-OR classification rules) and Logistic Regression (LogReg; allows to estimate the feature importance scores). In the second phase, we construct non-linear models such as Support Vector Machine (SVM) and Deep Neural Network (DNN) to obtain predictive performance on the volleyball matches' outcomes. We construct the ""post-hoc"" explanations for each data instance using ProtoDash, a method that finds prototypes in the training dataset that are most similar to the test instance, and SHAP, a method that estimates the contribution of each feature on the model's prediction. We evaluate the SHAP explanations using the faithfulness metric. Our results demonstrate the effectiveness of the explanations for the model's predictions. △ Less","18 June, 2022",https://arxiv.org/pdf/2206.09258
Secure Wireless Transmission for Reconfigurable Intelligent Surface Aided Full Duplex Systems,Pengxin Guan;Yiru Wang;Yuping Zhao,"This letter considers the secure communication in a reconfigurable intelligent surface (RIS) aided full duplex (FD) system. An FD base station (BS) serves an uplink (UL) user and a downlink (DL) user simultaneously over the same time-frequency dimension assisted by a RIS in the presence of an eavesdropper. In addition, the artificial noise (AN) is also applied to interfere the eavesdropper's channel. We aim to maximize the sum secrecy rate of UL and DL users by jointly optimizing the transmit beamforming, receive beamforming and AN covariance matrix at the BS, and passive beamforming at the RIS. To handle the non-convex problem, we decompose it into tractable subproblems and propose an efficient algorithm based on alternating optimization framework. Specifically, the receive beamforming is derived as a closed-form solution while other variables are obtained by using semidefinite relaxation (SDR) method and successive convex approximation (SCA) algorithm. Simulation results demonstrate the superior performance of our proposed scheme compared to other baseline schemes. △ Less","25 September, 2022",https://arxiv.org/pdf/2206.08797
Definition drives design: Disability models and mechanisms of bias in AI technologies,Denis Newman-Griffis;Jessica Sage Rauchberg;Rahaf Alharbi;Louise Hickman;Harry Hochheiser,"The increasing deployment of artificial intelligence (AI) tools to inform decision making across diverse areas including healthcare, employment, social benefits, and government policy, presents a serious risk for disabled people, who have been shown to face bias in AI implementations. While there has been significant work on analysing and mitigating algorithmic bias, the broader mechanisms of how bias emerges in AI applications are not well understood, hampering efforts to address bias where it begins. In this article, we illustrate how bias in AI-assisted decision making can arise from a range of specific design decisions, each of which may seem self-contained and non-biasing when considered separately. These design decisions include basic problem formulation, the data chosen for analysis, the use the AI technology is put to, and operational design elements in addition to the core algorithmic design. We draw on three historical models of disability common to different decision-making settings to demonstrate how differences in the definition of disability can lead to highly distinct decisions on each of these aspects of design, leading in turn to AI technologies with a variety of biases and downstream effects. We further show that the potential harms arising from inappropriate definitions of disability in fundamental design stages are further amplified by a lack of transparency and disabled participation throughout the AI design process. Our analysis provides a framework for critically examining AI technologies in decision-making contexts and guiding the development of a design praxis for disability-related AI analytics. We put forth this article to provide key questions to facilitate disability-led design and participatory development to produce more fair and equitable AI technologies in disability-related contexts. △ Less","23 November, 2022",https://arxiv.org/pdf/2206.08287
Backbones-Review: Feature Extraction Networks for Deep Learning and Deep Reinforcement Learning Approaches,Omar Elharrouss;Younes Akbari;Noor Almaadeed;Somaya Al-Maadeed,"To understand the real world using various types of data, Artificial Intelligence (AI) is the most used technique nowadays. While finding the pattern within the analyzed data represents the main task. This is performed by extracting representative features step, which is proceeded using the statistical algorithms or using some specific filters. However, the selection of useful features from large-scale data represented a crucial challenge. Now, with the development of convolution neural networks (CNNs), the feature extraction operation has become more automatic and easier. CNNs allow to work on large-scale size of data, as well as cover different scenarios for a specific task. For computer vision tasks, convolutional networks are used to extract features also for the other parts of a deep learning model. The selection of a suitable network for feature extraction or the other parts of a DL model is not random work. So, the implementation of such a model can be related to the target task as well as the computational complexity of it. Many networks have been proposed and become the famous networks used for any DL models in any AI task. These networks are exploited for feature extraction or at the beginning of any DL model which is named backbones. A backbone is a known network trained in many other tasks before and demonstrates its effectiveness. In this paper, an overview of the existing backbones, e.g. VGGs, ResNets, DenseNet, etc, is given with a detailed description. Also, a couple of computer vision tasks are discussed by providing a review of each task regarding the backbones used. In addition, a comparison in terms of performance is also provided, based on the backbone used for each task. △ Less","16 June, 2022",https://arxiv.org/pdf/2206.08016
Hardness prediction of age-hardening aluminum alloy based on ensemble learning,Houchen Zuo;Yongquan Jiang;Yan Yang;Baoying Liu;Jie Hu,"With the rapid development of artificial intelligence, the combination of material database and machine learning has driven the progress of material informatics. Because aluminum alloy is widely used in many fields, so it is significant to predict the properties of aluminum alloy. In this thesis, the data of Al-Cu-Mg-X (X: Zn, Zr, etc.) alloy are used to input the composition, aging conditions (time and temperature) and predict its hardness. An ensemble learning solution based on automatic machine learning and an attention mechanism introduced into the secondary learner of deep neural network are proposed respectively. The experimental results show that selecting the correct secondary learner can further improve the prediction accuracy of the model. This manuscript introduces the attention mechanism to improve the secondary learner based on deep neural network, and obtains a fusion model with better performance. The R-Square of the best model is 0.9697 and the MAE is 3.4518HV. △ Less","16 June, 2022",https://arxiv.org/pdf/2206.08011
Challenges and Opportunities in Deep Reinforcement Learning with Graph Neural Networks: A Comprehensive review of Algorithms and Applications,Sai Munikoti;Deepesh Agarwal;Laya Das;Mahantesh Halappanavar;Balasubramaniam Natarajan,"Deep reinforcement learning (DRL) has empowered a variety of artificial intelligence fields, including pattern recognition, robotics, recommendation-systems, and gaming. Similarly, graph neural networks (GNN) have also demonstrated their superior performance in supervised learning for graph-structured data. In recent times, the fusion of GNN with DRL for graph-structured environments has attracted a lot of attention. This paper provides a comprehensive review of these hybrid works. These works can be classified into two categories: (1) algorithmic enhancement, where DRL and GNN complement each other for better utility; (2) application-specific enhancement, where DRL and GNN support each other. This fusion effectively addresses various complex problems in engineering and life sciences. Based on the review, we further analyze the applicability and benefits of fusing these two domains, especially in terms of increasing generalizability and reducing computational complexity. Finally, the key challenges in integrating DRL and GNN, and potential future research directions are highlighted, which will be of interest to the broader machine learning community. △ Less","7 November, 2022",https://arxiv.org/pdf/2206.07922
Unifying Framework for Optimizations in non-boolean Formalisms,Yuliya Lierler,"Search-optimization problems are plentiful in scientific and engineering domains. Artificial intelligence has long contributed to the development of search algorithms and declarative programming languages geared towards solving and modeling search-optimization problems. Automated reasoning and knowledge representation are the subfields of AI that are particularly vested in these developments. Many popular automated reasoning paradigms provide users with languages supporting optimization statements. Recall integer linear programming, MaxSAT, optimization satisfiability modulo theory, and (constraint) answer set programming. These paradigms vary significantly in their languages in ways they express quality conditions on computed solutions. Here we propose a unifying framework of so called extended weight systems that eliminates syntactic distinctions between paradigms. They allow us to see essential similarities and differences between optimization statements provided by distinct automated reasoning languages. We also study formal properties of the proposed systems that immediately translate into formal properties of paradigms that can be captured within our framework. Under consideration in Theory and Practice of Logic Programming (TPLP). △ Less","15 June, 2022",https://arxiv.org/pdf/2206.07862
AI Ethics Issues in Real World: Evidence from AI Incident Database,Mengyi Wei;Zhixuan Zhou,"With the powerful performance of Artificial Intelligence (AI) also comes prevalent ethical issues. Though governments and corporations have curated multiple AI ethics guidelines to curb unethical behavior of AI, the effect has been limited, probably due to the vagueness of the guidelines. In this paper, we take a closer look at how AI ethics issues take place in real world, in order to have a more in-depth and nuanced understanding of different ethical issues as well as their social impact. With a content analysis of AI Incident Database, which is an effort to prevent repeated real world AI failures by cataloging incidents, we identified 13 application areas which often see unethical use of AI, with intelligent service robots, language/vision models and autonomous driving taking the lead. Ethical issues appear in 8 different forms, from inappropriate use and racial discrimination, to physical safety and unfair algorithm. With this taxonomy of AI ethics issues, we aim to provide AI practitioners with a practical guideline when trying to deploy AI applications ethically. △ Less","18 August, 2022",https://arxiv.org/pdf/2206.07635
HICEM: A High-Coverage Emotion Model for Artificial Emotional Intelligence,Benjamin Wortman;James Z. Wang,"As social robots and other intelligent machines enter the home, artificial emotional intelligence (AEI) is taking center stage to address users' desire for deeper, more meaningful human-machine interaction. To accomplish such efficacious interaction, the next-generation AEI need comprehensive human emotion models for training. Unlike theory of emotion, which has been the historical focus in psychology, emotion models are a descriptive tools. In practice, the strongest models need robust coverage, which means defining the smallest core set of emotions from which all others can be derived. To achieve the desired coverage, we turn to word embeddings from natural language processing. Using unsupervised clustering techniques, our experiments show that with as few as 15 discrete emotion categories, we can provide maximum coverage across six major languages--Arabic, Chinese, English, French, Spanish, and Russian. In support of our findings, we also examine annotations from two large-scale emotion recognition datasets to assess the validity of existing emotion models compared to human perception at scale. Because robust, comprehensive emotion models are foundational for developing real-world affective computing applications, this work has broad implications in social robotics, human-machine interaction, mental healthcare, and computational psychology. △ Less","15 June, 2022",https://arxiv.org/pdf/2206.07593
Towards ML Methods for Biodiversity: A Novel Wild Bee Dataset and Evaluations of XAI Methods for ML-Assisted Rare Species Annotations,Teodor Chiaburu;Felix Biessmann;Frank Hausser,"Insects are a crucial part of our ecosystem. Sadly, in the past few decades, their numbers have worryingly decreased. In an attempt to gain a better understanding of this process and monitor the insects populations, Deep Learning may offer viable solutions. However, given the breadth of their taxonomy and the typical hurdles of fine grained analysis, such as high intraclass variability compared to low interclass variability, insect classification remains a challenging task. There are few benchmark datasets, which impedes rapid development of better AI models. The annotation of rare species training data, however, requires expert knowledge. Explainable Artificial Intelligence (XAI) could assist biologists in these annotation tasks, but choosing the optimal XAI method is difficult. Our contribution to these research challenges is threefold: 1) a dataset of thoroughly annotated images of wild bees sampled from the iNaturalist database, 2) a ResNet model trained on the wild bee dataset achieving classification scores comparable to similar state-of-the-art models trained on other fine-grained datasets and 3) an investigation of XAI methods to support biologists in annotation tasks. △ Less","15 June, 2022",https://arxiv.org/pdf/2206.07497
Resource-Constrained Edge AI with Early Exit Prediction,Rongkang Dong;Yuyi Mao;Jun Zhang,"By leveraging the data sample diversity, the early-exit network recently emerges as a prominent neural network architecture to accelerate the deep learning inference process. However, intermediate classifiers of the early exits introduce additional computation overhead, which is unfavorable for resource-constrained edge artificial intelligence (AI). In this paper, we propose an early exit prediction mechanism to reduce the on-device computation overhead in a device-edge co-inference system supported by early-exit networks. Specifically, we design a low-complexity module, namely the Exit Predictor, to guide some distinctly ""hard"" samples to bypass the computation of the early exits. Besides, considering the varying communication bandwidth, we extend the early exit prediction mechanism for latency-aware edge inference, which adapts the prediction thresholds of the Exit Predictor and the confidence thresholds of the early-exit network via a few simple regression models. Extensive experiment results demonstrate the effectiveness of the Exit Predictor in achieving a better tradeoff between accuracy and on-device computation overhead for early-exit networks. Besides, compared with the baseline methods, the proposed method for latency-aware edge inference attains higher inference accuracy under different bandwidth conditions. △ Less","19 June, 2022",https://arxiv.org/pdf/2206.07269
Explainable expected goal models for performance analysis in football analytics,Mustafa Cavus;Przemysław Biecek,"The expected goal provides a more representative measure of the team and player performance which also suit the low-scoring nature of football instead of score in modern football. The score of a match involves randomness and often may not represent the performance of the teams and players, therefore it has been popular to use the alternative statistics in recent years such as shots on target, ball possessions, and drills. To measure the probability of a shot being a goal by the expected goal, several features are used to train an expected goal model which is based on the event and tracking football data. The selection of these features, the size and date of the data, and the model which are used as the parameters that may affect the performance of the model. Using black-box machine learning models for increasing the predictive performance of the model decreases its interpretability that causes the loss of information that can be gathered from the model. This paper proposes an accurate expected goal model trained consisting of 315,430 shots from seven seasons between 2014-15 and 2020-21 of the top-five European football leagues. Moreover, this model is explained by using explainable artificial intelligence tool to obtain an explainable expected goal model for evaluating a team or player performance. To the best of our knowledge, this is the first paper that demonstrates a practical application of an explainable artificial intelligence tool aggregated profiles to explain a group of observations on an accurate expected goal model for monitoring the team and player performance. Moreover, these methods can be generalized to other sports branches. △ Less","6 September, 2022",https://arxiv.org/pdf/2206.07212
Attributions Beyond Neural Networks: The Linear Program Case,Florian Peter Busch;Matej Zečević;Kristian Kersting;Devendra Singh Dhami,"Linear Programs (LPs) have been one of the building blocks in machine learning and have championed recent strides in differentiable optimizers for learning systems. While there exist solvers for even high-dimensional LPs, understanding said high-dimensional solutions poses an orthogonal and unresolved problem. We introduce an approach where we consider neural encodings for LPs that justify the application of attribution methods from explainable artificial intelligence (XAI) designed for neural learning systems. The several encoding functions we propose take into account aspects such as feasibility of the decision space, the cost attached to each input, or the distance to special points of interest. We investigate the mathematical consequences of several XAI methods on said neural LP encodings. We empirically show that the attribution methods Saliency and LIME reveal indistinguishable results up to perturbation levels, and we propose the property of Directedness as the main discriminative criterion between Saliency and LIME on one hand, and a perturbation-based Feature Permutation approach on the other hand. Directedness indicates whether an attribution method gives feature attributions with respect to an increase of that feature. We further notice the baseline selection problem beyond the classical computer vision setting for Integrated Gradients. △ Less","14 June, 2022",https://arxiv.org/pdf/2206.07203
"A Novel RIS-Aided EMF-Aware Beamforming Using Directional Spreading, Truncation and Boosting",Nour Awarkeh;Dinh-Thuy Phan-Huy;Raphael Visoz;Marco Di Renzo,"This paper addresses a drawback of massive multiple-input multiple-output Maximum Ratio Transmission beamforming. In some propagation conditions, when the base station serves the same target user equipment for a long period, it reduces the transmit power (and degrades the received power) to avoid creating high exposure regions located in the vicinity of the antenna and concentrated in few directions (corresponding to the best propagation paths between the antenna and the receiver). In this paper, we propose a novel electromagnetic field aware beamforming scheme, which (i) spreads the beamforming radiation pattern in the angular domain by adding artificial propagation paths thanks to reconfigurable intelligent surfaces, (ii) truncates the pattern in strong directions, and (iii) boosts it in weak directions. Compared to existing solutions, it maximizes the received power. However, it also consumes more power. Finally, truncation alone is the best trade-off between received power and energy efficiency, under exposure constrain. △ Less","14 June, 2022",https://arxiv.org/pdf/2206.07051
A Relative Church-Turing-Deutsch Thesis from Special Relativity and Undecidability,Blake Wilson;Ethan Dickey;Vaishnavi Iyer;Sabre Kais,"Beginning with Turing's seminal work in 1950, artificial intelligence proposes that consciousness can be simulated by a Turing machine. This implies a potential theory of everything where the universe is a simulation on a computer, which begs the question of whether we can prove we exist in a simulation. In this work, we construct a relative model of computation where a computable \textit{local} machine is simulated by a \textit{global}, classical Turing machine. We show that the problem of the local machine computing \textbf{simulation properties} of its global simulator is undecidable in the same sense as the Halting problem. Then, we show that computing the time, space, or error accumulated by the global simulator are simulation properties and therefore are undecidable. These simulation properties give rise to special relativistic effects in the relative model which we use to construct a relative Church-Turing-Deutsch thesis where a global, classical Turing machine computes quantum mechanics for a local machine with the same constant-time local computational complexity as experienced in our universe. △ Less","13 June, 2022",https://arxiv.org/pdf/2206.06419
AI-based Data Preparation and Data Analytics in Healthcare: The Case of Diabetes,Marianna Maranghi;Aris Anagnostopoulos;Irene Cannistraci;Ioannis Chatzigiannakis;Federico Croce;Giulia Di Teodoro;Michele Gentile;Giorgio Grani;Maurizio Lenzerini;Stefano Leonardi;Andrea Mastropietro;Laura Palagi;Massimiliano Pappa;Riccardo Rosati;Riccardo Valentini;Paola Velardi,"The Associazione Medici Diabetologi (AMD) collects and manages one of the largest worldwide-available collections of diabetic patient records, also known as the AMD database. This paper presents the initial results of an ongoing project whose focus is the application of Artificial Intelligence and Machine Learning techniques for conceptualizing, cleaning, and analyzing such an important and valuable dataset, with the goal of providing predictive insights to better support diabetologists in their diagnostic and therapeutic choices. △ Less","20 July, 2022",https://arxiv.org/pdf/2206.06182
"Tackling Algorithmic Disability Discrimination in the Hiring Process: An Ethical, Legal and Technical Analysis",Maarten Buyl;Christina Cociancig;Cristina Frattone;Nele Roekens,"Tackling algorithmic discrimination against persons with disabilities (PWDs) demands a distinctive approach that is fundamentally different to that applied to other protected characteristics, due to particular ethical, legal, and technical challenges. We address these challenges specifically in the context of artificial intelligence (AI) systems used in hiring processes (or automated hiring systems, AHSs), in which automated assessment procedures are subject to unique ethical and legal considerations and have an undeniable adverse impact on PWDs. In this paper, we discuss concerns and opportunities raised by AI-driven hiring in relation to disability discrimination. Ultimately, we aim to encourage further research into this topic. Hence, we establish some starting points and design a roadmap for ethicists, lawmakers, advocates as well as AI practitioners alike. △ Less","13 June, 2022",https://arxiv.org/pdf/2206.06149
SyntheX: Scaling Up Learning-based X-ray Image Analysis Through In Silico Experiments,Cong Gao;Benjamin D. Killeen;Yicheng Hu;Robert B. Grupp;Russell H. Taylor;Mehran Armand;Mathias Unberath,"Artificial intelligence (AI) now enables automated interpretation of medical images for clinical use. However, AI's potential use for interventional images (versus those involved in triage or diagnosis), such as for guidance during surgery, remains largely untapped. This is because surgical AI systems are currently trained using post hoc analysis of data collected during live surgeries, which has fundamental and practical limitations, including ethical considerations, expense, scalability, data integrity, and a lack of ground truth. Here, we demonstrate that creating realistic simulated images from human models is a viable alternative and complement to large-scale in situ data collection. We show that training AI image analysis models on realistically synthesized data, combined with contemporary domain generalization or adaptation techniques, results in models that on real data perform comparably to models trained on a precisely matched real data training set. Because synthetic generation of training data from human-based models scales easily, we find that our model transfer paradigm for X-ray image analysis, which we refer to as SyntheX, can even outperform real data-trained models due to the effectiveness of training on a larger dataset. We demonstrate the potential of SyntheX on three clinical tasks: Hip image analysis, surgical robotic tool detection, and COVID-19 lung lesion segmentation. SyntheX provides an opportunity to drastically accelerate the conception, design, and evaluation of intelligent systems for X-ray-based medicine. In addition, simulated image environments provide the opportunity to test novel instrumentation, design complementary surgical approaches, and envision novel techniques that improve outcomes, save time, or mitigate human error, freed from the ethical and practical considerations of live human data collection. △ Less","13 June, 2022",https://arxiv.org/pdf/2206.06127
Learning Feature Disentanglement and Dynamic Fusion for Recaptured Image Forensic,Shuyu Miao;Lin Zheng;Hong Jin,"Image recapture seriously breaks the fairness of artificial intelligent (AI) systems, which deceives the system by recapturing others' images. Most of the existing recapture models can only address a single pattern of recapture (e.g., moire, edge, artifact, and others) based on the datasets with simulated recaptured images using fixed electronic devices. In this paper, we explicitly redefine image recapture forensic task as four patterns of image recapture recognition, i.e., moire recapture, edge recapture, artifact recapture, and other recapture. Meanwhile, we propose a novel Feature Disentanglement and Dynamic Fusion (FDDF) model to adaptively learn the most effective recapture feature representation for covering different recapture pattern recognition. Furthermore, we collect a large-scale Real-scene Universal Recapture (RUR) dataset containing various recapture patterns, which is about five times the number of previously published datasets. To the best of our knowledge, we are the first to propose a general model and a general real-scene large-scale dataset for recaptured image forensic. Extensive experiments show that our proposed FDDF can achieve state-of-the-art performance on the RUR dataset. △ Less","13 June, 2022",https://arxiv.org/pdf/2206.06103
Mediators: Conversational Agents Explaining NLP Model Behavior,Nils Feldhus;Ajay Madhavan Ravichandran;Sebastian Möller,"The human-centric explainable artificial intelligence (HCXAI) community has raised the need for framing the explanation process as a conversation between human and machine. In this position paper, we establish desiderata for Mediators, text-based conversational agents which are capable of explaining the behavior of neural models interactively using natural language. From the perspective of natural language processing (NLP) research, we engineer a blueprint of such a Mediator for the task of sentiment analysis and assess how far along current research is on the path towards dialogue-based explanations. △ Less","13 June, 2022",https://arxiv.org/pdf/2206.06029
Fluorescence angiography classification in colorectal surgery -- A preliminary report,Antonio S Soares;Sophia Bano;Neil T Clancy;Laurence B Lovat;Danail Stoyanov;Manish Chand,"Background: Fluorescence angiography has shown very promising results in reducing anastomotic leaks by allowing the surgeon to select optimally perfused tissue. However, subjective interpretation of the fluorescent signal still hinders broad application of the technique, as significant variation between different surgeons exists. Our aim is to develop an artificial intelligence algorithm to classify colonic tissue as 'perfused' or 'not perfused' based on intraoperative fluorescence angiography data. Methods: A classification model with a Resnet architecture was trained on a dataset of fluorescence angiography videos of colorectal resections at a tertiary referral centre. Frames corresponding to fluorescent and non-fluorescent segments of colon were used to train a classification algorithm. Validation using frames from patients not used in the training set was performed, including both data collected using the same equipment and data collected using a different camera. Performance metrics were calculated, and saliency maps used to further analyse the output. A decision boundary was identified based on the tissue classification. Results: A convolutional neural network was successfully trained on 1790 frames from 7 patients and validated in 24 frames from 14 patients. The accuracy on the training set was 100%, on the validation set was 80%. Recall and precision were respectively 100% and 100% on the training set and 68.8% and 91.7% on the validation set. Conclusion: Automated classification of intraoperative fluorescence angiography with a high degree of accuracy is possible and allows automated decision boundary identification. This will enable surgeons to standardise the technique of fluorescence angiography. A web based app was made available to deploy the algorithm. △ Less","13 June, 2022",https://arxiv.org/pdf/2206.05935
X-Risk Analysis for AI Research,Dan Hendrycks;Mantas Mazeika,"Artificial intelligence (AI) has the potential to greatly improve society, but as with any powerful technology, it comes with heightened risks and responsibilities. Current AI research lacks a systematic discussion of how to manage long-tail risks from AI systems, including speculative long-term risks. Keeping in mind the potential benefits of AI, there is some concern that building ever more intelligent and powerful AI systems could eventually result in systems that are more powerful than us; some say this is like playing with fire and speculate that this could create existential risks (x-risks). To add precision and ground these discussions, we provide a guide for how to analyze AI x-risk, which consists of three parts: First, we review how systems can be made safer today, drawing on time-tested concepts from hazard analysis and systems safety that have been designed to steer large processes in safer directions. Next, we discuss strategies for having long-term impacts on the safety of future systems. Finally, we discuss a crucial concept in making AI systems safer by improving the balance between safety and general capabilities. We hope this document and the presented concepts and tools serve as a useful guide for understanding how to analyze AI x-risk. △ Less","20 September, 2022",https://arxiv.org/pdf/2206.05862
Efficiency Comparison of AI classification algorithms for Image Detection and Recognition in Real-time,Musarrat Saberin Nipun;Rejwan Bin Sulaiman;Amer Kareem,"Face detection and identification is the most difficult and often used task in Artificial Intelligence systems. The goal of this study is to present and compare the results of several face detection and recognition algorithms used in the system. This system begins with a training image of a human, then continues on to the test image, identifying the face, comparing it to the trained face, and finally classifying it using OpenCV classifiers. This research will discuss the most effective and successful tactics used in the system, which are implemented using Python, OpenCV, and Matplotlib. It may also be used in locations with CCTV, such as public spaces, shopping malls, and ATM booths. △ Less","12 June, 2022",https://arxiv.org/pdf/2206.05842
A Survey on Uncertainty Reasoning and Quantification for Decision Making: Belief Theory Meets Deep Learning,Zhen Guo;Zelin Wan;Qisheng Zhang;Xujiang Zhao;Feng Chen;Jin-Hee Cho;Qi Zhang;Lance M. Kaplan;Dong H. Jeong;Audun Jøsang,"An in-depth understanding of uncertainty is the first step to making effective decisions under uncertainty. Deep/machine learning (ML/DL) has been hugely leveraged to solve complex problems involved with processing high-dimensional data. However, reasoning and quantifying different types of uncertainties to achieve effective decision-making have been much less explored in ML/DL than in other Artificial Intelligence (AI) domains. In particular, belief/evidence theories have been studied in KRR since the 1960s to reason and measure uncertainties to enhance decision-making effectiveness. We found that only a few studies have leveraged the mature uncertainty research in belief/evidence theories in ML/DL to tackle complex problems under different types of uncertainty. In this survey paper, we discuss several popular belief theories and their core ideas dealing with uncertainty causes and types and quantifying them, along with the discussions of their applicability in ML/DL. In addition, we discuss three main approaches that leverage belief theories in Deep Neural Networks (DNNs), including Evidential DNNs, Fuzzy DNNs, and Rough DNNs, in terms of their uncertainty causes, types, and quantification methods along with their applicability in diverse problem domains. Based on our in-depth survey, we discuss insights, lessons learned, limitations of the current state-of-the-art bridging belief theories and ML/DL, and finally, future research directions. △ Less","13 June, 2022",https://arxiv.org/pdf/2206.05675
Indirect-Instant Attention Optimization for Crowd Counting in Dense Scenes,Suyu Han;Guodong Wang;Donghua Liu,"One of appealing approaches to guiding learnable parameter optimization, such as feature maps, is global attention, which enlightens network intelligence at a fraction of the cost. However, its loss calculation process still falls short: 1)We can only produce one-dimensional 'pseudo labels' for attention, since the artificial threshold involved in the procedure is not robust; 2) The attention awaiting loss calculation is necessarily high-dimensional, and decreasing it by convolution will inevitably introduce additional learnable parameters, thus confusing the source of the loss. To this end, we devise a simple but efficient Indirect-Instant Attention Optimization (IIAO) module based on SoftMax-Attention , which transforms high-dimensional attention map into a one-dimensional feature map in the mathematical sense for loss calculation midway through the network, while automatically providing adaptive multi-scale fusion to feature pyramid module. The special transformation yields relatively coarse features and, originally, the predictive fallibility of regions varies by crowd density distribution, so we tailor the Regional Correlation Loss (RCLoss) to retrieve continuous error-prone regions and smooth spatial information . Extensive experiments have proven that our approach surpasses previous SOTA methods in many benchmark datasets. △ Less","11 June, 2022",https://arxiv.org/pdf/2206.05648
Downlink Power Minimization in Intelligent Reconfigurable Surface-Aided Security Classification Wireless Communications System,Jintao Xing;Tiejun Lv;Yashuai Cao;Jie Zeng;Pingmu Huang,"User privacy protection is considered a critical issue in wireless networks, which drives the demand for various secure information interaction techniques. In this paper, we introduce an intelligent reflecting surface (IRS)-aided security classification wireless communication system, which reduces the transmit power of the base station (BS) by classifying users with different security requirements. Specifically, we divide the users into confidential subscribers with secure communication requirements and general communication users with simple communication requirements. During the communication period, we guarantee the secure rate of the confidential subscribers while ensuring the service quality of the general communication users, thereby reducing the transmit power of the BS. To realize such a secure and green information transmission, the BS implements a beamforming design on the transmitted signal superimposed with artificial noise (AN) and then broadcasts it to users with the assistance of the IRS's reflection. We develop an alternating optimization framework to minimize the BS downlink power with respect to the active beamformers of the BS, the AN vector at the BS, and the reflection phase shifts of the IRS. A successive convex approximation (SCA) method is proposed so that the nonconvex beamforming problems can be converted to tractable convex forms. The simulation results demonstrate that the proposed algorithm is convergent and can reduce the transmit power by 20\% compared to the best benchmark scheme. △ Less","11 June, 2022",https://arxiv.org/pdf/2206.05414
"A General Framework for the Representation of Function and Affordance: A Cognitive, Causal, and Grounded Approach, and a Step Toward AGI",Seng-Beng Ho,"In AI research, so far, the attention paid to the characterization and representation of function and affordance has been sporadic and sparse, even though this aspect features prominently in an intelligent system's functioning. In the sporadic and sparse, though commendable efforts so far devoted to the characterization and understanding of function and affordance, there has also been no general framework that could unify all the different use domains and situations related to the representation and application of functional concepts. This paper develops just such a general framework, with an approach that emphasizes the fact that the representations involved must be explicitly cognitive and conceptual, and they must also contain causal characterizations of the events and processes involved, as well as employ conceptual constructs that are grounded in the referents to which they refer, in order to achieve maximal generality. The basic general framework is described, along with a set of basic guiding principles with regards to the representation of functionality. To properly and adequately characterize and represent functionality, a descriptive representation language is needed. This language is defined and developed, and many examples of its use are described. The general framework is developed based on an extension of the general language meaning representational framework called conceptual dependency. To support the general characterization and representation of functionality, the basic conceptual dependency framework is enhanced with representational devices called structure anchor and conceptual dependency elaboration, together with the definition of a set of ground level concepts. These novel representational constructs are defined, developed, and described. A general framework dealing with functionality would represent a major step toward achieving Artificial General Intelligence. △ Less","17 August, 2022",https://arxiv.org/pdf/2206.05273
On Neural Architecture Inductive Biases for Relational Tasks,Giancarlo Kerg;Sarthak Mittal;David Rolnick;Yoshua Bengio;Blake Richards;Guillaume Lajoie,"Current deep learning approaches have shown good in-distribution generalization performance, but struggle with out-of-distribution generalization. This is especially true in the case of tasks involving abstract relations like recognizing rules in sequences, as we find in many intelligence tests. Recent work has explored how forcing relational representations to remain distinct from sensory representations, as it seems to be the case in the brain, can help artificial systems. Building on this work, we further explore and formalize the advantages afforded by 'partitioned' representations of relations and sensory details, and how this inductive bias can help recompose learned relational structure in newly encountered settings. We introduce a simple architecture based on similarity scores which we name Compositional Relational Network (CoRelNet). Using this model, we investigate a series of inductive biases that ensure abstract relations are learned and represented distinctly from sensory data, and explore their effects on out-of-distribution generalization for a series of relational psychophysics tasks. We find that simple architectural choices can outperform existing models in out-of-distribution generalization. Together, these results show that partitioning relational representations from other information streams may be a simple way to augment existing network architectures' robustness when performing out-of-distribution relational computations. △ Less","9 June, 2022",https://arxiv.org/pdf/2206.05056
Artificial Intelligence Enabled NOMA Towards Next Generation Multiple Access,Xiaoxia Xu;Yuanwei Liu;Xidong Mu;Qimei Chen;Hao Jiang;Zhiguo Ding,"This article focuses on the application of artificial intelligence (AI) in non-orthogonal multiple-access (NOMA), which aims to achieve automated, adaptive, and high-efficiency multi-user communications towards next generation multiple access (NGMA). First, the limitations of current scenario-specific multiple-antenna NOMA schemes are discussed, and the importance of AI for NGMA is highlighted. Then, to achieve the vision of NGMA, a novel cluster-free NOMA framework is proposed for providing scenario-adaptive NOMA communications, and several promising machine learning solutions are identified. To elaborate further, novel centralized and distributed machine learning paradigms are conceived for efficiently employing the proposed cluster-free NOMA framework in single-cell and multi-cell networks, where numerical results are provided to demonstrate the effectiveness. Furthermore, the interplays between the proposed cluster-free NOMA and emerging wireless techniques are presented. Finally, several open research issues of AI enabled NGMA are discussed. △ Less","13 December, 2022",https://arxiv.org/pdf/2206.04992
Deep Learning-based Massive MIMO CSI Acquisition for 5G Evolution and 6G,Xin Wang;Xiaolin Hou;Lan Chen;Yoshihisa Kishiyama;Takahiro Asai,"Recently, inspired by successful applications in many fields, deep learning (DL) technologies for CSI acquisition have received considerable research interest from both academia and industry. Considering the practical feedback mechanism of 5th generation (5G) New radio (NR) networks, we propose two implementation schemes for artificial intelligence for CSI (AI4CSI), the DL-based receiver and end-to-end design, respectively. The proposed AI4CSI schemes were evaluated in 5G NR networks in terms of spectrum efficiency (SE), feedback overhead, and computational complexity, and compared with legacy schemes. To demonstrate whether these schemes can be used in real-life scenarios, both the modeled-based channel data and practically measured channels were used in our investigations. When DL-based CSI acquisition is applied to the receiver only, which has little air interface impact, it provides approximately 25\% SE gain at a moderate feedback overhead level. It is feasible to deploy it in current 5G networks during 5G evolutions. For the end-to-end DL-based CSI enhancements, the evaluations also demonstrated their additional performance gain on SE, which is 6% -- 26% compared with DL-based receivers and 33% -- 58% compared with legacy CSI schemes. Considering its large impact on air-interface design, it will be a candidate technology for 6th generation (6G) networks, in which an air interface designed by artificial intelligence can be used. △ Less","14 June, 2022",https://arxiv.org/pdf/2206.04967
Explainable Artificial Intelligence (XAI) for Internet of Things: A Survey,Ibrahim Kok;Feyza Yildirim Okay;Ozgecan Muyanli;Suat Ozdemir,"Black-box nature of Artificial Intelligence (AI) models do not allow users to comprehend and sometimes trust the output created by such model. In AI applications, where not only the results but also the decision paths to the results are critical, such black-box AI models are not sufficient. Explainable Artificial Intelligence (XAI) addresses this problem and defines a set of AI models that are interpretable by the users. Recently, several number of XAI models have been to address the issues surrounding by lack of interpretability and explainability of black-box models in various application areas such as healthcare, military, energy, financial and industrial domains. Although the concept of XAI has gained great deal of attention recently, its integration into the IoT domain has not yet been fully defined. In this paper, we provide an in-depth and systematic review of recent studies using XAI models in the scope of IoT domain. We categorize the studies according to their methodology and applications areas. In addition, we aim to focus on the challenging problems and open issues and give future directions to guide the developers and researchers for prospective future investigations. △ Less","7 June, 2022",https://arxiv.org/pdf/2206.04800
Securing AI-based Healthcare Systems using Blockchain Technology: A State-of-the-Art Systematic Literature Review and Future Research Directions,Rucha Shinde;Shruti Patil;Ketan Kotecha;Vidyasagar Potdar;Ganeshsree Selvachandran;Ajith Abraham,"Healthcare systems are increasingly incorporating Artificial Intelligence into their systems, but it is not a solution for all difficulties. AI's extraordinary potential is being held back by challenges such as a lack of medical datasets for training AI models, adversarial attacks, and a lack of trust due to its black box working style. We explored how blockchain technology can improve the reliability and trustworthiness of AI-based healthcare. This paper has conducted a Systematic Literature Review to explore the state-of-the-art research studies conducted in healthcare applications developed with different AI techniques and Blockchain Technology. This systematic literature review proceeds with three different paths as natural language processing-based healthcare systems, computer vision-based healthcare systems and acoustic AI-based healthcare systems. We found that 1) Defence techniques for adversarial attacks on AI are available for specific kind of attacks and even adversarial training is AI based technique which in further prone to different attacks. 2) Blockchain can address security and privacy issues in healthcare fraternity. 3) Medical data verification and user provenance can be enabled with Blockchain. 4) Blockchain can protect distributed learning on heterogeneous medical data. 5) The issues like single point of failure, non-transparency in healthcare systems can be resolved with Blockchain. Nevertheless, it has been identified that research is at the initial stage. As a result, we have synthesized a conceptual framework using Blockchain Technology for AI-based healthcare applications that considers the needs of each NLP, Computer Vision, and Acoustic AI application. A global solution for all sort of adversarial attacks on AI based healthcare. However, this technique has significant limits and challenges that need to be addressed in future studies. △ Less","30 May, 2022",https://arxiv.org/pdf/2206.04793
AI-based Clinical Assessment of Optic Nerve Head Robustness Superseding Biomechanical Testing,Fabian A. Braeu;Thanadet Chuangsuwanich;Tin A. Tun;Alexandre H. Thiery;Tin Aung;George Barbastathis;Michaël J. A. Girard,"\mathbf{Purpose}: To use artificial intelligence (AI) to: (1) exploit biomechanical knowledge of the optic nerve head (ONH) from a relatively large population; (2) assess ONH robustness from a single optical coherence tomography (OCT) scan of the ONH; (3) identify what critical three-dimensional (3D) structural features make a given ONH robust. \mathbf{Design}: Retrospective cross-sectional study. \mathbf{Methods}: 316 subjects had their ONHs imaged with OCT before and after acute intraocular pressure (IOP) elevation through ophthalmo-dynamometry. IOP-induced lamina-cribrosa deformations were then mapped in 3D and used to classify ONHs. Those with LC deformations superior to 4% were considered fragile, while those with deformations inferior to 4% robust. Learning from these data, we compared three AI algorithms to predict ONH robustness strictly from a baseline (undeformed) OCT volume: (1) a random forest classifier; (2) an autoencoder; and (3) a dynamic graph CNN (DGCNN). The latter algorithm also allowed us to identify what critical 3D structural features make a given ONH robust. \mathbf{Results}: All 3 methods were able to predict ONH robustness from 3D structural information alone and without the need to perform biomechanical testing. The DGCNN (area under the receiver operating curve [AUC]: 0.76 \pm 0.08) outperformed the autoencoder (AUC: 0.70 \pm 0.07) and the random forest classifier (AUC: 0.69 \pm 0.05). Interestingly, to assess ONH robustness, the DGCNN mainly used information from the scleral canal and the LC insertion sites. \mathbf{Conclusions}: We propose an AI-driven approach that can assess the robustness of a given ONH solely from a single OCT scan of the ONH, and without the need to perform biomechanical testing. Longitudinal studies should establish whether ONH robustness could help us identify fast visual field loss progressors. △ Less","9 June, 2022",https://arxiv.org/pdf/2206.04689
Molecular dynamics without molecules: searching the conformational space of proteins with generative neural networks,Gregory Schwing;Luigi L. Palese;Ariel Fernández;Loren Schwiebert;Domenico L. Gatti,"All-atom and coarse-grained molecular dynamics are two widely used computational tools to study the conformational states of proteins. Yet, these two simulation methods suffer from the fact that without access to supercomputing resources, the time and length scales at which these states become detectable are difficult to achieve. One alternative to such methods is based on encoding the atomistic trajectory of molecular dynamics as a shorthand version devoid of physical particles, and then learning to propagate the encoded trajectory through the use of artificial intelligence. Here we show that a simple textual representation of the frames of molecular dynamics trajectories as vectors of Ramachandran basin classes retains most of the structural information of the full atomistic representation of a protein in each frame, and can be used to generate equivalent atom-less trajectories suitable to train different types of generative neural networks. In turn, the trained generative models can be used to extend indefinitely the atom-less dynamics or to sample the conformational space of proteins from their representation in the models latent space. We define intuitively this methodology as molecular dynamics without molecules, and show that it enables to cover physically relevant states of proteins that are difficult to access with traditional molecular dynamics. △ Less","8 June, 2022",https://arxiv.org/pdf/2206.04683
Uni-Perceiver-MoE: Learning Sparse Generalist Models with Conditional MoEs,Jinguo Zhu;Xizhou Zhu;Wenhai Wang;Xiaohua Wang;Hongsheng Li;Xiaogang Wang;Jifeng Dai,"To build an artificial neural network like the biological intelligence system, recent works have unified numerous tasks into a generalist model, which can process various tasks with shared parameters and do not have any task-specific modules. While generalist models achieve promising results on various benchmarks, they have performance degradation on some tasks compared with task-specialized models. In this work, we find that interference among different tasks and modalities is the main factor to this phenomenon. To mitigate such interference, we introduce the Conditional Mixture-of-Experts (Conditional MoEs) to generalist models. Routing strategies under different levels of conditions are proposed to take both the training/inference cost and generalization ability into account. By incorporating the proposed Conditional MoEs, the recently proposed generalist model Uni-Perceiver can effectively mitigate the interference across tasks and modalities, and achieves state-of-the-art results on a series of downstream tasks via prompt tuning on 1% of downstream data. Moreover, the introduction of Conditional MoEs still holds the generalization ability of generalist models to conduct zero-shot inference on new tasks, e.g., video-text retrieval and video caption. Code and pre-trained generalist models shall be released. △ Less","5 July, 2022",https://arxiv.org/pdf/2206.04674
Functional Code Building Genetic Programming,Edward Pantridge;Thomas Helmuth;Lee Spector,"General program synthesis has become an important application area for genetic programming (GP), and for artificial intelligence more generally. Code Building Genetic Programming (CBGP) is a recently introduced GP method for general program synthesis that leverages reflection and first class specifications to support the evolution of programs that may use arbitrary data types, polymorphism, and functions drawn from existing codebases. However, neither a formal description nor a thorough benchmarking of CBGP have yet been reported. In this work, we formalize the method of CBGP using algorithms from type theory. Specially, we show that a functional programming language and a Hindley-Milner type system can be used to evolve type-safe programs using the process abstractly described in the original CBGP paper. Furthermore, we perform a comprehensive analysis of the search performance of this functional variant of CBGP compared to other contemporary GP program synthesis methods. △ Less","9 June, 2022",https://arxiv.org/pdf/2206.04561
AAM-Gym: Artificial Intelligence Testbed for Advanced Air Mobility,Marc Brittain;Luis E. Alvarez;Kara Breeden;Ian Jessen,"We introduce AAM-Gym, a research and development testbed for Advanced Air Mobility (AAM). AAM has the potential to revolutionize travel by reducing ground traffic and emissions by leveraging new types of aircraft such as electric vertical take-off and landing (eVTOL) aircraft and new advanced artificial intelligence (AI) algorithms. Validation of AI algorithms require representative AAM scenarios, as well as a fast time simulation testbed to evaluate their performance. Until now, there has been no such testbed available for AAM to enable a common research platform for individuals in government, industry, or academia. MIT Lincoln Laboratory has developed AAM-Gym to address this gap by providing an ecosystem to develop, train, and validate new and established AI algorithms across a wide variety of AAM use-cases. In this paper, we use AAM-Gym to study the performance of two reinforcement learning algorithms on an AAM use-case, separation assurance in AAM corridors. The performance of the two algorithms is demonstrated based on a series of metrics provided by AAM-Gym, showing the testbed's utility to AAM research. △ Less","9 June, 2022",https://arxiv.org/pdf/2206.04513
BSM loss: A superior way in modeling aleatory uncertainty of fine_grained classification,Shuang Ge;Kehong Yuan;Maokun Han;Desheng Sun;Huabin Zhang;Qiongyu Ye,"Artificial intelligence(AI)-assisted method had received much attention in the risk field such as disease diagnosis. Different from the classification of disease types, it is a fine-grained task to classify the medical images as benign or malignant. However, most research only focuses on improving the diagnostic accuracy and ignores the evaluation of model reliability, which limits its clinical application. For clinical practice, calibration presents major challenges in the low-data regime extremely for over-parametrized models and inherent noises. In particular, we discovered that modeling data-dependent uncertainty is more conducive to confidence calibrations. Compared with test-time augmentation(TTA), we proposed a modified Bootstrapping loss(BS loss) function with Mixup data augmentation strategy that can better calibrate predictive uncertainty and capture data distribution transformation without additional inference time. Our experiments indicated that BS loss with Mixup(BSM) model can halve the Expected Calibration Error(ECE) compared to standard data augmentation, deep ensemble and MC dropout. The correlation between uncertainty and similarity of in-domain data is up to -0.4428 under the BSM model. Additionally, the BSM model is able to perceive the semantic distance of out-of-domain data, demonstrating high potential in real-world clinical practice. △ Less","9 June, 2022",https://arxiv.org/pdf/2206.04479
An Optimization Method-Assisted Ensemble Deep Reinforcement Learning Algorithm to Solve Unit Commitment Problems,Jingtao Qin;Yuanqi Gao;Mikhail Bragin;Nanpeng Yu,"Unit commitment (UC) is a fundamental problem in the day-ahead electricity market, and it is critical to solve UC problems efficiently. Mathematical optimization techniques like dynamic programming, Lagrangian relaxation, and mixed-integer quadratic programming (MIQP) are commonly adopted for UC problems. However, the calculation time of these methods increases at an exponential rate with the amount of generators and energy resources, which is still the main bottleneck in industry. Recent advances in artificial intelligence have demonstrated the capability of reinforcement learning (RL) to solve UC problems. Unfortunately, the existing research on solving UC problems with RL suffers from the curse of dimensionality when the size of UC problems grows. To deal with these problems, we propose an optimization method-assisted ensemble deep reinforcement learning algorithm, where UC problems are formulated as a Markov Decision Process (MDP) and solved by multi-step deep Q-learning in an ensemble framework. The proposed algorithm establishes a candidate action set by solving tailored optimization problems to ensure a relatively high performance and the satisfaction of operational constraints. Numerical studies on IEEE 118 and 300-bus systems show that our algorithm outperforms the baseline RL algorithm and MIQP. Furthermore, the proposed algorithm shows strong generalization capacity under unforeseen operational conditions. △ Less","8 June, 2022",https://arxiv.org/pdf/2206.04249
Automating Ambiguity: Challenges and Pitfalls of Artificial Intelligence,Abeba Birhane,"Machine learning (ML) and artificial intelligence (AI) tools increasingly permeate every possible social, political, and economic sphere; sorting, taxonomizing and predicting complex human behaviour and social phenomena. However, from fallacious and naive groundings regarding complex adaptive systems to datasets underlying models, these systems are beset by problems, challenges, and limitations. They remain opaque and unreliable, and fail to consider societal and structural oppressive systems, disproportionately negatively impacting those at the margins of society while benefiting the most powerful. The various challenges, problems and pitfalls of these systems are a hot topic of research in various areas, such as critical data/algorithm studies, science and technology studies (STS), embodied and enactive cognitive science, complexity science, Afro-feminism, and the broadly construed emerging field of Fairness, Accountability, and Transparency (FAccT). Yet, these fields of enquiry often proceed in silos. This thesis weaves together seemingly disparate fields of enquiry to examine core scientific and ethical challenges, pitfalls, and problems of AI. In this thesis I, a) review the historical and cultural ecology from which AI research emerges, b) examine the shaky scientific grounds of machine prediction of complex behaviour illustrating how predicting complex behaviour with precision is impossible in principle, c) audit large scale datasets behind current AI demonstrating how they embed societal historical and structural injustices, d) study the seemingly neutral values of ML research and put forward 67 prominent values underlying ML research, e) examine some of the insidious and worrying applications of computer vision research, and f) put forward a framework for approaching challenges, failures and problems surrounding ML systems as well as alternative ways forward. △ Less","8 June, 2022",https://arxiv.org/pdf/2206.04179
Forecasting AI Progress: Evidence from a Survey of Machine Learning Researchers,Baobao Zhang;Noemi Dreksler;Markus Anderljung;Lauren Kahn;Charlie Giattino;Allan Dafoe;Michael C. Horowitz,"Advances in artificial intelligence (AI) are shaping modern life, from transportation, health care, science, finance, to national defense. Forecasts of AI development could help improve policy- and decision-making. We report the results from a large survey of AI and machine learning (ML) researchers on their beliefs about progress in AI. The survey, fielded in late 2019, elicited forecasts for near-term AI development milestones and high- or human-level machine intelligence, defined as when machines are able to accomplish every or almost every task humans are able to do currently. As part of this study, we re-contacted respondents from a highly-cited study by Grace et al. (2018), in which AI/ML researchers gave forecasts about high-level machine intelligence and near-term milestones in AI development. Results from our 2019 survey show that, in aggregate, AI/ML researchers surveyed placed a 50% likelihood of human-level machine intelligence being achieved by 2060. The results show researchers newly contacted in 2019 expressed similar beliefs about the progress of advanced AI as respondents in the Grace et al. (2018) survey. For the recontacted participants from the Grace et al. (2018) study, the aggregate forecast for a 50% likelihood of high-level machine intelligence shifted from 2062 to 2076, although this change is not statistically significant, likely due to the small size of our panel sample. Forecasts of several near-term AI milestones have reduced in time, suggesting more optimism about AI progress. Finally, AI/ML researchers also exhibited significant optimism about how human-level machine intelligence will impact society. △ Less","8 June, 2022",https://arxiv.org/pdf/2206.04132
Deep Hierarchical Planning from Pixels,Danijar Hafner;Kuang-Huei Lee;Ian Fischer;Pieter Abbeel,"Intelligent agents need to select long sequences of actions to solve complex tasks. While humans easily break down tasks into subgoals and reach them through millions of muscle commands, current artificial intelligence is limited to tasks with horizons of a few hundred decisions, despite large compute budgets. Research on hierarchical reinforcement learning aims to overcome this limitation but has proven to be challenging, current methods rely on manually specified goal spaces or subtasks, and no general solution exists. We introduce Director, a practical method for learning hierarchical behaviors directly from pixels by planning inside the latent space of a learned world model. The high-level policy maximizes task and exploration rewards by selecting latent goals and the low-level policy learns to achieve the goals. Despite operating in latent space, the decisions are interpretable because the world model can decode goals into images for visualization. Director outperforms exploration methods on tasks with sparse rewards, including 3D maze traversal with a quadruped robot from an egocentric camera and proprioception, without access to the global position or top-down view that was used by prior work. Director also learns successful behaviors across a wide range of environments, including visual control, Atari games, and DMLab levels. △ Less","8 June, 2022",https://arxiv.org/pdf/2206.04114
Challenges in Applying Explainability Methods to Improve the Fairness of NLP Models,Esma Balkir;Svetlana Kiritchenko;Isar Nejadgholi;Kathleen C. Fraser,"Motivations for methods in explainable artificial intelligence (XAI) often include detecting, quantifying and mitigating bias, and contributing to making machine learning models fairer. However, exactly how an XAI method can help in combating biases is often left unspecified. In this paper, we briefly review trends in explainability and fairness in NLP research, identify the current practices in which explainability methods are applied to detect and mitigate bias, and investigate the barriers preventing XAI methods from being used more widely in tackling fairness issues. △ Less","8 June, 2022",https://arxiv.org/pdf/2206.03945
Sim2real for Reinforcement Learning Driven Next Generation Networks,Peizheng Li;Jonathan Thomas;Xiaoyang Wang;Hakan Erdol;Abdelrahim Ahmad;Rui Inacio;Shipra Kapoor;Arjun Parekh;Angela Doufexi;Arman Shojaeifard;Robert Piechocki,"The next generation of networks will actively embrace artificial intelligence (AI) and machine learning (ML) technologies for automation networks and optimal network operation strategies. The emerging network structure represented by Open RAN (O-RAN) conforms to this trend, and the radio intelligent controller (RIC) at the centre of its specification serves as an ML applications host. Various ML models, especially Reinforcement Learning (RL) models, are regarded as the key to solving RAN-related multi-objective optimization problems. However, it should be recognized that most of the current RL successes are confined to abstract and simplified simulation environments, which may not directly translate to high performance in complex real environments. One of the main reasons is the modelling gap between the simulation and the real environment, which could make the RL agent trained by simulation ill-equipped for the real environment. This issue is termed as the sim2real gap. This article brings to the fore the sim2real challenge within the context of O-RAN. Specifically, it emphasizes the characteristics, and benefits that the digital twins (DT) could have as a place for model development and verification. Several use cases are presented to exemplify and demonstrate failure modes of the simulations trained RL model in real environments. The effectiveness of DT in assisting the development of RL algorithms is discussed. Then the current state of the art learning-based methods commonly used to overcome the sim2real challenge are presented. Finally, the development and deployment concerns for the RL applications realisation in O-RAN are discussed from the view of the potential issues like data interaction, environment bottlenecks, and algorithm design. △ Less","8 June, 2022",https://arxiv.org/pdf/2206.03846
"XAI for Cybersecurity: State of the Art, Challenges, Open Issues and Future Directions",Gautam Srivastava;Rutvij H Jhaveri;Sweta Bhattacharya;Sharnil Pandya;Rajeswari;Praveen Kumar Reddy Maddikunta;Gokul Yenduri;Jon G. Hall;Mamoun Alazab;Thippa Reddy Gadekallu,"In the past few years, artificial intelligence (AI) techniques have been implemented in almost all verticals of human life. However, the results generated from the AI models often lag explainability. AI models often appear as a blackbox wherein developers are unable to explain or trace back the reasoning behind a specific decision. Explainable AI (XAI) is a rapid growing field of research which helps to extract information and also visualize the results generated with an optimum transparency. The present study provides and extensive review of the use of XAI in cybersecurity. Cybersecurity enables protection of systems, networks and programs from different types of attacks. The use of XAI has immense potential in predicting such attacks. The paper provides a brief overview on cybersecurity and the various forms of attack. Then the use of traditional AI techniques and its associated challenges are discussed which opens its doors towards use of XAI in various applications. The XAI implementations of various research projects and industry are also presented. Finally, the lessons learnt from these applications are highlighted which act as a guide for future scope of research. △ Less","2 June, 2022",https://arxiv.org/pdf/2206.03585
Formalization of the principles of brain Programming (Brain Principles Programming),E. E. Vityaev;A. G. Kolonin;A. V. Kurpatov A. A. Molchanov,"In the monograph ""Strong artificial intelligence. On the Approaches to Superintelligence"" contains an overview of general artificial intelligence (AGI). As an anthropomorphic research area, it includes Brain Principles Programming (BPP) -- the formalization of universal mechanisms (principles) of the brain work with information, which are implemented at all levels of the organization of nervous tissue. This monograph contains a formalization of these principles in terms of category theory. However, this formalization is not enough to develop algorithms for working with information. In this paper, for the description and modeling of BPP, it is proposed to apply mathematical models and algorithms developed earlier, which modeling cognitive functions and base on well-known physiological, psychological and other natural science theories. The paper uses mathematical models and algorithms of the following theories: P.K.Anokhin Theory of Functional Brain Systems, Eleanor Rosch prototypical categorization theory, Bob Rehder theory of causal models and ""natural"" classification. As a result, a formalization of BPP is obtained and computer experiments demonstrating the operation of algorithms are presented. △ Less","14 June, 2022",https://arxiv.org/pdf/2206.03487
Combining physics-based and data-driven techniques for reliable hybrid analysis and modeling using the corrective source term approach,Sindre Stenen Blakseth;Adil Rasheed;Trond Kvamsdal;Omer San,"Upcoming technologies like digital twins, autonomous, and artificial intelligent systems involving safety-critical applications require models which are accurate, interpretable, computationally efficient, and generalizable. Unfortunately, the two most commonly used modeling approaches, physics-based modeling (PBM) and data-driven modeling (DDM) fail to satisfy all these requirements. In the current work, we demonstrate how a hybrid approach combining the best of PBM and DDM can result in models which can outperform them both. We do so by combining partial differential equations based on first principles describing partially known physics with a black box DDM, in this case, a deep neural network model compensating for the unknown physics. First, we present a mathematical argument for why this approach should work and then apply the hybrid approach to model two dimensional heat diffusion problem with an unknown source term. The result demonstrates the method's superior performance in terms of accuracy, and generalizability. Additionally, it is shown how the DDM part can be interpreted within the hybrid framework to make the overall approach reliable. △ Less","7 June, 2022",https://arxiv.org/pdf/2206.03451
Neuro-Nav: A Library for Neurally-Plausible Reinforcement Learning,Arthur Juliani;Samuel Barnett;Brandon Davis;Margaret Sereno;Ida Momennejad,"In this work we propose Neuro-Nav, an open-source library for neurally plausible reinforcement learning (RL). RL is among the most common modeling frameworks for studying decision making, learning, and navigation in biological organisms. In utilizing RL, cognitive scientists often handcraft environments and agents to meet the needs of their particular studies. On the other hand, artificial intelligence researchers often struggle to find benchmarks for neurally and biologically plausible representation and behavior (e.g., in decision making or navigation). In order to streamline this process across both fields with transparency and reproducibility, Neuro-Nav offers a set of standardized environments and RL algorithms drawn from canonical behavioral and neural studies in rodents and humans. We demonstrate that the toolkit replicates relevant findings from a number of studies across both cognitive science and RL literatures. We furthermore describe ways in which the library can be extended with novel algorithms (including deep RL) and environments to address future research needs of the field. △ Less","6 June, 2022",https://arxiv.org/pdf/2206.03312
Future Artificial Intelligence tools and perspectives in medicine,Ahmad Chaddad;Yousef Katib;Lama Hassan,"Purpose of review: Artificial intelligence (AI) has become popular in medical applications, specifically as a clinical support tool for computer-aided diagnosis. These tools are typically employed on medical data (i.e., image, molecular data, clinical variables, etc.) and used the statistical and machine learning methods to measure the model performance. In this review, we summarized and discussed the most recent radiomic pipeline used for clinical analysis. Recent findings:Currently, limited management of cancers benefits from artificial intelligence, mostly related to a computer-aided diagnosis that avoids a biopsy analysis that presents additional risks and costs. Most AI tools are based on imaging features, known as radiomic analysis that can be refined into predictive models in non-invasively acquired imaging data. This review explores the progress of AI-based radiomic tools for clinical applications with a brief description of necessary technical steps. Explaining new radiomic approaches based on deep learning techniques will explain how the new radiomic models (deep radiomic analysis) can benefit from deep convolutional neural networks and be applied on limited data sets. Summary: To consider the radiomic algorithms, further investigations are recommended to involve deep learning in radiomic models with additional validation steps on various cancer types. △ Less","4 June, 2022",https://arxiv.org/pdf/2206.03289
Examining the Implementation of Digital Health to Strengthen the COVID-19 Pandemic Response and Recovery and Scale up Equitable Vaccine Access in African Countries,Olufunto A Olusanya;Brianna White;Chad A Melton;Arash Shaban-Nejad,"The COVID-19 pandemic has profoundly impacted the world, having taken the lives of over 6 million individuals. Accordingly, this pandemic has caused a shift in conversations surrounding the burden of diseases worldwide, welcoming insights from multidisciplinary fields including digital health and artificial intelligence. Africa faces a heavy disease burden that exacerbates the current COVID-19 pandemic and limits the scope of public health preparedness, response, containment, and case management. Herein, we examined the potential impact of transformative digital health technologies in mitigating the global health crisis with reference to African countries. Furthermore, we proposed recommendations for scaling up digital health technologies and artificial intelligence-based platforms to tackle the transmission of the SARS-CoV-2 and enable equitable vaccine access. Challenges related to the pandemic are numerous. Rapid response and management strategies - that is, contract tracing, case surveillance, diagnostic testing intensity, and most recently vaccine distribution mapping - can overwhelm the health care delivery system that is fragile. Although challenges are vast, digital health technologies can play an essential role in achieving sustainable resilient recovery and building back better. It is plausible that African nations are better equipped to rapidly identify, diagnose, and manage infected individuals for COVID-19, other diseases, future outbreaks, and pandemics. △ Less","3 June, 2022",https://arxiv.org/pdf/2206.03286
Ubiquitous knowledge empowers the Smart Factory: The impacts of a Service-oriented Digital Twin on enterprises' performance,Francesco Longo;Letizia Nicoletti;Antonio Padovano,"While the Industry 4.0 is idolizing the potential of an artificial intelligence embedded into ""things"", it is neglecting the role of the human component, which is still indispensable in different manufacturing activities, such as a machine setup or maintenance operations. The present research study first proposes an Industrial Internet pyramid as emergent human-centric manufacturing paradigm within Industry 4.0 in which central is the role of a Ubiquitous Knowledge about the manufacturing system intuitively accessed and used by the manufacturing employees. Second, the prototype of a Service-oriented Digital Twin, which leverage on a flexible ontology-oriented knowledge structure and on augmented reality combined to a vocal interaction system for an intuitive knowledge retrieval and fruition, has been designed and developed to deliver this manufacturing knowledge. Two test-beds, complimentary for the problems in practice (the former on the maintenance-production interface in a large enterprise, the latter majorly focused in production and setups in a small and medium enterprise), show the significant benefits in terms of time, costs and process quality, thus validating the approach proposed. This research shows that a human-centric and knowledge-driven approach can drive the performance of Industry 4.0 initiatives and lead a Smart Factory towards its full potential. △ Less","30 May, 2022",https://arxiv.org/pdf/2206.03268
Using sensitive data to prevent discrimination by artificial intelligence: Does the GDPR need a new exception?,Marvin van Bekkum;Frederik Zuiderveen Borgesius,"Organisations can use artificial intelligence to make decisions about people for a variety of reasons, for instance, to select the best candidates from many job applications. However, AI systems can have discriminatory effects when used for decision-making. To illustrate, an AI system could reject applications of people with a certain ethnicity, while the organisation did not plan such ethnicity discrimination. But in Europe, an organisation runs into a problem when it wants to assess whether its AI system accidentally discriminates based on ethnicity: the organisation may not know the applicants' ethnicity. In principle, the GDPR bans the use of certain 'special categories of data' (sometimes called 'sensitive data'), which include data on ethnicity, religion, and sexual preference. The proposal for an AI Act of the European Commission includes a provision that would enable organisations to use special categories of data for auditing their AI systems. This paper asks whether the GDPR's rules on special categories of personal data hinder the prevention of AI-driven discrimination. We argue that the GDPR does prohibit such use of special category data in many circumstances. We also map out the arguments for and against creating an exception to the GDPR's ban on using special categories of personal data, to enable preventing discrimination by AI systems. The paper discusses European law, but the paper can be relevant outside Europe too, as many policymakers in the world grapple with the tension between privacy and non-discrimination policy. △ Less","28 November, 2022",https://arxiv.org/pdf/2206.03262
Analyzing the impact of feature selection on the accuracy of heart disease prediction,Muhammad Salman Pathan;Avishek Nag;Muhammad Mohisn Pathan;Soumyabrata Dev,"Heart Disease has become one of the most serious diseases that has a significant impact on human life. It has emerged as one of the leading causes of mortality among the people across the globe during the last decade. In order to prevent patients from further damage, an accurate diagnosis of heart disease on time is an essential factor. Recently we have seen the usage of non-invasive medical procedures, such as artificial intelligence-based techniques in the field of medical. Specially machine learning employs several algorithms and techniques that are widely used and are highly useful in accurately diagnosing the heart disease with less amount of time. However, the prediction of heart disease is not an easy task. The increasing size of medical datasets has made it a complicated task for practitioners to understand the complex feature relations and make disease predictions. Accordingly, the aim of this research is to identify the most important risk-factors from a highly dimensional dataset which helps in the accurate classification of heart disease with less complications. For a broader analysis, we have used two heart disease datasets with various medical features. The classification results of the benchmarked models proved that there is a high impact of relevant features on the classification accuracy. Even with a reduced number of features, the performance of the classification models improved significantly with a reduced training time as compared with models trained on full feature set. △ Less","7 June, 2022",https://arxiv.org/pdf/2206.03239
The Different Faces of AI Ethics Across the World: A Principle-Implementation Gap Analysis,Lionel Nganyewou Tidjon;Foutse Khomh,"Artificial Intelligence (AI) is transforming our daily life with several applications in healthcare, space exploration, banking and finance. These rapid progresses in AI have brought increasing attention to the potential impacts of AI technologies on society, with ethically questionable consequences. In recent years, several ethical principles have been released by governments, national and international organisations. These principles outline high-level precepts to guide the ethical development, deployment, and governance of AI. However, the abstract nature, diversity, and context-dependency of these principles make them difficult to implement and operationalize, resulting in gaps between principles and their execution. Most recent work analysed and summarized existing AI principles and guidelines but they did not provide findings on principle-implementation gaps and how to mitigate them. These findings are particularly important to ensure that AI implementations are aligned with ethical principles and values. In this paper, we provide a contextual and global evaluation of current ethical AI principles for all continents, with the aim to identify potential principle characteristics tailored to specific countries or applicable across countries. Next, we analyze the current level of AI readiness and current implementations of ethical AI principles in different countries, to identify gaps in the implementation of AI principles and their causes. Finally, we propose recommendations to mitigate the principle-implementation gaps. △ Less","12 May, 2022",https://arxiv.org/pdf/2206.03225
Proteção intelectual de obras produzidas por sistemas baseados em inteligência artificial: uma visão tecnicista sobre o tema,Fábio Manoel França Lobato,"The pervasiveness of Artificial Intelligence (AI) is unquestionable in our society. Even in the arts, AI is present. A notorious case is the song ""Hey Ya!"" of the OutKast group, successful in the 2000s. At this time, the music industry began to make decisions based on data to strategize based on predictions of listeners' habits. This case is just one of the countless examples of AI applications in the arts. The advent of deep learning made it possible to build systems capable of accurately recognizing artistic style in paintings. Content generation is also possible; for example, Deepart customizes images from two \textit{inputs}: 1) an image to be customized; 2) a style of painting. The generation of songs according to specific styles from AI-based systems is also possible. Such possibilities raise questions about the intellectual property of such works. On this occasion, who owns the copyright of a work produced from a system based on Artificial Intelligence? To the creator of the AI? The company/corporation that subsidized the development of this system? Or AI itself as a creator? This essay aims to contribute with a technicist view on the discussion of copyright applicability from works produced by AI. △ Less","11 May, 2022",https://arxiv.org/pdf/2206.03215
Improving Students' Academic Performance with AI and Semantic Technologies,Yixin Cheng,"Artificial intelligence and semantic technologies are evolving and have been applied in various research areas, including the education domain. Higher Education institutions strive to improve students' academic performance. Early intervention to at-risk students and a reasonable curriculum is vital for students' success. Prior research opted for deploying traditional machine learning models to predict students' performance. In terms of curriculum semantic analysis, after conducting a comprehensive systematic review regarding the use of semantic technologies in the Computer Science curriculum, a major finding of the study is that technologies used to measure similarity have limitations in terms of accuracy and ambiguity in the representation of concepts, courses, etc. To fill these gaps, in this study, three implementations were developed, that is, to predict students' performance using marks from the previous semester, to model a course representation in a semantic way and compute the similarity, and to identify the prerequisite between two similar courses. Regarding performance prediction, we used the combination of Genetic Algorithm and Long-Short Term Memory (LSTM) on a dataset from a Brazilian university containing 248730 records. As for similarity measurement, we deployed BERT to encode the sentences and used cosine similarity to obtain the distance between courses. With respect to prerequisite identification, TextRazor was applied to extract concepts from course description, followed by employing SemRefD to measure the degree of prerequisite between two concepts. The outcomes of this study can be summarized as: (i) a breakthrough result improves Manrique's work by 2.5% in terms of accuracy in dropout prediction; (ii) uncover the similarity between courses based on course description; (iii) identify the prerequisite over three compulsory courses of School of Computing at ANU. △ Less","26 October, 2022",https://arxiv.org/pdf/2206.03213
The Structure of Interdisciplinary Science: Uncovering and Explaining Roles in Citation Graphs,Eoghan Cunningham;Derek Greene,"Role discovery is the task of dividing the set of nodes on a graph into classes of structurally similar roles. Modern strategies for role discovery typically rely on graph embedding techniques, which are capable of recognising complex local structures. However, when working with large, real-world networks, it is difficult to interpret or validate a set of roles identified according to these methods. In this work, motivated by advancements in the field of explainable artificial intelligence (XAI), we propose a new framework for interpreting role assignments on large graphs using small subgraph structures known as graphlets. We demonstrate our methods on a large, multidisciplinary citation network, where we successfully identify a number of important citation patterns which reflect interdisciplinary research △ Less","7 June, 2022",https://arxiv.org/pdf/2206.03159
Exploration of Systolic-Vector Architecture with Resource Scheduling for Dynamic ML Workloads,Jung-Hoon Kim;Sungyeob Yoo;Seungjae Moon;Joo-Young Kim,"As artificial intelligence (AI) and machine learning (ML) technologies disrupt a wide range of industries, cloud datacenters face ever-increasing demand in inference workloads. However, conventional CPU-based servers cannot handle excessive computational requirements of deep neural network (DNN) models, while GPU-based servers suffer from huge power consumption and high operating cost. In this paper, we present a scalable systolic-vector architecture that can cope with dynamically changing DNN workloads in cloud datacenters. We first devise a lightweight DNN model description format called unified model format (UMF) that enables general model representation and fast decoding in hardware accelerator. Based on this model format, we propose a heterogeneous architecture that features a load balancer that performs a high-level workload distribution and multiple systolic-vector clusters, in which each cluster consists of a programmable scheduler, throughput-oriented systolic arrays, and function-oriented vector processors. We also propose a heterogeneity-aware scheduling algorithm that enables concurrent execution of multiple DNN workloads while maximizing heterogeneous hardware utilization based on computation and memory access time estimation. Finally, we build an architecture simulation framework based on actual synthesis and place-and-route implementation results and conduct design space exploration for the proposed architecture. As a result, the proposed systolic-vector architecture achieves 10.9x higher throughput performance and 30.17x higher energy efficiency than a compatible GPU on realistic ML workloads. The proposed heterogeneity-aware scheduling algorithm improves the throughput and energy efficiency by 81% and 20%, respectively, compared to a standard round-robin scheduling. △ Less","7 June, 2022",https://arxiv.org/pdf/2206.03060
Explainability in Mechanism Design: Recent Advances and the Road Ahead,Sharadhi Alape Suryanarayana;David Sarne;Sarit Kraus,"Designing and implementing explainable systems is seen as the next step towards increasing user trust in, acceptance of and reliance on Artificial Intelligence (AI) systems. While explaining choices made by black-box algorithms such as machine learning and deep learning has occupied most of the limelight, systems that attempt to explain decisions (even simple ones) in the context of social choice are steadily catching up. In this paper, we provide a comprehensive survey of explainability in mechanism design, a domain characterized by economically motivated agents and often having no single choice that maximizes all individual utility functions. We discuss the main properties and goals of explainability in mechanism design, distinguishing them from those of Explainable AI in general. This discussion is followed by a thorough review of the challenges one may face when working on Explainable Mechanism Design and propose a few solution concepts to those. △ Less","21 August, 2022",https://arxiv.org/pdf/2206.03031
Mobile Health Solution for College Student Mental Health: Interview Study and Design Requirement Analysis,Xiaomei Wang;Alec Smith;Bruce Keller;Farzan Sasangohar,"Background: Mental health problems are prevalent in college students. The COVID-19 pandemic exacerbated the problems, and created a surge in the popularity of telehealth and mobile health solutions. Despite that mobile health is a promising approach to help students with mental health needs, few studies exist in investigating key features students need in a mental health self-management tool. Objective: The objective of our study was to identified key requirements and features for the design of a student-centered mental health self-management tool. Methods: An interview study was first conducted to understand college students' needs and preferences on a mental health self-management tool. Functional information requirement analysis was then conducted to translate the needs into design implications. Results: A total of 153 university students were recruited for the semi-structured interview. The participants mentioned several features including coping techniques, artificial intelligence, time management, tracking, and communication with others. Participant's preferences on usability and privacy settings were also collected. The desired functions were analyzed and turned into design-agnostic information requirements. Conclusions: This study documents findings from interviews with university students to understand their needs and preferences for a tool to help with self-management of mental health. △ Less","6 June, 2022",https://arxiv.org/pdf/2206.02960
Researching Alignment Research: Unsupervised Analysis,Jan H. Kirchner;Logan Smith;Jacques Thibodeau;Kyle McDonell;Laria Reynolds,"AI alignment research is the field of study dedicated to ensuring that artificial intelligence (AI) benefits humans. As machine intelligence gets more advanced, this research is becoming increasingly important. Researchers in the field share ideas across different media to speed up the exchange of information. However, this focus on speed means that the research landscape is opaque, making it difficult for young researchers to enter the field. In this project, we collected and analyzed existing AI alignment research. We found that the field is growing quickly, with several subfields emerging in parallel. We looked at the subfields and identified the prominent researchers, recurring topics, and different modes of communication in each. Furthermore, we found that a classifier trained on AI alignment research articles can detect relevant articles that we did not originally include in the dataset. We are sharing the dataset with the research community and hope to develop tools in the future that will help both established researchers and young researchers get more involved in the field. △ Less","6 June, 2022",https://arxiv.org/pdf/2206.02841
Intake Monitoring in Free-Living Conditions: Overview and Lessons we Have Learned,Christos Diou;Konstantinos Kyritsis;Vasileios Papapanagiotou;Ioannis Sarafis,"The progress in artificial intelligence and machine learning algorithms over the past decade has enabled the development of new methods for the objective measurement of eating, including both the measurement of eating episodes as well as the measurement of in-meal eating behavior. These allow the study of eating behavior outside the laboratory in free-living conditions, without the need for video recordings and laborious manual annotations. In this paper, we present a high-level overview of our recent work on intake monitoring using a smartwatch, as well as methods using an in-ear microphone. We also present evaluation results of these methods in challenging, real-world datasets. Furthermore, we discuss use-cases of such intake monitoring tools for advancing research in eating behavior, for improving dietary monitoring, as well as for developing evidence-based health policies. Our goal is to inform researchers and users of intake monitoring methods regarding (i) the development of new methods based on commercially available devices, (ii) what to expect in terms of effectiveness, and (iii) how these methods can be used in research as well as in practical applications. △ Less","4 June, 2022",https://arxiv.org/pdf/2206.02784
Predicting and Understanding Human Action Decisions during Skillful Joint-Action via Machine Learning and Explainable-AI,Fabrizia Auletta;Rachel W. Kallen;Mario di Bernardo;Micheal J. Richardson,"This study uses supervised machine learning (SML) and explainable artificial intelligence (AI) to model, predict and understand human decision-making during skillful joint-action. Long short-term memory networks were trained to predict the target selection decisions of expert and novice actors completing a dyadic herding task. Results revealed that the trained models were expertise specific and could not only accurately predict the target selection decisions of expert and novice herders but could do so at timescales that preceded an actor's conscious intent. To understand what differentiated the target selection decisions of expert and novice actors, we then employed the explainable-AI technique, SHapley Additive exPlanation, to identify the importance of informational features (variables) on model predictions. This analysis revealed that experts were more influenced by information about the state of their co-herders compared to novices. The utility of employing SML and explainable-AI techniques for investigating human decision-making is discussed. △ Less","6 June, 2022",https://arxiv.org/pdf/2206.02739
Automated visual inspection of silicon detectors in CMS experiment,Nupur Giri;Shashi Dugad;Amit Chhabria;Rashmi Manwani;Priyanka Asrani,"In the CMS experiment at CERN, Geneva, a large number of HGCAL sensor modules are fabricated in advanced laboratories around the world. Each sensor module contains about 700 checkpoints for visual inspection thus making it almost impossible to carry out such inspection manually. As artificial intelligence is more and more widely used in manufacturing, traditional detection technologies are gradually being intelligent. In order to more accurately evaluate the checkpoints, we propose to use deep learning-based object detection techniques to detect manufacturing defects in testing large numbers of modules automatically. △ Less","3 June, 2022",https://arxiv.org/pdf/2206.02572
Machine learning applications for electricity market agent-based models: A systematic literature review,Alexander J. M. Kell;Stephen McGough;Matthew Forshaw,"The electricity market has a vital role to play in the decarbonisation of the energy system. However, the electricity market is made up of many different variables and data inputs. These variables and data inputs behave in sometimes unpredictable ways which can not be predicted a-priori. It has therefore been suggested that agent-based simulations are used to better understand the dynamics of the electricity market. Agent-based models provide the opportunity to integrate machine learning and artificial intelligence to add intelligence, make better forecasts and control the power market in better and more efficient ways. In this systematic literature review, we review 55 papers published between 2016 and 2021 which focus on machine learning applied to agent-based electricity market models. We find that research clusters around popular topics, such as bidding strategies. However, there exists a long-tail of different research applications that could benefit from the high intensity research from the more investigated applications. △ Less","5 June, 2022",https://arxiv.org/pdf/2206.02196
The Spike Gating Flow: A Hierarchical Structure Based Spiking Neural Network for Online Gesture Recognition,Zihao Zhao;Yanhong Wang;Qiaosha Zou;Tie Xu;Fangbo Tao;Jiansong Zhang;Xiaoan Wang;C. -J. Richard Shi;Junwen Luo;Yuan Xie,"Action recognition is an exciting research avenue for artificial intelligence since it may be a game changer in the emerging industrial fields such as robotic visions and automobiles. However, current deep learning faces major challenges for such applications because of the huge computational cost and the inefficient learning. Hence, we develop a novel brain-inspired Spiking Neural Network (SNN) based system titled Spiking Gating Flow (SGF) for online action learning. The developed system consists of multiple SGF units which assembled in a hierarchical manner. A single SGF unit involves three layers: a feature extraction layer, an event-driven layer and a histogram-based training layer. To demonstrate the developed system capabilities, we employ a standard Dynamic Vision Sensor (DVS) gesture classification as a benchmark. The results indicate that we can achieve 87.5% accuracy which is comparable with Deep Learning (DL), but at smaller training/inference data number ratio 1.5:1. And only a single training epoch is required during the learning process. Meanwhile, to the best of our knowledge, this is the highest accuracy among the non-backpropagation algorithm based SNNs. At last, we conclude the few-shot learning paradigm of the developed network: 1) a hierarchical structure-based network design involves human prior knowledge; 2) SNNs for content based global dynamic feature detection. △ Less","7 June, 2022",https://arxiv.org/pdf/2206.01910
Automatic Quantification of Volumes and Biventricular Function in Cardiac Resonance. Validation of a New Artificial Intelligence Approach,Ariel H. Curiale;MatÍas E. Calandrelli;Lucca Dellazoppa;Mariano Trevisan;Jorge Luis BociÁn;Juan Pablo Bonifacio;GermÁn Mato,"Background: Artificial intelligence techniques have shown great potential in cardiology, especially in quantifying cardiac biventricular function, volume, mass, and ejection fraction (EF). However, its use in clinical practice is not straightforward due to its poor reproducibility with cases from daily practice, among other reasons. Objectives: To validate a new artificial intelligence tool in order to quantify the cardiac biventricular function (volume, mass, and EF). To analyze its robustness in the clinical area, and the computational times compared with conventional methods. Methods: A total of 189 patients were analyzed: 89 from a regional center and 100 from a public center. The method proposes two convolutional networks that include anatomical information of the heart to reduce classification errors. Results: A high concordance (Pearson coefficient) was observed between manual quantification and the proposed quantification of cardiac function (0.98, 0.92, 0.96 and 0.8 for volumes and biventricular EF) in about 5 seconds per study. Conclusions: This method quantifies biventricular function and volumes in seconds with an accuracy equivalent to that of a specialist. △ Less","3 June, 2022",https://arxiv.org/pdf/2206.01746
Detection of Fibrosis in Cine Magnetic Resonance Images Using Artificial Intelligence Techniques,Ariel. H. Curiale;Facundo Cabrera;Pablo Jimenez;Jorgelina Medus;GermÁn Mato;MatÍas E. Calandrelli,"Background: Artificial intelligence techniques have demonstrated great potential in cardiology, especially to detect imperceptible patterns for the human eye. In this sense, these techniques seem to be adequate to identify patterns in the myocardial texture which could lead to characterize and quantify fibrosis. Purpose: The aim of this study was to postulate a new artificial intelligence method to identify fibrosis in cine cardiac magnetic resonance (CMR) imaging. Methods: A retrospective observational study was carried out in a population of 75 subjects from a clinical center of San Carlos de Bariloche. The proposed method analyzes the myocardial texture in cine CMR images using a convolutional neural network to determine local myocardial tissue damage. Results: An accuracy of 89% for quantifying local tissue damage was observed for the validation data set and 70% for the test set. In addition, the qualitative analysis showed a high spatial correlation in lesion location. Conclusions: The postulated method enables to spatially identify fibrosis using only the information from cine nuclear magnetic resonance studies, demonstrating the potential of this technique to quantify myocardial viability in the future or to study the lesions etiology △ Less","3 June, 2022",https://arxiv.org/pdf/2206.01745
"A review of machine learning approaches, challenges and prospects for computational tumor pathology",Liangrui Pan;Zhichao Feng;Shaoliang Peng,"Computational pathology is part of precision oncology medicine. The integration of high-throughput data including genomics, transcriptomics, proteomics, metabolomics, pathomics, and radiomics into clinical practice improves cancer treatment plans, treatment cycles, and cure rates, and helps doctors open up innovative approaches to patient prognosis. In the past decade, rapid advances in artificial intelligence, chip design and manufacturing, and mobile computing have facilitated research in computational pathology and have the potential to provide better-integrated solutions for whole-slide images, multi-omics data, and clinical informatics. However, tumor computational pathology now brings some challenges to the application of tumour screening, diagnosis and prognosis in terms of data integration, hardware processing, network sharing bandwidth and machine learning technology. This review investigates image preprocessing methods in computational pathology from a pathological and technical perspective, machine learning-based methods, and applications of computational pathology in breast, colon, prostate, lung, and various tumour disease scenarios. Finally, the challenges and prospects of machine learning in computational pathology applications are discussed. △ Less","31 May, 2022",https://arxiv.org/pdf/2206.01728
ArgRewrite V.2: an Annotated Argumentative Revisions Corpus,Omid Kashefi;Tazin Afrin;Meghan Dale;Christopher Olshefski;Amanda Godley;Diane Litman;Rebecca Hwa,"Analyzing how humans revise their writings is an interesting research question, not only from an educational perspective but also in terms of artificial intelligence. Better understanding of this process could facilitate many NLP applications, from intelligent tutoring systems to supportive and collaborative writing environments. Developing these applications, however, requires revision corpora, which are not widely available. In this work, we present ArgRewrite V.2, a corpus of annotated argumentative revisions, collected from two cycles of revisions to argumentative essays about self-driving cars. Annotations are provided at different levels of purpose granularity (coarse and fine) and scope (sentential and subsentential). In addition, the corpus includes the revision goal given to each writer, essay scores, annotation verification, pre- and post-study surveys collected from participants as meta-data. The variety of revision unit scope and purpose granularity levels in ArgRewrite, along with the inclusion of new types of meta-data, can make it a useful resource for research and applications that involve revision analysis. We demonstrate some potential applications of ArgRewrite V.2 in the development of automatic revision purpose predictors, as a training source and benchmark. △ Less","3 June, 2022",https://arxiv.org/pdf/2206.01677
Can Requirements Engineering Support Explainable Artificial Intelligence? Towards a User-Centric Approach for Explainability Requirements,Umm-e-Habiba;Justus Bogner;Stefan Wagner,"With the recent proliferation of artificial intelligence systems, there has been a surge in the demand for explainability of these systems. Explanations help to reduce system opacity, support transparency, and increase stakeholder trust. In this position paper, we discuss synergies between requirements engineering (RE) and Explainable AI (XAI). We highlight challenges in the field of XAI, and propose a framework and research directions on how RE practices can help to mitigate these challenges. △ Less","3 June, 2022",https://arxiv.org/pdf/2206.01507
From Cities to Series: Complex Networks and Deep Learning for Improved Spatial and Temporal Analytics*,Gabriel Spadon;Jose F. Rodrigues-Jr,"Graphs have often been used to answer questions about the interaction between real-world entities by taking advantage of their capacity to represent complex topologies. Complex networks are known to be graphs that capture such non-trivial topologies; they are able to represent human phenomena such as epidemic processes, the dynamics of populations, and the urbanization of cities. The investigation of complex networks has been extrapolated to many fields of science, with particular emphasis on computing techniques, including artificial intelligence. In such a case, the analysis of the interaction between entities of interest is transposed to the internal learning of algorithms, a paradigm whose investigation is able to expand the state of the art in Computer Science. By exploring this paradigm, this thesis puts together complex networks and machine learning techniques to improve the understanding of the human phenomena observed in pandemics, pendular migration, and street networks. Accordingly, we contribute with: (i) a new neural network architecture capable of modeling dynamic processes observed in spatial and temporal data with applications in epidemics propagation, weather forecasting, and patient monitoring in intensive care units; (ii) a machine-learning methodology for analyzing and predicting links in the scope of human mobility between all the cities of Brazil; and, (iii) techniques for identifying inconsistencies in the urban planning of cities while tracking the most influential vertices, with applications over Brazilian and worldwide cities. We obtained results sustained by sound evidence of advances to the state of the art in artificial intelligence, rigorous formalisms, and ample experimentation. Our findings rely upon real-world applications in a range of domains, demonstrating the applicability of our methodologies. △ Less","1 June, 2022",https://arxiv.org/pdf/2206.01176
Language and Culture Internalisation for Human-Like Autotelic AI,Cédric Colas;Tristan Karch;Clément Moulin-Frier;Pierre-Yves Oudeyer,"Building autonomous agents able to grow open-ended repertoires of skills across their lives is a fundamental goal of artificial intelligence (AI). A promising developmental approach recommends the design of intrinsically motivated agents that learn new skills by generating and pursuing their own goals - autotelic agents. But despite recent progress, existing algorithms still show serious limitations in terms of goal diversity, exploration, generalisation or skill composition. This perspective calls for the immersion of autotelic agents into rich socio-cultural worlds, an immensely important attribute of our environment that shapes human cognition but is mostly omitted in modern AI. Inspired by the seminal work of Vygotsky, we propose Vygotskian autotelic agents - agents able to internalise their interactions with others and turn them into cognitive tools. We focus on language and show how its structure and informational content may support the development of new cognitive functions in artificial agents as it does in humans. We justify the approach by uncovering several examples of new artificial cognitive functions emerging from interactions between language and embodiment in recent works at the intersection of deep reinforcement learning and natural language processing. Looking forward, we highlight future opportunities and challenges for Vygotskian Autotelic AI research, including the use of language models as cultural models supporting artificial cognitive development. △ Less","16 November, 2022",https://arxiv.org/pdf/2206.01134
Artificial Open World for Evaluating AGI: a Conceptual Design,Bowen Xu;Quansheng Ren,"How to evaluate Artificial General Intelligence (AGI) is a critical problem that is discussed and unsolved for a long period. In the research of narrow AI, this seems not a severe problem, since researchers in that field focus on some specific problems as well as one or some aspects of cognition, and the criteria for evaluation are explicitly defined. By contrast, an AGI agent should solve problems that are never-encountered by both agents and developers. However, once a developer tests and debugs the agent with a problem, the never-encountered problem becomes the encountered problem, as a result, the problem is solved by the developers to some extent, exploiting their experience, rather than the agents. This conflict, as we call the trap of developers' experience, leads to that this kind of problems is probably hard to become an acknowledged criterion. In this paper, we propose an evaluation method named Artificial Open World, aiming to jump out of the trap. The intuition is that most of the experience in the actual world should not be necessary to be applied to the artificial world, and the world should be open in some sense, such that developers are unable to perceive the world and solve problems by themselves before testing, though after that they are allowed to check all the data. The world is generated in a similar way as the actual world, and a general form of problems is proposed. A metric is proposed aiming to quantify the progress of research. This paper describes the conceptual design of the Artificial Open World, though the formalization and the implementation are left to the future. △ Less","2 June, 2022",https://arxiv.org/pdf/2206.01044
Towards Responsible AI: A Design Space Exploration of Human-Centered Artificial Intelligence User Interfaces to Investigate Fairness,Yuri Nakao;Lorenzo Strappelli;Simone Stumpf;Aisha Naseer;Daniele Regoli;Giulia Del Gamba,"With Artificial intelligence (AI) to aid or automate decision-making advancing rapidly, a particular concern is its fairness. In order to create reliable, safe and trustworthy systems through human-centred artificial intelligence (HCAI) design, recent efforts have produced user interfaces (UIs) for AI experts to investigate the fairness of AI models. In this work, we provide a design space exploration that supports not only data scientists but also domain experts to investigate AI fairness. Using loan applications as an example, we held a series of workshops with loan officers and data scientists to elicit their requirements. We instantiated these requirements into FairHIL, a UI to support human-in-the-loop fairness investigations, and describe how this UI could be generalized to other use cases. We evaluated FairHIL through a think-aloud user study. Our work contributes better designs to investigate an AI model's fairness-and move closer towards responsible AI. △ Less","1 June, 2022",https://arxiv.org/pdf/2206.00474
Can Artificial Intelligence Transform DevOps?,Mamdouh Alenezi;Mohammad Zarour;Mohammad Akour,"DevOps and Artificial Intelligence (AI) are interconnected with each other. DevOps is a business-driven approach to providing quickly delivered quality software, and AI is the technology that can be used in the system to enhance its functionality. So, DevOps teams can use AI to test, code, release, monitor, and improve the system. Through AI, the automation process delivered by DevOps could be improved efficiently. This study aims to explore how AI can transform DevOps. The research is useful in terms of facilitating software developers and businesses to assess the importance of AI in DevOps. The study has practical implications as it elaborates on how AI transforms DevOps and in what way it can support businesses in their business. △ Less","1 June, 2022",https://arxiv.org/pdf/2206.00225
Benchmark of DNN Model Search at Deployment Time,Lixi Zhou;Arindam Jain;Zijie Wang;Amitabh Das;Yingzhen Yang;Jia Zou,"Deep learning has become the most popular direction in machine learning and artificial intelligence. However, the preparation of training data, as well as model training, are often time-consuming and become the bottleneck of the end-to-end machine learning lifecycle. Reusing models for inferring a dataset can avoid the costs of retraining. However, when there are multiple candidate models, it is challenging to discover the right model for reuse. Although there exist a number of model sharing platforms such as ModelDB, TensorFlow Hub, PyTorch Hub, and DLHub, most of these systems require model uploaders to manually specify the details of each model and model downloaders to screen keyword search results for selecting a model. We are lacking a highly productive model search tool that selects models for deployment without the need for any manual inspection and/or labeled data from the target domain. This paper proposes multiple model search strategies including various similarity-based approaches and non-similarity-based approaches. We design, implement, and evaluate these approaches on multiple model inference scenarios, including activity recognition, image recognition, text classification, natural language processing, and entity matching. The experimental evaluation showed that our proposed asymmetric similarity-based measurement, adaptivity, outperformed symmetric similarity-based measurements and non-similarity-based measurements in most of the workloads. △ Less","31 May, 2022",https://arxiv.org/pdf/2206.00188
Design and Simulation of an Autonomous Quantum Flying Robot Vehicle: An IBM Quantum Experience,Sudev Pradhan;Anshuman Padhi;Bikash Kumar Behera,"The application of quantum computation and information in robotics has caught the attention of researchers off late. The field of robotics has always put its effort on the minimization of the space occupied by the robot, and on making the robot `smarter. `The smartness of a robot is its sensitivity to its surroundings and the user input and its ability to react upon them desirably. Quantum phenomena in robotics make sure that the robots occupy less space and the ability of quantum computation to process the huge amount of information effectively, consequently making the robot smarter. Braitenberg vehicle is a simple circuited robot that moves according to the input that its sensors receive. Building upon that, we propose a quantum robot vehicle that is `smart' enough to understand the complex situations more than that of a simple Braitenberg vehicle and navigate itself as per the obstacles present. It can detect an obstacle-free path and can navigate itself accordingly. It also takes input from the user when there is more than one free path available. When left with no option on the ground, it can airlift itself off the ground. As these vehicles sort of `react to the surrounding conditions, this idea can be used to build artificial life and genetic algorithms, space exploration and deep-earth exploration probes, and a handy tool in defense and intelligence services. △ Less","31 May, 2022",https://arxiv.org/pdf/2206.00157
Calibrated Bagging Deep Learning for Image Semantic Segmentation: A Case Study on COVID-19 Chest X-ray Image,Lucy Nwosu;Xiangfang Li;Lijun Qian;Seungchan Kim;Xishuang Dong,"Severe acute respiratory syndrome coronavirus 2 (SARS-CoV-2) causes coronavirus disease 2019 (COVID-19). Imaging tests such as chest X-ray (CXR) and computed tomography (CT) can provide useful information to clinical staff for facilitating a diagnosis of COVID-19 in a more efficient and comprehensive manner. As a breakthrough of artificial intelligence (AI), deep learning has been applied to perform COVID-19 infection region segmentation and disease classification by analyzing CXR and CT data. However, prediction uncertainty of deep learning models for these tasks, which is very important to safety-critical applications like medical image processing, has not been comprehensively investigated. In this work, we propose a novel ensemble deep learning model through integrating bagging deep learning and model calibration to not only enhance segmentation performance, but also reduce prediction uncertainty. The proposed method has been validated on a large dataset that is associated with CXR image segmentation. Experimental results demonstrate that the proposed method can improve the segmentation performance, as well as decrease prediction uncertainties. △ Less","27 May, 2022",https://arxiv.org/pdf/2206.00002
Hide and Seek: on the Stealthiness of Attacks against Deep Learning Systems,Zeyan Liu;Fengjun Li;Jingqiang Lin;Zhu Li;Bo Luo,"With the growing popularity of artificial intelligence and machine learning, a wide spectrum of attacks against deep learning models have been proposed in the literature. Both the evasion attacks and the poisoning attacks attempt to utilize adversarially altered samples to fool the victim model to misclassify the adversarial sample. While such attacks claim to be or are expected to be stealthy, i.e., imperceptible to human eyes, such claims are rarely evaluated. In this paper, we present the first large-scale study on the stealthiness of adversarial samples used in the attacks against deep learning. We have implemented 20 representative adversarial ML attacks on six popular benchmarking datasets. We evaluate the stealthiness of the attack samples using two complementary approaches: (1) a numerical study that adopts 24 metrics for image similarity or quality assessment; and (2) a user study of 3 sets of questionnaires that has collected 20,000+ annotations from 1,000+ responses. Our results show that the majority of the existing attacks introduce nonnegligible perturbations that are not stealthy to human eyes. We further analyze the factors that contribute to attack stealthiness. We further examine the correlation between the numerical analysis and the user studies, and demonstrate that some image quality metrics may provide useful guidance in attack designs, while there is still a significant gap between assessed image quality and visual stealthiness of attacks. △ Less","12 August, 2022",https://arxiv.org/pdf/2205.15944
Generative Aging of Brain Images with Diffeomorphic Registration,Jingru Fu;Antonios Tzortzakakis;José Barroso;Eric Westman;Daniel Ferreira;Rodrigo Moreno,"Analyzing and predicting brain aging is essential for early prognosis and accurate diagnosis of cognitive diseases. The technique of neuroimaging, such as Magnetic Resonance Imaging (MRI), provides a noninvasive means of observing the aging process within the brain. With longitudinal image data collection, data-intensive Artificial Intelligence (AI) algorithms have been used to examine brain aging. However, existing state-of-the-art algorithms tend to be restricted to group-level predictions and suffer from unreal predictions. This paper proposes a methodology for generating longitudinal MRI scans that capture subject-specific neurodegeneration and retain anatomical plausibility in aging. The proposed methodology is developed within the framework of diffeomorphic registration and relies on three key novel technological advances to generate subject-level anatomically plausible predictions: i) a computationally efficient and individualized generative framework based on registration; ii) an aging generative module based on biological linear aging progression; iii) a quality control module to fit registration for generation task. Our methodology was evaluated on 2662 T1-weighted (T1-w) MRI scans from 796 participants from three different cohorts. First, we applied 6 commonly used criteria to demonstrate the aging simulation ability of the proposed methodology; Secondly, we evaluated the quality of the synthetic images using quantitative measurements and qualitative assessment by a neuroradiologist. Overall, the experimental results show that the proposed method can produce anatomically plausible predictions that can be used to enhance longitudinal datasets, in turn enabling data-hungry AI-driven healthcare tools. △ Less","31 May, 2022",https://arxiv.org/pdf/2205.15607
"Zero-Emission Delivery for Logistics and Transportation: Challenges, Research Issues, and Opportunities",J. Bukhari;A. G. Somanagoudar;L. Hou;O. Herrera;W. Merida,"Greenhouse gas, produced from various industries such as Power, Manufacturing, Transport, Chemical, or Agriculture, is the major source of global warming. While the transport industry is among the top three major contributors, accounting for 16.2% of global emissions. To counter this, many countries are responding actively to achieve net or absolute zero-emission goals by replacing fossil fuel with renewable energy sources. In response to this initiative, this chapter provides a systematic review of the use of zero-emission vehicles for a specific use case of package delivery. It first compares different green delivery systems that use unmanned aerial vehicles, electric vehicles, and fuel-cell trucks for certain weight categories. Specifically, a coordination of unmanned aerial vehicle and ground-based electric truck envisions a new paradigm of ground-based zero-emission vehicles where unmanned aerial vehicles can fly in the air beyond the visual line of sight empowered by future-generation wireless technologies. The integration of zero-emission vehicles for package delivery will encounter many challenges in analyzing, modelling, planning, and designing a green logistics system. This chapter investigates these challenges in the adoption of zero-emission vehicles with the existing research issues from a technical, environmental, economic, and political point of view. In addition, this study also sheds a new research perspective on artificial intelligence and integrated solutions for zero-emission deliveries. △ Less","31 May, 2022",https://arxiv.org/pdf/2205.15606
Comparing interpretation methods in mental state decoding analyses with deep learning models,Armin W. Thomas;Christopher Ré;Russell A. Poldrack,"Deep learning (DL) models find increasing application in mental state decoding, where researchers seek to understand the mapping between mental states (e.g., perceiving fear or joy) and brain activity by identifying those brain regions (and networks) whose activity allows to accurately identify (i.e., decode) these states. Once a DL model has been trained to accurately decode a set of mental states, neuroimaging researchers often make use of interpretation methods from explainable artificial intelligence research to understand the model's learned mappings between mental states and brain activity. Here, we compare the explanation performance of prominent interpretation methods in a mental state decoding analysis of three functional Magnetic Resonance Imaging (fMRI) datasets. Our findings demonstrate a gradient between two key characteristics of an explanation in mental state decoding, namely, its biological plausibility and faithfulness: interpretation methods with high explanation faithfulness, which capture the model's decision process well, generally provide explanations that are biologically less plausible than the explanations of interpretation methods with less explanation faithfulness. Based on this finding, we provide specific recommendations for the application of interpretation methods in mental state decoding. △ Less","14 October, 2022",https://arxiv.org/pdf/2205.15581
Automatic Short Math Answer Grading via In-context Meta-learning,Mengxue Zhang;Sami Baral;Neil Heffernan;Andrew Lan,"Automatic short answer grading is an important research direction in the exploration of how to use artificial intelligence (AI)-based tools to improve education. Current state-of-the-art approaches use neural language models to create vectorized representations of students responses, followed by classifiers to predict the score. However, these approaches have several key limitations, including i) they use pre-trained language models that are not well-adapted to educational subject domains and/or student-generated text and ii) they almost always train one model per question, ignoring the linkage across a question and result in a significant model storage problem due to the size of advanced language models. In this paper, we study the problem of automatic short answer grading for students' responses to math questions and propose a novel framework for this task. First, we use MathBERT, a variant of the popular language model BERT adapted to mathematical content, as our base model and fine-tune it for the downstream task of student response grading. Second, we use an in-context learning approach that provides scoring examples as input to the language model to provide additional context information and promote generalization to previously unseen questions. We evaluate our framework on a real-world dataset of student responses to open-ended math questions and show that our framework (often significantly) outperforms existing approaches, especially for new questions that are not seen during training. △ Less","8 July, 2022",https://arxiv.org/pdf/2205.15219
Do Deep Neural Networks Always Perform Better When Eating More Data?,Jiachen Yang;Zhuo Zhang;Yicheng Gong;Shukun Ma;Xiaolan Guo;Yue Yang;Shuai Xiao;Jiabao Wen;Yang Li;Xinbo Gao;Wen Lu;Qinggang Meng,"Data has now become a shortcoming of deep learning. Researchers in their own fields share the thinking that ""deep neural networks might not always perform better when they eat more data,"" which still lacks experimental validation and a convincing guiding theory. Here to fill this lack, we design experiments from Identically Independent Distribution(IID) and Out of Distribution(OOD), which give powerful answers. For the purpose of guidance, based on the discussion of results, two theories are proposed: under IID condition, the amount of information determines the effectivity of each sample, the contribution of samples and difference between classes determine the amount of sample information and the amount of class information; under OOD condition, the cross-domain degree of samples determine the contributions, and the bias-fitting caused by irrelevant elements is a significant factor of cross-domain. The above theories provide guidance from the perspective of data, which can promote a wide range of practical applications of artificial intelligence. △ Less","30 May, 2022",https://arxiv.org/pdf/2205.15187
Towards Supporting Intelligence in 5G/6G Core Networks: NWDAF Implementation and Initial Analysis,Ali Chouman;Dimitrios Michael Manias;Abdallah Shami,"Wireless networks, in the fifth-generation and beyond, must support diverse network applications which will support the numerous and demanding connections of today's and tomorrow's devices. Requirements such as high data rates, low latencies, and reliability are crucial considerations and artificial intelligence is incorporated to achieve these requirements for a large number of connected devices. Specifically, intelligent methods and frameworks for advanced analysis are employed by the 5G Core Network Data Analytics Function (NWDAF) to detect patterns and ascribe detailed action information to accommodate end users and improve network performance. To this end, the work presented in this paper incorporates a functional NWDAF into a 5G network developed using open source software. Furthermore, an analysis of the network data collected by the NWDAF and the valuable insights which can be drawn from it have been presented with detailed Network Function interactions. An example application of such insights used for intelligent network management is outlined. Finally, the expected limitations of 5G networks are discussed as motivation for the development of 6G networks. △ Less","30 May, 2022",https://arxiv.org/pdf/2205.15121
A Deep Learning Approach for Automatic Detection of Qualitative Features of Lecturing,Anna Wroblewska;Jozef Jasek;Bogdan Jastrzebski;Stanislaw Pawlak;Anna Grzywacz;Cheong Siew Ann;Tan Seng Chee;Tomasz Trzcinski;Janusz Holyst,"Artificial Intelligence in higher education opens new possibilities for improving the lecturing process, such as enriching didactic materials, helping in assessing students' works or even providing directions to the teachers on how to enhance the lectures. We follow this research path, and in this work, we explore how an academic lecture can be assessed automatically by quantitative features. First, we prepare a set of qualitative features based on teaching practices and then annotate the dataset of academic lecture videos collected for this purpose. We then show how these features could be detected automatically using machine learning and computer vision techniques. Our results show the potential usefulness of our work. △ Less","30 May, 2022",https://arxiv.org/pdf/2205.14919
"Lepton Flavour Violation Identification in Tau Decay (τ^{-} \rightarrow μ^{-}μ^{-}μ^{+}
) Using Artificial Intelligence",Reymond Mesuga,"The discovery of neutrino oscillation, proving that neutrinos do have masses, reveals the misfits of particles in the current Standard Model (SM) theory. In theory, neutrinos having masses could result in lepton flavour not being a symmetry called Lepton Flavour Violation (LFV). While SM theory extensions allowed LFV processes, their branching fractions are too small, making them unobservable even with the strongest equipment up-to-date. With that, scientists in recent years have generated LFV-like processes from the combined LHCb and Monte-Carlo-Simulated data in an attempt to identify LFV using Artificial Intelligence (AI), specifically Machine Learning (ML) and Deep Learning (DL). In this paper, the performance of several algorithms in AI has been presented, such as XGBoost, LightGBM, custom 1-D Dense Block Neural Networks (DBNNs), and custom 1-D Convolutional Neural Networks (CNNs) in identifying LFV signals, specifically τ^{-} \rightarrow μ^{-}μ^{-}μ^{+} decay from the combined LHCb and Monte-Carlo-Simulated data that imitates the signatures of the said decay. Kolmogorov-Smirnov (KS) and Cramer-von Mises (CvM) tests were also conducted to verify the validity of predictions for each of the trained algorithms. The result shows decent performances among algorithms, except for the LightGBM, for failing the CvM test, and a 20-layered CNN for having recorded a considerably low AUC. Meanwhile, XGBoost and a 10-layered DBNN recorded the highest AUC of 0.88. The main contribution of this paper is the extensive experiment involving custom DBNN and CNN algorithms in different layers, all of which have been rarely used in the past years in identifying LFV-like signatures, unlike GBMs and tree-based algorithms, which have been more popular in the said task. △ Less","11 June, 2022",https://arxiv.org/pdf/2205.14828
Risk of Stochastic Systems for Temporal Logic Specifications,Lars Lindemann;Lejun Jiang;Nikolai Matni;George J. Pappas,"The wide availability of data coupled with the computational advances in artificial intelligence and machine learning promise to enable many future technologies such as autonomous driving. While there has been a variety of successful demonstrations of these technologies, critical system failures have repeatedly been reported. Even if rare, such system failures pose a serious barrier to adoption without a rigorous risk assessment. This paper presents a framework for the systematic and rigorous risk verification of systems. We consider a wide range of system specifications formulated in signal temporal logic (STL) and model the system as a stochastic process, permitting discrete-time and continuous-time stochastic processes. We then define the STL robustness risk as the risk of lacking robustness against failure. This definition is motivated as system failures are often caused by missing robustness to modeling errors, system disturbances, and distribution shifts in the underlying data generating process. Within the definition, we permit general classes of risk measures and focus on tail risk measures such as the value-at-risk and the conditional value-at-risk. While the STL robustness risk is in general hard to compute, we propose the approximate STL robustness risk as a more tractable notion that upper bounds the STL robustness risk. We show how the approximate STL robustness risk can accurately be estimated from system trajectory data. For discrete-time stochastic processes, we show under which conditions the approximate STL robustness risk can even be computed exactly. We illustrate our verification algorithm in the autonomous driving simulator CARLA and show how a least risky controller can be selected among four neural network lane keeping controllers for five meaningful system specifications. △ Less","8 October, 2022",https://arxiv.org/pdf/2205.14523
Efficient Federated Learning with Spike Neural Networks for Traffic Sign Recognition,Kan Xie;Zhe Zhang;Bo Li;Jiawen Kang;Dusit Niyato;Shengli Xie;Yi Wu,"With the gradual popularization of self-driving, it is becoming increasingly important for vehicles to smartly make the right driving decisions and autonomously obey traffic rules by correctly recognizing traffic signs. However, for machine learning-based traffic sign recognition on the Internet of Vehicles (IoV), a large amount of traffic sign data from distributed vehicles is needed to be gathered in a centralized server for model training, which brings serious privacy leakage risk because of traffic sign data containing lots of location privacy information. To address this issue, we first exploit privacy-preserving federated learning to perform collaborative training for accurate recognition models without sharing raw traffic sign data. Nevertheless, due to the limited computing and energy resources of most devices, it is hard for vehicles to continuously undertake complex artificial intelligence tasks. Therefore, we introduce powerful Spike Neural Networks (SNNs) into traffic sign recognition for energy-efficient and fast model training, which is the next generation of neural networks and is practical and well-fitted to IoV scenarios. Furthermore, we design a novel encoding scheme for SNNs based on neuron receptive fields to extract information from the pixel and spatial dimensions of traffic signs to achieve high-accuracy training. Numerical results indicate that the proposed federated SNN outperforms traditional federated convolutional neural networks in terms of accuracy, noise immunity, and energy efficiency as well. △ Less","27 May, 2022",https://arxiv.org/pdf/2205.14315
Machine Learning-Based User Scheduling in Integrated Satellite-HAPS-Ground Networks,Hayssam Dahrouj;Shasha Liu;Mohamed-Slim Alouini,"Integrated space-air-ground networks promise to offer a valuable solution space for empowering the sixth generation of communication networks (6G), particularly in the context of connecting the unconnected and ultraconnecting the connected. Such digital inclusion thrive makes resource management problems, especially those accounting for load-balancing considerations, of particular interest. The conventional model-based optimization methods, however, often fail to meet the real-time processing and quality-of-service needs, due to the high heterogeneity of the space-air-ground networks, and the typical complexity of the classical algorithms. Given the premises of artificial intelligence at automating wireless networks design and the large-scale heterogeneity of non-terrestrial networks, this paper focuses on showcasing the prospects of machine learning in the context of user scheduling in integrated space-air-ground communications. The paper first overviews the most relevant state-of-the art in the context of machine learning applications to the resource allocation problems, with a dedicated attention to space-air-ground networks. The paper then proposes, and shows the benefit of, one specific use case that uses ensembling deep neural networks for optimizing the user scheduling policies in integrated space-high altitude platform station (HAPS)-ground networks. Finally, the paper sheds light on the challenges and open issues that promise to spur the integration of machine learning in space-air-ground networks, namely, online HAPS power adaptation, learning-based channel sensing, data-driven multi-HAPSs resource management, and intelligent flying taxis-empowered systems. △ Less","18 December, 2022",https://arxiv.org/pdf/2205.13958
EmoInHindi: A Multi-label Emotion and Intensity Annotated Dataset in Hindi for Emotion Recognition in Dialogues,Gopendra Vikram Singh;Priyanshu Priya;Mauajama Firdaus;Asif Ekbal;Pushpak Bhattacharyya,"The long-standing goal of Artificial Intelligence (AI) has been to create human-like conversational systems. Such systems should have the ability to develop an emotional connection with the users, hence emotion recognition in dialogues is an important task. Emotion detection in dialogues is a challenging task because humans usually convey multiple emotions with varying degrees of intensities in a single utterance. Moreover, emotion in an utterance of a dialogue may be dependent on previous utterances making the task more complex. Emotion recognition has always been in great demand. However, most of the existing datasets for multi-label emotion and intensity detection in conversations are in English. To this end, we create a large conversational dataset in Hindi named EmoInHindi for multi-label emotion and intensity recognition in conversations containing 1,814 dialogues with a total of 44,247 utterances. We prepare our dataset in a Wizard-of-Oz manner for mental health and legal counselling of crime victims. Each utterance of the dialogue is annotated with one or more emotion categories from the 16 emotion classes including neutral, and their corresponding intensity values. We further propose strong contextual baselines that can detect emotion(s) and the corresponding intensity of an utterance given the conversational context. △ Less","27 May, 2022",https://arxiv.org/pdf/2205.13908
Classification of COVID-19 Patients with their Severity Level from Chest CT Scans using Transfer Learning,Mansi Gupta;Aman Swaraj;Karan Verma,"Background and Objective: During pandemics, the use of artificial intelligence (AI) approaches combined with biomedical science play a significant role in reducing the burden on the healthcare systems and physicians. The rapid increment in cases of COVID-19 has led to an increase in demand for hospital beds and other medical equipment. However, since medical facilities are limited, it is recommended to diagnose patients as per the severity of the infection. Keeping this in mind, we share our research in detecting COVID-19 as well as assessing its severity using chest-CT scans and Deep Learning pre-trained models. Dataset: We have collected a total of 1966 CT Scan images for three different class labels, namely, Non-COVID, Severe COVID, and Non-Severe COVID, out of which 714 CT images belong to the Non-COVID category, 713 CT images are for Non-Severe COVID category and 539 CT images are of Severe COVID category. Methods: All of the images are initially pre-processed using the Contrast Limited Histogram Equalization (CLAHE) approach. The pre-processed images are then fed into the VGG-16 network for extracting features. Finally, the retrieved characteristics are categorized and the accuracy is evaluated using a support vector machine (SVM) with 10-fold cross-validation (CV). Result and Conclusion: In our study, we have combined well-known strategies for pre-processing, feature extraction, and classification which brings us to a remarkable success rate of disease and its severity recognition with an accuracy of 96.05% (97.7% for Non-Severe COVID-19 images and 93% for Severe COVID-19 images). Our model can therefore help radiologists detect COVID-19 and the extent of its severity. △ Less","27 May, 2022",https://arxiv.org/pdf/2205.13774
Harnessing Artificial Intelligence to Infer Novel Spatial Biomarkers for the Diagnosis of Eosinophilic Esophagitis,Ariel Larey;Eliel Aknin;Nati Daniel;Garrett A. Osswald;Julie M. Caldwell;Mark Rochman;Tanya Wasserman;Margaret H. Collins;Nicoleta C. Arva;Guang-Yu Yang;Marc E. Rothenberg;Yonatan Savir,"Eosinophilic esophagitis (EoE) is a chronic allergic inflammatory condition of the esophagus associated with elevated esophageal eosinophils. Second only to gastroesophageal reflux disease, EoE is one of the leading causes of chronic refractory dysphagia in adults and children. EoE diagnosis requires enumerating the density of esophageal eosinophils in esophageal biopsies, a somewhat subjective task that is time-consuming, thus reducing the ability to process the complex tissue structure. Previous artificial intelligence (AI) approaches that aimed to improve histology-based diagnosis focused on recapitulating identification and quantification of the area of maximal eosinophil density. However, this metric does not account for the distribution of eosinophils or other histological features, over the whole slide image. Here, we developed an artificial intelligence platform that infers local and spatial biomarkers based on semantic segmentation of intact eosinophils and basal zone distributions. Besides the maximal density of eosinophils (referred to as Peak Eosinophil Count [PEC]) and a maximal basal zone fraction, we identify two additional metrics that reflect the distribution of eosinophils and basal zone fractions. This approach enables a decision support system that predicts EoE activity and classifies the histological severity of EoE patients. We utilized a cohort that includes 1066 biopsy slides from 400 subjects to validate the system's performance and achieved a histological severity classification accuracy of 86.70%, sensitivity of 84.50%, and specificity of 90.09%. Our approach highlights the importance of systematically analyzing the distribution of biopsy features over the entire slide and paves the way towards a personalized decision support system that will assist not only in counting cells but can also potentially improve diagnosis and provide treatment prediction. △ Less","26 May, 2022",https://arxiv.org/pdf/2205.13583
Characterising Research Areas in the field of AI,Alessandra Belfiore;Angelo Salatino;Francesco Osborne,"Interest in Artificial Intelligence (AI) continues to grow rapidly, hence it is crucial to support researchers and organisations in understanding where AI research is heading. In this study, we conducted a bibliometric analysis on 257K articles in AI, retrieved from OpenAlex. We identified the main conceptual themes by performing clustering analysis on the co-occurrence network of topics. Finally, we observed how such themes evolved over time. The results highlight the growing academic interest in research themes like deep learning, machine learning, and internet of things. △ Less","26 May, 2022",https://arxiv.org/pdf/2205.13471
Evaluating Multimodal Interactive Agents,Josh Abramson;Arun Ahuja;Federico Carnevale;Petko Georgiev;Alex Goldin;Alden Hung;Jessica Landon;Timothy Lillicrap;Alistair Muldal;Blake Richards;Adam Santoro;Tamara von Glehn;Greg Wayne;Nathaniel Wong;Chen Yan,"Creating agents that can interact naturally with humans is a common goal in artificial intelligence (AI) research. However, evaluating these interactions is challenging: collecting online human-agent interactions is slow and expensive, yet faster proxy metrics often do not correlate well with interactive evaluation. In this paper, we assess the merits of these existing evaluation metrics and present a novel approach to evaluation called the Standardised Test Suite (STS). The STS uses behavioural scenarios mined from real human interaction data. Agents see replayed scenario context, receive an instruction, and are then given control to complete the interaction offline. These agent continuations are recorded and sent to human annotators to mark as success or failure, and agents are ranked according to the proportion of continuations in which they succeed. The resulting STS is fast, controlled, interpretable, and representative of naturalistic interactions. Altogether, the STS consolidates much of what is desirable across many of our standard evaluation metrics, allowing us to accelerate research progress towards producing agents that can interact naturally with humans. A video may be found at https://youtu.be/YR1TngGORGQ. △ Less","14 July, 2022",https://arxiv.org/pdf/2205.13274
TSEM: Temporally Weighted Spatiotemporal Explainable Neural Network for Multivariate Time Series,Anh-Duy Pham;Anastassia Kuestenmacher;Paul G. Ploeger,"Deep learning has become a one-size-fits-all solution for technical and business domains thanks to its flexibility and adaptability. It is implemented using opaque models, which unfortunately undermines the outcome trustworthiness. In order to have a better understanding of the behavior of a system, particularly one driven by time series, a look inside a deep learning model so-called posthoc eXplainable Artificial Intelligence (XAI) approaches, is important. There are two major types of XAI for time series data, namely model-agnostic and model-specific. Model-specific approach is considered in this work. While other approaches employ either Class Activation Mapping (CAM) or Attention Mechanism, we merge the two strategies into a single system, simply called the Temporally Weighted Spatiotemporal Explainable Neural Network for Multivariate Time Series (TSEM). TSEM combines the capabilities of RNN and CNN models in such a way that RNN hidden units are employed as attention weights for the CNN feature maps temporal axis. The result shows that TSEM outperforms XCM. It is similar to STAM in terms of accuracy, while also satisfying a number of interpretability criteria, including causality, fidelity, and spatiotemporality. △ Less","3 August, 2022",https://arxiv.org/pdf/2205.13012
COVID-19 Severity Classification on Chest X-ray Images,Aditi Sagar;Aman Swaraj;Karan Verma,"Biomedical imaging analysis combined with artificial intelligence (AI) methods has proven to be quite valuable in order to diagnose COVID-19. So far, various classification models have been used for diagnosing COVID-19. However, classification of patients based on their severity level is not yet analyzed. In this work, we classify covid images based on the severity of the infection. First, we pre-process the X-ray images using a median filter and histogram equalization. Enhanced X-ray images are then augmented using SMOTE technique for achieving a balanced dataset. Pre-trained Resnet50, VGG16 model and SVM classifier are then used for feature extraction and classification. The result of the classification model confirms that compared with the alternatives, with chest X-Ray images, the ResNet-50 model produced remarkable classification results in terms of accuracy (95%), recall (0.94), and F1-Score (0.92), and precision (0.91). △ Less","25 May, 2022",https://arxiv.org/pdf/2205.12705
Autoformalization with Large Language Models,Yuhuai Wu;Albert Q. Jiang;Wenda Li;Markus N. Rabe;Charles Staats;Mateja Jamnik;Christian Szegedy,"Autoformalization is the process of automatically translating from natural language mathematics to formal specifications and proofs. A successful autoformalization system could advance the fields of formal verification, program synthesis, and artificial intelligence. While the long-term goal of autoformalization seemed elusive for a long time, we show large language models provide new prospects towards this goal. We make the surprising observation that LLMs can correctly translate a significant portion (25.3\%) of mathematical competition problems perfectly to formal specifications in Isabelle/HOL. We demonstrate the usefulness of this process by improving a previously introduced neural theorem prover via training on these autoformalized theorems. Our methodology results in a new state-of-the-art result on the MiniF2F theorem proving benchmark, improving the proof rate from 29.6\% to 35.2\%. △ Less","25 May, 2022",https://arxiv.org/pdf/2205.12615
Eye-gaze-guided Vision Transformer for Rectifying Shortcut Learning,Chong Ma;Lin Zhao;Yuzhong Chen;Lu Zhang;Zhenxiang Xiao;Haixing Dai;David Liu;Zihao Wu;Zhengliang Liu;Sheng Wang;Jiaxing Gao;Changhe Li;Xi Jiang;Tuo Zhang;Qian Wang;Dinggang Shen;Dajiang Zhu;Tianming Liu,"Learning harmful shortcuts such as spurious correlations and biases prevents deep neural networks from learning the meaningful and useful representations, thus jeopardizing the generalizability and interpretability of the learned representation. The situation becomes even more serious in medical imaging, where the clinical data (e.g., MR images with pathology) are limited and scarce while the reliability, generalizability and transparency of the learned model are highly required. To address this problem, we propose to infuse human experts' intelligence and domain knowledge into the training of deep neural networks. The core idea is that we infuse the visual attention information from expert radiologists to proactively guide the deep model to focus on regions with potential pathology and avoid being trapped in learning harmful shortcuts. To do so, we propose a novel eye-gaze-guided vision transformer (EG-ViT) for diagnosis with limited medical image data. We mask the input image patches that are out of the radiologists' interest and add an additional residual connection in the last encoder layer of EG-ViT to maintain the correlations of all patches. The experiments on two public datasets of INbreast and SIIM-ACR demonstrate our EG-ViT model can effectively learn/transfer experts' domain knowledge and achieve much better performance than baselines. Meanwhile, it successfully rectifies the harmful shortcut learning and significantly improves the EG-ViT model's interpretability. In general, EG-ViT takes the advantages of both human expert's prior knowledge and the power of deep neural networks. This work opens new avenues for advancing current artificial intelligence paradigms by infusing human intelligence. △ Less","24 May, 2022",https://arxiv.org/pdf/2205.12466
"Women, artificial intelligence, and key positions in collaboration networks: Towards a more equal scientific ecosystem",Anahita Hajibabaei;Andrea Schiffauerova;Ashkan Ebadi,"Scientific collaboration in almost every discipline is mainly driven by the need of sharing knowledge, expertise, and pooled resources. Science is becoming more complex which has encouraged scientists to involve more in collaborative research projects in order to better address the challenges. As a highly interdisciplinary field with a rapidly evolving scientific landscape, artificial intelligence calls for researchers with special profiles covering a diverse set of skills and expertise. Understanding gender aspects of scientific collaboration is of paramount importance, especially in a field such as artificial intelligence that has been attracting large investments. Using social network analysis, natural language processing, and machine learning and focusing on artificial intelligence publications for the period from 2000 to 2019, in this work, we comprehensively investigated the effects of several driving factors on acquiring key positions in scientific collaboration networks through a gender lens. It was found that, regardless of gender, scientific performance in terms of quantity and impact plays a crucial in possessing the ""social researcher"" in the network. However, subtle differences were observed between female and male researchers in acquiring the ""local influencer"" role. △ Less","19 May, 2022",https://arxiv.org/pdf/2205.12339
Bias Discovery in Machine Learning Models for Mental Health,Pablo Mosteiro;Jesse Kuiper;Judith Masthoff;Floortje Scheepers;Marco Spruit,"Fairness and bias are crucial concepts in artificial intelligence, yet they are relatively ignored in machine learning applications in clinical psychiatry. We computed fairness metrics and present bias mitigation strategies using a model trained on clinical mental health data. We collected structured data related to the admission, diagnosis, and treatment of patients in the psychiatry department of the University Medical Center Utrecht. We trained a machine learning model to predict future administrations of benzodiazepines on the basis of past data. We found that gender plays an unexpected role in the predictions-this constitutes bias. Using the AI Fairness 360 package, we implemented reweighing and discrimination-aware regularization as bias mitigation strategies, and we explored their implications for model performance. This is the first application of bias exploration and mitigation in a machine learning model trained on real clinical psychiatry data. △ Less","24 May, 2022",https://arxiv.org/pdf/2205.12093
"Edge Semantic Cognitive Intelligence for 6G Networks: Novel Theoretical Models, Enabling Framework, and Typical Applications",Peihao Dong;Qihui Wu;Xiaofei Zhang;Guoru Ding,"Edge intelligence is anticipated to underlay the pathway to connected intelligence for 6G networks, but the organic confluence of edge computing and artificial intelligence still needs to be carefully treated. To this end, this article discusses the concepts of edge intelligence from the semantic cognitive perspective. Two instructive theoretical models for edge semantic cognitive intelligence (ESCI) are first established. Afterwards, the ESCI framework orchestrating deep learning with semantic communication is discussed. Two representative applications are present to shed light on the prospect of ESCI in 6G networks. Some open problems are finally listed to elicit the future research directions of ESCI. △ Less","9 July, 2022",https://arxiv.org/pdf/2205.12073
FedEntropy: Efficient Device Grouping for Federated Learning Using Maximum Entropy Judgment,Zhiwei Ling;Zhihao Yue;Jun Xia;Ming Hu;Ting Wang;Mingsong Chen,"Along with the popularity of Artificial Intelligence (AI) and Internet-of-Things (IoT), Federated Learning (FL) has attracted steadily increasing attentions as a promising distributed machine learning paradigm, which enables the training of a central model on for numerous decentralized devices without exposing their privacy. However, due to the biased data distributions on involved devices, FL inherently suffers from low classification accuracy in non-IID scenarios. Although various device grouping method have been proposed to address this problem, most of them neglect both i) distinct data distribution characteristics of heterogeneous devices, and ii) contributions and hazards of local models, which are extremely important in determining the quality of global model aggregation. In this paper, we present an effective FL method named FedEntropy with a novel dynamic device grouping scheme, which makes full use of the above two factors based on our proposed maximum entropy judgement heuristic.Unlike existing FL methods that directly aggregate local models returned from all the selected devices, in one FL round FedEntropy firstly makes a judgement based on the pre-collected soft labels of selected devices and then only aggregates the local models that can maximize the overall entropy of these soft labels. Without collecting local models that are harmful for aggregation, FedEntropy can effectively improve global model accuracy while reducing the overall communication overhead. Comprehensive experimental results on well-known benchmarks show that, FedEntropy not only outperforms state-of-the-art FL methods in terms of model accuracy and communication overhead, but also can be integrated into them to enhance their classification performance. △ Less","24 May, 2022",https://arxiv.org/pdf/2205.12038
mPLUG: Effective and Efficient Vision-Language Learning by Cross-modal Skip-connections,Chenliang Li;Haiyang Xu;Junfeng Tian;Wei Wang;Ming Yan;Bin Bi;Jiabo Ye;Hehong Chen;Guohai Xu;Zheng Cao;Ji Zhang;Songfang Huang;Fei Huang;Jingren Zhou;Luo Si,"Large-scale pretrained foundation models have been an emerging paradigm for building artificial intelligence (AI) systems, which can be quickly adapted to a wide range of downstream tasks. This paper presents mPLUG, a new vision-language foundation model for both cross-modal understanding and generation. Most existing pre-trained models suffer from the problems of low computational efficiency and information asymmetry brought by the long visual sequence in cross-modal alignment. To address these problems, mPLUG introduces an effective and efficient vision-language architecture with novel cross-modal skip-connections, which creates inter-layer shortcuts that skip a certain number of layers for time-consuming full self-attention on the vision side. mPLUG is pre-trained end-to-end on large-scale image-text pairs with both discriminative and generative objectives. It achieves state-of-the-art results on a wide range of vision-language downstream tasks, such as image captioning, image-text retrieval, visual grounding and visual question answering. mPLUG also demonstrates strong zero-shot transferability when directly transferred to multiple video-language tasks. △ Less","25 May, 2022",https://arxiv.org/pdf/2205.12005
UMSNet: An Universal Multi-sensor Network for Human Activity Recognition,Jialiang Wang;Haotian Wei;Yi Wang;Shu Yang;Chi Li,"Human activity recognition (HAR) based on multimodal sensors has become a rapidly growing branch of biometric recognition and artificial intelligence. However, how to fully mine multimodal time series data and effectively learn accurate behavioral features has always been a hot topic in this field. Practical applications also require a well-generalized framework that can quickly process a variety of raw sensor data and learn better feature representations. This paper proposes a universal multi-sensor network (UMSNet) for human activity recognition. In particular, we propose a new lightweight sensor residual block (called LSR block), which improves the performance by reducing the number of activation function and normalization layers, and adding inverted bottleneck structure and grouping convolution. Then, the Transformer is used to extract the relationship of series features to realize the classification and recognition of human activities. Our framework has a clear structure and can be directly applied to various types of multi-modal Time Series Classification (TSC) tasks after simple specialization. Extensive experiments show that the proposed UMSNet outperforms other state-of-the-art methods on two popular multi-sensor human activity recognition datasets (i.e. HHAR dataset and MHEALTH dataset). △ Less","23 May, 2022",https://arxiv.org/pdf/2205.11756
Fed-DART and FACT: A solution for Federated Learning in a production environment,Nico Weber;Patrick Holzer;Tania Jacob;Enislay Ramentol,"Federated Learning as a decentralized artificial intelligence (AI) solution solves a variety of problems in industrial applications. It enables a continuously self-improving AI, which can be deployed everywhere at the edge. However, bringing AI to production for generating a real business impact is a challenging task. Especially in the case of Federated Learning, expertise and resources from multiple domains are required to realize its full potential. Having this in mind we have developed an innovative Federated Learning framework FACT based on Fed-DART, enabling an easy and scalable deployment, helping the user to fully leverage the potential of their private and decentralized data. △ Less","23 May, 2022",https://arxiv.org/pdf/2205.11267
FaceMAE: Privacy-Preserving Face Recognition via Masked Autoencoders,Kai Wang;Bo Zhao;Xiangyu Peng;Zheng Zhu;Jiankang Deng;Xinchao Wang;Hakan Bilen;Yang You,"Face recognition, as one of the most successful applications in artificial intelligence, has been widely used in security, administration, advertising, and healthcare. However, the privacy issues of public face datasets have attracted increasing attention in recent years. Previous works simply mask most areas of faces or synthesize samples using generative models to construct privacy-preserving face datasets, which overlooks the trade-off between privacy protection and data utility. In this paper, we propose a novel framework FaceMAE, where the face privacy and recognition performance are considered simultaneously. Firstly, randomly masked face images are used to train the reconstruction module in FaceMAE. We tailor the instance relation matching (IRM) module to minimize the distribution gap between real faces and FaceMAE reconstructed ones. During the deployment phase, we use trained FaceMAE to reconstruct images from masked faces of unseen identities without extra training. The risk of privacy leakage is measured based on face retrieval between reconstructed and original datasets. Experiments prove that the identities of reconstructed images are difficult to be retrieved. We also perform sufficient privacy-preserving face recognition on several public face datasets (i.e. CASIA-WebFace and WebFace260M). Compared to previous state of the arts, FaceMAE consistently \textbf{reduces at least 50\% error rate} on LFW, CFP-FP and AgeDB. △ Less","23 May, 2022",https://arxiv.org/pdf/2205.11090
"Toward smart composites: small-scale, untethered prediction and control for soft sensor/actuator systems",Sarah Aguasvivas Manzano;Vani Sundaram;Artemis Xu;Khoi Ly;Mark Rentschler;Robert Shepherd;Nikolaus Correll,"We present formulation and open-source tools to achieve in-material model predictive control of sensor/actuator systems using learned forward kinematics and on-device computation. Microcontroller units (MCUs) that compute the prediction and control task while colocated with the sensors and actuators enable in-material untethered behaviors. In this approach, small parameter size neural network models learn forward kinematics offline. Our open-source compiler, nn4mc, generates code to offload these predictions onto MCUs. A Newton-Raphson solver then computes the control input in real time. We first benchmark this nonlinear control approach against a PID controller on a mass-spring-damper simulation. We then study experimental results on two experimental rigs with different sensing, actuation and computational hardware: a tendon-based platform with embedded LightLace sensors and a HASEL-based platform with magnetic sensors. Experimental results indicate effective high-bandwidth tracking of reference paths (greater than or equal to 120 Hz) with a small memory footprint (less than or equal to 6.4% of flash memory). The measured path following error does not exceed 2mm in the tendon-based platform. The simulated path following error does not exceed 1mm in the HASEL-based platform. The mean power consumption of this approach in an ARM Cortex-M4f device is 45.4 mW. This control approach is also compatible with Tensorflow Lite models and equivalent on-device code. In-material intelligence enables a new class of composites that infuse autonomy into structures and systems with refined artificial proprioception. △ Less","22 August, 2022",https://arxiv.org/pdf/2205.10940
Improved Healthcare Access in Low-resource Regions: A Review of Technological Solutions,Bishal Lamichhane;Navaraj Neupane,"Technological advancements have led to significant improvements in healthcare for prevention, diagnosis, treatments, and care. While resourceful regions can capitalize on state-of-the-art healthcare technologies, there might be barriers and delays in technology-enabled healthcare availability for a low-resource region. Unique innovations guided by the constraints of low-resource regions are required to truly make healthcare technologies ubiquitous and achieve the goal of ""healthcare for all"". In this review, we identified several research and development works that have investigated technology-based healthcare innovations targeted at low-resource regions. We found three main pillars of work towards this end: low-cost hardware for the affordability of medical devices, use of information and communication technology (ICT) tools for scalability and operational efficiencies in healthcare services, and mobile health solutions. Several emerging technologies are also promising for healthcare in low-resource regions, such as artificial intelligence, the Internet of Things (IoT), and blockchain technology. We discuss these emerging technologies too in this review. △ Less","22 May, 2022",https://arxiv.org/pdf/2205.10913
Responsible Artificial Intelligence -- from Principles to Practice,Virginia Dignum,"The impact of Artificial Intelligence does not depend only on fundamental research and technological developments, but for a large part on how these systems are introduced into society and used in everyday situations. AI is changing the way we work, live and solve challenges but concerns about fairness, transparency or privacy are also growing. Ensuring responsible, ethical AI is more than designing systems whose result can be trusted. It is about the way we design them, why we design them, and who is involved in designing them. In order to develop and use AI responsibly, we need to work towards technical, societal, institutional and legal methods and tools which provide concrete support to AI practitioners, as well as awareness and training to enable participation of all, to ensure the alignment of AI systems with our societies' principles and values. △ Less","22 May, 2022",https://arxiv.org/pdf/2205.10785
Neuro-Symbolic Artificial Intelligence (AI) for Intent based Semantic Communication,Christo Kurisummoottil Thomas;Walid Saad,"Intent-based networks that integrate sophisticated machine reasoning technologies will be a cornerstone of future wireless 6G systems. Intent-based communication requires the network to consider the semantics (meanings) and effectiveness (at end-user) of the data transmission. This is essential if 6G systems are to communicate reliably with fewer bits while simultaneously providing connectivity to heterogeneous users. In this paper, contrary to state of the art, which lacks explainability of data, the framework of neuro-symbolic artificial intelligence (NeSy AI) is proposed as a pillar for learning causal structure behind the observed data. In particular, the emerging concept of generative flow networks (GFlowNet) is leveraged for the first time in a wireless system to learn the probabilistic structure which generates the data. Further, a novel optimization problem for learning the optimal encoding and decoding functions is rigorously formulated with the intent of achieving higher semantic reliability. Novel analytical formulations are developed to define key metrics for semantic message transmission, including semantic distortion, semantic similarity, and semantic reliability. These semantic measure functions rely on the proposed definition of semantic content of the knowledge base and this information measure is reflective of the nodes' reasoning capabilities. Simulation results validate the ability to communicate efficiently (with less bits but same semantics) and significantly better compared to a conventional system which does not exploit the reasoning capabilities. △ Less","22 May, 2022",https://arxiv.org/pdf/2205.10768
Preparing data for pathological artificial intelligence with clinical-grade performance,Yuanqing Yang;Kai Sun;Yanhua Gao;Kuangsong Wang;Gang Yu,"[Purpose] The pathology is decisive for disease diagnosis, but relies heavily on the experienced pathologists. Recently, pathological artificial intelligence (PAI) is thought to improve diagnostic accuracy and efficiency. However, the high performance of PAI based on deep learning in the laboratory generally cannot be reproduced in the clinic. [Methods] Because the data preparation is important for PAI, the paper has reviewed PAI-related studies in the PubMed database published from January 2017 to February 2022, and 118 studies were included. The in-depth analysis of methods for preparing data is performed, including obtaining slides of pathological tissue, cleaning, screening, and then digitizing. Expert review, image annotation, dataset division for model training and validation are also discussed. We further discuss the reasons why the high performance of PAI is not reproducible in the clinical practices and show some effective ways to improve clinical performances of PAI. [Results] The robustness of PAI depend on randomized collection of representative disease slides, including rigorous quality control and screening, correction of digital discrepancies, reasonable annotation, and the amount of data. The digital pathology is fundamental of clinical-grade PAI, and the techniques of data standardization and weakly supervised learning methods based on whole slide image (WSI) are effective ways to overcome obstacles of performance reproduction. [Conclusion] The representative data, the amount of labeling and consistency from multi-centers is the key to performance reproduction. The digital pathology for clinical diagnosis, data standardization and technique of WSI-based weakly supervised learning hopefully build clinical-grade PAI. Keywords: pathological artificial intelligence; data preparation; clinical-grade; deep learning △ Less","22 May, 2022",https://arxiv.org/pdf/2205.10748
Deep Learning vs. Gradient Boosting: Benchmarking state-of-the-art machine learning algorithms for credit scoring,Marc Schmitt,"Artificial intelligence (AI) and machine learning (ML) have become vital to remain competitive for financial services companies around the globe. The two models currently competing for the pole position in credit risk management are deep learning (DL) and gradient boosting machines (GBM). This paper benchmarked those two algorithms in the context of credit scoring using three distinct datasets with different features to account for the reality that model choice/power is often dependent on the underlying characteristics of the dataset. The experiment has shown that GBM tends to be more powerful than DL and has also the advantage of speed due to lower computational requirements. This makes GBM the winner and choice for credit scoring. However, it was also shown that the outperformance of GBM is not always guaranteed and ultimately the concrete problem scenario or dataset will determine the final model choice. Overall, based on this study both algorithms can be considered state-of-the-art for binary classification tasks on structured datasets, while GBM should be the go-to solution for most problem scenarios due to easier use, significantly faster training time, and superior accuracy. △ Less","21 May, 2022",https://arxiv.org/pdf/2205.10535
Computable Artificial General Intelligence,Michael Timothy Bennett,"Artificial general intelligence (AGI) may herald our extinction, according to AI safety research. Yet claims regarding AGI must rely upon mathematical formalisms -- theoretical agents we may analyse or attempt to build. AIXI appears to be the only such formalism supported by proof that its behaviour is optimal, a consequence of its use of compression as a proxy for intelligence. Unfortunately, AIXI is incomputable and claims regarding its behaviour highly subjective. We argue that this is because AIXI formalises cognition as taking place in isolation from the environment in which goals are pursued (Cartesian dualism). We propose an alternative, supported by proof and experiment, which overcomes these problems. Integrating research from cognitive science with AI, we formalise an enactive model of learning and reasoning to address the problem of subjectivity. This allows us to formulate a different proxy for intelligence, called weakness, which addresses the problem of incomputability. We prove optimal behaviour is attained when weakness is maximised. This proof is supplemented by experimental results comparing weakness and description length (the closest analogue to compression possible without reintroducing subjectivity). Weakness outperforms description length, suggesting it is a better proxy. Furthermore we show that, if cognition is enactive, then minimisation of description length is neither necessary nor sufficient to attain optimal performance, undermining the notion that compression is closely related to intelligence. However, there remain open questions regarding the implementation of scale-able AGI. In the short term, these results may be best utilised to improve the performance of existing systems. For example, our results explain why Deepmind's Apperception Engine is able to generalise effectively, and how to replicate that performance by maximising weakness. △ Less","21 November, 2022",https://arxiv.org/pdf/2205.10513
De novo design of protein target specific scaffold-based Inhibitors via Reinforcement Learning,Andrew D. McNaughton;Mridula S. Bontha;Carter R. Knutson;Jenna A. Pope;Neeraj Kumar,"Efficient design and discovery of target-driven molecules is a critical step in facilitating lead optimization in drug discovery. Current approaches to develop molecules for a target protein are intuition-driven, hampered by slow iterative design-test cycles due to computational challenges in utilizing 3D structural data, and ultimately limited by the expertise of the chemist - leading to bottlenecks in molecular design. In this contribution, we propose a novel framework, called 3D-MolGNN_{RL}, coupling reinforcement learning (RL) to a deep generative model based on 3D-Scaffold to generate target candidates specific to a protein building up atom by atom from the starting core scaffold. 3D-MolGNN_{RL} provides an efficient way to optimize key features by multi-objective reward function within a protein pocket using parallel graph neural network models. The agent learns to build molecules in 3D space while optimizing the activity, binding affinity, potency, and synthetic accessibility of the candidates generated for infectious disease protein targets. Our approach can serve as an interpretable artificial intelligence (AI) tool for lead optimization with optimized activity, potency, and biophysical properties. △ Less","20 May, 2022",https://arxiv.org/pdf/2205.10473
SOL: Reducing the Maintenance Overhead for Integrating Hardware Support into AI Frameworks,Nicolas Weber,"The increased interest in Artificial Intelligence (AI) raised the need for highly optimized and sophisticated AI frameworks. Starting with the Lua-based Torch many frameworks have emerged over time, such as Theano, Caffe, Chainer, CNTK, MxNet, PyTorch, DL4J, or TensorFlow. All of these provide a high level scripting API that allows users to easily design neural networks and run these on various kinds of hardware. What the user usually does not see is the high effort put into these frameworks to provide peak execution performance. While mainstream CPUs and GPUs have the ""luxury"" to have a wide spread user base in the open source community, less mainstream CPU, GPU or accelerator vendors need to put in a high effort to get their hardware supported by these frameworks. This includes not only the development of highly efficient compute libraries such as CUDNN, OneDNN or VEDNN but also supporting an ever growing number of simpler compute operations such as summation and multiplications. Each of these frameworks, nowadays, supports several hundred of unique operations, with tensors of various sizes, shapes and data types, which end up in thousands of compute kernels required for each device type. And the number of operations keeps increasing. That is why NEC Laboratories Europe started developing the SOL AI Optimization project already years ago, to deliver optimal performance to users while keeping the maintenance burden minimal. △ Less","19 May, 2022",https://arxiv.org/pdf/2205.10357
Visualizing and Explaining Language Models,Adrian M. P. Braşoveanu;Răzvan Andonie,"During the last decade, Natural Language Processing has become, after Computer Vision, the second field of Artificial Intelligence that was massively changed by the advent of Deep Learning. Regardless of the architecture, the language models of the day need to be able to process or generate text, as well as predict missing words, sentences or relations depending on the task. Due to their black-box nature, such models are difficult to interpret and explain to third parties. Visualization is often the bridge that language model designers use to explain their work, as the coloring of the salient words and phrases, clustering or neuron activations can be used to quickly understand the underlying models. This paper showcases the techniques used in some of the most popular Deep Learning for NLP visualizations, with a special focus on interpretability and explainability. △ Less","30 April, 2022",https://arxiv.org/pdf/2205.10238
An Artificial Neural Network Functionalized by Evolution,Fabien Furfaro;Avner Bar-Hen;Geoffroy Berthelot,"The topology of artificial neural networks has a significant effect on their performance. Characterizing efficient topology is a field of promising research in Artificial Intelligence. However, it is not a trivial task and it is mainly experimented on through convolutional neural networks. We propose a hybrid model which combines the tensor calculus of feed-forward neural networks with Pseudo-Darwinian mechanisms. This allows for finding topologies that are well adapted for elaboration of strategies, control problems or pattern recognition tasks. In particular, the model can provide adapted topologies at early evolutionary stages, and 'structural convergence', which can found applications in robotics, big-data and artificial life. △ Less","16 May, 2022",https://arxiv.org/pdf/2205.10118
Assessing Demographic Bias Transfer from Dataset to Model: A Case Study in Facial Expression Recognition,Iris Dominguez-Catena;Daniel Paternain;Mikel Galar,"The increasing amount of applications of Artificial Intelligence (AI) has led researchers to study the social impact of these technologies and evaluate their fairness. Unfortunately, current fairness metrics are hard to apply in multi-class multi-demographic classification problems, such as Facial Expression Recognition (FER). We propose a new set of metrics to approach these problems. Of the three metrics proposed, two focus on the representational and stereotypical bias of the dataset, and the third one on the residual bias of the trained model. These metrics combined can potentially be used to study and compare diverse bias mitigation methods. We demonstrate the usefulness of the metrics by applying them to a FER problem based on the popular Affectnet dataset. Like many other datasets for FER, Affectnet is a large Internet-sourced dataset with 291,651 labeled images. Obtaining images from the Internet raises some concerns over the fairness of any system trained on this data and its ability to generalize properly to diverse populations. We first analyze the dataset and some variants, finding substantial racial bias and gender stereotypes. We then extract several subsets with different demographic properties and train a model on each one, observing the amount of residual bias in the different setups. We also provide a second analysis on a different dataset, FER+. △ Less","20 May, 2022",https://arxiv.org/pdf/2205.10049
Beyond Labels: Visual Representations for Bone Marrow Cell Morphology Recognition,Shayan Fazeli;Alireza Samiei;Thomas D. Lee;Majid Sarrafzadeh,"Analyzing and inspecting bone marrow cell cytomorphology is a critical but highly complex and time-consuming component of hematopathology diagnosis. Recent advancements in artificial intelligence have paved the way for the application of deep learning algorithms to complex medical tasks. Nevertheless, there are many challenges in applying effective learning algorithms to medical image analysis, such as the lack of sufficient and reliably annotated training datasets and the highly class-imbalanced nature of most medical data. Here, we improve on the state-of-the-art methodologies of bone marrow cell recognition by deviating from sole reliance on labeled data and leveraging self-supervision in training our learning models. We investigate our approach's effectiveness in identifying bone marrow cell types. Our experiments demonstrate significant performance improvements in conducting different bone marrow cell recognition tasks compared to the current state-of-the-art methodologies. △ Less","19 May, 2022",https://arxiv.org/pdf/2205.09880
Robust and Efficient Medical Imaging with Self-Supervision,Shekoofeh Azizi;Laura Culp;Jan Freyberg;Basil Mustafa;Sebastien Baur;Simon Kornblith;Ting Chen;Patricia MacWilliams;S. Sara Mahdavi;Ellery Wulczyn;Boris Babenko;Megan Wilson;Aaron Loh;Po-Hsuan Cameron Chen;Yuan Liu;Pinal Bavishi;Scott Mayer McKinney;Jim Winkens;Abhijit Guha Roy;Zach Beaver;Fiona Ryan;Justin Krogue;Mozziyar Etemadi;Umesh Telang;Yun Liu,"Recent progress in Medical Artificial Intelligence (AI) has delivered systems that can reach clinical expert level performance. However, such systems tend to demonstrate sub-optimal ""out-of-distribution"" performance when evaluated in clinical settings different from the training environment. A common mitigation strategy is to develop separate systems for each clinical setting using site-specific data [1]. However, this quickly becomes impractical as medical data is time-consuming to acquire and expensive to annotate [2]. Thus, the problem of ""data-efficient generalization"" presents an ongoing difficulty for Medical AI development. Although progress in representation learning shows promise, their benefits have not been rigorously studied, specifically for out-of-distribution settings. To meet these challenges, we present REMEDIS, a unified representation learning strategy to improve robustness and data-efficiency of medical imaging AI. REMEDIS uses a generic combination of large-scale supervised transfer learning with self-supervised learning and requires little task-specific customization. We study a diverse range of medical imaging tasks and simulate three realistic application scenarios using retrospective data. REMEDIS exhibits significantly improved in-distribution performance with up to 11.5% relative improvement in diagnostic accuracy over a strong supervised baseline. More importantly, our strategy leads to strong data-efficient generalization of medical imaging AI, matching strong supervised baselines using between 1% to 33% of retraining data across tasks. These results suggest that REMEDIS can significantly accelerate the life-cycle of medical imaging AI development thereby presenting an important step forward for medical imaging AI to deliver broad impact. △ Less","3 July, 2022",https://arxiv.org/pdf/2205.09723
Parallel bandit architecture based on laser chaos for reinforcement learning,Takashi Urushibara;Nicolas Chauvet;Satoshi Kochi;Satoshi Sunada;Kazutaka Kanno;Atsushi Uchida;Ryoichi Horisaki;Makoto Naruse,"Accelerating artificial intelligence by photonics is an active field of study aiming to exploit the unique properties of photons. Reinforcement learning is an important branch of machine learning, and photonic decision-making principles have been demonstrated with respect to the multi-armed bandit problems. However, reinforcement learning could involve a massive number of states, unlike previously demonstrated bandit problems where the number of states is only one. Q-learning is a well-known approach in reinforcement learning that can deal with many states. The architecture of Q-learning, however, does not fit well photonic implementations due to its separation of update rule and the action selection. In this study, we organize a new architecture for multi-state reinforcement learning as a parallel array of bandit problems in order to benefit from photonic decision-makers, which we call parallel bandit architecture for reinforcement learning or PBRL in short. Taking a cart-pole balancing problem as an instance, we demonstrate that PBRL adapts to the environment in fewer time steps than Q-learning. Furthermore, PBRL yields faster adaptation when operated with a chaotic laser time series than the case with uniformly distributed pseudorandom numbers where the autocorrelation inherent in the laser chaos provides a positive effect. We also find that the variety of states that the system undergoes during the learning phase exhibits completely different properties between PBRL and Q-learning. The insights obtained through the present study are also beneficial for existing computing platforms, not just photonic realizations, in accelerating performances by the PBRL algorithms and correlated random sequences. △ Less","19 May, 2022",https://arxiv.org/pdf/2205.09543
Mobility Support for Millimeter Wave Communications: Opportunities and Challenges,Jing Li;Yong Niu;Hao Wu;Bo Ai;Sheng Chen;Zhiyong Feng;Zhangdui Zhong;Ning Wang,"Millimeter-wave (mmWave) communication technology offers a potential and promising solution to support 5G and B5G wireless networks in dynamic scenarios and applications. However, mobility introduces many challenges as well as opportunities to mmWave applications. To address these problems, we conduct a survey of the opportunities and technologies to support mmWave communications in mobile scenarios. Firstly, we summarize the mobile scenarios where mmWave communications are exploited, including indoor wireless local area network (WLAN) or wireless personal area network (WPAN), cellular access, vehicle-to-everything (V2X), high speed train (HST), unmanned aerial vehicle (UAV), and the new space-air-ground-sea communication scenarios. Then, to address users' mobility impact on the system performance in different application scenarios, we introduce several representative mobility models in mmWave systems, including human mobility, vehicular mobility, high speed train mobility and ship mobility. Next we survey the key challenges and existing solutions to mmWave applications, such as channel modeling, channel estimation, anti-blockage, and capacity improvement. Lastly, we discuss the open issues concerning mobility-aware mmWave communications that deserve further investigation. In particular, we highlight future heterogeneous mobile networks, dynamic resource management, artificial intelligence (AI) for mobility and integration of geographical information, deployment of large intelligent surface and reconfigurable antenna technology, and finally, the evolution to Terahertz (THz) communications. △ Less","18 May, 2022",https://arxiv.org/pdf/2205.09214
AI-assisted Optimization of the ECCE Tracking System at the Electron Ion Collider,C. Fanelli;Z. Papandreou;K. Suresh;J. K. Adkins;Y. Akiba;A. Albataineh;M. Amaryan;I. C. Arsene;C. Ayerbe Gayoso;J. Bae;X. Bai;M. D. Baker;M. Bashkanov;R. Bellwied;F. Benmokhtar;V. Berdnikov;J. C. Bernauer;F. Bock;W. Boeglin;M. Borysova;E. Brash;P. Brindza;W. J. Briscoe;M. Brooks;S. Bueltmann,"The Electron-Ion Collider (EIC) is a cutting-edge accelerator facility that will study the nature of the ""glue"" that binds the building blocks of the visible matter in the universe. The proposed experiment will be realized at Brookhaven National Laboratory in approximately 10 years from now, with detector design and R&D currently ongoing. Notably, EIC is one of the first large-scale facilities to leverage Artificial Intelligence (AI) already starting from the design and R&D phases. The EIC Comprehensive Chromodynamics Experiment (ECCE) is a consortium that proposed a detector design based on a 1.5T solenoid. The EIC detector proposal review concluded that the ECCE design will serve as the reference design for an EIC detector. Herein we describe a comprehensive optimization of the ECCE tracker using AI. The work required a complex parametrization of the simulated detector system. Our approach dealt with an optimization problem in a multidimensional design space driven by multiple objectives that encode the detector performance, while satisfying several mechanical constraints. We describe our strategy and show results obtained for the ECCE tracking system. The AI-assisted design is agnostic to the simulation framework and can be extended to other sub-detectors or to a system of sub-detectors to further optimize the performance of the EIC detector. △ Less","19 May, 2022",https://arxiv.org/pdf/2205.09185
Relational representation learning with spike trains,Dominik Dold,"Relational representation learning has lately received an increase in interest due to its flexibility in modeling a variety of systems like interacting particles, materials and industrial projects for, e.g., the design of spacecraft. A prominent method for dealing with relational data are knowledge graph embedding algorithms, where entities and relations of a knowledge graph are mapped to a low-dimensional vector space while preserving its semantic structure. Recently, a graph embedding method has been proposed that maps graph elements to the temporal domain of spiking neural networks. However, it relies on encoding graph elements through populations of neurons that only spike once. Here, we present a model that allows us to learn spike train-based embeddings of knowledge graphs, requiring only one neuron per graph element by fully utilizing the temporal domain of spike patterns. This coding scheme can be implemented with arbitrary spiking neuron models as long as gradients with respect to spike times can be calculated, which we demonstrate for the integrate-and-fire neuron model. In general, the presented results show how relational knowledge can be integrated into spike-based systems, opening up the possibility of merging event-based computing and relational data to build powerful and energy efficient artificial intelligence applications and reasoning systems. △ Less","18 May, 2022",https://arxiv.org/pdf/2205.09140
Industry 5.0 is Coming: A Survey on Intelligent NextG Wireless Networks as Technological Enablers,Shah Zeb;Aamir Mahmood;Sunder Ali Khowaja;Kapal Dev;Syed Ali Hassan;Nawab Muhammad Faseeh Qureshi;Mikael Gidlund;Paolo Bellavista,"Industry 5.0 vision, a step toward the next industrial revolution and enhancement to Industry 4.0, envisioned the new goals of resilient, sustainable, and human-centric approaches in diverse emerging applications, e.g., factories-of-the-future, digital society. The vision seeks to leverage human intelligence and creativity in nexus with intelligent, efficient, and reliable cognitive collaborating robots (cobots) to achieve zero waste, zerodefect, and mass customization-based manufacturing solutions. However, the vision requires the merging of cyber-physical worlds through utilizing Industry 5.0 technological enablers, e.g., cognitive cobots, person-centric artificial intelligence (AI), cyberphysical systems, digital twins, hyperconverged data storage and computing, communication infrastructure, and others. In this regard, the convergence of the emerging computational intelligence (CI) paradigm and next-generation wireless networks (NGWNs) can fulfill the stringent communication and computation requirements of the technological enablers in the Industry 5.0 vision, which is the aim of this survey-based tutorial. In this article, we address this issue by reviewing and analyzing current emerging concepts and technologies, e.g., CI tools and frameworks, network-in-box architecture, open radio access networks, softwarized service architectures, potential enabling services, and others, essential for designing the objectives of CINGWNs to fulfill the Industry 5.0 vision requirements. Finally, we provide a list of lessons learned from our detailed review, research challenges, and open issues that should be addressed in CI-NGWNs to realize Industry 5.0. △ Less","18 May, 2022",https://arxiv.org/pdf/2205.09084
"Internet of Intelligence: A Survey on the Enabling Technologies, Applications, and Challenges",Qinqin Tang;F. Richard Yu;Renchao Xie;Azzedine Boukerche;Tao Huang;Yunjie Liu,"The Internet of intelligence is conceived as an emerging networking paradigm, which will make intelligence as easy to obtain as information. This paper provides an overview of the Internet of intelligence, focusing on motivations, architecture, enabling technologies, applications, and existing challenges. This can provide a good foundation for those who are interested to gain insights into the concept of the Internet of intelligence and the key enablers of this emerging networking paradigm. Specifically, this paper starts by investigating the evolution of networking paradigms and artificial intelligence (AI), based on which we present the motivations of the Internet of intelligence by demonstrating that networking needs intelligence and intelligence needs networking. We then present the layered architecture to characterize the Internet of intelligence systems and discuss the enabling technologies of each layer. Moreover, we discuss the critical applications and their integration with the Internet of intelligence paradigm. Finally, some technical challenges and open issues are summarized to fully exploit the benefits of the Internet of intelligence. △ Less","18 May, 2022",https://arxiv.org/pdf/2205.08977
The games we play: critical complexity improves machine learning,Abeba Birhane;David J. T. Sumpter,"When mathematical modelling is applied to capture a complex system, multiple models are often created that characterize different aspects of that system. Often, a model at one level will produce a prediction which is contradictory at another level but both models are accepted because they are both useful. Rather than aiming to build a single unified model of a complex system, the modeller acknowledges the infinity of ways of capturing the system of interest, while offering their own specific insight. We refer to this pragmatic applied approach to complex systems -- one which acknowledges that they are incompressible, dynamic, nonlinear, historical, contextual, and value-laden -- as Open Machine Learning (Open ML). In this paper we define Open ML and contrast it with some of the grand narratives of ML of two forms: 1) Closed ML, ML which emphasizes learning with minimal human input (e.g. Google's AlphaZero) and 2) Partially Open ML, ML which is used to parameterize existing models. To achieve this, we use theories of critical complexity to both evaluate these grand narratives and contrast them with the Open ML approach. Specifically, we deconstruct grand ML `theories' by identifying thirteen 'games' played in the ML community. These games lend false legitimacy to models, contribute to over-promise and hype about the capabilities of artificial intelligence, reduce wider participation in the subject, lead to models that exacerbate inequality and cause discrimination and ultimately stifle creativity in research. We argue that best practice in ML should be more consistent with critical complexity perspectives than with rationalist, grand narratives. △ Less","18 May, 2022",https://arxiv.org/pdf/2205.08922
World Value Functions: Knowledge Representation for Multitask Reinforcement Learning,Geraud Nangue Tasse;Steven James;Benjamin Rosman,"An open problem in artificial intelligence is how to learn and represent knowledge that is sufficient for a general agent that needs to solve multiple tasks in a given world. In this work we propose world value functions (WVFs), which are a type of general value function with mastery of the world - they represent not only how to solve a given task, but also how to solve any other goal-reaching task. To achieve this, we equip the agent with an internal goal space defined as all the world states where it experiences a terminal transition - a task outcome. The agent can then modify task rewards to define its own reward function, which provably drives it to learn how to achieve all achievable internal goals, and the value of doing so in the current task. We demonstrate a number of benefits of WVFs. When the agent's internal goal space is the entire state space, we demonstrate that the transition function can be inferred from the learned WVF, which allows the agent to plan using learned value functions. Additionally, we show that for tasks in the same world, a pretrained agent that has learned any WVF can then infer the policy and value function for any new task directly from its rewards. Finally, an important property for long-lived agents is the ability to reuse existing knowledge to solve new tasks. Using WVFs as the knowledge representation for learned tasks, we show that an agent is able to solve their logical combination zero-shot, resulting in a combinatorially increasing number of skills throughout their lifetime. △ Less","18 May, 2022",https://arxiv.org/pdf/2205.08827
Experimentally realized in situ backpropagation for deep learning in nanophotonic neural networks,Sunil Pai;Zhanghao Sun;Tyler W. Hughes;Taewon Park;Ben Bartlett;Ian A. D. Williamson;Momchil Minkov;Maziyar Milanizadeh;Nathnael Abebe;Francesco Morichetti;Andrea Melloni;Shanhui Fan;Olav Solgaard;David A. B. Miller,"Neural networks are widely deployed models across many scientific disciplines and commercial endeavors ranging from edge computing and sensing to large-scale signal processing in data centers. The most efficient and well-entrenched method to train such networks is backpropagation, or reverse-mode automatic differentiation. To counter an exponentially increasing energy budget in the artificial intelligence sector, there has been recent interest in analog implementations of neural networks, specifically nanophotonic neural networks for which no analog backpropagation demonstration exists. We design mass-manufacturable silicon photonic neural networks that alternately cascade our custom designed ""photonic mesh"" accelerator with digitally implemented nonlinearities. These reconfigurable photonic meshes program computationally intensive arbitrary matrix multiplication by setting physical voltages that tune the interference of optically encoded input data propagating through integrated Mach-Zehnder interferometer networks. Here, using our packaged photonic chip, we demonstrate in situ backpropagation for the first time to solve classification tasks and evaluate a new protocol to keep the entire gradient measurement and update of physical device voltages in the analog domain, improving on past theoretical proposals. Our method is made possible by introducing three changes to typical photonic meshes: (1) measurements at optical ""grating tap"" monitors, (2) bidirectional optical signal propagation automated by fiber switch, and (3) universal generation and readout of optical amplitude and phase. After training, our classification achieves accuracies similar to digital equivalents even in presence of systematic error. Our findings suggest a new training paradigm for photonics-accelerated artificial intelligence based entirely on a physical analog of the popular backpropagation technique. △ Less","17 May, 2022",https://arxiv.org/pdf/2205.08501
A Psychological Theory of Explainability,Scott Cheng-Hsin Yang;Tomas Folke;Patrick Shafto,"The goal of explainable Artificial Intelligence (XAI) is to generate human-interpretable explanations, but there are no computationally precise theories of how humans interpret AI generated explanations. The lack of theory means that validation of XAI must be done empirically, on a case-by-case basis, which prevents systematic theory-building in XAI. We propose a psychological theory of how humans draw conclusions from saliency maps, the most common form of XAI explanation, which for the first time allows for precise prediction of explainee inference conditioned on explanation. Our theory posits that absent explanation humans expect the AI to make similar decisions to themselves, and that they interpret an explanation by comparison to the explanations they themselves would give. Comparison is formalized via Shepard's universal law of generalization in a similarity space, a classic theory from cognitive science. A pre-registered user study on AI image classifications with saliency map explanations demonstrate that our theory quantitatively matches participants' predictions of the AI. △ Less","9 June, 2022",https://arxiv.org/pdf/2205.08452
MAS2HP: A Multi Agent System to Predict Protein Structure in 2D HP model,Hossein Parineh;Nasser Mozayani,"Protein Structure Prediction (PSP) is an unsolved problem in the field of computational biology. The problem of protein structure prediction is about predicting the native conformation of a protein, while its sequence of amino acids is known. Regarding processing limitations of current computer systems, all-atom simulations for proteins are typically unpractical; several reduced models of proteins have been proposed. Additionally, due to intrinsic hardness of calculations even in reduced models, many computational methods mainly based on artificial intelligence have been proposed to solve the problem. Agent-based modeling is a relatively new method for modeling systems composed of interacting items. In this paper we proposed a new approach for protein structure prediction by using agent-based modeling (ABM) in two dimensional hydrophobic-hydrophilic model. We broke the whole process of protein structure prediction into two steps: the first step, which was introduced in our previous paper, is about biasing the linear sequence to gain a primary energy, and the next step, which will be explained in this paper, is about using ABM with a predefined set of rules, to find the best conformation in the least possible amount of time and steps. This method was implemented in NETLOGO. We have tested this algorithm on several benchmark sequences ranging from 20 to 50-mers in two dimensional Hydrophobic-Hydrophilic lattice models. Comparing to the result of the other algorithms, our method is capable of finding the best known conformations in a significantly shorter time. A major problem in PSP simulation is that as the sequence length increases the time consumed to predict a valid structure will exponentially increase. In contrast, by using MAS2HP the effect of increase in sequence length on spent time has changed from exponentially to linear. △ Less","2 June, 2022",https://arxiv.org/pdf/2205.08451
A Comprehensive Study on Artificial Intelligence Algorithms to Implement Safety Using Communication Technologies,Rafia Inam;Alberto Yukinobu Hata;Vlasjov Prifti;Sara Abbaspour Asadollah,"The recent development of artificial intelligence (AI) has increased the interest of researchers and practitioners towards applying its techniques into multiple domains like automotive, health care and air space to achieve automation. Combined to these applications, the attempt to use AI techniques into carrying out safety issues is momentarily at a progressive state. As AI problems are getting even more complex, large processing power is demanded for safety-critical systems to fulfill real-time requirements. These challenges can be solved through edge or cloud computing, which makes the communication an integral part of the solution. This study aims at providing a comprehensive picture of the state of the art AI based safety solutions that uses different communication technologies in diverse application domains. To achieve this, a systematic mapping study is conducted and 565 relevant papers are shortlisted through a multistage selection process, which are then analyzed according to a systematically defined classification framework. The results of the study are based on these main objectives: to clarify current research gaps in the field, to identify the possibility of increased usage of cellular communication in multiple domains, to identify the mostly used AI algorithms and to summarize the emerging future research trends on the topic. The results demonstrate that automotive domain is the one applying AI and communication the most to implement safety and the most used AI in this domain is neural networks, clustering and computer vision; applying cellular communication to automotive domain is highest; the use of non-cellular communication technologies is dominant however a clear trend of a rapid increase in the use of cellular communication is observed specially from 2020 with the roll-out of 5G technology. △ Less","17 May, 2022",https://arxiv.org/pdf/2205.08404
Landing AI on Networks: An equipment vendor viewpoint on Autonomous Driving Networks,Dario Rossi;Liang Zhang,"The tremendous achievements of Artificial Intelligence (AI) in computer vision, natural language processing, games and robotics, has extended the reach of the AI hype to other fields: in telecommunication networks, the long term vision is to let AI fully manage, and autonomously drive, all aspects of network operation. In this industry vision paper, we discuss challenges and opportunities of Autonomous Driving Network (ADN) driven by AI technologies. To understand how AI can be successfully landed in current and future networks, we start by outlining challenges that are specific to the networking domain, putting them in perspective with advances that AI has achieved in other fields. We then present a system view, clarifying how AI can be fitted in the network architecture. We finally discuss current achievements as well as future promises of AI in networks, mentioning a roadmap to avoid bumps in the road that leads to true large-scale deployment of AI technologies in networks. △ Less","26 April, 2022",https://arxiv.org/pdf/2205.08347
"Using artificial intelligence to detect chest X-rays with no significant findings in a primary health care setting in Oulu, Finland",Tommi Keski-Filppula;Marko Nikki;Marianne Haapea;Naglis Ramanauskas;Osmo Tervonen,"Objectives: To assess the use of artificial intelligence-based software in ruling out chest X-ray cases, with no significant findings in a primary health care setting. Methods: In this retrospective study, a commercially available artificial intelligence (AI) software was used to analyse 10 000 chest X-rays of Finnish primary health care patients. In studies with a mismatch between an AI normal report and the original radiologist report, a consensus read by two board-certified radiologists was conducted to make the final diagnosis. Results: After the exclusion of cases not meeting the study criteria, 9579 cases were analysed by AI. Of these cases, 4451 were considered normal in the original radiologist report and 4644 after the consensus reading. The number of cases correctly found nonsignificant by AI was 1692 (17.7% of all studies and 36.4% of studies with no significant findings). After the consensus read, there were nine confirmed false-negative studies. These studies included four cases of slightly enlarged heart size, four cases of slightly increased pulmonary opacification and one case with a small unilateral pleural effusion. This gives the AI a sensitivity of 99.8% (95% CI= 99.65-99.92) and specificity of 36.4 % (95% CI= 35.05-37.84) for recognising significant pathology on a chest X-ray. Conclusions: AI was able to correctly rule out 36.4% of chest X-rays with no significant findings of primary health care patients, with a minimal number of false negatives that would lead to effectively no compromise on patient safety. No critical findings were missed by the software. △ Less","17 May, 2022",https://arxiv.org/pdf/2205.08123
CascadER: Cross-Modal Cascading for Knowledge Graph Link Prediction,Tara Safavi;Doug Downey;Tom Hope,"Knowledge graph (KG) link prediction is a fundamental task in artificial intelligence, with applications in natural language processing, information retrieval, and biomedicine. Recently, promising results have been achieved by leveraging cross-modal information in KGs, using ensembles that combine knowledge graph embeddings (KGEs) and contextual language models (LMs). However, existing ensembles are either (1) not consistently effective in terms of ranking accuracy gains or (2) impractically inefficient on larger datasets due to the combinatorial explosion problem of pairwise ranking with deep language models. In this paper, we propose a novel tiered ranking architecture CascadER to maintain the ranking accuracy of full ensembling while improving efficiency considerably. CascadER uses LMs to rerank the outputs of more efficient base KGEs, relying on an adaptive subset selection scheme aimed at invoking the LMs minimally while maximizing accuracy gain over the KGE. Extensive experiments demonstrate that CascadER improves MRR by up to 9 points over KGE baselines, setting new state-of-the-art performance on four benchmarks while improving efficiency by one or more orders of magnitude over competitive cross-modal baselines. Our empirical analyses reveal that diversity of models across modalities and preservation of individual models' confidence signals help explain the effectiveness of CascadER, and suggest promising directions for cross-modal cascaded architectures. Code and pretrained models are available at https://github.com/tsafavi/cascader. △ Less","23 September, 2022",https://arxiv.org/pdf/2205.08012
Developing patient-driven artificial intelligence based on personal rankings of care decision making steps,Lauri Lahti,"We propose and experimentally motivate a new methodology to support decision-making processes in healthcare with artificial intelligence based on personal rankings of care decision making steps that can be identified with our methodology, questionnaire data and its statistical patterns. Our longitudinal quantitative cross-sectional three-stage study gathered self-ratings for 437 expression statements concerning healthcare situations on Likert scales in respect to ""the need for help"", ""the advancement of health"", ""the hopefulness"", ""the indication of compassion"" and ""the health condition"", and 45 answers about the person's demographics, health and wellbeing, also the duration of giving answers. Online respondents between 1 June 2020 and 29 June 2021 were recruited from Finnish patient and disabled people's organizations, other health-related organizations and professionals, and educational institutions (n=1075). With Kruskal-Wallis test, Wilcoxon rank-sum test (i.e., Mann-Whitney U test), Wilcoxon rank-sum pairwise test, Welch's t test and one-way analysis of variance (ANOVA) between groups test we identified statistically significant differences of ratings and their durations for each expression statement in respect to respondent groupings based on the answer values of each background question. Frequencies of the later reordering of rating rankings showed dependencies with ratings given earlier in respect to various interpretation task entities, interpretation dimensions and respondent groupings. Our methodology, questionnaire data and its statistical patterns enable analyzing with self-rated expression statements the representations of decision making steps in healthcare situations and their chaining, agglomeration and branching in knowledge entities of personalized care paths. Our results support building artificial intelligence solutions to address the patient's needs concerning care. △ Less","15 May, 2022",https://arxiv.org/pdf/2205.07881
A Safety Assurable Human-Inspired Perception Architecture,Rick Salay;Krzysztof Czarnecki,"Although artificial intelligence-based perception (AIP) using deep neural networks (DNN) has achieved near human level performance, its well-known limitations are obstacles to the safety assurance needed in autonomous applications. These include vulnerability to adversarial inputs, inability to handle novel inputs and non-interpretability. While research in addressing these limitations is active, in this paper, we argue that a fundamentally different approach is needed to address them. Inspired by dual process models of human cognition, where Type 1 thinking is fast and non-conscious while Type 2 thinking is slow and based on conscious reasoning, we propose a dual process architecture for safe AIP. We review research on how humans address the simplest non-trivial perception problem, image classification, and sketch a corresponding AIP architecture for this task. We argue that this architecture can provide a systematic way of addressing the limitations of AIP using DNNs and an approach to assurance of human-level performance and beyond. We conclude by discussing what components of the architecture may already be addressed by existing work and what remains future work. △ Less","18 June, 2022",https://arxiv.org/pdf/2205.07862
A BenchCouncil View on Benchmarking Emerging and Future Computing,Jianfeng Zhan,"The measurable properties of the artifacts or objects in the computer, management, or finance disciplines are extrinsic, not inherent -- dependent on their problem definitions and solution instantiations. Only after the instantiation can the solutions to the problem be measured. The processes of definition, instantiation, and measurement are entangled, and they have complex mutual influences. Meanwhile, the technology inertia brings instantiation bias -- trapped into a subspace or even a point at a high-dimension solution space. These daunting challenges, which emerging computing aggravates, make metrology can not work for benchmark communities. It is pressing to establish independent benchmark science and engineering. This article presents a unifying benchmark definition, a conceptual framework, and a traceable and supervised learning-based benchmarking methodology, laying the foundation for benchmark science and engineering. I also discuss BenchCouncil's plans for emerging and future computing. The ongoing projects include defining the challenges of intelligence, instinct, quantum computers, Metaverse, planet-scale computers, and reformulating data centers, artificial intelligence for science, and CPU benchmark suites. Also, BenchCouncil will collaborate with ComputerCouncil on open-source computer systems for planet-scale computing, AI for science systems, and Metaverse. △ Less","16 May, 2022",https://arxiv.org/pdf/2205.07769
How Different Groups Prioritize Ethical Values for Responsible AI,Maurice Jakesch;Zana Buçinca;Saleema Amershi;Alexandra Olteanu,"Private companies, public sector organizations, and academic groups have outlined ethical values they consider important for responsible artificial intelligence technologies. While their recommendations converge on a set of central values, little is known about the values a more representative public would find important for the AI technologies they interact with and might be affected by. We conducted a survey examining how individuals perceive and prioritize responsible AI values across three groups: a representative sample of the US population (N=743), a sample of crowdworkers (N=755), and a sample of AI practitioners (N=175). Our results empirically confirm a common concern: AI practitioners' value priorities differ from those of the general public. Compared to the US-representative sample, AI practitioners appear to consider responsible AI values as less important and emphasize a different set of values. In contrast, self-identified women and black respondents found responsible AI values more important than other groups. Surprisingly, more liberal-leaning participants, rather than participants reporting experiences with discrimination, were more likely to prioritize fairness than other groups. Our findings highlight the importance of paying attention to who gets to define responsible AI. △ Less","15 November, 2022",https://arxiv.org/pdf/2205.07722
Generalizing to New Tasks via One-Shot Compositional Subgoals,Xihan Bian;Oscar Mendez;Simon Hadfield,"The ability to generalize to previously unseen tasks with little to no supervision is a key challenge in modern machine learning research. It is also a cornerstone of a future ""General AI"". Any artificially intelligent agent deployed in a real world application, must adapt on the fly to unknown environments. Researchers often rely on reinforcement and imitation learning to provide online adaptation to new tasks, through trial and error learning. However, this can be challenging for complex tasks which require many timesteps or large numbers of subtasks to complete. These ""long horizon"" tasks suffer from sample inefficiency and can require extremely long training times before the agent can learn to perform the necessary longterm planning. In this work, we introduce CASE which attempts to address these issues by training an Imitation Learning agent using adaptive ""near future"" subgoals. These subgoals are recalculated at each step using compositional arithmetic in a learned latent representation space. In addition to improving learning efficiency for standard long-term tasks, this approach also makes it possible to perform one-shot generalization to previously unseen tasks, given only a single reference trajectory for the task in a different environment. Our experiments show that the proposed approach consistently outperforms the previous state-of-the-art compositional Imitation Learning approach by 30%. △ Less","25 July, 2022",https://arxiv.org/pdf/2205.07716
L3-Net Deep Audio Embeddings to Improve COVID-19 Detection from Smartphone Data,Mattia Giovanni Campana;Andrea Rovati;Franca Delmastro;Elena Pagani,"Smartphones and wearable devices, along with Artificial Intelligence, can represent a game-changer in the pandemic control, by implementing low-cost and pervasive solutions to recognize the development of new diseases at their early stages and by potentially avoiding the rise of new outbreaks. Some recent works show promise in detecting diagnostic signals of COVID-19 from voice and coughs by using machine learning and hand-crafted acoustic features. In this paper, we decided to investigate the capabilities of the recently proposed deep embedding model L3-Net to automatically extract meaningful features from raw respiratory audio recordings in order to improve the performances of standard machine learning classifiers in discriminating between COVID-19 positive and negative subjects from smartphone data. We evaluated the proposed model on 3 datasets, comparing the obtained results with those of two reference works. Results show that the combination of L3-Net with hand-crafted features overcomes the performance of the other works of 28.57% in terms of AUC in a set of subject-independent experiments. This result paves the way to further investigation on different deep audio embeddings, also for the automatic detection of different diseases. △ Less","16 May, 2022",https://arxiv.org/pdf/2205.07682
A Precis of Language Models are not Models of Language,Csaba Veres,"Natural Language Processing is one of the leading application areas in the current resurgence of Artificial Intelligence, spearheaded by Artificial Neural Networks. We show that despite their many successes at performing linguistic tasks, Large Neural Language Models are ill-suited as comprehensive models of natural language. The wider implication is that, in spite of the often overbearing optimism about AI, modern neural models do not represent a revolution in our understanding of cognition. △ Less","16 May, 2022",https://arxiv.org/pdf/2205.07634
A Low-latency Communication Design for Brain Simulations,Xin Du,"Brain simulation, as one of the latest advances in artificial intelligence, facilitates better understanding about how information is represented and processed in the brain. The extreme complexity of human brain makes brain simulations only feasible upon high-performance computing platforms. Supercomputers with a large number of interconnected graphical processing units (GPUs) are currently employed for supporting brain simulations. Therefore, high-throughput low-latency inter-GPU communications in supercomputers play a crucial role in meeting the performance requirements of brain simulation as a highly time-sensitive application. In this paper, we first provide an overview of the current parallelizing technologies for brain simulations using multi-GPU architectures. Then, we analyze the challenges to communications for brain simulation and summarize guidelines for communication design to address such challenges. Furthermore, we propose a partitioning algorithm and a two-level routing method to achieve efficient low-latency communications in multi-GPU architecture for brain simulation. We report experiment results obtained on a supercomputer with 2,000 GPUs for simulating a brain model with 10 billion neurons to show that our approach can significantly improve communication performance. We also discuss open issues and identify some research directions for low-latency communication design for brain simulations. △ Less","14 May, 2022",https://arxiv.org/pdf/2205.07125
Robust Design of Federated Learning for Edge-Intelligent Networks,Qiao Qi;Xiaoming Chen,"Mass data traffics, low-latency wireless services and advanced artificial intelligence (AI) technologies have driven the emergence of a new paradigm for wireless networks, namely edge-intelligent networks, which are more efficient and flexible than traditional cloud-intelligent networks. Considering users' privacy, model sharing-based federated learning (FL) that migrates model parameters but not private data from edge devices to a central cloud is particularly attractive for edge-intelligent networks. Due to multiple rounds of iterative updating of high-dimensional model parameters between base station (BS) and edge devices, the communication reliability is a critical issue of FL for edge-intelligent networks. We reveal the impacts of the errors generated during model broadcast and model aggregation via wireless channels caused by channel fading, interference and noise on the accuracy of FL, especially when there exists channel uncertainty. To alleviate the impacts, we propose a robust FL algorithm for edge-intelligent networks with channel uncertainty, which is formulated as a worst-case optimization problem with joint device selection and transceiver design. Finally, simulation results validate the robustness and effectiveness of the proposed algorithm. △ Less","13 May, 2022",https://arxiv.org/pdf/2205.06955
"Differentiable programming: Generalization, characterization and limitations of deep learning",Adrián Hernández;Gilles Millerioux;José M. Amigó,"In the past years, deep learning models have been successfully applied in several cognitive tasks. Originally inspired by neuroscience, these models are specific examples of differentiable programs. In this paper we define and motivate differentiable programming, as well as specify some program characteristics that allow us to incorporate the structure of the problem in a differentiable program. We analyze different types of differentiable programs, from more general to more specific, and evaluate, for a specific problem with a graph dataset, its structure and knowledge with several differentiable programs using those characteristics. Finally, we discuss some inherent limitations of deep learning and differentiable programs, which are key challenges in advancing artificial intelligence, and then analyze possible solutions △ Less","13 May, 2022",https://arxiv.org/pdf/2205.06898
Physics guided neural networks for modelling of non-linear dynamics,Haakon Robinson;Suraj Pawar;Adil Rasheed;Omer San,"The success of the current wave of artificial intelligence can be partly attributed to deep neural networks, which have proven to be very effective in learning complex patterns from large datasets with minimal human intervention. However, it is difficult to train these models on complex dynamical systems from data alone due to their low data efficiency and sensitivity to hyperparameters and initialisation. This work demonstrates that injection of partially known information at an intermediate layer in a DNN can improve model accuracy, reduce model uncertainty, and yield improved convergence during the training. The value of these physics-guided neural networks has been demonstrated by learning the dynamics of a wide variety of nonlinear dynamical systems represented by five well-known equations in nonlinear systems theory: the Lotka-Volterra, Duffing, Van der Pol, Lorenz, and Henon-Heiles systems. △ Less","13 May, 2022",https://arxiv.org/pdf/2205.06858
Emergent Bartering Behaviour in Multi-Agent Reinforcement Learning,Michael Bradley Johanson;Edward Hughes;Finbarr Timbers;Joel Z. Leibo,"Advances in artificial intelligence often stem from the development of new environments that abstract real-world situations into a form where research can be done conveniently. This paper contributes such an environment based on ideas inspired by elementary Microeconomics. Agents learn to produce resources in a spatially complex world, trade them with one another, and consume those that they prefer. We show that the emergent production, consumption, and pricing behaviors respond to environmental conditions in the directions predicted by supply and demand shifts in Microeconomics. We also demonstrate settings where the agents' emergent prices for goods vary over space, reflecting the local abundance of goods. After the price disparities emerge, some agents then discover a niche of transporting goods between regions with different prevailing prices -- a profitable strategy because they can buy goods where they are cheap and sell them where they are expensive. Finally, in a series of ablation experiments, we investigate how choices in the environmental rewards, bartering actions, agent architecture, and ability to consume tradable goods can either aid or inhibit the emergence of this economic behavior. This work is part of the environment development branch of a research program that aims to build human-like artificial general intelligence through multi-agent interactions in simulated societies. By exploring which environment features are needed for the basic phenomena of elementary microeconomics to emerge automatically from learning, we arrive at an environment that differs from those studied in prior multi-agent reinforcement learning work along several dimensions. For example, the model incorporates heterogeneous tastes and physical abilities, and agents negotiate with one another as a grounded form of communication. △ Less","13 May, 2022",https://arxiv.org/pdf/2205.06760
Neurochaos Feature Transformation and Classification for Imbalanced Learning,Deeksha Sethi;Nithin Nagaraj;Harikrishnan N B,"Learning from limited and imbalanced data is a challenging problem in the Artificial Intelligence community. Real-time scenarios demand decision-making from rare events wherein the data are typically imbalanced. These situations commonly arise in medical applications, cybersecurity, catastrophic predictions etc. This motivates the development of learning algorithms capable of learning from imbalanced data. Human brain effortlessly learns from imbalanced data. Inspired by the chaotic neuronal firing in the human brain, a novel learning algorithm namely Neurochaos Learning (NL) was recently proposed. NL is categorized in three blocks: Feature Transformation, Neurochaos Feature Extraction (CFX), and Classification. In this work, the efficacy of neurochaos feature transformation and extraction for classification in imbalanced learning is studied. We propose a unique combination of neurochaos based feature transformation and extraction with traditional ML algorithms. The explored datasets in this study revolve around medical diagnosis, banknote fraud detection, environmental applications and spoken-digit classification. In this study, experiments are performed in both high and low training sample regime. In the former, five out of nine datasets have shown a performance boost in terms of macro F1-score after using CFX features. The highest performance boost obtained is 25.97% for Statlog (Heart) dataset using CFX+Decision Tree. In the low training sample regime (from just one to nine training samples per class), the highest performance boost of 144.38% is obtained for Haberman's Survival dataset using CFX+Random Forest. NL offers enormous flexibility of combining CFX with any ML classifier to boost its performance, especially for learning tasks with limited and imbalanced data. △ Less","16 May, 2022",https://arxiv.org/pdf/2205.06742
Controlling chaotic itinerancy in laser dynamics for reinforcement learning,Ryugo Iwami;Takatomo Mihana;Kazutaka Kanno;Satoshi Sunada;Makoto Naruse;Atsushi Uchida,"Photonic artificial intelligence has attracted considerable interest in accelerating machine learning; however, the unique optical properties have not been fully utilized for achieving higher-order functionalities. Chaotic itinerancy, with its spontaneous transient dynamics among multiple quasi-attractors, can be employed to realize brain-like functionalities. In this paper, we propose a method for controlling the chaotic itinerancy in a multi-mode semiconductor laser to solve a machine learning task, known as the multi-armed bandit problem, which is fundamental to reinforcement learning. The proposed method utilizes ultrafast chaotic itinerant motion in mode competition dynamics controlled via optical injection. We found that the exploration mechanism is completely different from a conventional searching algorithm and is highly scalable, outperforming the conventional approaches for large-scale bandit problems. This study paves the way to utilize chaotic itinerancy for effectively solving complex machine learning tasks as photonic hardware accelerators. △ Less","12 May, 2022",https://arxiv.org/pdf/2205.05987
E-Mail Assistant -- Automation of E-Mail Handling and Management using Robotic Process Automation,Arpit Khare;Sudhakar Singh;Richa Mishra;Shiv Prakash;Pratibha Dixit,"In this paper, a workflow for designing a bot using Robotic Process Automation (RPA), associated with Artificial Intelligence (AI) that is used for information extraction, classification, etc., is proposed. The bot is equipped with many features that make email handling a stress-free job. It automatically login into the mailbox through secured channels, distinguishes between the useful and not useful emails, classifies the emails into different labels, downloads the attached files, creates different directories, and stores the downloaded files into relevant directories. It moves the not useful emails into the trash. Further, the bot can also be trained to rename the attached files with the names of the sender/applicant in case of a job application for the sake of convenience. The bot is designed and tested using the UiPath tool to improve the performance of the system. The paper also discusses the further possible functionalities that can be added on to the bot. △ Less","12 May, 2022",https://arxiv.org/pdf/2205.05882
Computational behavior recognition in child and adolescent psychiatry: A statistical and machine learning analysis plan,Nicole N. Lønfeldt;Flavia D. Frumosu;A. -R. Cecilie Mora-Jensen;Nicklas Leander Lund;Sneha Das;A. Katrine Pagsberg;Line K. H. Clemmensen,"Motivation: Behavioral observations are an important resource in the study and evaluation of psychological phenomena, but it is costly, time-consuming, and susceptible to bias. Thus, we aim to automate coding of human behavior for use in psychotherapy and research with the help of artificial intelligence (AI) tools. Here, we present an analysis plan. Methods: Videos of a gold-standard semi-structured diagnostic interview of 25 youth with obsessive-compulsive disorder (OCD) and 12 youth without a psychiatric diagnosis (no-OCD) will be analyzed. Youth were between 8 and 17 years old. Features from the videos will be extracted and used to compute ratings of behavior, which will be compared to ratings of behavior produced by mental health professionals trained to use a specific behavioral coding manual. We will test the effect of OCD diagnosis on the computationally-derived behavior ratings using multivariate analysis of variance (MANOVA). Using the generated features, a binary classification model will be built and used to classify OCD/no-OCD classes. Discussion: Here, we present a pre-defined plan for how data will be pre-processed, analyzed and presented in the publication of results and their interpretation. A challenge for the proposed study is that the AI approach will attempt to derive behavioral ratings based solely on vision, whereas humans use visual, paralinguistic and linguistic cues to rate behavior. Another challenge will be using machine learning models for body and facial movement detection trained primarily on adults and not on children. If the AI tools show promising results, this pre-registered analysis plan may help reduce interpretation bias. Trial registration: ClinicalTrials.gov - H-18010607 △ Less","11 May, 2022",https://arxiv.org/pdf/2205.05737
Review on Panoramic Imaging and Its Applications in Scene Understanding,Shaohua Gao;Kailun Yang;Hao Shi;Kaiwei Wang;Jian Bai,"With the rapid development of high-speed communication and artificial intelligence technologies, human perception of real-world scenes is no longer limited to the use of small Field of View (FoV) and low-dimensional scene detection devices. Panoramic imaging emerges as the next generation of innovative intelligent instruments for environmental perception and measurement. However, while satisfying the need for large-FoV photographic imaging, panoramic imaging instruments are expected to have high resolution, no blind area, miniaturization, and multidimensional intelligent perception, and can be combined with artificial intelligence methods towards the next generation of intelligent instruments, enabling deeper understanding and more holistic perception of 360-degree real-world surrounding environments. Fortunately, recent advances in freeform surfaces, thin-plate optics, and metasurfaces provide innovative approaches to address human perception of the environment, offering promising ideas beyond conventional optical imaging. In this review, we begin with introducing the basic principles of panoramic imaging systems, and then describe the architectures, features, and functions of various panoramic imaging systems. Afterwards, we discuss in detail the broad application prospects and great design potential of freeform surfaces, thin-plate optics, and metasurfaces in panoramic imaging. We then provide a detailed analysis on how these techniques can help enhance the performance of panoramic imaging systems. We further offer a detailed analysis of applications of panoramic imaging in scene understanding for autonomous driving and robotics, spanning panoramic semantic image segmentation, panoramic depth estimation, panoramic visual localization, and so on. Finally, we cast a perspective on future potential and research directions for panoramic imaging instruments. △ Less","14 October, 2022",https://arxiv.org/pdf/2205.05570
Performance of a deep learning system for detection of referable diabetic retinopathy in real clinical settings,Verónica Sánchez-Gutiérrez;Paula Hernández-Martínez;Francisco J. Muñoz-Negrete;Jonne Engelberts;Allison M. Luger;Mark J. J. P. van Grinsven,"Background: To determine the ability of a commercially available deep learning system, RetCAD v.1.3.1 (Thirona, Nijmegen, The Netherlands) for the automatic detection of referable diabetic retinopathy (DR) on a dataset of colour fundus images acquired during routine clinical practice in a tertiary hospital screening program, analyzing the reduction of workload that can be released incorporating this artificial intelligence-based technology. Methods: Evaluation of the software was performed on a dataset of 7195 nonmydriatic fundus images from 6325 eyes of 3189 diabetic patients attending our screening program between February to December of 2019. The software generated a DR severity score for each colour fundus image which was combined into an eye-level score. This score was then compared with a reference standard as set by a human expert using receiver operating characteristic (ROC) curve analysis. Results: The artificial intelligence (AI) software achieved an area under the ROC curve (AUC) value of 0.988 [0.981:0.993] for the detection of referable DR. At the proposed operating point, the sensitivity of the RetCAD software for DR is 90.53% and specificity is 97.13%. A workload reduction of 96% could be achieved at the cost of only 6 false negatives. Conclusions: The AI software correctly identified the vast majority of referable DR cases, with a workload reduction of 96% of the cases that would need to be checked, while missing almost no true cases, so it may therefore be used as an instrument for triage. △ Less","11 May, 2022",https://arxiv.org/pdf/2205.05554
Keep Your Friends Close and Your Counterfactuals Closer: Improved Learning From Closest Rather Than Plausible Counterfactual Explanations in an Abstract Setting,Ulrike Kuhl;André Artelt;Barbara Hammer,"Counterfactual explanations (CFEs) highlight what changes to a model's input would have changed its prediction in a particular way. CFEs have gained considerable traction as a psychologically grounded solution for explainable artificial intelligence (XAI). Recent innovations introduce the notion of computational plausibility for automatically generated CFEs, enhancing their robustness by exclusively creating plausible explanations. However, practical benefits of such a constraint on user experience and behavior is yet unclear. In this study, we evaluate objective and subjective usability of computationally plausible CFEs in an iterative learning design targeting novice users. We rely on a novel, game-like experimental design, revolving around an abstract scenario. Our results show that novice users actually benefit less from receiving computationally plausible rather than closest CFEs that produce minimal changes leading to the desired outcome. Responses in a post-game survey reveal no differences in terms of subjective user experience between both groups. Following the view of psychological plausibility as comparative similarity, this may be explained by the fact that users in the closest condition experience their CFEs as more psychologically plausible than the computationally plausible counterpart. In sum, our work highlights a little-considered divergence of definitions of computational plausibility and psychological plausibility, critically confirming the need to incorporate human behavior, preferences and mental models already at the design stages of XAI approaches. In the interest of reproducible research, all source code, acquired user data, and evaluation scripts of the current study are available: https://github.com/ukuhl/PlausibleAlienZoo △ Less","11 May, 2022",https://arxiv.org/pdf/2205.05515
Knowledge-powered Explainable Artificial Intelligence (XAI) for Network Automation Towards 6G,Yulei Wu;Guozhi Lin;Jingguo Ge,"Communication networks are becoming increasingly complex towards 6G. Manual management is no longer an option for network operators. Network automation has been widely discussed in the networking community, and it is a sensible means to manage the complex communication network. Deep learning models developed to enable network automation for given operation practices have the limitations of 1) lack of explainability and 2) inapplicable across different networks and/or network settings. To tackle the above issues, in this article we propose a new knowledge-powered framework that provides a human-understandable explainable artificial intelligence (XAI) agent for network automation. A case study of path selection is developed to demonstrate the feasibility of the proposed framework. Research on network automation is still in its infancy. Therefore, at the end of this article, we provide a list of challenges and open issues that can guide further research in this important area. △ Less","11 May, 2022",https://arxiv.org/pdf/2205.05406
The Conflict Between Explainable and Accountable Decision-Making Algorithms,Gabriel Lima;Nina Grgić-Hlača;Jin Keun Jeong;Meeyoung Cha,"Decision-making algorithms are being used in important decisions, such as who should be enrolled in health care programs and be hired. Even though these systems are currently deployed in high-stakes scenarios, many of them cannot explain their decisions. This limitation has prompted the Explainable Artificial Intelligence (XAI) initiative, which aims to make algorithms explainable to comply with legal requirements, promote trust, and maintain accountability. This paper questions whether and to what extent explainability can help solve the responsibility issues posed by autonomous AI systems. We suggest that XAI systems that provide post-hoc explanations could be seen as blameworthy agents, obscuring the responsibility of developers in the decision-making process. Furthermore, we argue that XAI could result in incorrect attributions of responsibility to vulnerable stakeholders, such as those who are subjected to algorithmic decisions (i.e., patients), due to a misguided perception that they have control over explainable algorithms. This conflict between explainability and accountability can be exacerbated if designers choose to use algorithms and patients as moral and legal scapegoats. We conclude with a set of recommendations for how to approach this tension in the socio-technical process of algorithmic decision-making and a defense of hard regulation to prevent designers from escaping responsibility. △ Less","11 May, 2022",https://arxiv.org/pdf/2205.05306
Social Inclusion in Curated Contexts: Insights from Museum Practices,Han-Yin Huang;Cynthia C. S. Liem,"Artificial intelligence literature suggests that minority and fragile communities in society can be negatively impacted by machine learning algorithms due to inherent biases in the design process, which lead to socially exclusive decisions and policies. Faced with similar challenges in dealing with an increasingly diversified audience, the museum sector has seen changes in theory and practice, particularly in the areas of representation and meaning-making. While rarity and grandeur used to be at the centre stage of the early museum practices, folk life and museums' relationships with the diverse communities they serve become a widely integrated part of the contemporary practices. These changes address issues of diversity and accessibility in order to offer more socially inclusive services. Drawing on these changes and reflecting back on the AI world, we argue that the museum experience provides useful lessons for building AI with socially inclusive approaches, especially in situations in which both a collection and access to it will need to be curated or filtered, as frequently happens in search engines, recommender systems and digital libraries. We highlight three principles: (1) Instead of upholding the value of neutrality, practitioners are aware of the influences of their own backgrounds and those of others on their work. By not claiming to be neutral but practising cultural humility, the chances of addressing potential biases can be increased. (2) There should be room for situational interpretation beyond the stages of data collection and machine learning. Before applying models and predictions, the contexts in which relevant parties exist should be taken into account. (3) Community participation serves the needs of communities and has the added benefit of bringing practitioners and communities together. △ Less","10 May, 2022",https://arxiv.org/pdf/2205.05192
Massively Digitized Power Grid: Opportunities and Challenges of Use-inspired AI,Le Xie;Xiangtian Zheng;Yannan Sun;Tong Huang;Tony Bruton,"This article presents a use-inspired perspective of the opportunities and challenges in a massively digitized power grid. It argues that the intricate interplay of data availability, computing capability, and artificial intelligence (AI) algorithm development are the three key factors driving the adoption of digitized solutions in the power grid. The impact of these three factors on critical functions of power system operation and planning practices are reviewed and illustrated with industrial practice case studies. Open challenges and research opportunities for data, computing, and AI algorithms are articulated within the context of the power industry's tremendous decarbonization efforts. △ Less","10 May, 2022",https://arxiv.org/pdf/2205.05180
A Meta-Analysis of the Utility of Explainable Artificial Intelligence in Human-AI Decision-Making,Max Schemmer;Patrick Hemmer;Maximilian Nitsche;Niklas Kühl;Michael Vössing,"Research in artificial intelligence (AI)-assisted decision-making is experiencing tremendous growth with a constantly rising number of studies evaluating the effect of AI with and without techniques from the field of explainable AI (XAI) on human decision-making performance. However, as tasks and experimental setups vary due to different objectives, some studies report improved user decision-making performance through XAI, while others report only negligible effects. Therefore, in this article, we present an initial synthesis of existing research on XAI studies using a statistical meta-analysis to derive implications across existing research. We observe a statistically positive impact of XAI on users' performance. Additionally, the first results indicate that human-AI decision-making tends to yield better task performance on text data. However, we find no effect of explanations on users' performance compared to sole AI predictions. Our initial synthesis gives rise to future research investigating the underlying causes and contributes to further developing algorithms that effectively benefit human decision-makers by providing meaningful explanations. △ Less","1 June, 2022",https://arxiv.org/pdf/2205.05126
Model-Contrastive Learning for Backdoor Defense,Zhihao Yue;Jun Xia;Zhiwei Ling;Ming Hu;Ting Wang;Xian Wei;Mingsong Chen,"Due to the popularity of Artificial Intelligence (AI) techniques, we are witnessing an increasing number of backdoor injection attacks that are designed to maliciously threaten Deep Neural Networks (DNNs) causing misclassification. Although there exist various defense methods that can effectively erase backdoors from DNNs, they greatly suffer from both high Attack Success Rate (ASR) and a non-negligible loss in Benign Accuracy (BA). Inspired by the observation that a backdoored DNN tends to form a new cluster in its feature spaces for poisoned data, in this paper we propose a novel two-stage backdoor defense method, named MCLDef, based on Model-Contrastive Learning (MCL). In the first stage, our approach performs trigger inversion based on trigger synthesis, where the resultant trigger can be used to generate poisoned data. In the second stage, under the guidance of MCL and our defined positive and negative pairs, MCLDef can purify the backdoored model by pulling the feature representations of poisoned data towards those of their clean data counterparts. Due to the shrunken cluster of poisoned data, the backdoor formed by end-to-end supervised learning is eliminated. Comprehensive experimental results show that, with only 5% of clean data, MCLDef significantly outperforms state-of-the-art defense methods by up to 95.79% reduction in ASR, while in most cases the BA degradation can be controlled within less than 2%. Our code is available at https://github.com/WeCanShow/MCL. △ Less","17 May, 2022",https://arxiv.org/pdf/2205.04411
Aligned with Whom? Direct and social goals for AI systems,Anton Korinek;Avital Balwit,"As artificial intelligence (AI) becomes more powerful and widespread, the AI alignment problem - how to ensure that AI systems pursue the goals that we want them to pursue - has garnered growing attention. This article distinguishes two types of alignment problems depending on whose goals we consider, and analyzes the different solutions necessitated by each. The direct alignment problem considers whether an AI system accomplishes the goals of the entity operating it. In contrast, the social alignment problem considers the effects of an AI system on larger groups or on society more broadly. In particular, it also considers whether the system imposes externalities on others. Whereas solutions to the direct alignment problem center around more robust implementation, social alignment problems typically arise because of conflicts between individual and group-level goals, elevating the importance of AI governance to mediate such conflicts. Addressing the social alignment problem requires both enforcing existing norms on their developers and operators and designing new norms that apply directly to AI systems. △ Less","9 May, 2022",https://arxiv.org/pdf/2205.04279
Robotic Maintenance of Road Infrastructures: The HERON Project,Iason Katsamenis;Matthaios Bimpas;Eftychios Protopapadakis;Charalampos Zafeiropoulos;Dimitris Kalogeras;Anastasios Doulamis;Nikolaos Doulamis;Carlos Martín-Portugués Montoliu;Yannis Handanos;Franziska Schmidt;Lionel Ott;Miquel Cantero;Rafael Lopez,"Of all public assets, road infrastructure tops the list. Roads are crucial for economic development and growth, providing access to education, health, and employment. The maintenance, repair, and upgrade of roads are therefore vital to road users' health and safety as well as to a well-functioning and prosperous modern economy. The EU-funded HERON project will develop an integrated automated system to adequately maintain road infrastructure. In turn, this will reduce accidents, lower maintenance costs, and increase road network capacity and efficiency. To coordinate maintenance works, the project will design an autonomous ground robotic vehicle that will be supported by autonomous drones. Sensors and scanners for 3D mapping will be used in addition to artificial intelligence toolkits to help coordinate road maintenance and upgrade workflows. △ Less","9 May, 2022",https://arxiv.org/pdf/2205.04164
The Roles and Modes of Human Interactions with Automated Machine Learning Systems,Thanh Tung Khuat;David Jacob Kedziora;Bogdan Gabrys,"As automated machine learning (AutoML) systems continue to progress in both sophistication and performance, it becomes important to understand the `how' and `why' of human-computer interaction (HCI) within these frameworks, both current and expected. Such a discussion is necessary for optimal system design, leveraging advanced data-processing capabilities to support decision-making involving humans, but it is also key to identifying the opportunities and risks presented by ever-increasing levels of machine autonomy. Within this context, we focus on the following questions: (i) How does HCI currently look like for state-of-the-art AutoML algorithms, especially during the stages of development, deployment, and maintenance? (ii) Do the expectations of HCI within AutoML frameworks vary for different types of users and stakeholders? (iii) How can HCI be managed so that AutoML solutions acquire human trust and broad acceptance? (iv) As AutoML systems become more autonomous and capable of learning from complex open-ended environments, will the fundamental nature of HCI evolve? To consider these questions, we project existing literature in HCI into the space of AutoML; this connection has, to date, largely been unexplored. In so doing, we review topics including user-interface design, human-bias mitigation, and trust in artificial intelligence (AI). Additionally, to rigorously gauge the future of HCI, we contemplate how AutoML may manifest in effectively open-ended environments. This discussion necessarily reviews projected developmental pathways for AutoML, such as the incorporation of reasoning, although the focus remains on how and why HCI may occur in such a framework rather than on any implementational details. Ultimately, this review serves to identify key research directions aimed at better facilitating the roles and modes of human interactions with both current and future AutoML systems. △ Less","9 May, 2022",https://arxiv.org/pdf/2205.04139
AI Based Digital Twin Model for Cattle Caring,Xue Han;Zihuai Lin,"In this paper, we developed innovative digital twins of cattle status that are powered by artificial intelligence (AI). The work was built on a farm IoT system that remotely monitors and tracks the state of cattle. A digital twin model of cattle health based on Deep Learning (DL) was generated using the sensor data acquired from the farm IoT system. The health and physiological cycle of cattle can be monitored in real time, and the state of the next physiological cycle of cattle can be anticipated using this model. The basis of this work is the vast amount of data which is required to validate the legitimacy of the digital twins model. In terms of behavioural state, it was found that the cattle treated with a combination of topical anaesthetic and meloxicam exhibits the least pain reaction. The digital twins model developed in this work can be used to monitor the health of cattle △ Less","9 May, 2022",https://arxiv.org/pdf/2205.04034
Data Augmentation with Paraphrase Generation and Entity Extraction for Multimodal Dialogue System,Eda Okur;Saurav Sahay;Lama Nachman,"Contextually aware intelligent agents are often required to understand the users and their surroundings in real-time. Our goal is to build Artificial Intelligence (AI) systems that can assist children in their learning process. Within such complex frameworks, Spoken Dialogue Systems (SDS) are crucial building blocks to handle efficient task-oriented communication with children in game-based learning settings. We are working towards a multimodal dialogue system for younger kids learning basic math concepts. Our focus is on improving the Natural Language Understanding (NLU) module of the task-oriented SDS pipeline with limited datasets. This work explores the potential benefits of data augmentation with paraphrase generation for the NLU models trained on small task-specific datasets. We also investigate the effects of extracting entities for conceivably further data expansion. We have shown that paraphrasing with model-in-the-loop (MITL) strategies using small seed data is a promising approach yielding improved performance results for the Intent Recognition task. △ Less","8 May, 2022",https://arxiv.org/pdf/2205.04006
A Survey on AI Sustainability: Emerging Trends on Learning Algorithms and Research Challenges,Zhenghua Chen;Min Wu;Alvin Chan;Xiaoli Li;Yew-Soon Ong,"Artificial Intelligence (AI) is a fast-growing research and development (R&D) discipline which is attracting increasing attention because of its promises to bring vast benefits for consumers and businesses, with considerable benefits promised in productivity growth and innovation. To date it has reported significant accomplishments in many areas that have been deemed as challenging for machines, ranging from computer vision, natural language processing, audio analysis to smart sensing and many others. The technical trend in realizing the successes has been towards increasing complex and large size AI models so as to solve more complex problems at superior performance and robustness. This rapid progress, however, has taken place at the expense of substantial environmental costs and resources. Besides, debates on the societal impacts of AI, such as fairness, safety and privacy, have continued to grow in intensity. These issues have presented major concerns pertaining to the sustainable development of AI. In this work, we review major trends in machine learning approaches that can address the sustainability problem of AI. Specifically, we examine emerging AI methodologies and algorithms for addressing the sustainability issue of AI in two major aspects, i.e., environmental sustainability and social sustainability of AI. We will also highlight the major limitations of existing studies and propose potential research challenges and directions for the development of next generation of sustainable AI techniques. We believe that this technical review can help to promote a sustainable development of AI R&D activities for the research community. △ Less","8 May, 2022",https://arxiv.org/pdf/2205.03824
Pervasive Machine Learning for Smart Radio Environments Enabled by Reconfigurable Intelligent Surfaces,George C. Alexandropoulos;Kyriakos Stylianopoulos;Chongwen Huang;Chau Yuen;Mehdi Bennis;Mérouane Debbah,"The emerging technology of Reconfigurable Intelligent Surfaces (RISs) is provisioned as an enabler of smart wireless environments, offering a highly scalable, low-cost, hardware-efficient, and almost energy-neutral solution for dynamic control of the propagation of electromagnetic signals over the wireless medium, ultimately providing increased environmental intelligence for diverse operation objectives. One of the major challenges with the envisioned dense deployment of RISs in such reconfigurable radio environments is the efficient configuration of multiple metasurfaces with limited, or even the absence of, computing hardware. In this paper, we consider multi-user and multi-RIS-empowered wireless systems, and present a thorough survey of the online machine learning approaches for the orchestration of their various tunable components. Focusing on the sum-rate maximization as a representative design objective, we present a comprehensive problem formulation based on Deep Reinforcement Learning (DRL). We detail the correspondences among the parameters of the wireless system and the DRL terminology, and devise generic algorithmic steps for the artificial neural network training and deployment, while discussing their implementation details. Further practical considerations for multi-RIS-empowered wireless communications in the sixth Generation (6G) era are presented along with some key open research challenges. Differently from the DRL-based status quo, we leverage the independence between the configuration of the system design parameters and the future states of the wireless environment, and present efficient multi-armed bandits approaches, whose resulting sum-rate performances are numerically shown to outperform random configurations, while being sufficiently close to the conventional Deep Q-Network (DQN) algorithm, but with lower implementation complexity. △ Less","8 May, 2022",https://arxiv.org/pdf/2205.03793
Playing Tic-Tac-Toe Games with Intelligent Single-pixel Imaging,Shuming Jiao;Jiaxiang Li;Wei Huang;Zibang Zhang,"Single-pixel imaging (SPI) is a novel optical imaging technique by replacing a two-dimensional pixelated sensor with a single-pixel detector and pattern illuminations. SPI have been extensively used for various tasks related to image acquisition and processing. In this work, a novel non-image-based task of playing Tic-Tac-Toe games interactively is merged into the framework of SPI. An optoelectronic artificial intelligent (AI) player with minimal digital computation can detect the game states, generate optimal moves and display output results mainly by pattern illumination and single-pixel detection. Simulated and experimental results demonstrate the feasibility of proposed scheme and its unbeatable performance against human players. △ Less","7 May, 2022",https://arxiv.org/pdf/2205.03663
Anomaly Detection in Intra-Vehicle Networks,Ajeet Kumar Dwivedi,"The progression of innovation and technology and ease of inter-connectivity among networks has allowed us to evolve towards one of the promising areas, the Internet of Vehicles. Nowadays, modern vehicles are connected to a range of networks, including intra-vehicle networks and external networks. However, a primary challenge in the automotive industry is to make the vehicle safe and reliable; particularly with the loopholes in the existing traditional protocols, cyber-attacks on the vehicle network are rising drastically. Practically every vehicle uses the universal Controller Area Network (CAN) bus protocol for the communication between electronic control units to transmit key vehicle functionality and messages related to driver safety. The CAN bus system, although its critical significance, lacks the key feature of any protocol authentication and authorization. Resulting in compromises of CAN bus security leads to serious issues to both car and driver safety. This paper discusses the security issues of the CAN bus protocol and proposes an Intrusion Detection System (IDS) that detects known attacks on in-vehicle networks. Multiple Artificial Intelligence (AI) algorithms are employed to provide recognition of known potential cyber-attacks based on messages, timestamps, and data packets traveling through the CAN. The main objective of this paper is to accurately detect cyberattacks by considering time-series features and attack frequency. The majority of the evaluated AI algorithms, when considering attack frequency, correctly identify known attacks with remarkable accuracy of more than 99%. However, these models achieve approximately 92% to 97% accuracy when timestamps are not taken into account. Long Short Term Memory (LSTM), Xgboost, and SVC have proved to the well-performing classifiers. △ Less","6 May, 2022",https://arxiv.org/pdf/2205.03537
The AI Index 2022 Annual Report,Daniel Zhang;Nestor Maslej;Erik Brynjolfsson;John Etchemendy;Terah Lyons;James Manyika;Helen Ngo;Juan Carlos Niebles;Michael Sellitto;Ellie Sakhaee;Yoav Shoham;Jack Clark;Raymond Perrault,"Welcome to the fifth edition of the AI Index Report! The latest edition includes data from a broad set of academic, private, and nonprofit organizations as well as more self-collected data and original analysis than any previous editions, including an expanded technical performance chapter, a new survey of robotics researchers around the world, data on global AI legislation records in 25 countries, and a new chapter with an in-depth analysis of technical AI ethics metrics. The AI Index Report tracks, collates, distills, and visualizes data related to artificial intelligence. Its mission is to provide unbiased, rigorously vetted, and globally sourced data for policymakers, researchers, executives, journalists, and the general public to develop a more thorough and nuanced understanding of the complex field of AI. The report aims to be the world's most credible and authoritative source for data and insights about AI. △ Less","2 May, 2022",https://arxiv.org/pdf/2205.03468
Let's Go to the Alien Zoo: Introducing an Experimental Framework to Study Usability of Counterfactual Explanations for Machine Learning,Ulrike Kuhl;André Artelt;Barbara Hammer,"To foster usefulness and accountability of machine learning (ML), it is essential to explain a model's decisions in addition to evaluating its performance. Accordingly, the field of explainable artificial intelligence (XAI) has resurfaced as a topic of active research, offering approaches to address the ""how"" and ""why"" of automated decision-making. Within this domain, counterfactual explanations (CFEs) have gained considerable traction as a psychologically grounded approach to generate post-hoc explanations. To do so, CFEs highlight what changes to a model's input would have changed its prediction in a particular way. However, despite the introduction of numerous CFE approaches, their usability has yet to be thoroughly validated at the human level. Thus, to advance the field of XAI, we introduce the Alien Zoo, an engaging, web-based and game-inspired experimental framework. The Alien Zoo provides the means to evaluate usability of CFEs for gaining new knowledge from an automated system, targeting novice users in a domain-general context. As a proof of concept, we demonstrate the practical efficacy and feasibility of this approach in a user study. Our results suggest that users benefit from receiving CFEs compared to no explanation, both in terms of objective performance in the proposed iterative learning task, and subjective usability. With this work, we aim to equip research groups and practitioners with the means to easily run controlled and well-powered user studies to complement their otherwise often more technology-oriented work. Thus, in the interest of reproducible research, we provide the entire code, together with the underlying models and user data. △ Less","6 May, 2022",https://arxiv.org/pdf/2205.03398
A CNN Approach for 5G mmWave Positioning Using Beamformed CSI Measurements,Ghazaleh Kia;Laura Ruotsalainen;Jukka Talvitie,"The advent of Artificial Intelligence (AI) has impacted all aspects of human life. One of the concrete examples of AI impact is visible in radio positioning. In this article, for the first time we utilize the power of AI by training a Convolutional Neural Network (CNN) using 5G New Radio (NR) fingerprints consisting of beamformed Channel State Information (CSI). By observing CSI, it is possible to characterize the multipath channel between the transmitter and the receiver, and thus provide a good source of spatiotemporal data to find the position of a User Equipment (UE). We collect ray-tracing-based 5G NR CSI from an urban area. The CSI data of the signals from one Base Station (BS) is collected at the reference points with known positions to train a CNN. We evaluate our work by testing: a) the robustness of the trained network for estimating the positions for the new measurements on the same reference points and b) the accuracy of the CNN-based position estimation while the UE is on points other than the reference points. The results prove that our trained network for a specific urban environment can estimate the UE position with a minimum mean error of 0.98 m. △ Less","30 April, 2022",https://arxiv.org/pdf/2205.03236
"A Review on Text-Based Emotion Detection -- Techniques, Applications, Datasets, and Future Directions",Sheetal Kusal;Shruti Patil;Jyoti Choudrie;Ketan Kotecha;Deepali Vora;Ilias Pappas,"Artificial Intelligence (AI) has been used for processing data to make decisions, interact with humans, and understand their feelings and emotions. With the advent of the internet, people share and express their thoughts on day-to-day activities and global and local events through text messaging applications. Hence, it is essential for machines to understand emotions in opinions, feedback, and textual dialogues to provide emotionally aware responses to users in today's online world. The field of text-based emotion detection (TBED) is advancing to provide automated solutions to various applications, such as businesses, and finances, to name a few. TBED has gained a lot of attention in recent times. The paper presents a systematic literature review of the existing literature published between 2005 to 2021 in TBED. This review has meticulously examined 63 research papers from IEEE, Science Direct, Scopus, and Web of Science databases to address four primary research questions. It also reviews the different applications of TBED across various research domains and highlights its use. An overview of various emotion models, techniques, feature extraction methods, datasets, and research challenges with future directions has also been represented. △ Less","26 April, 2022",https://arxiv.org/pdf/2205.03235
Trust-SIoT: Towards Trustworthy Object Classification in the Social Internet of Things,Subhash Sagar;Adnan Mahmood;Kai Wang;Quan Z. Sheng;Wei Emma Zhang,"The recent emergence of the promising paradigm of the Social Internet of Things (SIoT) is a result of an intelligent amalgamation of the social networking concepts with the Internet of Things (IoT) objects (also referred to as ""things"") in an attempt to unravel the challenges of network discovery, navigability, and service composition. This is realized by facilitating the IoT objects to socialize with one another, i.e., similar to the social interactions amongst the human beings. A fundamental issue that mandates careful attention is to thus establish, and over time, maintain trustworthy relationships amongst these IoT objects. Therefore, a trust framework for SIoT must include object-object interactions, the aspects of social relationships, credible recommendations, etc., however, the existing literature has only focused on some aspects of trust by primarily relying on the conventional approaches that govern linear relationships between input and output. In this paper, an artificial neural network-based trust framework, Trust-SIoT, has been envisaged for identifying the complex non-linear relationships between input and output in a bid to classify the trustworthy objects. Moreover, Trust-SIoT has been designed for capturing a number of key trust metrics as input, i.e., direct trust by integrating both current and past interactions, reliability, and benevolence of an object, credible recommendations, and the degree of relationship by employing a knowledge graph embedding. Finally, we have performed extensive experiments to evaluate the performance of Trust-SIoT vis-a-vis state-of-the-art heuristics on two real-world datasets. The results demonstrate that Trust-SIoT achieves a higher F1 and lower MAE and MSE scores. △ Less","3 May, 2022",https://arxiv.org/pdf/2205.03226
"Metaversal Learning Environments: Measuring, predicting and improving interpersonal effectiveness",Arjun Nagendran;Scott Compton;William Follette;Artem Golenchenko;Anna Compton;Jonathan Grizou,"Experiential learning has been known to be an engaging and effective modality for personal and professional development. The Metaverse provides ample opportunities for the creation of environments in which such experiential learning can occur. In this work, we introduce a novel architecture that combines Artificial intelligence and Virtual Reality to create a highly immersive and efficient learning experience using avatars. The framework allows us to measure the interpersonal effectiveness of an individual interacting with the avatar. We first present a small pilot study and its results which were used to enhance the framework. We then present a larger study using the enhanced framework to measure, assess, and predict the interpersonal effectiveness of individuals interacting with an avatar. Results reveal that individuals with deficits in their interpersonal effectiveness show a significant improvement in performance after multiple interactions with an avatar. The results also reveal that individuals interact naturally with avatars within this framework, and exhibit similar behavioral traits as they would in the real world. We use this as a basis to analyze the underlying audio and video data streams of individuals during these interactions. Finally, we extract relevant features from these data and present a machine-learning based approach to predict interpersonal effectiveness during human-avatar conversation. We conclude by discussing the implications of these findings to build beneficial applications for the real world. △ Less","5 May, 2022",https://arxiv.org/pdf/2205.02875
Understanding Transfer Learning for Chest Radiograph Clinical Report Generation with Modified Transformer Architectures,Edward Vendrow;Ethan Schonfeld,"The image captioning task is increasingly prevalent in artificial intelligence applications for medicine. One important application is clinical report generation from chest radiographs. The clinical writing of unstructured reports is time consuming and error-prone. An automated system would improve standardization, error reduction, time consumption, and medical accessibility. In this paper we demonstrate the importance of domain specific pre-training and propose a modified transformer architecture for the medical image captioning task. To accomplish this, we train a series of modified transformers to generate clinical reports from chest radiograph image input. These modified transformers include: a meshed-memory augmented transformer architecture with visual extractor using ImageNet pre-trained weights, a meshed-memory augmented transformer architecture with visual extractor using CheXpert pre-trained weights, and a meshed-memory augmented transformer whose encoder is passed the concatenated embeddings using both ImageNet pre-trained weights and CheXpert pre-trained weights. We use BLEU(1-4), ROUGE-L, CIDEr, and the clinical CheXbert F1 scores to validate our models and demonstrate competitive scores with state of the art models. We provide evidence that ImageNet pre-training is ill-suited for the medical image captioning task, especially for less frequent conditions (eg: enlarged cardiomediastinum, lung lesion, pneumothorax). Furthermore, we demonstrate that the double feature model improves performance for specific medical conditions (edema, consolidation, pneumothorax, support devices) and overall CheXbert F1 score, and should be further developed in future work. Such a double feature model, including both ImageNet pre-training as well as domain specific pre-training, could be used in a wide range of image captioning models in medicine. △ Less","4 May, 2022",https://arxiv.org/pdf/2205.02841
RaFoLa: A Rationale-Annotated Corpus for Detecting Indicators of Forced Labour,Erick Mendez Guzman;Viktor Schlegel;Riza Batista-Navarro,"Forced labour is the most common type of modern slavery, and it is increasingly gaining the attention of the research and social community. Recent studies suggest that artificial intelligence (AI) holds immense potential for augmenting anti-slavery action. However, AI tools need to be developed transparently in cooperation with different stakeholders. Such tools are contingent on the availability and access to domain-specific data, which are scarce due to the near-invisible nature of forced labour. To the best of our knowledge, this paper presents the first openly accessible English corpus annotated for multi-class and multi-label forced labour detection. The corpus consists of 989 news articles retrieved from specialised data sources and annotated according to risk indicators defined by the International Labour Organization (ILO). Each news article was annotated for two aspects: (1) indicators of forced labour as classification labels and (2) snippets of the text that justify labelling decisions. We hope that our data set can help promote research on explainability for multi-class and multi-label text classification. In this work, we explain our process for collecting the data underpinning the proposed corpus, describe our annotation guidelines and present some statistical analysis of its content. Finally, we summarise the results of baseline experiments based on different variants of the Bidirectional Encoder Representation from Transformer (BERT) model. △ Less","5 May, 2022",https://arxiv.org/pdf/2205.02684
Time-multiplexed Neural Holography: A flexible framework for holographic near-eye displays with fast heavily-quantized spatial light modulators,Suyeon Choi;Manu Gopakumar;Yifan;Peng;Jonghyun Kim;Matthew O'Toole;Gordon Wetzstein,"Holographic near-eye displays offer unprecedented capabilities for virtual and augmented reality systems, including perceptually important focus cues. Although artificial intelligence--driven algorithms for computer-generated holography (CGH) have recently made much progress in improving the image quality and synthesis efficiency of holograms, these algorithms are not directly applicable to emerging phase-only spatial light modulators (SLM) that are extremely fast but offer phase control with very limited precision. The speed of these SLMs offers time multiplexing capabilities, essentially enabling partially-coherent holographic display modes. Here we report advances in camera-calibrated wave propagation models for these types of holographic near-eye displays and we develop a CGH framework that robustly optimizes the heavily quantized phase patterns of fast SLMs. Our framework is flexible in supporting runtime supervision with different types of content, including 2D and 2.5D RGBD images, 3D focal stacks, and 4D light fields. Using our framework, we demonstrate state-of-the-art results for all of these scenarios in simulation and experiment. △ Less","4 May, 2022",https://arxiv.org/pdf/2205.02367
GitRank: A Framework to Rank GitHub Repositories,Niranjan Hasabnis,"Open-source repositories provide wealth of information and are increasingly being used to build artificial intelligence (AI) based systems to solve problems in software engineering. Open-source repositories could be of varying quality levels, and bad-quality repositories could degrade performance of these systems. Evaluating quality of open-source repositories, which is not available directly on code hosting sites such as GitHub, is thus important. In this hackathon, we utilize known code quality measures and GrimoireLab toolkit to implement a framework, named GitRank, to rank open-source repositories on three different criteria. We discuss our findings and preliminary evaluation in this hackathon report. △ Less","4 May, 2022",https://arxiv.org/pdf/2205.02360
VSCNN: Convolution Neural Network Accelerator With Vector Sparsity,Kuo-Wei Chang;Tian-Sheuan Chang,"Hardware accelerator for convolution neural network (CNNs) enables real time applications of artificial intelligence technology. However, most of the accelerators only support dense CNN computations or suffers complex control to support fine grained sparse networks. To solve above problem, this paper presents an efficient CNN accelerator with 1-D vector broadcasted input to support both dense network as well as vector sparse network with the same hardware and low overhead. The presented design achieves 1.93X speedup over the dense CNN computations. △ Less","2 May, 2022",https://arxiv.org/pdf/2205.02271
VWA: Hardware Efficient Vectorwise Accelerator for Convolutional Neural Network,Kuo-Wei Chang;Tian-Sheuan Chang,"Hardware accelerators for convolution neural networks (CNNs) enable real-time applications of artificial intelligence technology. However, most of the existing designs suffer from low hardware utilization or high area cost due to complex dataflow. This paper proposes a hardware efficient vectorwise CNN accelerator that adopts a 3\times3 filter optimized systolic array using 1-D broadcast dataflow to generate partial sum. This enables easy reconfiguration for different kinds of kernels with interleaved input or elementwise input dataflow. This simple and regular data flow results in low area cost while attains high hardware utilization. The presented design achieves 99\%, 97\%, 93.7\%, 94\% hardware utilization for VGG-16, ResNet-34, GoogLeNet, and Mobilenet, respectively. Hardware implementation with TSMC 40nm technology takes 266.9K NAND gate count and 191KB SRAM to support 168GOPS throughput and consumes only 154.98mW when running at 500MHz operating frequency, which has superior area and power efficiency than other designs. △ Less","2 May, 2022",https://arxiv.org/pdf/2205.02270
ASP-Based Declarative Process Mining (Extended Abstract),Francesco Chiariello;Fabrizio Maria Maggi;Fabio Patrizi,"We propose Answer Set Programming (ASP) as an approach for modeling and solving problems from the area of Declarative Process Mining (DPM). We consider here three classical problems, namely, Log Generation, Conformance Checking, and Query Checking. These problems are addressed from both a control-flow and a data-aware perspective. The approach is based on the representation of process specifications as (finite-state) automata. Since these are strictly more expressive than the de facto DPM standard specification language DECLARE, more general specifications than those typical of DPM can be handled, such as formulas in linear-time temporal logic over finite traces. (Full version available in the Proceedings of the 36th AAAI Conference on Artificial Intelligence). △ Less","26 September, 2022",https://arxiv.org/pdf/2205.01979
Great Truths are Always Simple: A Rather Simple Knowledge Encoder for Enhancing the Commonsense Reasoning Capacity of Pre-Trained Models,Jinhao Jiang;Kun Zhou;Wayne Xin Zhao;Ji-Rong Wen,"Commonsense reasoning in natural language is a desired ability of artificial intelligent systems. For solving complex commonsense reasoning tasks, a typical solution is to enhance pre-trained language models~(PTMs) with a knowledge-aware graph neural network~(GNN) encoder that models a commonsense knowledge graph~(CSKG). Despite the effectiveness, these approaches are built on heavy architectures, and can't clearly explain how external knowledge resources improve the reasoning capacity of PTMs. Considering this issue, we conduct a deep empirical analysis, and find that it is indeed relation features from CSKGs (but not node features) that mainly contribute to the performance improvement of PTMs. Based on this finding, we design a simple MLP-based knowledge encoder that utilizes statistical relation paths as features. Extensive experiments conducted on five benchmarks demonstrate the effectiveness of our approach, which also largely reduces the parameters for encoding CSKGs. Our codes and data are publicly available at https://github.com/RUCAIBox/SAFE. △ Less","3 May, 2022",https://arxiv.org/pdf/2205.01841
The Brazilian Data at Risk in the Age of AI?,Raoni F. da S. Teixeira;Rafael B. Januzi;Fabio A. Faria,"Advances in image processing and analysis as well as machine learning techniques have contributed to the use of biometric recognition systems in daily people tasks. These tasks range from simple access to mobile devices to tagging friends in photos shared on social networks and complex financial operations on self-service devices for banking transactions. In China, the use of these systems goes beyond personal use becoming a country's government policy with the objective of monitoring the behavior of its population. On July 05th 2021, the Brazilian government announced acquisition of a biometric recognition system to be used nationwide. In the opposite direction to China, Europe and some American cities have already started the discussion about the legality of using biometric systems in public places, even banning this practice in their territory. In order to open a deeper discussion about the risks and legality of using these systems, this work exposes the vulnerabilities of biometric recognition systems, focusing its efforts on the face modality. Furthermore, it shows how it is possible to fool a biometric system through a well-known presentation attack approach in the literature called morphing. Finally, a list of ten concerns was created to start the discussion about the security of citizen data and data privacy law in the Age of Artificial Intelligence (AI). △ Less","14 December, 2022",https://arxiv.org/pdf/2205.01772
On the Effect of Information Asymmetry in Human-AI Teams,Patrick Hemmer;Max Schemmer;Niklas Kühl;Michael Vössing;Gerhard Satzger,"Over the last years, the rising capabilities of artificial intelligence (AI) have improved human decision-making in many application areas. Teaming between AI and humans may even lead to complementary team performance (CTP), i.e., a level of performance beyond the ones that can be reached by AI or humans individually. Many researchers have proposed using explainable AI (XAI) to enable humans to rely on AI advice appropriately and thereby reach CTP. However, CTP is rarely demonstrated in previous work as often the focus is on the design of explainability, while a fundamental prerequisite -- the presence of complementarity potential between humans and AI -- is often neglected. Therefore, we focus on the existence of this potential for effective human-AI decision-making. Specifically, we identify information asymmetry as an essential source of complementarity potential, as in many real-world situations, humans have access to different contextual information. By conducting an online experiment, we demonstrate that humans can use such contextual information to adjust the AI's decision, finally resulting in CTP. △ Less","3 May, 2022",https://arxiv.org/pdf/2205.01467
Copy Motion From One to Another: Fake Motion Video Generation,Zhenguang Liu;Sifan Wu;Chejian Xu;Xiang Wang;Lei Zhu;Shuang Wu;Fuli Feng,"One compelling application of artificial intelligence is to generate a video of a target person performing arbitrary desired motion (from a source person). While the state-of-the-art methods are able to synthesize a video demonstrating similar broad stroke motion details, they are generally lacking in texture details. A pertinent manifestation appears as distorted face, feet, and hands, and such flaws are very sensitively perceived by human observers. Furthermore, current methods typically employ GANs with a L2 loss to assess the authenticity of the generated videos, inherently requiring a large amount of training samples to learn the texture details for adequate video generation. In this work, we tackle these challenges from three aspects: 1) We disentangle each video frame into foreground (the person) and background, focusing on generating the foreground to reduce the underlying dimension of the network output. 2) We propose a theoretically motivated Gromov-Wasserstein loss that facilitates learning the mapping from a pose to a foreground image. 3) To enhance texture details, we encode facial features with geometric guidance and employ local GANs to refine the face, feet, and hands. Extensive experiments show that our method is able to generate realistic target person videos, faithfully copying complex motions from a source person. △ Less","23 December, 2022",https://arxiv.org/pdf/2205.01373
Visual Knowledge Discovery with Artificial Intelligence: Challenges and Future Directions,Boris Kovalerchuk;Răzvan Andonie;Nuno Datia;Kawa Nazemi;Ebad Banissi,"This volume is devoted to the emerging field of Integrated Visual Knowledge Discovery that combines advances in Artificial Intelligence/Machine Learning (AI/ML) and Visualization/Visual Analytics. Chapters included are extended versions of the selected AI and Visual Analytics papers and related symposia at the recent International Information Visualization Conferences (IV2019 and IV2020). AI/ML face a long-standing challenge of explaining models to humans. Models explanation is fundamentally human activity, not only an algorithmic one. In this chapter we aim to present challenges and future directions within the field of Visual Analytics, Visual Knowledge Discovery and AI/ML, and to discuss the role of visualization in visual AI/ML. In addition, we describe progress in emerging Full 2D ML, natural language processing, and AI/ML in multidimensional data aided by visual means. △ Less","4 May, 2022",https://arxiv.org/pdf/2205.01296
Retrieval-Enhanced Machine Learning,Hamed Zamani;Fernando Diaz;Mostafa Dehghani;Donald Metzler;Michael Bendersky,"Although information access systems have long supported people in accomplishing a wide range of tasks, we propose broadening the scope of users of information access systems to include task-driven machines, such as machine learning models. In this way, the core principles of indexing, representation, retrieval, and ranking can be applied and extended to substantially improve model generalization, scalability, robustness, and interpretability. We describe a generic retrieval-enhanced machine learning (REML) framework, which includes a number of existing models as special cases. REML challenges information retrieval conventions, presenting opportunities for novel advances in core areas, including optimization. The REML research agenda lays a foundation for a new style of information access research and paves a path towards advancing machine learning and artificial intelligence. △ Less","2 May, 2022",https://arxiv.org/pdf/2205.01230
The Theory of Artificial Immutability: Protecting Algorithmic Groups Under Anti-Discrimination Law,Sandra Wachter,"Artificial Intelligence (AI) is increasingly used to make important decisions about people. While issues of AI bias and proxy discrimination are well explored, less focus has been paid to the harms created by profiling based on groups that do not map to or correlate with legally protected groups such as sex or ethnicity. This raises a question: are existing equality laws able to protect against emergent AI-driven inequality? This article examines the legal status of algorithmic groups in North American and European non-discrimination doctrine, law, and jurisprudence and will show that algorithmic groups are not comparable to traditional protected groups. Nonetheless, these new groups are worthy of protection. I propose a new theory of harm - ""the theory of artificial immutability"" - that aims to bring AI groups within the scope of the law. My theory describes how algorithmic groups act as de facto immutable characteristics in practice that limit people's autonomy and prevent them from achieving important goals. △ Less","2 May, 2022",https://arxiv.org/pdf/2205.01166
Distributed intelligence on the Edge-to-Cloud Continuum: A systematic literature review,Daniel Rosendo;Alexandru Costan;Patrick Valduriez;Gabriel Antoniu,"The explosion of data volumes generated by an increasing number of applications is strongly impacting the evolution of distributed digital infrastructures for data analytics and machine learning (ML). While data analytics used to be mainly performed on cloud infrastructures, the rapid development of IoT infrastructures and the requirements for low-latency, secure processing has motivated the development of edge analytics. Today, to balance various trade-offs, ML-based analytics tends to increasingly leverage an interconnected ecosystem that allows complex applications to be executed on hybrid infrastructures where IoT Edge devices are interconnected to Cloud/HPC systems in what is called the Computing Continuum, the Digital Continuum, or the Transcontinuum.Enabling learning-based analytics on such complex infrastructures is challenging. The large scale and optimized deployment of learning-based workflows across the Edge-to-Cloud Continuum requires extensive and reproducible experimental analysis of the application execution on representative testbeds. This is necessary to help understand the performance trade-offs that result from combining a variety of learning paradigms and supportive frameworks. A thorough experimental analysis requires the assessment of the impact of multiple factors, such as: model accuracy, training time, network overhead, energy consumption, processing latency, among others.This review aims at providing a comprehensive vision of the main state-of-the-art libraries and frameworks for machine learning and data analytics available today. It describes the main learning paradigms enabling learning-based analytics on the Edge-to-Cloud Continuum. The main simulation, emulation, deployment systems, and testbeds for experimental research on the Edge-to-Cloud Continuum available today are also surveyed. Furthermore, we analyze how the selected systems provide support for experiment reproducibility. We conclude our review with a detailed discussion of relevant open research challenges and of future directions in this domain such as: holistic understanding of performance; performance optimization of applications;efficient deployment of Artificial Intelligence (AI) workflows on highly heterogeneous infrastructures; and reproducible analysis of experiments on the Computing Continuum. △ Less","29 April, 2022",https://arxiv.org/pdf/2205.01081
Analyzing the Adoption Challenges of the Internet of Things (IoT) and Artificial Intelligence (AI) for Smart Cities in China,Ke Wang;Yafei Zhao;Rajan Kumar Gangadhari;Zhixing Li,"Smart cities play a vital role in the growth of a nation. In recent years, several countries have made huge investments in developing smart cities to offer sustainable living. However, there are some challenges to overcome in smart city development, such as traffic and transportation man-agement, energy and water distribution and management, air quality and waste management monitoring, etc. The capabilities of the Internet of Things (IoT) and artificial intelligence (AI) can help to achieve some goals of smart cities, and there are proven examples from some cities like Singapore, Copenhagen, etc. However, the adoption of AI and the IoT in developing countries has some challenges. The analysis of challenges hindering the adoption of AI and the IoT are very limited. This study aims to fill this research gap by analyzing the causal relationships among the challenges in smart city development, and contains several parts that conclude the previous scholars work, as well as independent research and investigation, such as data collection and analysis based on DEMATEL. In this paper, we have reviewed the literature to extract key chal-lenges for the adoption of AI and the IoT. These helped us to proceed with the investigation and analyze the adoption status. Therefore, using the PRISMA method, 10 challenges were identified from the literature review. Subsequently, determination of the causal inter-relationships among the key challenges based on expert opinions using DEMATEL is performed. This study explored the driving and dependent power of the challenges, and causal relationships between the barriers were established. △ Less","22 April, 2022",https://arxiv.org/pdf/2205.01067
Machine Learning and Artificial Intelligence in Circular Economy: A Bibliometric Analysis and Systematic Literature Review,Abdulla All noman;Umma Habiba Akter;Tahmid Hasan Pranto;AKM Bahalul Haque,"With unorganized, unplanned and improper use of limited raw materials, an abundant amount of waste is being produced, which is harmful to our environment and ecosystem. While traditional linear production lines fail to address far-reaching issues like waste production and a shorter product life cycle, a prospective concept, namely circular economy (CE), has shown promising prospects to be adopted at industrial and governmental levels. CE aims to complete the product life cycle loop by bringing out the highest values from raw materials in the design phase and later on by reusing, recycling, and remanufacturing. Innovative technologies like artificial intelligence (AI) and machine learning(ML) provide vital assistance in effectively adopting and implementing CE in real-world practices. This study explores the adoption and integration of applied AI techniques in CE. First, we conducted bibliometric analysis on a collection of 104 SCOPUS indexed documents exploring the critical research criteria in AI and CE. Forty papers were picked to conduct a systematic literature review from these documents. The selected documents were further divided into six categories: sustainable development, reverse logistics, waste management, supply chain management, recycle & reuse, and manufacturing development. Comprehensive research insights and trends have been extracted and delineated. Finally, the research gap needing further attention has been identified and the future research directions have also been discussed. △ Less","1 April, 2022",https://arxiv.org/pdf/2205.01042
Big Tech Companies Impact on Research at the Faculty of Information Technology and Electrical Engineering,Ahmad Hassanpour;An Thi Nguyen;Anshul Rani;Sarang Shaikh;Ying Xu;Haoyu Zhang,"Artificial intelligence is gaining momentum, ongoing pandemic is fuel to that with more opportunities in every sector specially in health and education sector. But with the growth in technology, challenges associated with ethics also grow (Katharine Schwab, 2021). Whenever a new AI product is developed, companies publicize that their systems are transparent, fair, and are in accordance with the existing laws and regulations as the methods and procedures followed by a big tech company for ensuring AI ethics, not only affect the trust and perception of public, but it also challenges the capabilities of the companies towards business strategies in different regions, and the kind of brains it can attract for their projects. AI Big Tech companies have influence over AI ethics as many influencing ethical-AI researchers have roots in Big Tech or its associated labs. △ Less","10 April, 2022",https://arxiv.org/pdf/2205.01039
AI-Driven Contextual Advertising: A Technology Report and Implication Analysis,Emil Häglund;Johanna Björklund,"Programmatic advertising consists in automated auctioning of digital ad space. Every time a user requests a web page, placeholders on the page are populated with ads from the highest-bidding advertisers. The bids are typically based on information about the user, and to an increasing extent, on information about the surrounding media context. The growing interest in contextual advertising is in part a counterreaction to the current dependency on personal data, which is problematic from legal and ethical standpoints. The transition is further accelerated by developments in Artificial Intelligence (AI), which allow for a deeper semantic understanding of context and, by extension, more effective ad placement. In this article, we begin by identifying context factors that have been shown in previous research to positively influence how ads are received. We then continue to discuss applications of AI in contextual advertising, where it adds value by, e.g., extracting high-level information about media context and optimising bidding strategies. However, left unchecked, these new practices can lead to unfair ad delivery and manipulative use of context. We summarize these and other concerns for consumers, publishers and advertisers in an implication analysis. △ Less","2 May, 2022",https://arxiv.org/pdf/2205.00911
The Rise and Fall of Robotic World (A case study of WALL-E),Faisal Ghaffar,"The current trend in technology shows that the robots will soon be seen interacting with humans and handling different tasks more efficiently in future. With advancement in artificial intelligence it can be foreseen that robots will definitely take over all of the major and minor jobs, will socially interact with humans and will minimize the burden of human significantly. With the passage of time humans will forget about their reality and the labour work they currently do by themselves. That age will be exactly the peak age of robots and we can call it age of the rise of the robots. With the rise of robots, they will start taking decision by themselves regarding the human life and the planet. Human will again start to minimize the influence of robots and take control of their life with the help of some robots. The WALL-E narrates such a story of human life dependency on robots in spaceship when earth is destroyed because of some catastrophe. The clash between human and robots occurs when humans find the earth is survivable and should go back but robots do not allow them. With the help of some social robots humans again take control and take the ship back to earth. The major part of the movie is about the a lonely robot living on planet earth, the love of two robots, their interaction. Those two robots help humans to move back to earth. △ Less","8 April, 2022",https://arxiv.org/pdf/2205.00838
Re-defining Radiology Quality Assurance (QA) -- Artificial Intelligence (AI)-Based QA by Restricted Investigation of Unequal Scores (AQUARIUS),Axel Wismueller;Larry Stockmaster;Ali Vosoughi,"There is an urgent need for streamlining radiology Quality Assurance (QA) programs to make them better and faster. Here, we present a novel approach, Artificial Intelligence (AI)-Based QUality Assurance by Restricted Investigation of Unequal Scores (AQUARIUS), for re-defining radiology QA, which reduces human effort by up to several orders of magnitude over existing approaches. AQUARIUS typically includes automatic comparison of AI-based image analysis with natural language processing (NLP) on radiology reports. Only the usually small subset of cases with discordant reads is subsequently reviewed by human experts. To demonstrate the clinical applicability of AQUARIUS, we performed a clinical QA study on Intracranial Hemorrhage (ICH) detection in 1936 head CT scans from a large academic hospital. Immediately following image acquisition, scans were automatically analyzed for ICH using a commercially available software (Aidoc, Tel Aviv, Israel). Cases rated positive for ICH by AI (ICH-AI+) were automatically flagged in radiologists' reading worklists, where flagging was randomly switched off with probability 50%. Using AQUARIUS with NLP on final radiology reports and targeted expert neuroradiology review of only 29 discordantly classified cases reduced the human QA effort by 98.5%, where we found a total of six non-reported true ICH+ cases, with radiologists' missed ICH detection rates of 0.52% and 2.5% for flagged and non-flagged cases, respectively. We conclude that AQUARIUS, by combining AI-based image analysis with NLP-based pre-selection of cases for targeted human expert review, can efficiently identify missed findings in radiology studies and significantly expedite radiology QA programs in a hybrid human-machine interoperability approach. △ Less","3 May, 2022",https://arxiv.org/pdf/2205.00629
The use of Data Augmentation as a technique for improving neural network accuracy in detecting fake news about COVID-19,Wilton O. Júnior;Mauricio S. da Cruz;Andre Brasil Vieira Wyzykowski;Arnaldo Bispo de Jesus,"This paper aims to present how the application of Natural Language Processing (NLP) and data augmentation techniques can improve the performance of a neural network for better detection of fake news in the Portuguese language. Fake news is one of the main controversies during the growth of the internet in the last decade. Verifying what is fact and what is false has proven to be a difficult task, while the dissemination of false news is much faster, which leads to the need for the creation of tools that, automated, assist in the process of verification of what is fact and what is false. In order to bring a solution, an experiment was developed with neural network using news, real and fake, which were never seen by artificial intelligence (AI). There was a significant performance in the news classification after the application of the mentioned techniques. △ Less","1 May, 2022",https://arxiv.org/pdf/2205.00452
Artificial Intelligence and Medicine: A literature review,Chottiwatt Jittprasong,"In practically every industry today, artificial intelligence is one of the most effective ways for machines to assist humans. Since its inception, a large number of researchers throughout the globe have been pioneering the application of artificial intelligence in medicine. Although artificial intelligence may seem to be a 21st-century concept, Alan Turing pioneered the first foundation concept in the 1940s. Artificial intelligence in medicine has a huge variety of applications that researchers are continually exploring. The tremendous increase in computer and human resources has hastened progress in the 21st century, and it will continue to do so for many years to come. This review of the literature will highlight the emerging field of artificial intelligence in medicine and its current level of development. △ Less","5 May, 2022",https://arxiv.org/pdf/2205.00322
A Theory of Natural Intelligence,Christoph von der Malsburg;Thilo Stadelmann;Benjamin F. Grewe,"Introduction: In contrast to current AI technology, natural intelligence -- the kind of autonomous intelligence that is realized in the brains of animals and humans to attain in their natural environment goals defined by a repertoire of innate behavioral schemata -- is far superior in terms of learning speed, generalization capabilities, autonomy and creativity. How are these strengths, by what means are ideas and imagination produced in natural neural networks? Methods: Reviewing the literature, we put forward the argument that both our natural environment and the brain are of low complexity, that is, require for their generation very little information and are consequently both highly structured. We further argue that the structures of brain and natural environment are closely related. Results: We propose that the structural regularity of the brain takes the form of net fragments (self-organized network patterns) and that these serve as the powerful inductive bias that enables the brain to learn quickly, generalize from few examples and bridge the gap between abstractly defined general goals and concrete situations. Conclusions: Our results have important bearings on open problems in artificial neural network research. △ Less","22 April, 2022",https://arxiv.org/pdf/2205.00002
Brainish: Formalizing A Multimodal Language for Intelligence and Consciousness,Paul Pu Liang,"Having a rich multimodal inner language is an important component of human intelligence that enables several necessary core cognitive functions such as multimodal prediction, translation, and generation. Building upon the Conscious Turing Machine (CTM), a machine model for consciousness proposed by Blum and Blum (2021), we describe the desiderata of a multimodal language called Brainish, comprising words, images, audio, and sensations combined in representations that the CTM's processors use to communicate with each other. We define the syntax and semantics of Brainish before operationalizing this language through the lens of multimodal artificial intelligence, a vibrant research area studying the computational tools necessary for processing and relating information from heterogeneous signals. Our general framework for learning Brainish involves designing (1) unimodal encoders to segment and represent unimodal data, (2) a coordinated representation space that relates and composes unimodal features to derive holistic meaning across multimodal inputs, and (3) decoders to map multimodal representations into predictions (for fusion) or raw data (for translation or generation). Through discussing how Brainish is crucial for communication and coordination in order to achieve consciousness in the CTM, and by implementing a simple version of Brainish and evaluating its capability of demonstrating intelligence on multimodal prediction and retrieval tasks on several real-world image, text, and audio datasets, we argue that such an inner language will be important for advances in machine models of intelligence and consciousness. △ Less","6 July, 2022",https://arxiv.org/pdf/2205.00001
Recommendations on test datasets for evaluating AI solutions in pathology,André Homeyer;Christian Geißler;Lars Ole Schwen;Falk Zakrzewski;Theodore Evans;Klaus Strohmenger;Max Westphal;Roman David Bülow;Michaela Kargl;Aray Karjauv;Isidre Munné-Bertran;Carl Orge Retzlaff;Adrià Romero-López;Tomasz Sołtysiński;Markus Plass;Rita Carvalho;Peter Steinbach;Yu-Chia Lan;Nassim Bouteldja;David Haber;Mateo Rojas-Carulla;Alireza Vafaei Sadr;Matthias Kraft;Daniel Krüger;Rutger Fick,"Artificial intelligence (AI) solutions that automatically extract information from digital histology images have shown great promise for improving pathological diagnosis. Prior to routine use, it is important to evaluate their predictive performance and obtain regulatory approval. This assessment requires appropriate test datasets. However, compiling such datasets is challenging and specific recommendations are missing. A committee of various stakeholders, including commercial AI developers, pathologists, and researchers, discussed key aspects and conducted extensive literature reviews on test datasets in pathology. Here, we summarize the results and derive general recommendations for the collection of test datasets. We address several questions: Which and how many images are needed? How to deal with low-prevalence subsets? How can potential bias be detected? How should datasets be reported? What are the regulatory requirements in different countries? The recommendations are intended to help AI developers demonstrate the utility of their products and to help regulatory agencies and end users verify reported performance measures. Further research is needed to formulate criteria for sufficiently representative test datasets so that AI solutions can operate with less user intervention and better support diagnostic workflows in the future. △ Less","21 April, 2022",https://arxiv.org/pdf/2204.14226
Local Explanation of Dimensionality Reduction,Avraam Bardos;Ioannis Mollas;Nick Bassiliades;Grigorios Tsoumakas,"Dimensionality reduction (DR) is a popular method for preparing and analyzing high-dimensional data. Reduced data representations are less computationally intensive and easier to manage and visualize, while retaining a significant percentage of their original information. Aside from these advantages, these reduced representations can be difficult or impossible to interpret in most circumstances, especially when the DR approach does not provide further information about which features of the original space led to their construction. This problem is addressed by Interpretable Machine Learning, a subfield of Explainable Artificial Intelligence that addresses the opacity of machine learning models. However, current research on Interpretable Machine Learning has been focused on supervised tasks, leaving unsupervised tasks like Dimensionality Reduction unexplored. In this paper, we introduce LXDR, a technique capable of providing local interpretations of the output of DR techniques. Experiment results and two LXDR use case examples are presented to evaluate its usefulness. △ Less","29 April, 2022",https://arxiv.org/pdf/2204.14012
Physical Deep Learning with Biologically Plausible Training Method,Mitsumasa Nakajima;Katsuma Inoue;Kenji Tanaka;Yasuo Kuniyoshi;Toshikazu Hashimoto;Kohei Nakajima,"The ever-growing demand for further advances in artificial intelligence motivated research on unconventional computation based on analog physical devices. While such computation devices mimic brain-inspired analog information processing, learning procedures still relies on methods optimized for digital processing such as backpropagation. Here, we present physical deep learning by extending a biologically plausible training algorithm called direct feedback alignment. As the proposed method is based on random projection with arbitrary nonlinear activation, we can train a physical neural network without knowledge about the physical system. In addition, we can emulate and accelerate the computation for this training on a simple and scalable physical system. We demonstrate the proof-of-concept using a hierarchically connected optoelectronic recurrent neural network called deep reservoir computer. By constructing an FPGA-assisted optoelectronic benchtop, we confirmed the potential for accelerated computation with competitive performance on benchmarks. Our results provide practical solutions for the training and acceleration of neuromorphic computation. △ Less","1 April, 2022",https://arxiv.org/pdf/2204.13991
Noise-reducing attention cross fusion learning transformer for histological image classification of osteosarcoma,Liangrui Pan;Hetian Wang;Lian Wang;Boya Ji;Mingting Liu;Mitchai Chongcheawchamnan;Jin Yuan;Shaoliang Peng,"The degree of malignancy of osteosarcoma and its tendency to metastasize/spread mainly depend on the pathological grade (determined by observing the morphology of the tumor under a microscope). The purpose of this study is to use artificial intelligence to classify osteosarcoma histological images and to assess tumor survival and necrosis, which will help doctors reduce their workload, improve the accuracy of osteosarcoma cancer detection, and make a better prognosis for patients. The study proposes a typical transformer image classification framework by integrating noise reduction convolutional autoencoder and feature cross fusion learning (NRCA-FCFL) to classify osteosarcoma histological images. Noise reduction convolutional autoencoder could well denoise histological images of osteosarcoma, resulting in more pure images for osteosarcoma classification. Moreover, we introduce feature cross fusion learning, which integrates two scale image patches, to sufficiently explore their interactions by using additional classification tokens. As a result, a refined fusion feature is generated, which is fed to the residual neural network for label predictions. We conduct extensive experiments to evaluate the performance of the proposed approach. The experimental results demonstrate that our method outperforms the traditional and deep learning approaches on various evaluation metrics, with an accuracy of 99.17% to support osteosarcoma diagnosis. △ Less","28 April, 2022",https://arxiv.org/pdf/2204.13838
Tragedy Plus Time: Capturing Unintended Human Activities from Weakly-labeled Videos,Arnav Chakravarthy;Zhiyuan Fang;Yezhou Yang,"In videos that contain actions performed unintentionally, agents do not achieve their desired goals. In such videos, it is challenging for computer vision systems to understand high-level concepts such as goal-directed behavior, an ability present in humans from a very early age. Inculcating this ability in artificially intelligent agents would make them better social learners by allowing them to evaluate human action under a teleological lens. To validate the ability of deep learning models to perform this task, we curate the W-Oops dataset, built upon the Oops dataset [15]. W-Oops consists of 2,100 unintentional human action videos, with 44 goal-directed and 30 unintentional video-level activity labels collected through human annotations. Due to the expensive segment annotation procedure, we propose a weakly supervised algorithm for localizing the goal-directed as well as unintentional temporal regions in the video leveraging solely video-level labels. In particular, we employ an attention mechanism-based strategy that predicts the temporal regions which contribute the most to a classification task. Meanwhile, our designed overlap regularization allows the model to focus on distinct portions of the video for inferring the goal-directed and unintentional activity while guaranteeing their temporal ordering. Extensive quantitative experiments verify the validity of our localization method. We further conduct a video captioning experiment which demonstrates that the proposed localization module does indeed assist teleological action understanding. △ Less","28 April, 2022",https://arxiv.org/pdf/2204.13548
Fuzzy Expert System for Stock Portfolio Selection: An Application to Bombay Stock Exchange,Gour Sundar Mitra Thakur;Rupak Bhattacharyya;Seema Sarkar,"Selection of proper stocks, before allocating investment ratios, is always a crucial task for the investors. Presence of many influencing factors in stock performance have motivated researchers to adopt various Artificial Intelligence (AI) techniques to make this challenging task easier. In this paper a novel fuzzy expert system model is proposed to evaluate and rank the stocks under Bombay Stock Exchange (BSE). Dempster-Shafer (DS) evidence theory is used for the first time to automatically generate the consequents of the fuzzy rule base to reduce the effort in knowledge base development of the expert system. Later a portfolio optimization model is constructed where the objective function is considered as the ratio of the difference of fuzzy portfolio return and the risk free return to the weighted mean semi-variance of the assets that has been used. The model is solved by applying Ant Colony Optimization (ACO) algorithm by giving preference to the top ranked stocks. The performance of the model proved to be satisfactory for short-term investment period when compared with the recent performance of the stocks. △ Less","4 May, 2022",https://arxiv.org/pdf/2204.13385
Phase Shift Design in RIS Empowered Wireless Networks: From Optimization to AI-Based Methods,Zongze Li;Shuai Wang;Qingfeng Lin;Yang Li;Miaowen Wen;Yik-Chung Wu;H. Vincent Poor,"Reconfigurable intelligent surfaces (RISs) have a revolutionary capability to customize the radio propagation environment for wireless networks. To fully exploit the advantages of RISs in wireless systems, the phases of the reflecting elements must be jointly designed with conventional communication resources, such as beamformers, transmit power, and computation time. However, due to the unique constraints on the phase shift, and massive numbers of reflecting units and users in large-scale networks, the resulting optimization problems are challenging to solve. This paper provides a review of current optimization methods and artificial intelligence-based methods for handling the constraints imposed by RIS and compares them in terms of solution quality and computational complexity. Future challenges in phase shift optimization involving RISs are also described and potential solutions are discussed. △ Less","28 April, 2022",https://arxiv.org/pdf/2204.13372
Cross-modal Memory Networks for Radiology Report Generation,Zhihong Chen;Yaling Shen;Yan Song;Xiang Wan,"Medical imaging plays a significant role in clinical practice of medical diagnosis, where the text reports of the images are essential in understanding them and facilitating later treatments. By generating the reports automatically, it is beneficial to help lighten the burden of radiologists and significantly promote clinical automation, which already attracts much attention in applying artificial intelligence to medical domain. Previous studies mainly follow the encoder-decoder paradigm and focus on the aspect of text generation, with few studies considering the importance of cross-modal mappings and explicitly exploit such mappings to facilitate radiology report generation. In this paper, we propose a cross-modal memory networks (CMN) to enhance the encoder-decoder framework for radiology report generation, where a shared memory is designed to record the alignment between images and texts so as to facilitate the interaction and generation across modalities. Experimental results illustrate the effectiveness of our proposed model, where state-of-the-art performance is achieved on two widely used benchmark datasets, i.e., IU X-Ray and MIMIC-CXR. Further analyses also prove that our model is able to better align information from radiology images and texts so as to help generating more accurate reports in terms of clinical indicators. △ Less","27 April, 2022",https://arxiv.org/pdf/2204.13258
Systematic Literature Review: Anti-Phishing Defences and Their Application to Before-the-click Phishing Email Detection,Trevor Wood;Vitor Basto-Fernandes;Eerke Boiten;Iryna Yevseyeva,"Most research into anti-phishing defence assumes that the mal-actor is attempting to harvest end-users' personally identifiable information or login credentials and, hence, focuses on detecting phishing websites. The defences for this type of attack are usually activated after the end-user clicks on a link, at which point the link is checked. This is known as after-the-click detection. However, more sophisticated phishing attacks (such as spear-phishing and whaling) are rarely designed to get the end-user to visit a website. Instead, they attempt to get the end-user to perform some other action, for example, transferring money from their bank account to the mal-actors account. These attacks are rarer, and before-the-click defence has been investigated less than after-the-click defence. To better integrate and contextualize these studies in the overall anti-phishing research, this paper presents a systematic literature review of proposed anti-phishing defences. From a total of 6330 papers, 21 primary studies and 335 secondary studies were identified and examined. The current research was grouped into six primary categories, blocklist/allowlist, heuristics, content, visual, artificial intelligence/machine learning and proactive, with an additional category of ""other"" for detection techniques that do not fit into any of the primary categories. It then discusses the performance and suitability of using these techniques for detecting phishing emails before the end-user even reads the email. Finally, it suggests some promising areas for further research. △ Less","27 April, 2022",https://arxiv.org/pdf/2204.13054
High-quality Conversational Systems,Samuel Ackerman;Ateret Anaby-Tavor;Eitan Farchi;Esther Goldbraich;George Kour;Ella Rabinovich;Orna Raz;Saritha Route;Marcel Zalmanovici;Naama Zwerdling,"Conversational systems or chatbots are an example of AI-Infused Applications (AIIA). Chatbots are especially important as they are often the first interaction of clients with a business and are the entry point of a business into the AI (Artificial Intelligence) world. The quality of the chatbot is, therefore, key. However, as is the case in general with AIIAs, it is especially challenging to assess and control the quality of chatbot systems. Beyond the inherent statistical nature of these systems, where occasional failure is acceptable, we identify two major challenges. The first is to release an initial system that is of sufficient quality such that humans will interact with it. The second is to maintain the quality, enhance its capabilities, improve it and make necessary adjustments based on changing user requests or drift. These challenges exist because it is impossible to predict the real distribution of user requests and the natural language they will use to express these requests. Moreover, any empirical distribution of requests is likely to change over time. This may be due to periodicity, changing usage, and drift of topics. We provide a methodology and set of technologies to address these challenges and to provide automated assistance through a human-in-the-loop approach. We notice that it is crucial to connect between the different phases in the lifecycle of the chatbot development and to make sure it provides its expected business value. For example, that it frees human agents to deal with tasks other than answering human users. Our methodology and technologies apply during chatbot training in the pre-production phase, through to chatbot usage in the field in the post-production phase. They implement the `test first' paradigm by assisting in agile design, and support continuous integration through actionable insights. △ Less","28 April, 2022",https://arxiv.org/pdf/2204.13043
Rationality in current era -- A recent survey,Dibakar Das,"Rationality has been an intriguing topic for several decades. Even the scope of definition of rationality across different subjects varies. Several theories (e.g., game theory) initially evolved on the basis that agents (e.g., humans) are perfectly rational. One interpretation of perfect rationality is that agents always make the optimal decision which maximizes their expected utilities. However, subsequently this assumption was relaxed to include bounded rationality where agents have limitations in terms of computing resources and biases which prevents them to take the optimal decision. However, with recent advances in (quantum) computing, artificial intelligence (AI), science and technology etc., has led to the thought that perhaps the concept of rationality would be augmented with machine intelligence which will enable agents to take decision optimally with higher regularity. However, there are divergent views on this topic. The paper attempts to put forward a recent survey (last five years) of research on these divergent views. These viewsmay be grouped into three schools of thoughts. The first school is the one which is sceptical of progress of AI and believes that human intelligencewill always supersede machine intelligence. The second school of thought thinks that advent of AI and advances in computing will help in better understanding of bounded rationality. Third school of thought believes that bounds of bounded rationality will be extended by advances in AI and various other fields. This survey hopes to provide a starting point for further research. △ Less","27 April, 2022",https://arxiv.org/pdf/2204.12872
Evaluation and Learning in Two-Player Symmetric Games via Best and Better Responses,Rui Yan;Weixian Zhang;Ruiliang Deng;Xiaoming Duan;Zongying Shi;Yisheng Zhong,"Artificial intelligence and robotic competitions are accompanied by a class of game paradigms in which each player privately commits a strategy to a game system which simulates the game using the collected joint strategy and then returns payoffs to players. This paper considers the strategy commitment for two-player symmetric games in which the players' strategy spaces are identical and their payoffs are symmetric. First, we introduce two digraph-based metrics at a meta-level for strategy evaluation in two-agent reinforcement learning, grounded on sink equilibrium. The metrics rank the strategies of a single player and determine the set of strategies which are preferred for the private commitment. Then, in order to find the preferred strategies under the metrics, we propose two variants of the classical learning algorithm self-play, called strictly best-response and weakly better-response self-plays. By modeling learning processes as walks over joint-strategy response digraphs, we prove that the learnt strategies by two variants are preferred under two metrics, respectively. The preferred strategies under both two metrics are identified and adjacency matrices induced by one metric and one variant are connected. Finally, simulations are provided to illustrate the results. △ Less","27 April, 2022",https://arxiv.org/pdf/2204.12791
Zero-Touch Network on Industrial IoT: An End-to-End Machine Learning Approach,Shih-Chun Lin;Chia-Hung Lin;Wei-Chi Chen,"Industry 4.0-enabled smart factory is expected to realize the next revolution for manufacturers. Although artificial intelligence (AI) technologies have improved productivity, current use cases belong to small-scale and single-task operations. To unbound the potential of smart factory, this paper develops zero-touch network systems for intelligent manufacturing and facilitates distributed AI applications in both training and inferring stages in a large-scale manner. The open radio access network (O-RAN) architecture is first introduced for the zero-touch platform to enable globally controlling communications and computation infrastructure capability in the field. The designed serverless framework allows intelligent and efficient learning assignments and resource allocations. Hence, requested learning tasks can be assigned to appropriate robots, and the underlying infrastructure can be used to support the learning tasks without expert knowledge. Moreover, due to the proposed network system's flexibility, powerful AI-enabled networking algorithms can be utilized to ensure service-level agreements and superior performances for factory workloads. Finally, three open research directions of backward compatibility, end-to-end enhancements, and cybersecurity are discussed for zero-touch smart factory. △ Less","26 April, 2022",https://arxiv.org/pdf/2204.12605
Process Knowledge-infused Learning for Suicidality Assessment on Social Media,Kaushik Roy;Manas Gaur;Qi Zhang;Amit Sheth,"Improving the performance and natural language explanations of deep learning algorithms is a priority for adoption by humans in the real world. In several domains, such as healthcare, such technology has significant potential to reduce the burden on humans by providing quality assistance at scale. However, current methods rely on the traditional pipeline of predicting labels from data, thus completely ignoring the process and guidelines used to obtain the labels. Furthermore, post hoc explanations on the data to label prediction using explainable AI (XAI) models, while satisfactory to computer scientists, leave much to be desired to the end-users due to lacking explanations of the process in terms of human-understandable concepts. We \textit{introduce}, \textit{formalize}, and \textit{develop} a novel Artificial Intelligence (A) paradigm -- Process Knowledge-infused Learning (PK-iL). PK-iL utilizes a structured process knowledge that explicitly explains the underlying prediction process that makes sense to end-users. The qualitative human evaluation confirms through a annotator agreement of 0.72, that humans are understand explanations for the predictions. PK-iL also performs competitively with the state-of-the-art (SOTA) baselines. △ Less","26 April, 2022",https://arxiv.org/pdf/2204.12560
"AI-Assisted Authentication: State of the Art, Taxonomy and Future Roadmap",Guangyi Zhu;Yasir Al-Qaraghuli,"Artificial Intelligence (AI) has found its applications in a variety of environments ranging from data science to cybersecurity. AI helps break through the limitations of traditional algorithms and provides more efficient and flexible methods for solving problems. In this paper, we focus on the applications of artificial intelligence in authentication, which is used in a wide range of scenarios including facial recognition to access buildings, keystroke dynamics to unlock smartphones. With the emerging AI-assisted authentication schemes, our comprehensive survey provides an overall understanding on a high level, which paves the way for future research in this area. In contrast to other relevant surveys, our research is the first of its kind to focus on the roles of AI in authentication. △ Less","25 April, 2022",https://arxiv.org/pdf/2204.12492
RadioPathomics: Multimodal Learning in Non-Small Cell Lung Cancer for Adaptive Radiotherapy,Matteo Tortora;Ermanno Cordelli;Rosa Sicilia;Lorenzo Nibid;Edy Ippolito;Giuseppe Perrone;Sara Ramella;Paolo Soda,"The current cancer treatment practice collects multimodal data, such as radiology images, histopathology slides, genomics and clinical data. The importance of these data sources taken individually has fostered the recent raise of radiomics and pathomics, i.e. the extraction of quantitative features from radiology and histopathology images routinely collected to predict clinical outcomes or to guide clinical decisions using artificial intelligence algorithms. Nevertheless, how to combine them into a single multimodal framework is still an open issue. In this work we therefore develop a multimodal late fusion approach that combines hand-crafted features computed from radiomics, pathomics and clinical data to predict radiation therapy treatment outcomes for non-small-cell lung cancer patients. Within this context, we investigate eight different late fusion rules (i.e. product, maximum, minimum, mean, decision template, Dempster-Shafer, majority voting, and confidence rule) and two patient-wise aggregation rules leveraging the richness of information given by computer tomography images and whole-slide scans. The experiments in leave-one-patient-out cross-validation on an in-house cohort of 33 patients show that the proposed multimodal paradigm with an AUC equal to 90.9\% outperforms each unimodal approach, suggesting that data integration can advance precision medicine. As a further contribution, we also compare the hand-crafted representations with features automatically computed by deep networks, and the late fusion paradigm with early fusion, another popular multimodal approach. In both cases, the experiments show that the proposed multimodal approach provides the best results. △ Less","26 April, 2022",https://arxiv.org/pdf/2204.12423
Information Retrieval in Friction Stir Welding of Aluminum Alloys by using Natural Language Processing based Algorithms,Akshansh Mishra,"Text summarization is a technique for condensing a big piece of text into a few key elements that give a general impression of the content. When someone requires a quick and precise summary of a large amount of information, it becomes vital. If done manually, summarizing text can be costly and time-consuming. Natural Language Processing (NLP) is the sub-division of Artificial Intelligence that narrows down the gap between technology and human cognition by extracting the relevant information from the pile of data. In the present work, scientific information regarding the Friction Stir Welding of Aluminum alloys was collected from the abstract of scholarly research papers. For extracting the relevant information from these research abstracts four Natural Language Processing based algorithms i.e. Latent Semantic Analysis (LSA), Luhn Algorithm, Lex Rank Algorithm, and KL-Algorithm were used. In order to evaluate the accuracy score of these algorithms, Recall-Oriented Understudy for Gisting Evaluation (ROUGE) was used. The results showed that the Luhn Algorithm resulted in the highest f1-Score of 0.413 in comparison to other algorithms. △ Less","25 April, 2022",https://arxiv.org/pdf/2204.12309
Stochastic Coherence Over Attention Trajectory For Continuous Learning In Video Streams,Matteo Tiezzi;Simone Marullo;Lapo Faggi;Enrico Meloni;Alessandro Betti;Stefano Melacci,"Devising intelligent agents able to live in an environment and learn by observing the surroundings is a longstanding goal of Artificial Intelligence. From a bare Machine Learning perspective, challenges arise when the agent is prevented from leveraging large fully-annotated dataset, but rather the interactions with supervisory signals are sparsely distributed over space and time. This paper proposes a novel neural-network-based approach to progressively and autonomously develop pixel-wise representations in a video stream. The proposed method is based on a human-like attention mechanism that allows the agent to learn by observing what is moving in the attended locations. Spatio-temporal stochastic coherence along the attention trajectory, paired with a contrastive term, leads to an unsupervised learning criterion that naturally copes with the considered setting. Differently from most existing works, the learned representations are used in open-set class-incremental classification of each frame pixel, relying on few supervisions. Our experiments leverage 3D virtual environments and they show that the proposed agents can learn to distinguish objects just by observing the video stream. Inheriting features from state-of-the art models is not as powerful as one might expect. △ Less","26 April, 2022",https://arxiv.org/pdf/2204.12193
Crystal Transformer: Self-learning neural language model for Generative and Tinkering Design of Materials,Lai Wei;Qinyang Li;Yuqi Song;Stanislav Stefanov;Edirisuriya M. D. Siriwardane;Fanglin Chen;Jianjun Hu,"Self-supervised neural language models have recently achieved unprecedented success, from natural language processing to learning the languages of biological sequences and organic molecules. These models have demonstrated superior performance in the generation, structure classification, and functional predictions for proteins and molecules with learned representations. However, most of the masking-based pre-trained language models are not designed for generative design, and their black-box nature makes it difficult to interpret their design logic. Here we propose BLMM Crystal Transformer, a neural network based probabilistic generative model for generative and tinkering design of inorganic materials. Our model is built on the blank filling language model for text generation and has demonstrated unique advantages in learning the ""materials grammars"" together with high-quality generation, interpretability, and data efficiency. It can generate chemically valid materials compositions with as high as 89.7\% charge neutrality and 84.8\% balanced electronegativity, which are more than 4 and 8 times higher compared to a pseudo random sampling baseline. The probabilistic generation process of BLMM allows it to recommend tinkering operations based on learned materials chemistry and makes it useful for materials doping. Combined with the TCSP crysal structure prediction algorithm, We have applied our model to discover a set of new materials as validated using DFT calculations. Our work thus brings the unsupervised transformer language models based generative artificial intelligence to inorganic materials. A user-friendly web app has been developed for computational materials doping and can be accessed freely at \url{www.materialsatlas.org/blmtinker}. △ Less","25 April, 2022",https://arxiv.org/pdf/2204.11953
Automated detection of dark patterns in cookie banners: how to do it poorly and why it is hard to do it any other way,Than Htut Soe;Cristiana Teixeira Santos;Marija Slavkovik,"Cookie banners, the pop ups that appear to collect your consent for data collection, are a tempting ground for dark patterns. Dark patterns are design elements that are used to influence the user's choice towards an option that is not in their interest. The use of dark patterns renders consent elicitation meaningless and voids the attempts to improve a fair collection and use of data. Can machine learning be used to automatically detect the presence of dark patterns in cookie banners? In this work, a dataset of cookie banners of 300 news websites was used to train a prediction model that does exactly that. The machine learning pipeline we used includes feature engineering, parameter search, training a Gradient Boosted Tree classifier and evaluation. The accuracy of the trained model is promising, but allows a lot of room for improvement. We provide an in-depth analysis of the interdisciplinary challenges that automated dark pattern detection poses to artificial intelligence. The dataset and all the code created using machine learning is available at the url to repository removed for review. △ Less","21 April, 2022",https://arxiv.org/pdf/2204.11836
"Enable Deep Learning on Mobile Devices: Methods, Systems, and Applications",Han Cai;Ji Lin;Yujun Lin;Zhijian Liu;Haotian Tang;Hanrui Wang;Ligeng Zhu;Song Han,"Deep neural networks (DNNs) have achieved unprecedented success in the field of artificial intelligence (AI), including computer vision, natural language processing and speech recognition. However, their superior performance comes at the considerable cost of computational complexity, which greatly hinders their applications in many resource-constrained devices, such as mobile phones and Internet of Things (IoT) devices. Therefore, methods and techniques that are able to lift the efficiency bottleneck while preserving the high accuracy of DNNs are in great demand in order to enable numerous edge AI applications. This paper provides an overview of efficient deep learning methods, systems and applications. We start from introducing popular model compression methods, including pruning, factorization, quantization as well as compact model design. To reduce the large design cost of these manual solutions, we discuss the AutoML framework for each of them, such as neural architecture search (NAS) and automated pruning and quantization. We then cover efficient on-device training to enable user customization based on the local data on mobile devices. Apart from general acceleration techniques, we also showcase several task-specific accelerations for point cloud, video and natural language processing by exploiting their spatial sparsity and temporal/token redundancy. Finally, to support all these algorithmic advancements, we introduce the efficient deep learning system design from both software and hardware perspectives. △ Less","25 April, 2022",https://arxiv.org/pdf/2204.11786
Integrating Prior Knowledge in Post-hoc Explanations,Adulam Jeyasothy;Thibault Laugel;Marie-Jeanne Lesot;Christophe Marsala;Marcin Detyniecki,"In the field of eXplainable Artificial Intelligence (XAI), post-hoc interpretability methods aim at explaining to a user the predictions of a trained decision model. Integrating prior knowledge into such interpretability methods aims at improving the explanation understandability and allowing for personalised explanations adapted to each user. In this paper, we propose to define a cost function that explicitly integrates prior knowledge into the interpretability objectives: we present a general framework for the optimization problem of post-hoc interpretability methods, and show that user knowledge can thus be integrated to any method by adding a compatibility term in the cost function. We instantiate the proposed formalization in the case of counterfactual explanations and propose a new interpretability method called Knowledge Integration in Counterfactual Explanation (KICE) to optimize it. The paper performs an experimental study on several benchmark data sets to characterize the counterfactual instances generated by KICE, as compared to reference methods. △ Less","25 April, 2022",https://arxiv.org/pdf/2204.11634
Adaptive cognitive fit: Artificial intelligence augmented management of information facets and representations,Jim Samuel;Rajiv Kashyap;Yana Samuel;Alexander Pelaez,"Explosive growth in big data technologies and artificial intelligence [AI] applications have led to increasing pervasiveness of information facets and a rapidly growing array of information representations. Information facets, such as equivocality and veracity, can dominate and significantly influence human perceptions of information and consequently affect human performance. Extant research in cognitive fit, which preceded the big data and AI era, focused on the effects of aligning information representation and task on performance, without sufficient consideration to information facets and attendant cognitive challenges. Therefore, there is a compelling need to understand the interplay of these dominant information facets with information representations and tasks, and their influence on human performance. We suggest that artificially intelligent technologies that can adapt information representations to overcome cognitive limitations are necessary for these complex information environments. To this end, we propose and test a novel *Adaptive Cognitive Fit* [ACF] framework that explains the influence of information facets and AI-augmented information representations on human performance. We draw on information processing theory and cognitive dissonance theory to advance the ACF framework and a set of propositions. We empirically validate the ACF propositions with an economic experiment that demonstrates the influence of information facets, and a machine learning simulation that establishes the viability of using AI to improve human performance. △ Less","24 April, 2022",https://arxiv.org/pdf/2204.11405
RealNet: Combining Optimized Object Detection with Information Fusion Depth Estimation Co-Design Method on IoT,Zhuohao Li;Fandi Gou;Qixin De;Leqi Ding;Yuanhang Zhang;Yunze Cai,"Depth Estimation and Object Detection Recognition play an important role in autonomous driving technology under the guidance of deep learning artificial intelligence. We propose a hybrid structure called RealNet: a co-design method combining the model-streamlined recognition algorithm, the depth estimation algorithm with information fusion, and deploying them on the Jetson-Nano for unmanned vehicles with monocular vision sensors. We use ROS for experiment. The method proposed in this paper is suitable for mobile platforms with high real-time request. Innovation of our method is using information fusion to compensate the problem of insufficient frame rate of output image, and improve the robustness of target detection and depth estimation under monocular vision.Object Detection is based on YOLO-v5. We have simplified the network structure of its DarkNet53 and realized a prediction speed up to 0.01s. Depth Estimation is based on the VNL Depth Estimation, which considers multiple geometric constraints in 3D global space. It calculates the loss function by calculating the deviation of the virtual normal vector VN and the label, which can obtain deeper depth information. We use PnP fusion algorithm to solve the problem of insufficient frame rate of depth map output. It solves the motion estimation depth from three-dimensional target to two-dimensional point based on corner feature matching, which is faster than VNL calculation. We interpolate VNL output and PnP output to achieve information fusion. Experiments show that this can effectively eliminate the jitter of depth information and improve robustness. At the control end, this method combines the results of target detection and depth estimation to calculate the target position, and uses a pure tracking control algorithm to track it. △ Less","24 April, 2022",https://arxiv.org/pdf/2204.11216
"Turning the Hunted into the Hunter via Threat Hunting: Life Cycle, Ecosystem, Challenges and the Great Promise of AI",Caroline Hillier;Talieh Karroubi,"The threat hunting lifecycle is a complex atmosphere that requires special attention from professionals to maintain security. This paper is a collection of recent work that gives a holistic view of the threat hunting ecosystem, identifies challenges, and discusses the future with the integration of artificial intelligence (AI). We specifically establish a life cycle and ecosystem for privacy-threat hunting in addition to identifying the related challenges. We also discovered how critical the use of AI is in threat hunting. This work paves the way for future work in this area as it provides the foundational knowledge to make meaningful advancements for threat hunting. △ Less","23 April, 2022",https://arxiv.org/pdf/2204.11076
FPGA-based AI Smart NICs for Scalable Distributed AI Training Systems,Rui Ma;Evangelos Georganas;Alexander Heinecke;Andrew Boutros;Eriko Nurvitadhi,"Rapid advances in artificial intelligence (AI) technology have led to significant accuracy improvements in a myriad of application domains at the cost of larger and more compute-intensive models. Training such models on massive amounts of data typically requires scaling to many compute nodes and relies heavily on collective communication algorithms, such as all-reduce, to exchange the weight gradients between different nodes. The overhead of these collective communication operations in a distributed AI training system can bottleneck its performance, with more pronounced effects as the number of nodes increases. In this paper, we first characterize the all-reduce operation overhead by profiling distributed AI training. Then, we propose a new smart network interface card (NIC) for distributed AI training systems using field-programmable gate arrays (FPGAs) to accelerate all-reduce operations and optimize network bandwidth utilization via data compression. The AI smart NIC frees up the system's compute resources to perform the more compute-intensive tensor operations and increases the overall node-to-node communication efficiency. We perform real measurements on a prototype distributed AI training system comprised of 6 compute nodes to evaluate the performance gains of our proposed FPGA-based AI smart NIC compared to a baseline system with regular NICs. We also use these measurements to validate an analytical model that we formulate to predict performance when scaling to larger systems. Our proposed FPGA-based AI smart NIC enhances overall training performance by 1.6x at 6 nodes, with an estimated 2.5x performance improvement at 32 nodes, compared to the baseline system using conventional NICs. △ Less","22 April, 2022",https://arxiv.org/pdf/2204.10943
Development of an algorithm for medical image segmentation of bone tissue in interaction with metallic implants,Fernando García-Torres;Carmen Mínguez-Porter;Julia Tomás-Chenoll;Sofía Iranzo-Egea;Juan-Manuel Belda-Lois,"This preliminary study focuses on the development of a medical image segmentation algorithm based on artificial intelligence for calculating bone growth in contact with metallic implants. %as a result of the problem of estimating the growth of new bone tissue due to artifacts. %the presence of various types of distortions and errors, known as artifacts. Two databases consisting of computerized microtomography images have been used throughout this work: 100 images for training and 196 images for testing. Both bone and implant tissue were manually segmented in the training data set. The type of network constructed follows the U-Net architecture, a convolutional neural network explicitly used for medical image segmentation. In terms of network accuracy, the model reached around 98\%. Once the prediction was obtained from the new data set (test set), the total number of pixels belonging to bone tissue was calculated. This volume is around 15\% of the volume estimated by conventional techniques, which are usually overestimated. This method has shown its good performance and results, although it has a wide margin for improvement, modifying various parameters of the networks or using larger databases to improve training. △ Less","22 April, 2022",https://arxiv.org/pdf/2204.10560
Application of Federated Learning in Building a Robust COVID-19 Chest X-ray Classification Model,Amartya Bhattacharya;Manish Gawali;Jitesh Seth;Viraj Kulkarni,"While developing artificial intelligence (AI)-based algorithms to solve problems, the amount of data plays a pivotal role - large amount of data helps the researchers and engineers to develop robust AI algorithms. In the case of building AI-based models for problems related to medical imaging, these data need to be transferred from the medical institutions where they were acquired to the organizations developing the algorithms. This movement of data involves time-consuming formalities like complying with HIPAA, GDPR, etc.There is also a risk of patients' private data getting leaked, compromising their confidentiality. One solution to these problems is using the Federated Learning framework. Federated Learning (FL) helps AI models to generalize better and create a robust AI model by using data from different sources having different distributions and data characteristics without moving all the data to a central server. In our paper, we apply the FL framework for training a deep learning model to solve a binary classification problem of predicting the presence or absence of COVID-19. We took three different sources of data and trained individual models on each source. Then we trained an FL model on the complete data and compared all the model performances. We demonstrated that the FL model performs better than the individual models. Moreover, the FL model performed at par with the model trained on all the data combined at a central server. Thus Federated Learning leads to generalized AI models without the cost of data transfer and regulatory overhead. △ Less","22 April, 2022",https://arxiv.org/pdf/2204.10505
Towards Involving End-users in Interactive Human-in-the-loop AI Fairness,Yuri Nakao;Simone Stumpf;Subeida Ahmed;Aisha Naseer;Lorenzo Strappelli,"Ensuring fairness in artificial intelligence (AI) is important to counteract bias and discrimination in far-reaching applications. Recent work has started to investigate how humans judge fairness and how to support machine learning (ML) experts in making their AI models fairer. Drawing inspiration from an Explainable AI (XAI) approach called \emph{explanatory debugging} used in interactive machine learning, our work explores designing interpretable and interactive human-in-the-loop interfaces that allow ordinary end-users without any technical or domain background to identify potential fairness issues and possibly fix them in the context of loan decisions. Through workshops with end-users, we co-designed and implemented a prototype system that allowed end-users to see why predictions were made, and then to change weights on features to ""debug"" fairness issues. We evaluated the use of this prototype system through an online study. To investigate the implications of diverse human values about fairness around the globe, we also explored how cultural dimensions might play a role in using this prototype. Our results contribute to the design of interfaces to allow end-users to be involved in judging and addressing AI fairness through a human-in-the-loop approach. △ Less","21 April, 2022",https://arxiv.org/pdf/2204.10464
"Facilitating automated conversion of scientific knowledge into scientific simulation models with the Machine Assisted Generation, Calibration, and Comparison (MAGCC) Framework",Chase Cockrell;Scott Christley;Gary An,"The Machine Assisted Generation, Comparison, and Calibration (MAGCC) framework provides machine assistance and automation of recurrent crucial steps and processes in the development, implementation, testing, and use of scientific simulation models. MAGCC bridges systems for knowledge extraction via natural language processing or extracted from existing mathematical models and provides a comprehensive workflow encompassing the composition of scientific models and artificial intelligence (AI) assisted code generation. MAGCC accomplishes this through: 1) the development of a comprehensively expressive formal knowledge representation knowledgebase, the Structured Scientific Knowledge Representation (SSKR) that encompasses all the types of information needed to make any simulation model, 2) the use of an artificially intelligent logic reasoning system, the Computational Modeling Assistant (CMA), that takes information from the SSKR and generates, in a traceable fashion, model specifications across a range of simulation modeling methods, and 3) the use of the CMA to generate executable code for a simulation model from those model specifications. The MAGCC framework can be customized any scientific domain, and future work will integrate newly developed code-generating AI systems. △ Less","21 April, 2022",https://arxiv.org/pdf/2204.10382
The 6th AI City Challenge,Milind Naphade;Shuo Wang;David C. Anastasiu;Zheng Tang;Ming-Ching Chang;Yue Yao;Liang Zheng;Mohammed Shaiqur Rahman;Archana Venkatachalapathy;Anuj Sharma;Qi Feng;Vitaly Ablavsky;Stan Sclaroff;Pranamesh Chakraborty;Alice Li;Shangru Li;Rama Chellappa,"The 6th edition of the AI City Challenge specifically focuses on problems in two domains where there is tremendous unlocked potential at the intersection of computer vision and artificial intelligence: Intelligent Traffic Systems (ITS), and brick and mortar retail businesses. The four challenge tracks of the 2022 AI City Challenge received participation requests from 254 teams across 27 countries. Track 1 addressed city-scale multi-target multi-camera (MTMC) vehicle tracking. Track 2 addressed natural-language-based vehicle track retrieval. Track 3 was a brand new track for naturalistic driving analysis, where the data were captured by several cameras mounted inside the vehicle focusing on driver safety, and the task was to classify driver actions. Track 4 was another new track aiming to achieve retail store automated checkout using only a single view camera. We released two leader boards for submissions based on different methods, including a public leader board for the contest, where no use of external data is allowed, and a general leader board for all submitted results. The top performance of participating teams established strong baselines and even outperformed the state-of-the-art in the proposed challenge tracks. △ Less","9 June, 2022",https://arxiv.org/pdf/2204.10380
Creative Problem Solving in Artificially Intelligent Agents: A Survey and Framework,Evana Gizzi;Lakshmi Nair;Sonia Chernova;Jivko Sinapov,"Creative Problem Solving (CPS) is a sub-area within Artificial Intelligence (AI) that focuses on methods for solving off-nominal, or anomalous problems in autonomous systems. Despite many advancements in planning and learning, resolving novel problems or adapting existing knowledge to a new context, especially in cases where the environment may change in unpredictable ways post deployment, remains a limiting factor in the safe and useful integration of intelligent systems. The emergence of increasingly autonomous systems dictates the necessity for AI agents to deal with environmental uncertainty through creativity. To stimulate further research in CPS, we present a definition and a framework of CPS, which we adopt to categorize existing AI methods in this field. Our framework consists of four main components of a CPS problem, namely, 1) problem formulation, 2) knowledge representation, 3) method of knowledge manipulation, and 4) method of evaluation. We conclude our survey with open research questions, and suggested directions for the future. △ Less","21 April, 2022",https://arxiv.org/pdf/2204.10358
Condition Monitoring of Transformer Bushings Using Computational Intelligence,Joshua Tshifhiwa Maumela,"Dissolved Gas-in-oil analysis (DGA) is used to monitor the condition of bushings on large power transformers. There are different techniques used in determining the conditions from the data collected, but in this work the Artificial Intelligence techniques are investigated. This work investigates which gases in DGA are related to each other and which ones are important for making decisions. When the related and crucial gases are determined, the other gases are discarded thereby reducing the number of attributes in DGA. Hence a further investigation is done to see how these new datasets influence the performance of the classifiers used to classify the DGA of full attributes. The classifiers used in these experiments were Backpropagation Neural Networks (BPNN) and Support Vector Machines (SVM) whereas the Principal Component Analysis (PCA), Rough Set (RS), Incremental Granular Ranking (GR++) and Decision Trees (DT) were used to reduce the attributes of the dataset. The parameters used when training the BPNN and SVM classifiers are kept fixed to create a controlled test environment when investigating the effects of reducing the number of gases. This work further introduced a new classifier that can handle high dimension dataset and noisy dataset, Rough Neural Network (RNN). △ Less","20 April, 2022",https://arxiv.org/pdf/2204.10193
Social Media Sentiment Analysis for Cryptocurrency Market Prediction,Ali Raheman;Anton Kolonin;Igors Fridkins;Ikram Ansari;Mukul Vishwas,"In this paper, we explore the usability of different natural language processing models for the sentiment analysis of social media applied to financial market prediction, using the cryptocurrency domain as a reference. We study how the different sentiment metrics are correlated with the price movements of Bitcoin. For this purpose, we explore different methods to calculate the sentiment metrics from a text finding most of them not very accurate for this prediction task. We find that one of the models outperforms more than 20 other public ones and makes it possible to fine-tune it efficiently given its interpretable nature. Thus we confirm that interpretable artificial intelligence and natural language processing methods might be more valuable practically than non-explainable and non-interpretable ones. In the end, we analyse potential causal connections between the different sentiment metrics and the price movements. △ Less","18 April, 2022",https://arxiv.org/pdf/2204.10185
Distributed Learning for Vehicular Dynamic Spectrum Access in Autonomous Driving,Pawe\{l} Sroka;Adrian Kliks,"Reliable wireless communication between the autonomously driving cars is one of the fundamental needs for guaranteeing passenger safety and comfort. However, when the number of communicating cars increases, the transmission quality may be significantly degraded due to too high occupancy radio of the used frequency band. In this paper, we concentrate on the autonomous vehicle-platooning use-case, where intra-platoon communication is done in the dynamically selected frequency band, other than nominally devoted for such purposes. The carrier selection is done in a flexible manner with the support of the context database located at the roadside unit (edge of wireless communication infrastructure). However, as the database delivers only context information to the platoons' leaders, the final decision is made separately by the individual platoons, following the suggestions made by the artificial intelligence algorithms. In this work, we concentrate on a lightweight Q-learning solution, that could be successfully implemented in each car for dynamic channel selection. △ Less","22 March, 2022",https://arxiv.org/pdf/2204.10179
Eliminating Backdoor Triggers for Deep Neural Networks Using Attention Relation Graph Distillation,Jun Xia;Ting Wang;Jiepin Ding;Xian Wei;Mingsong Chen,"Due to the prosperity of Artificial Intelligence (AI) techniques, more and more backdoors are designed by adversaries to attack Deep Neural Networks (DNNs).Although the state-of-the-art method Neural Attention Distillation (NAD) can effectively erase backdoor triggers from DNNs, it still suffers from non-negligible Attack Success Rate (ASR) together with lowered classification ACCuracy (ACC), since NAD focuses on backdoor defense using attention features (i.e., attention maps) of the same order. In this paper, we introduce a novel backdoor defense framework named Attention Relation Graph Distillation (ARGD), which fully explores the correlation among attention features with different orders using our proposed Attention Relation Graphs (ARGs). Based on the alignment of ARGs between both teacher and student models during knowledge distillation, ARGD can eradicate more backdoor triggers than NAD. Comprehensive experimental results show that, against six latest backdoor attacks, ARGD outperforms NAD by up to 94.85% reduction in ASR, while ACC can be improved by up to 3.23%. △ Less","23 April, 2022",https://arxiv.org/pdf/2204.09975
Perception Visualization: Seeing Through the Eyes of a DNN,Loris Giulivi;Mark James Carman;Giacomo Boracchi,"Artificial intelligence (AI) systems power the world we live in. Deep neural networks (DNNs) are able to solve tasks in an ever-expanding landscape of scenarios, but our eagerness to apply these powerful models leads us to focus on their performance and deprioritises our ability to understand them. Current research in the field of explainable AI tries to bridge this gap by developing various perturbation or gradient-based explanation techniques. For images, these techniques fail to fully capture and convey the semantic information needed to elucidate why the model makes the predictions it does. In this work, we develop a new form of explanation that is radically different in nature from current explanation methods, such as Grad-CAM. Perception visualization provides a visual representation of what the DNN perceives in the input image by depicting what visual patterns the latent representation corresponds to. Visualizations are obtained through a reconstruction model that inverts the encoded features, such that the parameters and predictions of the original models are not modified. Results of our user study demonstrate that humans can better understand and predict the system's decisions when perception visualizations are available, thus easing the debugging and deployment of deep models as trusted systems. △ Less","21 April, 2022",https://arxiv.org/pdf/2204.09920
Recent Progress in Conversational AI,Zijun Xue;Ruirui Li;Mingda Li,"Conversational artificial intelligence (AI) is becoming an increasingly popular topic among industry and academia. With the fast development of neural network-based models, a lot of neural-based conversational AI system are developed. We will provide a brief review of the recent progress in the Conversational AI, including the commonly adopted techniques, notable works, famous competitions from academia and industry and widely used datasets. △ Less","8 April, 2022",https://arxiv.org/pdf/2204.09719
Generative Pre-Trained Transformers for Biologically Inspired Design,Qihao Zhu;Xinyu Zhang;Jianxi Luo,"Biological systems in nature have evolved for millions of years to adapt and survive the environment. Many features they developed can be inspirational and beneficial for solving technical problems in modern industries. This leads to a novel form of design-by-analogy called bio-inspired design (BID). Although BID as a design method has been proven beneficial, the gap between biology and engineering continuously hinders designers from effectively applying the method. Therefore, we explore the recent advance of artificial intelligence (AI) for a computational approach to bridge the gap. This paper proposes a generative design approach based on the pre-trained language model (PLM) to automatically retrieve and map biological analogy and generate BID in the form of natural language. The latest generative pre-trained transformer, namely GPT-3, is used as the base PLM. Three types of design concept generators are identified and fine-tuned from the PLM according to the looseness of the problem space representation. Machine evaluators are also fine-tuned to assess the correlation between the domains within the generated BID concepts. The approach is then tested via a case study in which the fine-tuned models are applied to generate and evaluate light-weighted flying car concepts inspired by nature. The results show our approach can generate BID concepts with good performance. △ Less","31 March, 2022",https://arxiv.org/pdf/2204.09714
Generative Design Ideation: A Natural Language Generation Approach,Qihao Zhu;Jianxi Luo,"This paper aims to explore a generative approach for knowledge-based design ideation by applying the latest pre-trained language models in artificial intelligence (AI). Specifically, a method of fine-tuning the generative pre-trained transformer using the USPTO patent database is proposed. The AI-generated ideas are not only in concise and understandable language but also able to synthesize the target design with external knowledge sources with controllable knowledge distance. The method is tested in a case study of rolling toy design and the results show good performance in generating ideas of varied novelty with near-field and far-field source knowledge. △ Less","28 March, 2022",https://arxiv.org/pdf/2204.09658
The MIT Voice Name System,Brian Subirana;Harry Levinson;Ferran Hueto;Prithvi Rajasekaran;Alexander Gaidis;Esteve Tarragó;Peter Oliveira-Soens,"This RFC white Paper summarizes our progress on the MIT Voice Name System (VNS) and Huey. The VNS, similar in name and function to the DNS, is a system to reserve and use ""wake words"" to activate Artificial Intelligence (AI) devices. Just like you can say ""Hey Siri"" to activate Apple's personal assistant, we propose using the VNS in smart speakers and other devices to route wake requests based on commands such as ""turn off"", ""open grocery shopping list"" or ""271, start flash card review of my computer vision class"". We also introduce Huey, an unambiguous Natural Language to interact with AI devices. We aim to standardize voice interactions to a universal reach similar to that of other systems such as phone numbering, with an agreed world-wide approach to assign and use numbers, or the Internet's DNS, with a standard naming system, that has helped flourish popular services including the World-Wide-Web, FTP, and email. Just like these standards are ""neutral"", we also aim to endow the VNS with ""wake neutrality"" so that each participant can develop its own digital voice. We focus on voice as a starting point to talk to any IoT object and explain briefly how the VNS may be expanded to other AI technologies enabling person-to-machine conversations (really machine-to-machine), including computer vision or neural interfaces. We also describe briefly considerations for a broader set of standards, MIT Open AI (MOA), including a reference architecture to serve as a starting point for the development of a general conversational commerce infrastructure that has standard ""Wake Words"", NLP commands such as ""Shopping Lists"" or ""Flash Card Reviews"", and personalities such as Pi or 271. Privacy and security are key elements considered because of speech-to-text errors and the amount of personal information contained in a voice sample. △ Less","28 March, 2022",https://arxiv.org/pdf/2204.09657
A Survey and Perspective on Artificial Intelligence for Security-Aware Electronic Design Automation,David Selasi Koblah;Rabin Yu Acharya;Daniel Capecci;Olivia P. Dizon-Paradis;Shahin Tajik;Fatemeh Ganji;Damon L. Woodard;Domenic Forte,"Artificial intelligence (AI) and machine learning (ML) techniques have been increasingly used in several fields to improve performance and the level of automation. In recent years, this use has exponentially increased due to the advancement of high-performance computing and the ever increasing size of data. One of such fields is that of hardware design; specifically the design of digital and analog integrated circuits~(ICs), where AI/ ML techniques have been extensively used to address ever-increasing design complexity, aggressive time-to-market, and the growing number of ubiquitous interconnected devices (IoT). However, the security concerns and issues related to IC design have been highly overlooked. In this paper, we summarize the state-of-the-art in AL/ML for circuit design/optimization, security and engineering challenges, research in security-aware CAD/EDA, and future research directions and needs for using AI/ML for security-aware circuit design. △ Less","20 April, 2022",https://arxiv.org/pdf/2204.09579
Noise mitigation strategies in physical feedforward neural networks,Nadezhda Semenova;Daniel Brunner,"Physical neural networks are promising candidates for next generation artificial intelligence hardware. In such architectures, neurons and connections are physically realized and do not leverage digital concepts with their practically infinite signal-to-noise ratio to encode, transduce and transform information. They therefore are prone to noise with a variety of statistical and architectural properties, and effective strategies leveraging network-inherent assets to mitigate noise in an hardware-efficient manner are important in the pursuit of next generation neural network hardware. Based on analytical derivations, we here introduce and analyse a variety of different noise-mitigation approaches. We analytically show that intra-layer connections in which the connection matrix's squared mean exceeds the mean of its square fully suppresses uncorrelated noise. We go beyond and develop two synergistic strategies for noise that is uncorrelated and correlated across populations of neurons. First, we introduce the concept of ghost neurons, where each group of neurons perturbed by correlated noise has a negative connection to a single neuron, yet without receiving any input information. Secondly, we show that pooling of neuron populations is an efficient approach to suppress uncorrelated noise. As such, we developed a general noise mitigation strategy leveraging the statistical properties of the different noise terms most relevant in analogue hardware. Finally, we demonstrate the effectiveness of this combined approach for trained neural network classifying the MNIST handwritten digits, for which we achieve a 4-fold improvement of the output signal-to-noise ratio and increase the classification accuracy almost to the level of the noise-free network. △ Less","18 May, 2022",https://arxiv.org/pdf/2204.09461
"Enabling Dynamic and Intelligent Workflows for HPC, Data Analytics, and AI Convergence",Jorge Ejarque;Rosa M. Badia;Loïc Albertin;Giovanni Aloisio;Enrico Baglione;Yolanda Becerra;Stefan Boschert;Julian R. Berlin;Alessandro D'Anca;Donatello Elia;François Exertier;Sandro Fiore;José Flich;Arnau Folch;Steven J Gibbons;Nikolay Koldunov;Francesc Lordan;Stefano Lorito;Finn Løvholt;Jorge Macías;Fabrizio Marozzo;Alberto Michelini;Marisol Monterrubio-Velasco;Marta Pienkowska;Josep de la Puente,"The evolution of High-Performance Computing (HPC) platforms enables the design and execution of progressively larger and more complex workflow applications in these systems. The complexity comes not only from the number of elements that compose the workflows but also from the type of computations they perform. While traditional HPC workflows target simulations and modelling of physical phenomena, current needs require in addition data analytics (DA) and artificial intelligence (AI) tasks. However, the development of these workflows is hampered by the lack of proper programming models and environments that support the integration of HPC, DA, and AI, as well as the lack of tools to easily deploy and execute the workflows in HPC systems. To progress in this direction, this paper presents use cases where complex workflows are required and investigates the main issues to be addressed for the HPC/DA/AI convergence. Based on this study, the paper identifies the challenges of a new workflow platform to manage complex workflows. Finally, it proposes a development approach for such a workflow platform addressing these challenges in two directions: first, by defining a software stack that provides the functionalities to manage these complex workflows; and second, by proposing the HPC Workflow as a Service (HPCWaaS) paradigm, which leverages the software stack to facilitate the reusability of complex workflows in federated HPC infrastructures. Proposals presented in this work are subject to study and development as part of the EuroHPC eFlows4HPC project. △ Less","13 May, 2022",https://arxiv.org/pdf/2204.09287
Characterization and Optimization of Integrated Silicon-Photonic Neural Networks under Fabrication-Process Variations,Asif Mirza;Amin Shafiee;Sanmitra Banerjee;Krishnendu Chakrabarty;Sudeep Pasricha;Mahdi Nikdast,"Silicon-photonic neural networks (SPNNs) have emerged as promising successors to electronic artificial intelligence (AI) accelerators by offering orders of magnitude lower latency and higher energy efficiency. Nevertheless, the underlying silicon photonic devices in SPNNs are sensitive to inevitable fabrication-process variations (FPVs) stemming from optical lithography imperfections. Consequently, the inferencing accuracy in an SPNN can be highly impacted by FPVs -- e.g., can drop to below 10% -- the impact of which is yet to be fully studied. In this paper, we, for the first time, model and explore the impact of FPVs in the waveguide width and silicon-on-insulator (SOI) thickness in coherent SPNNs that use Mach-Zehnder Interferometers (MZIs). Leveraging such models, we propose a novel variation-aware, design-time optimization solution to improve MZI tolerance to different FPVs in SPNNs. Simulation results for two example SPNNs of different scales under realistic and correlated FPVs indicate that the optimized MZIs can improve the inferencing accuracy by up to 93.95% for the MNIST handwritten digit dataset -- considered as an example in this paper -- which corresponds to a <0.5% accuracy loss compared to the variation-free case. The proposed one-time optimization method imposes low area overhead, and hence is applicable even to resource-constrained designs △ Less","19 April, 2022",https://arxiv.org/pdf/2204.09153
GAM(e) changer or not? An evaluation of interpretable machine learning models based on additive model constraints,Patrick Zschech;Sven Weinzierl;Nico Hambauer;Sandra Zilker;Mathias Kraus,"The number of information systems (IS) studies dealing with explainable artificial intelligence (XAI) is currently exploding as the field demands more transparency about the internal decision logic of machine learning (ML) models. However, most techniques subsumed under XAI provide post-hoc-analytical explanations, which have to be considered with caution as they only use approximations of the underlying ML model. Therefore, our paper investigates a series of intrinsically interpretable ML models and discusses their suitability for the IS community. More specifically, our focus is on advanced extensions of generalized additive models (GAM) in which predictors are modeled independently in a non-linear way to generate shape functions that can capture arbitrary patterns but remain fully interpretable. In our study, we evaluate the prediction qualities of five GAMs as compared to six traditional ML models and assess their visual outputs for model interpretability. On this basis, we investigate their merits and limitations and derive design implications for further improvements. △ Less","19 April, 2022",https://arxiv.org/pdf/2204.09123
Factors that influence the adoption of human-AI collaboration in clinical decision-making,Patrick Hemmer;Max Schemmer;Lara Riefle;Nico Rosellen;Michael Vössing;Niklas Kühl,"Recent developments in Artificial Intelligence (AI) have fueled the emergence of human-AI collaboration, a setting where AI is a coequal partner. Especially in clinical decision-making, it has the potential to improve treatment quality by assisting overworked medical professionals. Even though research has started to investigate the utilization of AI for clinical decision-making, its potential benefits do not imply its adoption by medical professionals. While several studies have started to analyze adoption criteria from a technical perspective, research providing a human-centered perspective with a focus on AI's potential for becoming a coequal team member in the decision-making process remains limited. Therefore, in this work, we identify factors for the adoption of human-AI collaboration by conducting a series of semi-structured interviews with experts in the healthcare domain. We identify six relevant adoption factors and highlight existing tensions between them and effective human-AI collaboration. △ Less","19 April, 2022",https://arxiv.org/pdf/2204.09082
Sampling Strategies for Static Powergrid Models,Stephan Balduin;Eric MSP Veith;Sebastian Lehnhoff,"Machine learning and computational intelligence technologies gain more and more popularity as possible solution for issues related to the power grid. One of these issues, the power flow calculation, is an iterative method to compute the voltage magnitudes of the power grid's buses from power values. Machine learning and, especially, artificial neural networks were successfully used as surrogates for the power flow calculation. Artificial neural networks highly rely on the quality and size of the training data, but this aspect of the process is apparently often neglected in the works we found. However, since the availability of high quality historical data for power grids is limited, we propose the Correlation Sampling algorithm. We show that this approach is able to cover a larger area of the sampling space compared to different random sampling algorithms from the literature and a copula-based approach, while at the same time inter-dependencies of the inputs are taken into account, which, from the other algorithms, only the copula-based approach does. △ Less","19 April, 2022",https://arxiv.org/pdf/2204.09053
On the Influence of Explainable AI on Automation Bias,Max Schemmer;Niklas Kühl;Carina Benz;Gerhard Satzger,"Artificial intelligence (AI) is gaining momentum, and its importance for the future of work in many areas, such as medicine and banking, is continuously rising. However, insights on the effective collaboration of humans and AI are still rare. Typically, AI supports humans in decision-making by addressing human limitations. However, it may also evoke human bias, especially in the form of automation bias as an over-reliance on AI advice. We aim to shed light on the potential to influence automation bias by explainable AI (XAI). In this pre-test, we derive a research model and describe our study design. Subsequentially, we conduct an online experiment with regard to hotel review classifications and discuss first results. We expect our research to contribute to the design and development of safe hybrid intelligence systems. △ Less","19 April, 2022",https://arxiv.org/pdf/2204.08859
Metamorphic Testing-based Adversarial Attack to Fool Deepfake Detectors,Nyee Thoang Lim;Meng Yi Kuan;Muxin Pu;Mei Kuan Lim;Chun Yong Chong,"Deepfakes utilise Artificial Intelligence (AI) techniques to create synthetic media where the likeness of one person is replaced with another. There are growing concerns that deepfakes can be maliciously used to create misleading and harmful digital contents. As deepfakes become more common, there is a dire need for deepfake detection technology to help spot deepfake media. Present deepfake detection models are able to achieve outstanding accuracy (>90%). However, most of them are limited to within-dataset scenario, where the same dataset is used for training and testing. Most models do not generalise well enough in cross-dataset scenario, where models are tested on unseen datasets from another source. Furthermore, state-of-the-art deepfake detection models rely on neural network-based classification models that are known to be vulnerable to adversarial attacks. Motivated by the need for a robust deepfake detection model, this study adapts metamorphic testing (MT) principles to help identify potential factors that could influence the robustness of the examined model, while overcoming the test oracle problem in this domain. Metamorphic testing is specifically chosen as the testing technique as it fits our demand to address learning-based system testing with probabilistic outcomes from largely black-box components, based on potentially large input domains. We performed our evaluations on MesoInception-4 and TwoStreamNet models, which are the state-of-the-art deepfake detection models. This study identified makeup application as an adversarial attack that could fool deepfake detectors. Our experimental results demonstrate that both the MesoInception-4 and TwoStreamNet models degrade in their performance by up to 30\% when the input data is perturbed with makeup. △ Less","31 May, 2022",https://arxiv.org/pdf/2204.08612
LwHBench: A low-level hardware component benchmark and dataset for Single Board Computers,Pedro Miguel Sánchez Sánchez;José María Jorquera Valero;Alberto Huertas Celdrán;Gérôme Bovet;Manuel Gil Pérez;Gregorio Martínez Pérez,"In today's computing environment, where Artificial Intelligence (AI) and data processing are moving toward the Internet of Things (IoT) and Edge computing paradigms, benchmarking resource-constrained devices is a critical task to evaluate their suitability and performance. Between the employed devices, Single-Board Computers arise as multi-purpose and affordable systems. The literature has explored Single-Board Computers performance when running high-level benchmarks specialized in particular application scenarios, such as AI or medical applications. However, lower-level benchmarking applications and datasets are needed to enable new Edge-based AI solutions for network, system and service management based on device and component performance, such as individual device identification. Thus, this paper presents LwHBench, a low-level hardware benchmarking application for Single-Board Computers that measures the performance of CPU, GPU, Memory and Storage taking into account the component constraints in these types of devices. LwHBench has been implemented for Raspberry Pi devices and run for 100 days on a set of 45 devices to generate an extensive dataset that allows the usage of AI techniques in scenarios where performance data can help in the device management process. Besides, to demonstrate the inter-scenario capability of the dataset, a series of AI-enabled use cases about device identification and context impact on performance are presented as exploration of the published data. Finally, the benchmark application has been adapted and applied to an agriculture-focused scenario where three RockPro64 devices are present. △ Less","24 October, 2022",https://arxiv.org/pdf/2204.08516
Comparative analysis of machine learning and numerical modeling for combined heat transfer in Polymethylmethacrylate,Mahsa Dehghan Manshadi;Nima Alafchi;Alireza Taat;Milad Mousavi;Amir Mosavi,"This study compares different methods to predict the simultaneous effects of conductive and radiative heat transfer in a Polymethylmethacrylate (PMMA) sample. PMMA is a kind of polymer utilized in various sensors and actuator devices. One-dimensional combined heat transfer is considered in numerical analysis. Computer implementation was obtained for the numerical solution of governing equation with the implicit finite difference method in the case of discretization. Kirchhoff transformation was used to get data from a non-linear equation of conductive heat transfer by considering monochromatic radiation intensity and temperature conditions applied to the PMMA sample boundaries. For Deep Neural Network (DNN) method, the novel Long Short Term Memory (LSTM) method was introduced to find accurate results in the least processing time than the numerical method. A recent study derived the combined heat transfers and their temperature profiles for the PMMA sample. Furthermore, the transient temperature profile is validated by another study. A comparison proves a perfect agreement. It shows the temperature gradient in the primary positions that makes a spectral amount of conductive heat transfer from a PMMA sample. It is more straightforward when they are compared with the novel DNN method. Results demonstrate that this artificial intelligence method is accurate and fast in predicting problems. By analyzing the results from the numerical solution it can be understood that the conductive and radiative heat flux is similar in the case of gradient behavior, but it is also twice in its amount approximately. Hence, total heat flux has a constant value in an approximated steady state condition. In addition to analyzing their composition, ROC curve and confusion matrix were implemented to evaluate the algorithm performance. △ Less","12 April, 2022",https://arxiv.org/pdf/2204.08459
Intelligent Explorations of the String Theory Landscape,Andrei Constantin,"The goal of identifying the Standard Model of particle physics and its extensions within string theory has been one of the principal driving forces in string phenomenology. Recently, the incorporation of artificial intelligence in string theory and certain theoretical advancements have brought to light unexpected solutions to mathematical hurdles that have so far hindered progress in this direction. In this review we focus on model building efforts in the context of the E_8\times E_8 heterotic string compactified on smooth Calabi-Yau threefolds and discuss several areas in which machine learning is expected to make a difference. △ Less","28 April, 2022",https://arxiv.org/pdf/2204.08073
COVIBOT: A Smart Chatbot for Assistance and E-Awareness during COVID-19 Pandemic,Maha Driss;Iman Almomani;Leen Alahmadi;Linah Alhajjam;Raghad Alharbi;Shahad Alanazi,"The coronavirus pandemic has spread over the past two years in our highly connected and information-dense society. Nonetheless, disseminating accurate and up-to-date information on the spread of this pandemic remains a challenge. In this context, opting for a solution based on conversational artificial intelligence, also known under the name of the chatbot, is proving to be an unavoidable solution, especially since it has already shown its effectiveness in fighting the coronavirus crisis in several countries. This work proposes to design and implement a smart chatbot on the theme of COVID-19, called COVIBOT, which will be useful in the context of Saudi Arabia. COVIBOT is a generative-based contextual chatbot, which is built using machine learning APIs that are offered by the cloud-based Azure Cognitive Services. Two versions of COVIBOT are offered: English and Arabic versions. Use cases of COVIBOT are tested and validated using a scenario-based approach. △ Less","16 April, 2022",https://arxiv.org/pdf/2204.07851
"Contextualizing Artificially Intelligent Morality: A Meta-Ethnography of Top-Down, Bottom-Up, and Hybrid Models for Theoretical and Applied Ethics in Artificial Intelligence",Jennafer S. Roberts;Laura N. Montoya,"In this meta-ethnography, we explore three different angles of ethical artificial intelligence (AI) design implementation including the philosophical ethical viewpoint, the technical perspective, and framing through a political lens. Our qualitative research includes a literature review that highlights the cross-referencing of these angles by discussing the value and drawbacks of contrastive top-down, bottom-up, and hybrid approaches previously published. The novel contribution to this framework is the political angle, which constitutes ethics in AI either being determined by corporations and governments and imposed through policies or law (coming from the top), or ethics being called for by the people (coming from the bottom), as well as top-down, bottom-up, and hybrid technicalities of how AI is developed within a moral construct and in consideration of its users, with expected and unexpected consequences and long-term impact in the world. There is a focus on reinforcement learning as an example of a bottom-up applied technical approach and AI ethics principles as a practical top-down approach. This investigation includes real-world case studies to impart a global perspective, as well as philosophical debate on the ethics of AI and theoretical future thought experimentation based on historical facts, current world circumstances, and possible ensuing realities. △ Less","8 September, 2022",https://arxiv.org/pdf/2204.07612
An Introductory Review of Spiking Neural Network and Artificial Neural Network: From Biological Intelligence to Artificial Intelligence,Shengjie Zheng;Lang Qian;Pingsheng Li;Chenggang He;Xiaoqin Qin;Xiaojian Li,"Recently, stemming from the rapid development of artificial intelligence, which has gained expansive success in pattern recognition, robotics, and bioinformatics, neuroscience is also gaining tremendous progress. A kind of spiking neural network with biological interpretability is gradually receiving wide attention, and this kind of neural network is also regarded as one of the directions toward general artificial intelligence. This review introduces the following sections, the biological background of spiking neurons and the theoretical basis, different neuronal models, the connectivity of neural circuits, the mainstream neural network learning mechanisms and network architectures, etc. This review hopes to attract different researchers and advance the development of brain-inspired intelligence and artificial intelligence. △ Less","9 April, 2022",https://arxiv.org/pdf/2204.07519
Cryogenic Neuromorphic Hardware,Md Mazharul Islam;Shamiul Alam;Md Shafayat Hossain;Kaushik Roy;Ahmedullah Aziz,"The revolution in artificial intelligence (AI) brings up an enormous storage and data processing requirement. Large power consumption and hardware overhead have become the main challenges for building next-generation AI hardware. To mitigate this, Neuromorphic computing has drawn immense attention due to its excellent capability for data processing with very low power consumption. While relentless research has been underway for years to minimize the power consumption in neuromorphic hardware, we are still a long way off from reaching the energy efficiency of the human brain. Furthermore, design complexity and process variation hinder the large-scale implementation of current neuromorphic platforms. Recently, the concept of implementing neuromorphic computing systems in cryogenic temperature has garnered intense interest thanks to their excellent speed and power metric. Several cryogenic devices can be engineered to work as neuromorphic primitives with ultra-low demand for power. Here we comprehensively review the cryogenic neuromorphic hardware. We classify the existing cryogenic neuromorphic hardware into several hierarchical categories and sketch a comparative analysis based on key performance metrics. Our analysis concisely describes the operation of the associated circuit topology and outlines the advantages and challenges encountered by the state-of-the-art technology platforms. Finally, we provide insights to circumvent these challenges for the future progression of research. △ Less","24 August, 2022",https://arxiv.org/pdf/2204.07503
An interpretable machine learning approach for ferroalloys consumptions,Nick Knyazev,"This paper is devoted to a practical method for ferroalloys consumption modeling and optimization. We consider the problem of selecting the optimal process control parameters based on the analysis of historical data from sensors. We developed approach, which predicts results of chemical reactions and give ferroalloys consumption recommendation. The main features of our method are easy interpretation and noise resistance. Our approach is based on k-means clustering algorithm, decision trees and linear regression. The main idea of the method is to identify situations where processes go similarly. For this, we propose using a k-means based dataset clustering algorithm and a classification algorithm to determine the cluster. This algorithm can be also applied to various technological processes, in this article, we demonstrate its application in metallurgy. To test the application of the proposed method, we used it to optimize ferroalloys consumption in Basic Oxygen Furnace steelmaking when finishing steel in a ladle furnace. The minimum required element content for a given steel grade was selected as the predictive model's target variable, and the required amount of the element to be added to the melt as the optimized variable. Keywords: Clustering, Machine Learning, Linear Regression, Steelmaking, Optimization, Gradient Boosting, Artificial Intelligence, Decision Trees, Recommendation services △ Less","15 April, 2022",https://arxiv.org/pdf/2204.07421
EXPERT: Public Benchmarks for Dynamic Heterogeneous Academic Graphs,Sameera Horawalavithana;Ellyn Ayton;Anastasiya Usenko;Shivam Sharma;Jasmine Eshun;Robin Cosbey;Maria Glenski;Svitlana Volkova,"Machine learning models that learn from dynamic graphs face nontrivial challenges in learning and inference as both nodes and edges change over time. The existing large-scale graph benchmark datasets that are widely used by the community primarily focus on homogeneous node and edge attributes and are static. In this work, we present a variety of large scale, dynamic heterogeneous academic graphs to test the effectiveness of models developed for multi-step graph forecasting tasks. Our novel datasets cover both context and content information extracted from scientific publications across two communities: Artificial Intelligence (AI) and Nuclear Nonproliferation (NN). In addition, we propose a systematic approach to improve the existing evaluation procedures used in the graph forecasting models. △ Less","14 April, 2022",https://arxiv.org/pdf/2204.07203
Recent Advances and New Frontiers in Spiking Neural Networks,Duzhen Zhang;Shuncheng Jia;Qingyu Wang,"In recent years, spiking neural networks (SNNs) have received extensive attention in brain-inspired intelligence due to their rich spatially-temporal dynamics, various encoding methods, and event-driven characteristics that naturally fit the neuromorphic hardware. With the development of SNNs, brain-inspired intelligence, an emerging research field inspired by brain science achievements and aiming at artificial general intelligence, is becoming hot. This paper reviews recent advances and discusses new frontiers in SNNs from five major research topics, including essential elements (i.e., spiking neuron models, encoding methods, and topology structures), neuromorphic datasets, optimization algorithms, software, and hardware frameworks. We hope our survey can help researchers understand SNNs better and inspire new works to advance this field. △ Less","15 October, 2022",https://arxiv.org/pdf/2204.07050
State of the Art in Artificial Intelligence applied to the Legal Domain,João Dias;Pedro A. Santos;Nuno Cordeiro;Ana Antunes;Bruno Martins;Jorge Baptista;Carlos Gonçalves,"While Artificial Intelligence applied to the legal domain is a topic with origins in the last century, recent advances in Artificial Intelligence are posed to revolutionize it. This work presents an overview and contextualizes the main advances on the field of Natural Language Processing and how these advances have been used to further the state of the art in legal text analysis. △ Less","10 March, 2022",https://arxiv.org/pdf/2204.07047
XLMRQA: Open-Domain Question Answering on Vietnamese Wikipedia-based Textual Knowledge Source,Kiet Van Nguyen;Phong Nguyen-Thuan Do;Nhat Duy Nguyen;Tin Van Huynh;Anh Gia-Tuan Nguyen;Ngan Luu-Thuy Nguyen,"Question answering (QA) is a natural language understanding task within the fields of information retrieval and information extraction that has attracted much attention from the computational linguistics and artificial intelligence research community in recent years because of the strong development of machine reading comprehension-based models. A reader-based QA system is a high-level search engine that can find correct answers to queries or questions in open-domain or domain-specific texts using machine reading comprehension (MRC) techniques. The majority of advancements in data resources and machine-learning approaches in the MRC and QA systems especially are developed significantly in two resource-rich languages such as English and Chinese. A low-resource language like Vietnamese has witnessed a scarcity of research on QA systems. This paper presents XLMRQA, the first Vietnamese QA system using a supervised transformer-based reader on the Wikipedia-based textual knowledge source (using the UIT-ViQuAD corpus), outperforming the two robust QA systems using deep neural network models: DrQA and BERTserini with 24.46% and 6.28%, respectively. From the results obtained on the three systems, we analyze the influence of question types on the performance of the QA systems. △ Less","13 August, 2022",https://arxiv.org/pdf/2204.07002
Should I Follow AI-based Advice? Measuring Appropriate Reliance in Human-AI Decision-Making,Max Schemmer;Patrick Hemmer;Niklas Kühl;Carina Benz;Gerhard Satzger,"Many important decisions in daily life are made with the help of advisors, e.g., decisions about medical treatments or financial investments. Whereas in the past, advice has often been received from human experts, friends, or family, advisors based on artificial intelligence (AI) have become more and more present nowadays. Typically, the advice generated by AI is judged by a human and either deemed reliable or rejected. However, recent work has shown that AI advice is not always beneficial, as humans have shown to be unable to ignore incorrect AI advice, essentially representing an over-reliance on AI. Therefore, the aspired goal should be to enable humans not to rely on AI advice blindly but rather to distinguish its quality and act upon it to make better decisions. Specifically, that means that humans should rely on the AI in the presence of correct advice and self-rely when confronted with incorrect advice, i.e., establish appropriate reliance (AR) on AI advice on a case-by-case basis. Current research lacks a metric for AR. This prevents a rigorous evaluation of factors impacting AR and hinders further development of human-AI decision-making. Therefore, based on the literature, we derive a measurement concept of AR. We propose to view AR as a two-dimensional construct that measures the ability to discriminate advice quality and behave accordingly. In this article, we derive the measurement concept, illustrate its application and outline potential future research. △ Less","14 April, 2022",https://arxiv.org/pdf/2204.06916
Explainable Analysis of Deep Learning Methods for SAR Image Classification,Shenghan Su;Ziteng Cui;Weiwei Guo;Zenghui Zhang;Wenxian Yu,"Deep learning methods exhibit outstanding performance in synthetic aperture radar (SAR) image interpretation tasks. However, these are black box models that limit the comprehension of their predictions. Therefore, to meet this challenge, we have utilized explainable artificial intelligence (XAI) methods for the SAR image classification task. Specifically, we trained state-of-the-art convolutional neural networks for each polarization format on OpenSARUrban dataset and then investigate eight explanation methods to analyze the predictions of the CNN classifiers of SAR images. These XAI methods are also evaluated qualitatively and quantitatively which shows that Occlusion achieves the most reliable interpretation performance in terms of Max-Sensitivity but with a low-resolution explanation heatmap. The explanation results provide some insights into the internal mechanism of black-box decisions for SAR image classification. △ Less","14 April, 2022",https://arxiv.org/pdf/2204.06783
HCFL: A High Compression Approach for Communication-Efficient Federated Learning in Very Large Scale IoT Networks,Minh-Duong Nguyen;Sang-Min Lee;Quoc-Viet Pham;Dinh Thai Hoang;Diep N. Nguyen;Won-Joo Hwang,"Federated learning (FL) is a new artificial intelligence concept that enables Internet-of-Things (IoT) devices to learn a collaborative model without sending the raw data to centralized nodes for processing. Despite numerous advantages, low computing resources at IoT devices and high communication costs for exchanging model parameters make applications of FL in massive IoT networks very limited. In this work, we develop a novel compression scheme for FL, called high-compression federated learning (HCFL), for very large scale IoT networks. HCFL can reduce the data load for FL processes without changing their structure and hyperparameters. In this way, we not only can significantly reduce communication costs, but also make intensive learning processes more adaptable on low-computing resource IoT devices. Furthermore, we investigate a relationship between the number of IoT devices and the convergence level of the FL model and thereby better assess the quality of the FL process. We demonstrate our HCFL scheme in both simulations and mathematical analyses. Our proposed theoretical research can be used as a minimum level of satisfaction, proving that the FL process can achieve good performance when a determined configuration is met. Therefore, we show that HCFL is applicable in any FL-integrated networks with numerous IoT devices. △ Less","21 June, 2022",https://arxiv.org/pdf/2204.06760
"Information and Communication Technology in Migration: A Framework for Applications, Customization, and Research",Ali Arya;Luciara Nardon;Md Riyadh,"This paper addresses the role of Information and Communication Technology (ICT) in migration governance, support, and experience with particular attention to emerging technologies such as artificial intelligence, social media, and virtual reality. We propose a framework for technology use based on user groups and process types. We provide examples of using emerging technologies for migration-related tasks within the context of this framework. We then identify how such technologies can be applied to migration-related tasks, developed for customized use, and improved through research to add new features that can help different migration stakeholders. We suggest a series of possible directions for future research and development to take advantage of specific affordances of those emerging technologies more effectively. △ Less","13 April, 2022",https://arxiv.org/pdf/2204.06611
Optimally Designing Cybersecurity Insurance Contracts to Encourage the Sharing of Medical Data,Yoon Lee;Anil Aswani,"Though the sharing of medical data has the potential to lead to breakthroughs in health care, the sharing process itself exposes patients and health care providers to various risks. Patients face risks due to the possible loss in privacy or livelihood that can occur when medical data is stolen or used in non-permitted ways, whereas health care providers face risks due to the associated liability. For medical data, these risks persist even after anonymizing/deidentifying, according to the standards defined in existing legislation, the data sets prior to sharing, because shared medical data can often be deanonymized/reidentified using advanced artificial intelligence and machine learning methodologies. As a result, health care providers are hesitant to share medical data. One possible solution to encourage health care providers to responsibly share data is through the use of cybersecurity insurance contracts. This paper studies the problem of designing optimal cybersecurity insurance contracts, with the goal of encouraging the sharing of the medical data. We use a principal-agent model with moral hazard to model various scenarios, derive the optimal contract, discuss its implications, and perform numerical case studies. In particular, we consider two scenarios: the first scenario is where a health care provider is selling medical data to a technology firm who is developing an artificial intelligence algorithm using the shared data. The second scenario is where a group of health care providers share health data amongst themselves for the purpose of furthering medical research using the aggregated medical data. △ Less","19 September, 2022",https://arxiv.org/pdf/2204.06549
DL4SciVis: A State-of-the-Art Survey on Deep Learning for Scientific Visualization,Chaoli Wang;Jun Han,"Since 2016, we have witnessed the tremendous growth of artificial intelligence+visualization (AI+VIS) research. However, existing survey papers on AI+VIS focus on visual analytics and information visualization, not scientific visualization (SciVis). In this paper, we survey related deep learning (DL) works in SciVis, specifically in the direction of DL4SciVis: designing DL solutions for solving SciVis problems. To stay focused, we primarily consider works that handle scalar and vector field data but exclude mesh data. We classify and discuss these works along six dimensions: domain setting, research task, learning type, network architecture, loss function, and evaluation metric. The paper concludes with a discussion of the remaining gaps to fill along the discussed dimensions and the grand challenges we need to tackle as a community. This state-of-the-art survey guides SciVis researchers in gaining an overview of this emerging topic and points out future directions to grow this research. △ Less","13 April, 2022",https://arxiv.org/pdf/2204.06504
Enabling Synthetic Data adoption in regulated domains,Giorgio Visani;Giacomo Graffi;Mattia Alfero;Enrico Bagli;Davide Capuzzo;Federico Chesani,"The switch from a Model-Centric to a Data-Centric mindset is putting emphasis on data and its quality rather than algorithms, bringing forward new challenges. In particular, the sensitive nature of the information in highly regulated scenarios needs to be accounted for. Specific approaches to address the privacy issue have been developed, as Privacy Enhancing Technologies. However, they frequently cause loss of information, putting forward a crucial trade-off among data quality and privacy. A clever way to bypass such a conundrum relies on Synthetic Data: data obtained from a generative process, learning the real data properties. Both Academia and Industry realized the importance of evaluating synthetic data quality: without all-round reliable metrics, the innovative data generation task has no proper objective function to maximize. Despite that, the topic remains under-explored. For this reason, we systematically catalog the important traits of synthetic data quality and privacy, and devise a specific methodology to test them. The result is DAISYnt (aDoption of Artificial Intelligence SYnthesis): a comprehensive suite of advanced tests, which sets a de facto standard for synthetic data evaluation. As a practical use-case, a variety of generative algorithms have been trained on real-world Credit Bureau Data. The best model has been assessed, using DAISYnt on the different synthetic replicas. Further potential uses, among others, entail auditing and fine-tuning of generative models or ensuring high quality of a given synthetic dataset. From a prescriptive viewpoint, eventually, DAISYnt may pave the way to synthetic data adoption in highly regulated domains, ranging from Finance to Healthcare, through Insurance and Education. △ Less","13 April, 2022",https://arxiv.org/pdf/2204.06297
Dynamic Dialogue Policy for Continual Reinforcement Learning,Christian Geishauser;Carel van Niekerk;Nurul Lubis;Michael Heck;Hsien-Chin Lin;Shutong Feng;Milica Gašić,"Continual learning is one of the key components of human learning and a necessary requirement of artificial intelligence. As dialogue can potentially span infinitely many topics and tasks, a task-oriented dialogue system must have the capability to continually learn, dynamically adapting to new challenges while preserving the knowledge it already acquired. Despite the importance, continual reinforcement learning of the dialogue policy has remained largely unaddressed. The lack of a framework with training protocols, baseline models and suitable metrics, has so far hindered research in this direction. In this work we fill precisely this gap, enabling research in dialogue policy optimisation to go from static to dynamic learning. We provide a continual learning algorithm, baseline architectures and metrics for assessing continual learning models. Moreover, we propose the dynamic dialogue policy transformer (DDPT), a novel dynamic architecture that can integrate new knowledge seamlessly, is capable of handling large state spaces and obtains significant zero-shot performance when being exposed to unseen domains, without any growth in network parameter size. △ Less","10 October, 2022",https://arxiv.org/pdf/2204.05928
A Survey on Sustainable Software Ecosystems to Support Experimental and Observational Science at Oak Ridge National Laboratory,David E Bernholdt;Mathieu Doucet;William F Godoy;Addi Malviya-Thakur;Gregory R Watson,"In the search for a sustainable approach for software ecosystems that supports experimental and observational science (EOS) across Oak Ridge National Laboratory (ORNL), we conducted a survey to understand the current and future landscape of EOS software and data. This paper describes the survey design we used to identify significant areas of interest, gaps, and potential opportunities, followed by a discussion on the obtained responses. The survey formulates questions about project demographics, technical approach, and skills required for the present and the next five years. The study was conducted among 38 ORNL participants between June and July of 2021 and followed the required guidelines for human subjects training. We plan to use the collected information to help guide a vision for sustainable, community-based, and reusable scientific software ecosystems that need to adapt effectively to: i) the evolving landscape of heterogeneous hardware in the next generation of instruments and computing (e.g. edge, distributed, accelerators), and ii) data management requirements for data-driven science using artificial intelligence. △ Less","12 April, 2022",https://arxiv.org/pdf/2204.05896
The MIT Supercloud Workload Classification Challenge,Benny J. Tang;Qiqi Chen;Matthew L. Weiss;Nathan Frey;Joseph McDonald;David Bestor;Charles Yee;William Arcand;Chansup Byun;Daniel Edelman;Matthew Hubbell;Michael Jones;Jeremy Kepner;Anna Klein;Adam Michaleas;Peter Michaleas;Lauren Milechin;Julia Mullen;Andrew Prout;Albert Reuther;Antonio Rosa;Andrew Bowne;Lindsey McEvoy;Baolin Li;Devesh Tiwari,"High-Performance Computing (HPC) centers and cloud providers support an increasingly diverse set of applications on heterogenous hardware. As Artificial Intelligence (AI) and Machine Learning (ML) workloads have become an increasingly larger share of the compute workloads, new approaches to optimized resource usage, allocation, and deployment of new AI frameworks are needed. By identifying compute workloads and their utilization characteristics, HPC systems may be able to better match available resources with the application demand. By leveraging datacenter instrumentation, it may be possible to develop AI-based approaches that can identify workloads and provide feedback to researchers and datacenter operators for improving operational efficiency. To enable this research, we released the MIT Supercloud Dataset, which provides detailed monitoring logs from the MIT Supercloud cluster. This dataset includes CPU and GPU usage by jobs, memory usage, and file system logs. In this paper, we present a workload classification challenge based on this dataset. We introduce a labelled dataset that can be used to develop new approaches to workload classification and present initial results based on existing approaches. The goal of this challenge is to foster algorithmic innovations in the analysis of compute workloads that can achieve higher accuracy than existing methods. Data and code will be made publicly available via the Datacenter Challenge website : https://dcc.mit.edu. △ Less","13 April, 2022",https://arxiv.org/pdf/2204.05839
Benchmarking Active Learning Strategies for Materials Optimization and Discovery,Alex Wang;Haotong Liang;Austin McDannald;Ichiro Takeuchi;A. Gilad Kusne,"Autonomous physical science is revolutionizing materials science. In these systems, machine learning controls experiment design, execution, and analysis in a closed loop. Active learning, the machine learning field of optimal experiment design, selects each subsequent experiment to maximize knowledge toward the user goal. Autonomous system performance can be further improved with implementation of scientific machine learning, also known as inductive bias-engineered artificial intelligence, which folds prior knowledge of physical laws (e.g., Gibbs phase rule) into the algorithm. As the number, diversity, and uses for active learning strategies grow, there is an associated growing necessity for real-world reference datasets to benchmark strategies. We present a reference dataset and demonstrate its use to benchmark active learning strategies in the form of various acquisition functions. Active learning strategies are used to rapidly identify materials with optimal physical properties within a ternary materials system. The data is from an actual Fe-Co-Ni thin-film library and includes previously acquired experimental data for materials compositions, X-ray diffraction patterns, and two functional properties of magnetic coercivity and the Kerr rotation. Popular active learning methods along with a recent scientific active learning method are benchmarked for their materials optimization performance. We discuss the relationship between algorithm performance, materials search space complexity, and the incorporation of prior knowledge. △ Less","12 April, 2022",https://arxiv.org/pdf/2204.05838
Automatic detection of glaucoma via fundus imaging and artificial intelligence: A review,Lauren Coan;Bryan Williams;Krishna Adithya Venkatesh;Swati Upadhyaya;Silvester Czanner;Rengaraj Venkatesh;Colin E. Willoughby;Srinivasan Kavitha;Gabriela Czanner,"Glaucoma is a leading cause of irreversible vision impairment globally and cases are continuously rising worldwide. Early detection is crucial, allowing timely intervention which can prevent further visual field loss. To detect glaucoma, examination of the optic nerve head via fundus imaging can be performed, at the centre of which is the assessment of the optic cup and disc boundaries. Fundus imaging is non-invasive and low-cost; however, the image examination relies on subjective, time-consuming, and costly expert assessments. A timely question to ask is can artificial intelligence mimic glaucoma assessments made by experts. Namely, can artificial intelligence automatically find the boundaries of the optic cup and disc (providing a so-called segmented fundus image) and then use the segmented image to identify glaucoma with high accuracy. We conducted a comprehensive review on artificial intelligence-enabled glaucoma detection frameworks that produce and use segmented fundus images. We found 28 papers and identified two main approaches: 1) logical rule-based frameworks, based on a set of simplistic decision rules; and 2) machine learning/statistical modelling based frameworks. We summarise the state-of-art of the two approaches and highlight the key hurdles to overcome for artificial intelligence-enabled glaucoma detection frameworks to be translated into clinical practice. △ Less","12 April, 2022",https://arxiv.org/pdf/2204.05591
Enriching Artificial Intelligence Explanations with Knowledge Fragments,Jože M. Rožanec;Elena Trajkova;Inna Novalija;Patrik Zajec;Klemen Kenda;Blaž Fortuna;Dunja Mladenić,"Artificial Intelligence models are increasingly used in manufacturing to inform decision-making. Responsible decision-making requires accurate forecasts and an understanding of the models' behavior. Furthermore, the insights into models' rationale can be enriched with domain knowledge. This research builds explanations considering feature rankings for a particular forecast, enriching them with media news entries, datasets' metadata, and entries from the Google Knowledge Graph. We compare two approaches (embeddings-based and semantic-based) on a real-world use case regarding demand forecasting. △ Less","12 April, 2022",https://arxiv.org/pdf/2204.05579
Accelerated Design and Deployment of Low-Carbon Concrete for Data Centers,Xiou Ge;Richard T. Goodwin;Haizi Yu;Pablo Romero;Omar Abdelrahman;Amruta Sudhalkar;Julius Kusuma;Ryan Cialdella;Nishant Garg;Lav R. Varshney,"Concrete is the most widely used engineered material in the world with more than 10 billion tons produced annually. Unfortunately, with that scale comes a significant burden in terms of energy, water, and release of greenhouse gases and other pollutants; indeed 8% of worldwide carbon emissions are attributed to the production of cement, a key ingredient in concrete. As such, there is interest in creating concrete formulas that minimize this environmental burden, while satisfying engineering performance requirements including compressive strength. Specifically for computing, concrete is a major ingredient in the construction of data centers. In this work, we use conditional variational autoencoders (CVAEs), a type of semi-supervised generative artificial intelligence (AI) model, to discover concrete formulas with desired properties. Our model is trained just using a small open dataset from the UCI Machine Learning Repository joined with environmental impact data from standard lifecycle analysis. Computational predictions demonstrate CVAEs can design concrete formulas with much lower carbon requirements than existing formulations while meeting design requirements. Next we report laboratory-based compressive strength experiments for five AI-generated formulations, which demonstrate that the formulations exceed design requirements. The resulting formulations were then used by Ozinga Ready Mix -- a concrete supplier -- to generate field-ready concrete formulations, based on local conditions and their expertise in concrete design. Finally, we report on how these formulations were used in the construction of buildings and structures in a Meta data center in DeKalb, IL, USA. Results from field experiments as part of this real-world deployment corroborate the efficacy of AI-generated low-carbon concrete mixes. △ Less","11 April, 2022",https://arxiv.org/pdf/2204.05397
The self-learning AI controller for adaptive power beaming with fiber-array laser transmitter system,A. M. Vorontsov;G. A. Filimonov,"In this study we consider adaptive power beaming with fiber-array laser transmitter system in presence of atmospheric turbulence. For optimization of power transition through the atmosphere fiber-array is traditionally controlled by stochastic parallel gradient descent (SPGD) algorithm where control feedback is provided via radio frequency link by an optical-to-electrical power conversion sensor, attached to a cooperative target. The SPGD algorithm continuously and randomly perturbs voltages applied to fiber-array phase shifters and fiber tip positioners in order to maximize sensor signal, i.e. uses, so-called, ""blind"" optimization principle. In opposite to this approach a perspective artificially intelligent (AI) control systems for synthesis of optimal control can utilize various pupil- or target-plane data available for the analysis including wavefront sensor data, photo-voltaic array (PVA) data, other optical or atmospheric parameters, and potentially can eliminate well-known drawbacks of SPGD-based controllers. In this study an optimal control is synthesized by a deep neural network (DNN) using target-plane PVA sensor data as its input. A DNN training is occurred online in sync with control system operation and is performed by applying of small perturbations to DNN's outputs. This approach does not require initial DNN's pre-training as well as guarantees optimization of system performance in time. All theoretical results are verified by numerical experiments. △ Less","8 April, 2022",https://arxiv.org/pdf/2204.05227
A Post-Processing Tool and Feasibility Study for Three-Dimensional Imaging with Electrical Impedance Tomography During Deep Brain Stimulation Surgery,Sebastien Martin,"Electrical impedance tomography (EIT) is a promising technique for biomedical imaging. The strength of EIT is its ability to reconstruct images of the body's internal structures through radiation-safe techniques. EIT is regarded as safe for patients' health, and it is currently being actively researched. This paper investigates the application of EIT during deep brain stimulation (DBS) surgery as a means to identify targets during operations. DBS involves a surgical procedure in which a lead or electrode array is implanted in a specific target area in the brain. Electrical stimulations are then used to modulate neural circuits within the target area to reduce disabling neurological symptoms. The main difficulty in performing DBS surgery is to accurately position the lead in the target area before commencing the treatment. Brain tissue shifts during DBS surgery can be as large as the target size when compared with the pre-operative magnetic resonance imaging (MRI) or computed tomography (CT) images. To address this problem, a solution based on open-domain EIT to reconstruct images surrounding the probe during DBS surgery is proposed. Data acquisition and image reconstruction were performed, and artificial intelligence was applied to enhance the resulting images. The results showed that the proposed method is rapid, produces valuable high-quality images, and constitutes a first step towards in-vivo study. △ Less","26 June, 2022",https://arxiv.org/pdf/2204.05201
Metaethical Perspectives on 'Benchmarking' AI Ethics,Travis LaCroix;Alexandra Sasha Luccioni,"Benchmarks are seen as the cornerstone for measuring technical progress in Artificial Intelligence (AI) research and have been developed for a variety of tasks ranging from question answering to facial recognition. An increasingly prominent research area in AI is ethics, which currently has no set of benchmarks nor commonly accepted way for measuring the 'ethicality' of an AI system. In this paper, drawing upon research in moral philosophy and metaethics, we argue that it is impossible to develop such a benchmark. As such, alternative mechanisms are necessary for evaluating whether an AI system is 'ethical'. This is especially pressing in light of the prevalence of applied, industrial AI research. We argue that it makes more sense to talk about 'values' (and 'value alignment') rather than 'ethics' when considering the possible actions of present and future AI systems. We further highlight that, because values are unambiguously relative, focusing on values forces us to consider explicitly what the values are and whose values they are. Shifting the emphasis from ethics to values therefore gives rise to several new ways of understanding how researchers might advance research programmes for robustly safe or beneficial AI. We conclude by highlighting a number of possible ways forward for the field as a whole, and we advocate for different approaches towards more value-aligned AI research. △ Less","11 April, 2022",https://arxiv.org/pdf/2204.05151
Learning Object-Centered Autotelic Behaviors with Graph Neural Networks,Ahmed Akakzia;Olivier Sigaud,"Although humans live in an open-ended world and endlessly face new challenges, they do not have to learn from scratch each time they face the next one. Rather, they have access to a handful of previously learned skills, which they rapidly adapt to new situations. In artificial intelligence, autotelic agents, which are intrinsically motivated to represent and set their own goals, exhibit promising skill adaptation capabilities. However, these capabilities are highly constrained by their policy and goal space representations. In this paper, we propose to investigate the impact of these representations on the learning and transfer capabilities of autotelic agents. We study different implementations of autotelic agents using four types of Graph Neural Networks policy representations and two types of goal spaces, either geometric or predicate-based. By testing agents on unseen goals, we show that combining object-centered architectures that are expressive enough with semantic relational goals helps learning to reach more difficult goals. We also release our graph-based implementations to encourage further research in this direction. △ Less","20 July, 2022",https://arxiv.org/pdf/2204.05141
"Artificial Intelligence Software Structured to Simulate Human Working Memory, Mental Imagery, and Mental Continuity",Jared Edward Reser,"This article presents an artificial intelligence (AI) architecture intended to simulate the human working memory system as well as the manner in which it is updated iteratively. It features several interconnected neural networks designed to emulate the specialized modules of the cerebral cortex. These are structured hierarchically and integrated into a global workspace. They are capable of temporarily maintaining high-level patterns akin to the psychological items maintained in working memory. This maintenance is made possible by persistent neural activity in the form of two modalities: sustained neural firing (resulting in a focus of attention) and synaptic potentiation (resulting in a short-term store). This persistent activity is updated iteratively resulting in incremental changes to the content of the working memory system. As the content stored in working memory gradually evolves, successive states overlap and are continuous with one another. The present article will explore how this architecture can lead to gradual shift in the distribution of coactive representations, ultimately leading to mental continuity between processing states, and thus to human-like cognition. △ Less","29 March, 2022",https://arxiv.org/pdf/2204.05138
On the link between conscious function and general intelligence in humans and machines,Arthur Juliani;Kai Arulkumaran;Shuntaro Sasai;Ryota Kanai,"In popular media, there is often a connection drawn between the advent of awareness in artificial agents and those same agents simultaneously achieving human or superhuman level intelligence. In this work, we explore the validity and potential application of this seemingly intuitive link between consciousness and intelligence. We do so by examining the cognitive abilities associated with three contemporary theories of conscious function: Global Workspace Theory (GWT), Information Generation Theory (IGT), and Attention Schema Theory (AST). We find that all three theories specifically relate conscious function to some aspect of domain-general intelligence in humans. With this insight, we turn to the field of Artificial Intelligence (AI) and find that, while still far from demonstrating general intelligence, many state-of-the-art deep learning methods have begun to incorporate key aspects of each of the three functional theories. Having identified this trend, we use the motivating example of mental time travel in humans to propose ways in which insights from each of the three theories may be combined into a single unified and implementable model. Given that it is made possible by cognitive abilities underlying each of the three functional theories, artificial agents capable of mental time travel would not only possess greater general intelligence than current approaches, but also be more consistent with our current understanding of the functional role of consciousness in humans, thus making it a promising near-term goal for AI research. △ Less","19 July, 2022",https://arxiv.org/pdf/2204.05133
Multimodal Machine Learning in Precision Health,Adrienne Kline;Hanyin Wang;Yikuan Li;Saya Dennis;Meghan Hutch;Zhenxing Xu;Fei Wang;Feixiong Cheng;Yuan Luo,"As machine learning and artificial intelligence are more frequently being leveraged to tackle problems in the health sector, there has been increased interest in utilizing them in clinical decision-support. This has historically been the case in single modal data such as electronic health record data. Attempts to improve prediction and resemble the multimodal nature of clinical expert decision-making this has been met in the computational field of machine learning by a fusion of disparate data. This review was conducted to summarize this field and identify topics ripe for future research. We conducted this review in accordance with the PRISMA (Preferred Reporting Items for Systematic reviews and Meta-Analyses) extension for Scoping Reviews to characterize multi-modal data fusion in health. We used a combination of content analysis and literature searches to establish search strings and databases of PubMed, Google Scholar, and IEEEXplore from 2011 to 2021. A final set of 125 articles were included in the analysis. The most common health areas utilizing multi-modal methods were neurology and oncology. However, there exist a wide breadth of current applications. The most common form of information fusion was early fusion. Notably, there was an improvement in predictive performance performing heterogeneous data fusion. Lacking from the papers were clear clinical deployment strategies and pursuit of FDA-approved tools. These findings provide a map of the current literature on multimodal data fusion as applied to health diagnosis/prognosis problems. Multi-modal machine learning, while more robust in its estimations over unimodal methods, has drawbacks in its scalability and the time-consuming nature of information concatenation. △ Less","10 April, 2022",https://arxiv.org/pdf/2204.04777
Generative Adversarial Networks for Image Augmentation in Agriculture: A Systematic Review,Ebenezer Olaniyi;Dong Chen;Yuzhen Lu;Yanbo Huang,"In agricultural image analysis, optimal model performance is keenly pursued for better fulfilling visual recognition tasks (e.g., image classification, segmentation, object detection and localization), in the presence of challenges with biological variability and unstructured environments. Large-scale, balanced and ground-truthed image datasets, however, are often difficult to obtain to fuel the development of advanced, high-performance models. As artificial intelligence through deep learning is impacting analysis and modeling of agricultural images, data augmentation plays a crucial role in boosting model performance while reducing manual efforts for data preparation, by algorithmically expanding training datasets. Beyond traditional data augmentation techniques, generative adversarial network (GAN) invented in 2014 in the computer vision community, provides a suite of novel approaches that can learn good data representations and generate highly realistic samples. Since 2017, there has been a growth of research into GANs for image augmentation or synthesis in agriculture for improved model performance. This paper presents an overview of the evolution of GAN architectures followed by a systematic review of their application to agriculture (https://github.com/Derekabc/GANs-Agriculture), involving various vision tasks for plant health, weeds, fruits, aquaculture, animal farming, plant phenotyping as well as postharvest detection of fruit defects. Challenges and opportunities of GANs are discussed for future research. △ Less","12 April, 2022",https://arxiv.org/pdf/2204.04707
Effective Out-of-Distribution Detection in Classifier Based on PEDCC-Loss,Qiuyu Zhu;Guohui Zheng;Yingying Yan,"Deep neural networks suffer from the overconfidence issue in the open world, meaning that classifiers could yield confident, incorrect predictions for out-of-distribution (OOD) samples. Thus, it is an urgent and challenging task to detect these samples drawn far away from training distribution based on the security considerations of artificial intelligence. Many current methods based on neural networks mainly rely on complex processing strategies, such as temperature scaling and input preprocessing, to obtain satisfactory results. In this paper, we propose an effective algorithm for detecting out-of-distribution examples utilizing PEDCC-Loss. We mathematically analyze the nature of the confidence score output by the PEDCC (Predefined Evenly-Distribution Class Centroids) classifier, and then construct a more effective scoring function to distinguish in-distribution (ID) and out-of-distribution. In this method, there is no need to preprocess the input samples and the computational burden of the algorithm is reduced. Experiments demonstrate that our method can achieve better OOD detection performance. △ Less","10 April, 2022",https://arxiv.org/pdf/2204.04665
Explain yourself! Effects of Explanations in Human-Robot Interaction,Jakob Ambsdorf;Alina Munir;Yiyao Wei;Klaas Degkwitz;Harm Matthias Harms;Susanne Stannek;Kyra Ahrens;Dennis Becker;Erik Strahl;Tom Weber;Stefan Wermter,"Recent developments in explainable artificial intelligence promise the potential to transform human-robot interaction: Explanations of robot decisions could affect user perceptions, justify their reliability, and increase trust. However, the effects on human perceptions of robots that explain their decisions have not been studied thoroughly. To analyze the effect of explainable robots, we conduct a study in which two simulated robots play a competitive board game. While one robot explains its moves, the other robot only announces them. Providing explanations for its actions was not sufficient to change the perceived competence, intelligence, likeability or safety ratings of the robot. However, the results show that the robot that explains its moves is perceived as more lively and human-like. This study demonstrates the need for and potential of explainable human-robot interaction and the wider assessment of its effects as a novel research direction. △ Less","14 June, 2022",https://arxiv.org/pdf/2204.04501
Controlling Traffic with Humanoid Social Robot,Faisal Ghaffar,"The advancement of technology such as artificial intelligence, machine learning and internet of things it became easy to develop more humanoid robots and automate different processes. An interactive robot must have high social behavior so that it can be easily accepted by the people using it. In this study we designed a traffic police robot (TRAPROB) to automate the traffic control at intersection. The human police officer experiences high stress because of long duty hours as well as pose the risk of accidents. The digital electronic signals are automatic but we want to create a system which is more human like and looks like an officer controlling the traffic at intersection. We used Thiago++ robot in this study and modified its look to like a police officer, and then programmed it to imitate and make gestures just like traffic police officer makes gestures for controlling traffic. We evaluated the looks, gestures, functionality, and social behavior of the robot. We asked a limited sample of two participants to identify the TRAPBOT, rate its look, the social behaviors and gestures in comparison to a real life police officer. we found that people can identify the robot as traffic police robot. Our analysis also shows that TRAPBOT has appearance like a traffic robot and can make similar signal gestures as a traffic police officer. △ Less","8 April, 2022",https://arxiv.org/pdf/2204.04240
Measuring AI Systems Beyond Accuracy,Violet Turri;Rachel Dzombak;Eric Heim;Nathan VanHoudnos;Jay Palat;Anusha Sinha,"Current test and evaluation (T&E) methods for assessing machine learning (ML) system performance often rely on incomplete metrics. Testing is additionally often siloed from the other phases of the ML system lifecycle. Research investigating cross-domain approaches to ML T&E is needed to drive the state of the art forward and to build an Artificial Intelligence (AI) engineering discipline. This paper advocates for a robust, integrated approach to testing by outlining six key questions for guiding a holistic T&E strategy. △ Less","7 April, 2022",https://arxiv.org/pdf/2204.04211
LoCI: An Analysis of the Impact of Optical Loss and Crosstalk Noise in Integrated Silicon-Photonic Neural Networks,Amin Shafiee;Sanmitra Banerjee;Krishnendu Chakrabarty;Sudeep Pasricha;Mahdi Nikdast,"Compared to electronic accelerators, integrated silicon-photonic neural networks (SP-NNs) promise higher speed and energy efficiency for emerging artificial-intelligence applications. However, a hitherto overlooked problem in SP-NNs is that the underlying silicon photonic devices suffer from intrinsic optical loss and crosstalk noise, the impact of which accumulates as the network scales up. Leveraging precise device-level models, this paper presents the first comprehensive and systematic optical loss and crosstalk modeling framework for SP-NNs. For an SP-NN case study with two hidden layers and 1380 tunable parameters, we show a catastrophic 84% drop in inferencing accuracy due to optical loss and crosstalk noise. △ Less","8 April, 2022",https://arxiv.org/pdf/2204.03835
Improving Urban Mobility: using artificial intelligence and new technologies to connect supply and demand,Ana L. C. Bazzan,"As the demand for mobility in our society seems to increase, the various issues centered on urban mobility are among those that worry most city inhabitants in this planet. For instance, how to go from A to B in an efficient (but also less stressful) way? These questions and concerns have not changed even during the covid-19 pandemic; on the contrary, as the current stand, people who are avoiding public transportation are only contributing to an increase in the vehicular traffic. The are of intelligent transportation systems (ITS) aims at investigating how to employ information and communication technologies to problems related to transportation. This may mean monitoring and managing the infrastructure (e.g., traffic roads, traffic signals, etc.). However, currently, ITS is also targeting the management of demand. In this panorama, artificial intelligence plays an important role, especially with the advances in machine learning that translates in the use of computational vision, connected and autonomous vehicles, agent-based simulation, among others. In the present work, a survey of several works developed by our group are discussed in a holistic perspective, i.e., they cover not only the supply side (as commonly found in ITS works), but also the demand side, and, in an novel perspective, the integration of both. △ Less","18 March, 2022",https://arxiv.org/pdf/2204.03570
Multi-Task Distributed Learning using Vision Transformer with Random Patch Permutation,Sangjoon Park;Jong Chul Ye,"The widespread application of artificial intelligence in health research is currently hampered by limitations in data availability. Distributed learning methods such as federated learning (FL) and shared learning (SL) are introduced to solve this problem as well as data management and ownership issues with their different strengths and weaknesses. The recent proposal of federated split task-agnostic (FeSTA) learning tries to reconcile the distinct merits of FL and SL by enabling the multi-task collaboration between participants through Vision Transformer (ViT) architecture, but they suffer from higher communication overhead. To address this, here we present a multi-task distributed learning using ViT with random patch permutation. Instead of using a CNN based head as in FeSTA, p-FeSTA adopts a randomly permuting simple patch embedder, improving the multi-task learning performance without sacrificing privacy. Experimental results confirm that the proposed method significantly enhances the benefit of multi-task collaboration, communication efficiency, and privacy preservation, shedding light on practical multi-task distributed learning in the field of medical imaging. △ Less","7 April, 2022",https://arxiv.org/pdf/2204.03500
Human-AI ecosystem with abrupt changes as a function of the composition,Pierluigi Contucci;János Kertész;Godwin Osabutey,"The progressive advent of artificial intelligence machines may represent both an opportunity or a threat. In order to have an idea of what is coming we propose a model that simulate a Human-AI ecosystem. In particular we consider systems where agents present biases, peer-to-peer interactions and also three body interactions that are crucial and describe two humans interacting with an artificial agent and two artificial intelligence agents interacting with a human. We focus our analysis by exploring how the relative fraction of artificial intelligence agents affect that ecosystem. We find evidence that for suitable values of the interaction parameters, arbitrarily small changes in such percentage may trigger dramatic changes for the system that can be either in one of the two polarised states or in an undecided state. △ Less","7 April, 2022",https://arxiv.org/pdf/2204.03372
Predictive coding and stochastic resonance as fundamental principles of auditory perception,Achim Schilling;William Sedley;Richard Gerum;Claus Metzner;Konstantin Tziridis;Andreas Maier;Holger Schulze;Fan-Gang Zeng;Karl J. Friston;Patrick Krauss,"How is information processed in the brain during perception? Mechanistic insight is achieved only when experiments are employed to test formal or computational models. In analogy to lesion studies, phantom perception may serve as a vehicle to understand the fundamental processing principles underlying auditory perception. With a special focus on tinnitus -- as the prime example of auditory phantom perception -- we review recent work at the intersection of artificial intelligence, psychology, and neuroscience. In particular, we discuss why everyone with tinnitus suffers from hearing loss, but not everyone with hearing loss suffers from tinnitus. We argue that the increase of sensory precision due to Bayesian inference could be caused by intrinsic neural noise and lead to a prediction error in the cerebral cortex. Hence, two fundamental processing principles - being ubiquitous in the brain - provide the most explanatory power for the emergence of tinnitus: predictive coding as a top-down, and stochastic resonance as a complementary bottom-up mechanism. We conclude that both principles play a crucial role in healthy auditory perception. △ Less","23 May, 2022",https://arxiv.org/pdf/2204.03354
Predicting Performance of Heterogeneous AI Systems with Discrete-Event Simulations,Vyacheslav Zhdanovskiy;Lev Teplyakov;Anton Grigoryev,"In recent years, artificial intelligence (AI) technologies have found industrial applications in various fields. AI systems typically possess complex software and heterogeneous CPU/GPU hardware architecture, making it difficult to answer basic questions considering performance evaluation and software optimization. Where is the bottleneck impeding the system? How does the performance scale with the workload? How the speed-up of a specific module would contribute to the whole system? Finding the answers to these questions through experiments on the real system could require a lot of computational, human, financial, and time resources. A solution to cut these costs is to use a fast and accurate simulation model preparatory to implementing anything in the real system. In this paper, we propose a discrete-event simulation model of a high-load heterogeneous AI system in the context of video analytics. Using the proposed model, we estimate: 1) the performance scalability with the increasing number of cameras; 2) the performance impact of integrating a new module; 3) the performance gain from optimizing a single module. We show that the performance estimation accuracy of the proposed model is higher than 90%. We also demonstrate, that the considered system possesses a counter-intuitive relationship between workload and performance, which nevertheless is correctly inferred by the proposed simulation model. △ Less","7 April, 2022",https://arxiv.org/pdf/2204.03332
Autoencoding Language Model Based Ensemble Learning for Commonsense Validation and Explanation,Ngo Quang Huy;Tu Minh Phuong;Ngo Xuan Bach,"An ultimate goal of artificial intelligence is to build computer systems that can understand human languages. Understanding commonsense knowledge about the world expressed in text is one of the foundational and challenging problems to create such intelligent systems. As a step towards this goal, we present in this paper ALMEn, an Autoencoding Language Model based Ensemble learning method for commonsense validation and explanation. By ensembling several advanced pre-trained language models including RoBERTa, DeBERTa, and ELECTRA with Siamese neural networks, our method can distinguish natural language statements that are against commonsense (validation subtask) and correctly identify the reason for making against commonsense (explanation selection subtask). Experimental results on the benchmark dataset of SemEval-2020 Task 4 show that our method outperforms state-of-the-art models, reaching 97.9% and 95.4% accuracies on the validation and explanation selection subtasks, respectively. △ Less","7 April, 2022",https://arxiv.org/pdf/2204.03324
Swarm behavior tracking based on a deep vision algorithm,Meihong Wu;Xiaoyan Cao;Shihui Guo,"The intelligent swarm behavior of social insects (such as ants) springs up in different environments, promising to provide insights for the study of embodied intelligence. Researching swarm behavior requires that researchers could accurately track each individual over time. Obviously, manually labeling individual insects in a video is labor-intensive. Automatic tracking methods, however, also poses serious challenges: (1) individuals are small and similar in appearance; (2) frequent interactions with each other cause severe and long-term occlusion. With the advances of artificial intelligence and computing vision technologies, we are hopeful to provide a tool to automate monitor multiple insects to address the above challenges. In this paper, we propose a detection and tracking framework for multi-ant tracking in the videos by: (1) adopting a two-stage object detection framework using ResNet-50 as backbone and coding the position of regions of interest to locate ants accurately; (2) using the ResNet model to develop the appearance descriptors of ants; (3) constructing long-term appearance sequences and combining them with motion information to achieve online tracking. To validate our method, we construct an ant database including 10 videos of ants from different indoor and outdoor scenes. We achieve a state-of-the-art performance of 95.7\% mMOTA and 81.1\% mMOTP in indoor videos, 81.8\% mMOTA and 81.9\% mMOTP in outdoor videos. Additionally, Our method runs 6-10 times faster than existing methods for insect tracking. Experimental results demonstrate that our method provides a powerful tool for accelerating the unraveling of the mechanisms underlying the swarm behavior of social insects. △ Less","7 April, 2022",https://arxiv.org/pdf/2204.03319
Deep Learning for Real Time Satellite Pose Estimation on Low Power Edge TPU,Alessandro Lotti;Dario Modenini;Paolo Tortora;Massimiliano Saponara;Maria A. Perino,"Pose estimation of an uncooperative space resident object is a key asset towards autonomy in close proximity operations. In this context monocular cameras are a valuable solution because of their low system requirements. However, the associated image processing algorithms are either too computationally expensive for real time on-board implementation, or not enough accurate. In this paper we propose a pose estimation software exploiting neural network architectures which can be scaled to different accuracy-latency trade-offs. We designed our pipeline to be compatible with Edge Tensor Processing Units to show how low power machine learning accelerators could enable Artificial Intelligence exploitation in space. The neural networks were tested both on the benchmark Spacecraft Pose Estimation Dataset, and on the purposely developed Cosmo Photorealistic Dataset, which depicts a COSMO-SkyMed satellite in a variety of random poses and steerable solar panels orientations. The lightest version of our architecture achieves state-of-the-art accuracy on both datasets but at a fraction of networks complexity, running at 7.7 frames per second on a Coral Dev Board Mini consuming just 2.2W. △ Less","22 June, 2022",https://arxiv.org/pdf/2204.03296
"Software Testing, AI and Robotics (STAIR) Learning Lab",Simon Haller-Seeber;Thomas Gatterer;Patrick Hofmann;Christopher Kelter;Thomas Auer;Michael Felderer,"In this paper we presented the Software Testing, AI and Robotics (STAIR) Learning Lab. STAIR is an initiative started at the University of Innsbruck to bring robotics, Artificial Intelligence (AI) and software testing into schools. In the lab physical and virtual learning units are developed in parallel and in sync with each other. Its core learning approach is based the develop of both a physical and simulated robotics environment. In both environments AI scenarios (like traffic sign recognition) are deployed and tested. We present and focus on our newly designed MiniBot that are both built on hardware which was designed for educational and research purposes as well as the simulation environment. Additionally, we describe first learning design concepts and a showcase scenario (i.e., AI-based traffic sign recognition) with different exercises which can easily be extended. △ Less","6 April, 2022",https://arxiv.org/pdf/2204.03028
"Contextual Attention Mechanism, SRGAN Based Inpainting System for Eliminating Interruptions from Images",Narayana Darapaneni;Vaibhav Kherde;Kameswara Rao;Deepali Nikam;Swanand Katdare;Anima Shukla;Anagha Lomate;Anwesh Reddy Paduri,"The new alternative is to use deep learning to inpaint any image by utilizing image classification and computer vision techniques. In general, image inpainting is a task of recreating or reconstructing any broken image which could be a photograph or oil/acrylic painting. With the advancement in the field of Artificial Intelligence, this topic has become popular among AI enthusiasts. With our approach, we propose an initial end-to-end pipeline for inpainting images using a complete Machine Learning approach instead of a conventional application-based approach. We first use the YOLO model to automatically identify and localize the object we wish to remove from the image. Using the result obtained from the model we can generate a mask for the same. After this, we provide the masked image and original image to the GAN model which uses the Contextual Attention method to fill in the region. It consists of two generator networks and two discriminator networks and is also called a coarse-to-fine network structure. The two generators use fully convolutional networks while the global discriminator gets hold of the entire image as input while the local discriminator gets the grip of the filled region as input. The contextual Attention mechanism is proposed to effectively borrow the neighbor information from distant spatial locations for reconstructing the missing pixels. The third part of our implementation uses SRGAN to resolve the inpainted image back to its original size. Our work is inspired by the paper Free-Form Image Inpainting with Gated Convolution and Generative Image Inpainting with Contextual Attention. △ Less","6 April, 2022",https://arxiv.org/pdf/2204.02591
Challenges and Opportunities of Edge AI for Next-Generation Implantable BMIs,MohammadAli Shaeri;Arshia Afzal;Mahsa Shoaran,"Neuroscience and neurotechnology are currently being revolutionized by artificial intelligence (AI) and machine learning. AI is widely used to study and interpret neural signals (analytical applications), assist people with disabilities (prosthetic applications), and treat underlying neurological symptoms (therapeutic applications). In this brief, we will review the emerging opportunities of on-chip AI for the next-generation implantable brain-machine interfaces (BMIs), with a focus on state-of-the-art prosthetic BMIs. Major technological challenges for the effectiveness of AI models will be discussed. Finally, we will present algorithmic and IC design solutions to enable a new generation of AI-enhanced and high-channel-count BMIs. △ Less","13 April, 2022",https://arxiv.org/pdf/2204.02362
"Scientometric Review of Artificial Intelligence for Operations & Maintenance of Wind Turbines: The Past, Present and Future",Joyjit Chatterjee;Nina Dethlefs,"Wind energy has emerged as a highly promising source of renewable energy in recent times. However, wind turbines regularly suffer from operational inconsistencies, leading to significant costs and challenges in operations and maintenance (O&M). Condition-based monitoring (CBM) and performance assessment/analysis of turbines are vital aspects for ensuring efficient O&M planning and cost minimisation. Data-driven decision making techniques have witnessed rapid evolution in the wind industry for such O&M tasks during the last decade, from applying signal processing methods in early 2010 to artificial intelligence (AI) techniques, especially deep learning in 2020. In this article, we utilise statistical computing to present a scientometric review of the conceptual and thematic evolution of AI in the wind energy sector, providing evidence-based insights into present strengths and limitations of data-driven decision making in the wind industry. We provide a perspective into the future and on current key challenges in data availability and quality, lack of transparency in black box-natured AI models, and prevailing issues in deploying models for real-time decision support, along with possible strategies to overcome these problems. We hope that a systematic analysis of the past, present and future of CBM and performance assessment can encourage more organisations to adopt data-driven decision making techniques in O&M towards making wind energy sources more reliable, contributing to the global efforts of tackling climate change. △ Less","30 March, 2022",https://arxiv.org/pdf/2204.02360
Towards Explainable Meta-Learning for DDoS Detection,Qianru Zhou;Rongzhen Li;Lei Xu;Arumugam Nallanathan;Jian Yang;Anmin Fu,"The Internet is the most complex machine humankind has ever built, and how to defense it from intrusions is even more complex. With the ever increasing of new intrusions, intrusion detection task rely on Artificial Intelligence more and more. Interpretability and transparency of the machine learning model is the foundation of trust in AI-driven intrusion detection results. Current interpretation Artificial Intelligence technologies in intrusion detection are heuristic, which is neither accurate nor sufficient. This paper proposed a rigorous interpretable Artificial Intelligence driven intrusion detection approach, based on artificial immune system. Details of rigorous interpretation calculation process for a decision tree model is presented. Prime implicant explanation for benign traffic flow are given in detail as rule for negative selection of the cyber immune system. Experiments are carried out in real-life traffic. △ Less","16 August, 2022",https://arxiv.org/pdf/2204.02255
Real-time Hyperspectral Imaging in Hardware via Trained Metasurface Encoders,Maksim Makarenko;Arturo Burguete-Lopez;Qizhou Wang;Fedor Getman;Silvio Giancola;Bernard Ghanem;Andrea Fratalocchi,"Hyperspectral imaging has attracted significant attention to identify spectral signatures for image classification and automated pattern recognition in computer vision. State-of-the-art implementations of snapshot hyperspectral imaging rely on bulky, non-integrated, and expensive optical elements, including lenses, spectrometers, and filters. These macroscopic components do not allow fast data processing for, e.g real-time and high-resolution videos. This work introduces Hyplex, a new integrated architecture addressing the limitations discussed above. Hyplex is a CMOS-compatible, fast hyperspectral camera that replaces bulk optics with nanoscale metasurfaces inversely designed through artificial intelligence. Hyplex does not require spectrometers but makes use of conventional monochrome cameras, opening up the possibility for real-time and high-resolution hyperspectral imaging at inexpensive costs. Hyplex exploits a model-driven optimization, which connects the physical metasurfaces layer with modern visual computing approaches based on end-to-end training. We design and implement a prototype version of Hyplex and compare its performance against the state-of-the-art for typical imaging tasks such as spectral reconstruction and semantic segmentation. In all benchmarks, Hyplex reports the smallest reconstruction error. We additionally present what is, to the best of our knowledge, the largest publicly available labeled hyperspectral dataset for semantic segmentation. △ Less","5 April, 2022",https://arxiv.org/pdf/2204.02084
"Aerial Computing: A New Computing Paradigm, Applications, and Challenges",Quoc-Viet Pham;Rukhsana Ruby;Fang Fang;Dinh C. Nguyen;Zhaohui Yang;Mai Le;Zhiguo Ding;Won-Joo Hwang,"In existing computing systems, such as edge computing and cloud computing, several emerging applications and practical scenarios are mostly unavailable or only partially implemented. To overcome the limitations that restrict such applications, the development of a comprehensive computing paradigm has garnered attention in both academia and industry. However, a gap exists in the literature owing to the scarce research, and a comprehensive computing paradigm is yet to be systematically designed and reviewed. This study introduces a novel concept, called aerial computing, via the amalgamation of aerial radio access networks and edge computing, which attempts to bridge the gap. Specifically, first, we propose a novel comprehensive computing architecture that is composed of low-altitude computing, high-altitude computing, and satellite computing platforms, along with conventional computing systems. We determine that aerial computing offers several desirable attributes: global computing service, better mobility, higher scalability and availability, and simultaneity. Second, we comprehensively discuss key technologies that facilitate aerial computing, including energy refilling, edge computing, network softwarization, frequency spectrum, multi-access techniques, artificial intelligence, and big data. In addition, we discuss vertical domain applications (e.g., smart cities, smart vehicles, smart factories, and smart grids) supported by aerial computing. Finally, we highlight several challenges that need to be addressed and their possible solutions. △ Less","5 April, 2022",https://arxiv.org/pdf/2204.02005
Digital Twin Virtualization with Machine Learning for IoT and Beyond 5G Networks: Research Directions for Security and Optimal Control,Jithin Jagannath;Keyvan Ramezanpour;Anu Jagannath,"Digital twin (DT) technologies have emerged as a solution for real-time data-driven modeling of cyber physical systems (CPS) using the vast amount of data available by Internet of Things (IoT) networks. In this position paper, we elucidate unique characteristics and capabilities of a DT framework that enables realization of such promises as online learning of a physical environment, real-time monitoring of assets, Monte Carlo heuristic search for predictive prevention, on-policy, and off-policy reinforcement learning in real-time. We establish a conceptual layered architecture for a DT framework with decentralized implementation on cloud computing and enabled by artificial intelligence (AI) services for modeling, event detection, and decision-making processes. The DT framework separates the control functions, deployed as a system of logically centralized process, from the physical devices under control, much like software-defined networking (SDN) in fifth generation (5G) wireless networks. We discuss the moment of the DT framework in facilitating implementation of network-based control processes and its implications for critical infrastructure. To clarify the significance of DT in lowering the risk of development and deployment of innovative technologies on existing system, we discuss the application of implementing zero trust architecture (ZTA) as a necessary security framework in future data-driven communication networks. △ Less","10 April, 2022",https://arxiv.org/pdf/2204.01950
A Data-Driven Framework for Identifying Investment Opportunities in Private Equity,Samantha Petersone;Alwin Tan;Richard Allmendinger;Sujit Roy;James Hales,"The core activity of a Private Equity (PE) firm is to invest into companies in order to provide the investors with profit, usually within 4-7 years. To invest into a company or not is typically done manually by looking at various performance indicators of the company and then making a decision often based on instinct. This process is rather unmanageable given the large number of companies to potentially invest. Moreover, as more data about company performance indicators becomes available and the number of different indicators one may want to consider increases, manual crawling and assessment of investment opportunities becomes inefficient and ultimately impossible. To address these issues, this paper proposes a framework for automated data-driven screening of investment opportunities and thus the recommendation of businesses to invest in. The framework draws on data from several sources to assess the financial and managerial position of a company, and then uses an explainable artificial intelligence (XAI) engine to suggest investment recommendations. The robustness of the model is validated using different AI algorithms, class imbalance-handling methods, and features extracted from the available data sources. △ Less","4 April, 2022",https://arxiv.org/pdf/2204.01852
Experimental quantum adversarial learning with programmable superconducting qubits,Wenhui Ren;Weikang Li;Shibo Xu;Ke Wang;Wenjie Jiang;Feitong Jin;Xuhao Zhu;Jiachen Chen;Zixuan Song;Pengfei Zhang;Hang Dong;Xu Zhang;Jinfeng Deng;Yu Gao;Chuanyu Zhang;Yaozu Wu;Bing Zhang;Qiujiang Guo;Hekang Li;Zhen Wang;Jacob Biamonte;Chao Song;Dong-Ling Deng;H. Wang,"Quantum computing promises to enhance machine learning and artificial intelligence. Different quantum algorithms have been proposed to improve a wide spectrum of machine learning tasks. Yet, recent theoretical works show that, similar to traditional classifiers based on deep classical neural networks, quantum classifiers would suffer from the vulnerability problem: adding tiny carefully-crafted perturbations to the legitimate original data samples would facilitate incorrect predictions at a notably high confidence level. This will pose serious problems for future quantum machine learning applications in safety and security-critical scenarios. Here, we report the first experimental demonstration of quantum adversarial learning with programmable superconducting qubits. We train quantum classifiers, which are built upon variational quantum circuits consisting of ten transmon qubits featuring average lifetimes of 150 μs, and average fidelities of simultaneous single- and two-qubit gates above 99.94% and 99.4% respectively, with both real-life images (e.g., medical magnetic resonance imaging scans) and quantum data. We demonstrate that these well-trained classifiers (with testing accuracy up to 99%) can be practically deceived by small adversarial perturbations, whereas an adversarial training process would significantly enhance their robustness to such perturbations. Our results reveal experimentally a crucial vulnerability aspect of quantum learning systems under adversarial scenarios and demonstrate an effective defense strategy against adversarial attacks, which provide a valuable guide for quantum artificial intelligence applications with both near-term and future quantum devices. △ Less","4 April, 2022",https://arxiv.org/pdf/2204.01738
MetaAID: A Flexible Framework for Developing Metaverse Applications via AI Technology and Human Editing,Hongyin Zhu,"Achieving the expansion of domestic demand and the economic internal circulation requires balanced and coordinated support from multiple industries (domains) such as consumption, education, entertainment, engineering infrastructure, etc., which is indispensable for maintaining economic development. Metaverse applications may help with this task and can make many industries more interesting, more efficient, and provide a better user experience. The first challenge is that metaverse application development inevitably requires the support of various artificial intelligence (AI) technologies such as natural language processing (NLP), knowledge graph (KG), computer vision (CV), and machine learning (ML), etc. However, existing metaverse application development lacks a lightweight AI technology framework. This paper proposes a flexible metaverse AI technology framework metaAID that aims to support language and semantic technologies in the development of digital twins and virtual humans. The second challenge is that the development process of metaverse applications involves both technical development tasks and manual editing work, and often becomes a heavyweight multi-team collaboration project, not to mention the development of metaverse applications in multiple industries. Our framework summarizes common AI technologies and application development templates with common functional modules and interfaces. Based on this framework, we have designed 5 applications for 3 industries around the expansion of domestic demand and economic internal circulation. Experimental results show that our framework can support AI technologies when developing metaverse applications in different industries. △ Less","4 April, 2022",https://arxiv.org/pdf/2204.01614
Modern Views of Machine Learning for Precision Psychiatry,Zhe Sage Chen;Prathamesh;Kulkarni;Isaac R. Galatzer-Levy;Benedetta Bigio;Carla Nasca;Yu Zhang,"In light of the NIMH's Research Domain Criteria (RDoC), the advent of functional neuroimaging, novel technologies and methods provide new opportunities to develop precise and personalized prognosis and diagnosis of mental disorders. Machine learning (ML) and artificial intelligence (AI) technologies are playing an increasingly critical role in the new era of precision psychiatry. Combining ML/AI with neuromodulation technologies can potentially provide explainable solutions in clinical practice and effective therapeutic treatment. Advanced wearable and mobile technologies also call for the new role of ML/AI for digital phenotyping in mobile mental health. In this review, we provide a comprehensive review of the ML methodologies and applications by combining neuroimaging, neuromodulation, and advanced mobile technologies in psychiatry practice. Additionally, we review the role of ML in molecular phenotyping and cross-species biomarker identification in precision psychiatry. We further discuss explainable AI (XAI) and causality testing in a closed-human-in-the-loop manner, and highlight the ML potential in multimedia information extraction and multimodal data fusion. Finally, we discuss conceptual and practical challenges in precision psychiatry and highlight ML opportunities in future research. △ Less","11 July, 2022",https://arxiv.org/pdf/2204.01607
"Artificial Intelligence: Framework of driving triggers to past, present and future applications and influencers of industry sector adoption",Richard Fulton;Diane Fulton;Susan Kaplan,"To gain a sense of the development of Artificial Intelligence (AI), this research analyzes what has been done in the past, presently in the last decade and what is predicted for the next several decades. The paper will highlight the biggest changes in AI and give examples of how these technologies are applied in several key industry sectors along with influencers that can affect adoption speed. Lastly, the research examines the driving triggers such as cost, speed, accuracy, diversity/inclusion and interdisciplinary research/collaboration that propel AI into an essential transformative technology. △ Less","30 March, 2022",https://arxiv.org/pdf/2204.01518
On scientific understanding with artificial intelligence,Mario Krenn;Robert Pollice;Si Yue Guo;Matteo Aldeghi;Alba Cervera-Lierta;Pascal Friederich;Gabriel dos Passos Gomes;Florian Häse;Adrian Jinich;AkshatKumar Nigam;Zhenpeng Yao;Alán Aspuru-Guzik,"Imagine an oracle that correctly predicts the outcome of every particle physics experiment, the products of every chemical reaction, or the function of every protein. Such an oracle would revolutionize science and technology as we know them. However, as scientists, we would not be satisfied with the oracle itself. We want more. We want to comprehend how the oracle conceived these predictions. This feat, denoted as scientific understanding, has frequently been recognized as the essential aim of science. Now, the ever-growing power of computers and artificial intelligence poses one ultimate question: How can advanced artificial systems contribute to scientific understanding or achieve it autonomously? We are convinced that this is not a mere technical question but lies at the core of science. Therefore, here we set out to answer where we are and where we can go from here. We first seek advice from the philosophy of science to understand scientific understanding. Then we review the current state of the art, both from literature and by collecting dozens of anecdotes from scientists about how they acquired new conceptual understanding with the help of computers. Those combined insights help us to define three dimensions of android-assisted scientific understanding: The android as a I) computational microscope, II) resource of inspiration and the ultimate, not yet existent III) agent of understanding. For each dimension, we explain new avenues to push beyond the status quo and unleash the full power of artificial intelligence's contribution to the central aim of science. We hope our perspective inspires and focuses research towards androids that get new scientific understanding and ultimately bring us closer to true artificial scientists. △ Less","4 April, 2022",https://arxiv.org/pdf/2204.01467
Computer-Aided Extraction of Select MRI Markers of Cerebral Small Vessel Disease: A Systematic Review,Jiyang Jiang;Dadong Wang;Yang Song;Perminder S. Sachdev;Wei Wen,"Cerebral small vessel disease (CSVD) is a major vascular contributor to cognitive impairment in ageing, including dementias. Imaging remains the most promising method for in vivo studies of CSVD. To replace the subjective and laborious visual rating approaches, emerging studies have applied state-of-the-art artificial intelligence to extract imaging biomarkers of CSVD from MRI scans. We aimed to summarise published computer-aided methods to examine three imaging biomarkers of CSVD, namely cerebral microbleeds (CMB), dilated perivascular spaces (PVS), and lacunes of presumed vascular origin. Seventy-one classical image processing, classical machine learning, and deep learning studies were identified. CMB and PVS have been better studied, compared to lacunes. While good performance metrics have been achieved in local test datasets, there have not been generalisable pipelines validated in different research or clinical cohorts. Transfer learning and weak supervision techniques have been applied to accommodate the limitations in training data. Future studies could consider pooling data from multiple sources to increase diversity, and validating the performance of the methods using both image processing metrics and associations with clinical measures. △ Less","4 April, 2022",https://arxiv.org/pdf/2204.01411
Explainable Online Lane Change Predictions on a Digital Twin with a Layer Normalized LSTM and Layer-wise Relevance Propagation,Christoph Wehner;Francis Powlesland;Bashar Altakrouri;Ute Schmid,"Artificial Intelligence and Digital Twins play an integral role in driving innovation in the domain of intelligent driving. Long short-term memory (LSTM) is a leading driver in the field of lane change prediction for manoeuvre anticipation. However, the decision-making process of such models is complex and non-transparent, hence reducing the trustworthiness of the smart solution. This work presents an innovative approach and a technical implementation for explaining lane change predictions of layer normalized LSTMs using Layer-wise Relevance Propagation (LRP). The core implementation includes consuming live data from a digital twin on a German highway, live predictions and explanations of lane changes by extending LRP to layer normalized LSTMs, and an interface for communicating and explaining the predictions to a human user. We aim to demonstrate faithful, understandable, and adaptable explanations of lane change prediction to increase the adoption and trustworthiness of AI systems that involve humans. Our research also emphases that explainability and state-of-the-art performance of ML models for manoeuvre anticipation go hand in hand without negatively affecting predictive effectiveness. △ Less","4 April, 2022",https://arxiv.org/pdf/2204.01292
Introduction to the Artificial Intelligence that can be applied to the Network Automation Journey,Gilbert Moisio;Alexandre Gonzalvez;Noam Zeitoun,"The computer network world is changing and the NetDevOps approach has brought the dynamics of applications and systems into the field of communication infrastructure. Businesses are changing and businesses are faced with difficulties related to the diversity of hardware and software that make up those infrastructures. The ""Intent-Based Networking - Concepts and Definitions"" document describes the different parts of the ecosystem that could be involved in NetDevOps. The recognize, generate intent, translate and refine features need a new way to implement algorithms. This is where artificial intelligence comes in. △ Less","2 April, 2022",https://arxiv.org/pdf/2204.00800
A neural network based heading and position control system of a ship,Shahroz Unar;Mukhtiar Ali Unar;Zubair Ahmed Memon;Sanam Narejo,"Heading and position control system of ships has remained a challenging control problem. It is a nonlinear multiple input multiple output system. Moreover, the dynamics of the system vary with operating as well as environmental conditions. Conventionally, simple Proportional Integral Derivative controller is used which has well known limitations. Other conventional control techniques have also been investigated but they require an accurate mathematical model of a ship. Unfortunately, accuracy of mathematical models is very difficult to achieve. During the past few decades computational intelligence techniques such as artificial neural networks have been very successful when an accurate mathematical model is not available. Therefore, in this paper, an artificial neural network controller is proposed for heading and position control system. For simulation purposes, a mathematical model with four effective thrusters have been chosen to test the performance of the proposed controller. The final closed loop system has been analyzed and tested through simulation studies. The results are very encouraging. △ Less","2 April, 2022",https://arxiv.org/pdf/2204.00757
Visual explanations for polyp detection: How medical doctors assess intrinsic versus extrinsic explanations,Steven Hicks;Andrea Storås;Michael Riegler;Cise Midoglu;Malek Hammou;Thomas de Lange;Sravanthi Parasa;Pål Halvorsen;Inga Strümke,"Deep learning has in recent years achieved immense success in all areas of computer vision and has the potential of assisting medical doctors in analyzing visual content for disease and other abnormalities. However, the current state of deep learning is very much a black box, making medical professionals highly skeptical about integrating these methods into clinical practice. Several methods have been proposed in order to shine some light onto these black boxes, but there is no consensus on the opinion of the medical doctors that will consume these explanations. This paper presents a study asking medical doctors about their opinion of current state-of-the-art explainable artificial intelligence methods when applied to a gastrointestinal disease detection use case. We compare two different categories of explanation methods, intrinsic and extrinsic, and gauge their opinion of the current value of these explanations. The results indicate that intrinsic explanations are preferred and that explanation. △ Less","23 March, 2022",https://arxiv.org/pdf/2204.00617
From Statistical to Causal Learning,Bernhard Schölkopf;Julius von Kügelgen,"We describe basic ideas underlying research to build and understand artificially intelligent systems: from symbolic approaches via statistical learning to interventional models relying on concepts of causality. Some of the hard open problems of machine learning and AI are intrinsically related to causality, and progress may require advances in our understanding of how to model and infer causality from data. △ Less","1 April, 2022",https://arxiv.org/pdf/2204.00607
SELFIES and the future of molecular string representations,Mario Krenn;Qianxiang Ai;Senja Barthel;Nessa Carson;Angelo Frei;Nathan C. Frey;Pascal Friederich;Théophile Gaudin;Alberto Alexander Gayle;Kevin Maik Jablonka;Rafael F. Lameiro;Dominik Lemm;Alston Lo;Seyed Mohamad Moosavi;José Manuel Nápoles-Duarte;AkshatKumar Nigam;Robert Pollice;Kohulan Rajan;Ulrich Schatzschneider;Philippe Schwaller;Marta Skreta;Berend Smit;Felix Strieth-Kalthoff;Chong Sun;Gary Tom,"Artificial intelligence (AI) and machine learning (ML) are expanding in popularity for broad applications to challenging tasks in chemistry and materials science. Examples include the prediction of properties, the discovery of new reaction pathways, or the design of new molecules. The machine needs to read and write fluently in a chemical language for each of these tasks. Strings are a common tool to represent molecular graphs, and the most popular molecular string representation, SMILES, has powered cheminformatics since the late 1980s. However, in the context of AI and ML in chemistry, SMILES has several shortcomings -- most pertinently, most combinations of symbols lead to invalid results with no valid chemical interpretation. To overcome this issue, a new language for molecules was introduced in 2020 that guarantees 100\% robustness: SELFIES (SELF-referencIng Embedded Strings). SELFIES has since simplified and enabled numerous new applications in chemistry. In this manuscript, we look to the future and discuss molecular string representations, along with their respective opportunities and challenges. We propose 16 concrete Future Projects for robust molecular representations. These involve the extension toward new chemical domains, exciting questions at the interface of AI and robust languages and interpretability for both humans and machines. We hope that these proposals will inspire several follow-up works exploiting the full potential of molecular string representations for the future of AI in chemistry and materials science. △ Less","31 March, 2022",https://arxiv.org/pdf/2204.00056
Differentially Private Federated Learning via Reconfigurable Intelligent Surface,Yuhan Yang;Yong Zhou;Youlong Wu;Yuanming Shi,"Federated learning (FL), as a disruptive machine learning paradigm, enables the collaborative training of a global model over decentralized local datasets without sharing them. It spans a wide scope of applications from Internet-of-Things (IoT) to biomedical engineering and drug discovery. To support low-latency and high-privacy FL over wireless networks, in this paper, we propose a reconfigurable intelligent surface (RIS) empowered over-the-air FL system to alleviate the dilemma between learning accuracy and privacy. This is achieved by simultaneously exploiting the channel propagation reconfigurability with RIS for boosting the receive signal power, as well as waveform superposition property with over-the-air computation (AirComp) for fast model aggregation. By considering a practical scenario where high-dimensional local model updates are transmitted across multiple communication blocks, we characterize the convergence behaviors of the differentially private federated optimization algorithm. We further formulate a system optimization problem to optimize the learning accuracy while satisfying privacy and power constraints via the joint design of transmit power, artificial noise, and phase shifts at RIS, for which a two-step alternating minimization framework is developed. Simulation results validate our systematic, theoretical, and algorithmic achievements and demonstrate that RIS can achieve a better trade-off between privacy and accuracy for over-the-air FL systems. △ Less","31 March, 2022",https://arxiv.org/pdf/2203.17028
A unified theory of learning,Taisuke Katayose,"Recently machine learning using neural networks (NN) has been developed, and many new methods have been suggested. These methods are optimized for the type of input data and work very effectively, but they cannot be used with any kind of input data universally. On the other hand, the human brain is universal for any kind of problem, and we will be able to construct artificial general intelligence if we can mimic the system of how the human brain works. We consider how the human brain learns things uniformly, and find that the essence of learning is the compression of information. We suggest a toy NN model which mimics the system of the human brain, and we show that the NN can compress the input information without ad hoc treatment, only by setting the loss function properly. The loss function is expressed as the sum of the self-information to remember and the loss of the information along with the compression, and its minimum corresponds to the self-information of the original data. To evaluate the self-information to remember, we provided the concept of memory. The memory expresses the compressed information, and the learning proceeds by referring to previous memories. There are many similarities between this NN and the human brain, and this NN is a realization of the free-energy principle which is considered to be a unified theory of the human brain. This work can be applied to any kind of data analysis and cognitive science. △ Less","24 April, 2022",https://arxiv.org/pdf/2203.16941
Ransomware Detection using Process Memory,Avinash Singh;Richard Adeyemi Ikuesan;Hein Venter,"Ransomware attacks have increased significantly in recent years, causing great destruction and damage to critical systems and business operations. Attackers are unfailingly finding innovative ways to bypass detection mechanisms, whichencouraged the adoption of artificial intelligence. However, most research summarizes the general features of AI and induces many false positives, as the behavior of ransomware constantly differs to bypass detection. Focusing on the key indicating features of ransomware becomes vital as this guides the investigator to the inner workings and main function of ransomware itself. By utilizing access privileges in process memory, the main function of the ransomware can be detected more easily and accurately. Furthermore, new signatures and fingerprints of ransomware families can be identified to classify novel ransomware attacks correctly. The current research used the process memory access privileges of the different memory regions of the behavior of an executable to quickly determine its intent before serious harm can occur. To achieve this aim, several well-known machine learning algorithms were explored with an accuracy range of 81.38 to 96.28 percents. The study thus confirms the feasibility of utilizing process memory as a detection mechanism for ransomware. △ Less","31 March, 2022",https://arxiv.org/pdf/2203.16871
"An Artificial Intelligence Browser Architecture (AIBA) For Our Kind and Others: A Voice Name System Speech implementation with two warrants, Wake Neutrality and Value Preservation of Personally Identifiable Information",Brian Subirana,"Conversational commerce, first pioneered by Apple's Siri, is the first of may applications based on always-on artificial intelligence systems that decide on its own when to interact with the environment, potentially collecting 24x7 longitudinal training data that is often Personally Identifiable Information (PII). A large body of scholarly papers, on the order of a million according to a simple Google Scholar search, suggests that the treatment of many health conditions, including COVID-19 and dementia, can be vastly improved by this data if the dataset is large enough as it has happened in other domains (e.g. GPT3). In contrast, current dominant systems are closed garden solutions without wake neutrality and that can't fully exploit the PII data they have because of IRB and Cohues-type constraints. We present a voice browser-and-server architecture that aims to address these two limitations by offering wake neutrality and the possibility to handle PII aiming to maximize its value. We have implemented this browser for the collection of speech samples and have successfully demonstrated it can capture over 200.000 samples of COVID-19 coughs. The architecture we propose is designed so it can grow beyond our kind into other domains such as collecting sound samples from vehicles, video images from nature, ingestible robotics, multi-modal signals (EEG, EKG,...), or even interacting with other kinds such as dogs and cats. △ Less","31 March, 2022",https://arxiv.org/pdf/2203.16497
Longitudinal Fairness with Censorship,Wenbin Zhang;Jeremy C. Weiss,"Recent works in artificial intelligence fairness attempt to mitigate discrimination by proposing constrained optimization programs that achieve parity for some fairness statistic. Most assume availability of the class label, which is impractical in many real-world applications such as precision medicine, actuarial analysis and recidivism prediction. Here we consider fairness in longitudinal right-censored environments, where the time to event might be unknown, resulting in censorship of the class label and inapplicability of existing fairness studies. We devise applicable fairness measures, propose a debiasing algorithm, and provide necessary theoretical constructs to bridge fairness with and without censorship for these important and socially-sensitive tasks. Our experiments on four censored datasets confirm the utility of our approach. △ Less","30 March, 2022",https://arxiv.org/pdf/2203.16024
Theory of Acceleration of Decision Making by Correlated Time Sequences,Norihiro Okada;Tomoki Yamagami;Nicolas Chauvet;Yusuke Ito;Mikio Hasegawa;Makoto Naruse,"Photonic accelerators have been intensively studied to provide enhanced information processing capability to benefit from the unique attributes of physical processes. Recently, it has been reported that chaotically oscillating ultrafast time series from a laser, called laser chaos, provide the ability to solve multi-armed bandit (MAB) problems or decision-making problems at GHz order. Furthermore, it has been confirmed that the negatively correlated time-domain structure of laser chaos contributes to the acceleration of decision-making. However, the underlying mechanism of why decision-making is accelerated by correlated time series is unknown. In this study, we demonstrate a theoretical model to account for accelerating decision-making by correlated time sequence. We first confirm the effectiveness of the negative autocorrelation inherent in time series for solving two-armed bandit problems using Fourier transform surrogate methods. We propose a theoretical model that concerns the correlated time series subjected to the decision-making system and the internal status of the system therein in a unified manner, inspired by correlated random walks. We demonstrate that the performance derived analytically by the theory agrees well with the numerical simulations, which confirms the validity of the proposed model and leads to optimal system design. The present study paves the way for improving the effectiveness of correlated time series for decision-making, impacting artificial intelligence and other applications. △ Less","15 July, 2022",https://arxiv.org/pdf/2203.16004
Implementation of an Automated Learning System for Non-experts,Phoenix X. Huang;Zhiwei Zhao;Chao Liu;Jingyi Liu;Wenze Hu;Xiaoyu Wang,"Automated machine learning systems for non-experts could be critical for industries to adopt artificial intelligence to their own applications. This paper detailed the engineering system implementation of an automated machine learning system called YMIR, which completely relies on graphical interface to interact with users. After importing training/validation data into the system, a user without AI knowledge can label the data, train models, perform data mining and evaluation by simply clicking buttons. The paper described: 1) Open implementation of model training and inference through docker containers. 2) Implementation of task and resource management. 3) Integration of Labeling software. 4) Implementation of HCI (Human Computer Interaction) with a rebuilt collaborative development paradigm. We also provide subsequent case study on training models with the system. We hope this paper can facilitate the prosperity of our automated machine learning community from industry application perspective. The code of the system has already been released to GitHub (https://github.com/industryessentials/ymir). △ Less","25 March, 2022",https://arxiv.org/pdf/2203.15784
Exploring Opportunities in Usable Hazard Analysis Processes for AI Engineering,Nikolas Martelaro;Carol J. Smith;Tamara Zilovic,"Embedding artificial intelligence into systems introduces significant challenges to modern engineering practices. Hazard analysis tools and processes have not yet been adequately adapted to the new paradigm. This paper describes initial research and findings regarding current practices in AI-related hazard analysis and on the tools used to conduct this work. Our goal with this initial research is to better understand the needs of practitioners and the emerging challenges of considering hazards and risks for AI-enabled products and services. Our primary research question is: Can we develop new structured thinking methods and systems engineering tools to support effective and engaging ways for preemptively considering failure modes in AI systems? The preliminary findings from our review of the literature and interviews with practitioners highlight various challenges around integrating hazard analysis into modern AI development processes and suggest opportunities for exploration of usable, human-centered hazard analysis tools. △ Less","29 March, 2022",https://arxiv.org/pdf/2203.15628
Human Response to an AI-Based Decision Support System: A User Study on the Effects of Accuracy and Bias,David Solans;Andrea Beretta;Manuel Portela;Carlos Castillo;Anna Monreale,"Artificial Intelligence (AI) is increasingly used to build Decision Support Systems (DSS) across many domains. This paper describes a series of experiments designed to observe human response to different characteristics of a DSS such as accuracy and bias, particularly the extent to which participants rely on the DSS, and the performance they achieve. In our experiments, participants play a simple online game inspired by so-called ""wildcat"" (i.e., exploratory) drilling for oil. The landscape has two layers: a visible layer describing the costs (terrain), and a hidden layer describing the reward (oil yield). Participants in the control group play the game without receiving any assistance, while in treatment groups they are assisted by a DSS suggesting places to drill. For certain treatments, the DSS does not consider costs, but only rewards, which introduces a bias that is observable by users. Between subjects, we vary the accuracy and bias of the DSS, and observe the participants' total score, time to completion, the extent to which they follow or ignore suggestions. We also measure the acceptability of the DSS in an exit survey. Our results show that participants tend to score better with the DSS, that the score increase is due to users following the DSS advice, and related to the difficulty of the game and the accuracy of the DSS. We observe that this setting elicits mostly rational behavior from participants, who place a moderate amount of trust in the DSS and show neither algorithmic aversion (under-reliance) nor automation bias (over-reliance).However, their stated willingness to accept the DSS in the exit survey seems less sensitive to the accuracy of the DSS than their behavior, suggesting that users are only partially aware of the (lack of) accuracy of the DSS. △ Less","24 March, 2022",https://arxiv.org/pdf/2203.15514
Spatiotemporal Patterns in Neurobiology: An Overview for Future Artificial Intelligence,Sean Knight;Navjot Gadda,"In recent years, there has been increasing interest in developing models and tools to address the complex patterns of connectivity found in brain tissue. Specifically, this is due to a need to understand how emergent properties emerge from these network structures at multiple spatiotemporal scales. We argue that computational models are key tools for elucidating the possible functionalities that can emerge from interactions of heterogeneous neurons connected by complex networks on multi-scale temporal and spatial domains. Here we review several classes of models including spiking neurons, integrate and fire neurons with short term plasticity (STP), conductance based integrate-and-fire models with STP, and population density neural field (PDNF) models using simple examples with emphasis on neuroscience applications while also providing some potential future research directions for AI. These computational approaches allow us to explore the impact of changing underlying mechanisms on resulting network function both experimentally as well as theoretically. Thus we hope these studies will inform future developments in artificial intelligence algorithms as well as help validate our understanding of brain processes based on experiments in animals or humans. △ Less","14 April, 2022",https://arxiv.org/pdf/2203.15415
Human-AI Collaboration Enables More Empathic Conversations in Text-based Peer-to-Peer Mental Health Support,Ashish Sharma;Inna W. Lin;Adam S. Miner;David C. Atkins;Tim Althoff,"Advances in artificial intelligence (AI) are enabling systems that augment and collaborate with humans to perform simple, mechanistic tasks like scheduling meetings and grammar-checking text. However, such Human-AI collaboration poses challenges for more complex, creative tasks, such as carrying out empathic conversations, due to difficulties of AI systems in understanding complex human emotions and the open-ended nature of these tasks. Here, we focus on peer-to-peer mental health support, a setting in which empathy is critical for success, and examine how AI can collaborate with humans to facilitate peer empathy during textual, online supportive conversations. We develop Hailey, an AI-in-the-loop agent that provides just-in-time feedback to help participants who provide support (peer supporters) respond more empathically to those seeking help (support seekers). We evaluate Hailey in a non-clinical randomized controlled trial with real-world peer supporters on TalkLife (N=300), a large online peer-to-peer support platform. We show that our Human-AI collaboration approach leads to a 19.60% increase in conversational empathy between peers overall. Furthermore, we find a larger 38.88% increase in empathy within the subsample of peer supporters who self-identify as experiencing difficulty providing support. We systematically analyze the Human-AI collaboration patterns and find that peer supporters are able to use the AI feedback both directly and indirectly without becoming overly reliant on AI while reporting improved self-efficacy post-feedback. Our findings demonstrate the potential of feedback-driven, AI-in-the-loop writing systems to empower humans in open-ended, social, creative tasks such as empathic conversations. △ Less","28 March, 2022",https://arxiv.org/pdf/2203.15144
Solving Disjunctive Temporal Networks with Uncertainty under Restricted Time-Based Controllability using Tree Search and Graph Neural Networks,Kevin Osanlou;Jeremy Frank;Andrei Bursuc;Tristan Cazenave;Eric Jacopin;Christophe Guettier;J. Benton,"Planning under uncertainty is an area of interest in artificial intelligence. We present a novel approach based on tree search and graph machine learning for the scheduling problem known as Disjunctive Temporal Networks with Uncertainty (DTNU). Dynamic Controllability (DC) of DTNUs seeks a reactive scheduling strategy to satisfy temporal constraints in response to uncontrollable action durations. We introduce new semantics for reactive scheduling: Time-based Dynamic Controllability (TDC) and a restricted subset of TDC, R-TDC. We design a tree search algorithm to determine whether or not a DTNU is R-TDC. Moreover, we leverage a graph neural network as a heuristic for tree search guidance. Finally, we conduct experiments on a known benchmark on which we show R-TDC to retain significant completeness with regard to DC, while being faster to prove. This results in the tree search processing fifty percent more DTNU problems in R-TDC than the state-of-the-art DC solver does in DC with the same time budget. We also observe that graph neural network search guidance leads to substantial performance gains on benchmarks of more complex DTNUs, with up to eleven times more problems solved than the baseline tree search. △ Less","30 March, 2022",https://arxiv.org/pdf/2203.15030
Deep Learning and Artificial General Intelligence: Still a Long Way to Go,Maciej Świechowski,"In recent years, deep learning using neural network architecture, i.e. deep neural networks, has been on the frontier of computer science research. It has even lead to superhuman performance in some problems, e.g., in computer vision, games and biology, and as a result the term deep learning revolution was coined. The undisputed success and rapid growth of deep learning suggests that, in future, it might become an enabler for Artificial General Intelligence (AGI). In this article, we approach this statement critically showing five major reasons of why deep neural networks, as of the current state, are not ready to be the technique of choice for reaching AGI. △ Less","5 April, 2022",https://arxiv.org/pdf/2203.14963
A Novel Remote Sensing Approach to Recognize and Monitor Red Palm Weevil in Date Palm Trees,Yashu Kang;Chunlei Chen;Fujian Cheng;Jianyong Zhang,"The spread of the Red Pal Weevil (RPW) has become an existential threat for palm trees around the world. In the Middle East, RPW is causing wide-spread damage to date palm Phoenix dactylifera L., having both agricultural impacts on the palm production and environmental impacts. Early detection of RPW is very challenging, especially at large scale. This research proposes a novel remote sensing approach to recognize and monitor red palm weevil in date palm trees, using a combination of vegetation indices, object detection and semantic segmentation techniques. The study area consists of date palm trees with three classes, including healthy palms, smallish palms and severely infected palms. This proposed method achieved a promising 0.947 F1 score on test data set. This work paves the way for deploying artificial intelligence approaches to monitor RPW in large-scale as well as provide guidance for practitioners. △ Less","27 March, 2022",https://arxiv.org/pdf/2203.14476
MFSNet: A Multi Focus Segmentation Network for Skin Lesion Segmentation,Hritam Basak;Rohit Kundu;Ram Sarkar,"Segmentation is essential for medical image analysis to identify and localize diseases, monitor morphological changes, and extract discriminative features for further diagnosis. Skin cancer is one of the most common types of cancer globally, and its early diagnosis is pivotal for the complete elimination of malignant tumors from the body. This research develops an Artificial Intelligence (AI) framework for supervised skin lesion segmentation employing the deep learning approach. The proposed framework, called MFSNet (Multi-Focus Segmentation Network), uses differently scaled feature maps for computing the final segmentation mask using raw input RGB images of skin lesions. In doing so, initially, the images are preprocessed to remove unwanted artifacts and noises. The MFSNet employs the Res2Net backbone, a recently proposed convolutional neural network (CNN), for obtaining deep features used in a Parallel Partial Decoder (PPD) module to get a global map of the segmentation mask. In different stages of the network, convolution features and multi-scale maps are used in two boundary attention (BA) modules and two reverse attention (RA) modules to generate the final segmentation output. MFSNet, when evaluated on three publicly available datasets: PH^2, ISIC 2017, and HAM10000, outperforms state-of-the-art methods, justifying the reliability of the framework. The relevant codes for the proposed approach are accessible at https://github.com/Rohit-Kundu/MFSNet △ Less","29 March, 2022",https://arxiv.org/pdf/2203.14341
Quantum continual learning of quantum data realizing knowledge backward transfer,Haozhen Situ;Tianxiang Lu;Minghua Pan;Lvzhou Li,"For the goal of strong artificial intelligence that can mimic human-level intelligence, AI systems would have the ability to adapt to ever-changing scenarios and learn new knowledge continuously without forgetting previously acquired knowledge. When a machine learning model is consecutively trained on multiple tasks that come in sequence, its performance on previously learned tasks may drop dramatically during the learning process of the newly seen task. To avoid this phenomenon termed catastrophic forgetting, continual learning, also known as lifelong learning, has been proposed and become one of the most up-to-date research areas of machine learning. As quantum machine learning blossoms in recent years, it is interesting to develop quantum continual learning. This paper focuses on the case of quantum models for quantum data where the computation model and the data to be processed are both quantum. The gradient episodic memory method is incorporated to design a quantum continual learning scheme that overcomes catastrophic forgetting and realizes knowledge backward transfer. Specifically, a sequence of quantum state classification tasks is continually learned by a variational quantum classifier whose parameters are optimized by a classical gradient-based optimizer. The gradient of the current task is projected to the closest gradient, avoiding the increase of the loss at previous tasks, but allowing the decrease. Numerical simulation results show that our scheme not only overcomes catastrophic forgetting, but also realize knowledge backward transfer, which means the classifier's performance on previous tasks is enhanced rather than compromised while learning a new task. △ Less","7 June, 2022",https://arxiv.org/pdf/2203.14032
A Meta Survey of Quality Evaluation Criteria in Explanation Methods,Helena Löfström;Karl Hammar;Ulf Johansson,"Explanation methods and their evaluation have become a significant issue in explainable artificial intelligence (XAI) due to the recent surge of opaque AI models in decision support systems (DSS). Since the most accurate AI models are opaque with low transparency and comprehensibility, explanations are essential for bias detection and control of uncertainty. There are a plethora of criteria to choose from when evaluating explanation method quality. However, since existing criteria focus on evaluating single explanation methods, it is not obvious how to compare the quality of different methods. This lack of consensus creates a critical shortage of rigour in the field, although little is written about comparative evaluations of explanation methods. In this paper, we have conducted a semi-systematic meta-survey over fifteen literature surveys covering the evaluation of explainability to identify existing criteria usable for comparative evaluations of explanation methods. The main contribution in the paper is the suggestion to use appropriate trust as a criterion to measure the outcome of the subjective evaluation criteria and consequently make comparative evaluations possible. We also present a model of explanation quality aspects. In the model, criteria with similar definitions are grouped and related to three identified aspects of quality; model, explanation, and user. We also notice four commonly accepted criteria (groups) in the literature, covering all aspects of explanation quality: Performance, appropriate trust, explanation satisfaction, and fidelity. We suggest the model be used as a chart for comparative evaluations to create more generalisable research in explanation quality. △ Less","25 March, 2022",https://arxiv.org/pdf/2203.13929
Concept Embedding Analysis: A Review,Gesina Schwalbe,"Deep neural networks (DNNs) have found their way into many applications with potential impact on the safety, security, and fairness of human-machine-systems. Such require basic understanding and sufficient trust by the users. This motivated the research field of explainable artificial intelligence (XAI), i.e. finding methods for opening the ""black-boxes"" DNNs represent. For the computer vision domain in specific, practical assessment of DNNs requires a globally valid association of human interpretable concepts with internals of the model. The research field of concept (embedding) analysis (CA) tackles this problem: CA aims to find global, assessable associations of humanly interpretable semantic concepts (e.g., eye, bearded) with internal representations of a DNN. This work establishes a general definition of CA and a taxonomy for CA methods, uniting several ideas from literature. That allows to easily position and compare CA approaches. Guided by the defined notions, the current state-of-the-art research regarding CA methods and interesting applications are reviewed. More than thirty relevant methods are discussed, compared, and categorized. Finally, for practitioners, a survey of fifteen datasets is provided that have been used for supervised concept analysis. Open challenges and research directions are pointed out at the end. △ Less","25 March, 2022",https://arxiv.org/pdf/2203.13909
A World-Self Model Towards Understanding Intelligence,Yutao Yue,"The symbolism, connectionism and behaviorism approaches of artificial intelligence have achieved a lot of successes in various tasks, while we still do not have a clear definition of ""intelligence"" with enough consensus in the community (although there are over 70 different ""versions"" of definitions). The nature of intelligence is still in darkness. In this work we do not take any of these three traditional approaches, instead we try to identify certain fundamental aspects of the nature of intelligence, and construct a mathematical model to represent and potentially reproduce these fundamental aspects. We first stress the importance of defining the scope of discussion and granularity of investigation. We carefully compare human and artificial intelligence, and qualitatively demonstrate an information abstraction process, which we propose to be the key to connect perception and cognition. We then present the broader idea of ""concept"", separate the idea of self model out of the world model, and construct a new model called world-self model (WSM). We show the mechanisms of creating and connecting concepts, and the flow of how the WSM receives, processes and outputs information with respect to an arbitrary type of problem to solve. We also consider and discuss the potential computer implementation issues of the proposed theoretical framework, and finally we propose a unified general framework of intelligence based on WSM. △ Less","15 June, 2022",https://arxiv.org/pdf/2203.13762
Multi-modal multi-objective model-based genetic programming to find multiple diverse high-quality models,E. M. C. Sijben;T. Alderliesten;P. A. N. Bosman,"Explainable artificial intelligence (XAI) is an important and rapidly expanding research topic. The goal of XAI is to gain trust in a machine learning (ML) model through clear insights into how the model arrives at its predictions. Genetic programming (GP) is often cited as being uniquely well-suited to contribute to XAI because of its capacity to learn (small) symbolic models that have the potential to be interpreted. Nevertheless, like many ML algorithms, GP typically results in a single best model. However, in practice, the best model in terms of training error may well not be the most suitable one as judged by a domain expert for various reasons, including overfitting, multiple different models existing that have similar accuracy, and unwanted errors on particular data points due to typical accuracy measures like mean squared error. Hence, to increase chances that domain experts deem a resulting model plausible, it becomes important to be able to explicitly search for multiple, diverse, high-quality models that trade-off different meanings of accuracy. In this paper, we achieve exactly this with a novel multi-modal multi-tree multi-objective GP approach that extends a modern model-based GP algorithm known as GP-GOMEA that is already effective at searching for small expressions. △ Less","24 March, 2022",https://arxiv.org/pdf/2203.13347
Searching for fingerspelled content in American Sign Language,Bowen Shi;Diane Brentari;Greg Shakhnarovich;Karen Livescu,"Natural language processing for sign language video - including tasks like recognition, translation, and search - is crucial for making artificial intelligence technologies accessible to deaf individuals, and is gaining research interest in recent years. In this paper, we address the problem of searching for fingerspelled key-words or key phrases in raw sign language videos. This is an important task since significant content in sign language is often conveyed via fingerspelling, and to our knowledge the task has not been studied before. We propose an end-to-end model for this task, FSS-Net, that jointly detects fingerspelling and matches it to a text sequence. Our experiments, done on a large public dataset of ASL fingerspelling in the wild, show the importance of fingerspelling detection as a component of a search and retrieval model. Our model significantly outperforms baseline methods adapted from prior work on related tasks △ Less","24 March, 2022",https://arxiv.org/pdf/2203.13291
Six Insights into 6G: Orientation and Input for Developing Your Strategic 6G Research Plan,Kimberley Parsons Trommler;Matthias Hafner;Wolfgang Kellerer;Peter Merz;Sigurd Schuster;Josef Urban;Uwe Baeder;Bertram Gunzelmann;Andreas Kornbichler,"This paper is a summary of the findings from a series of workshops which were held by Thinknet 6G and MUENCHNER KREIS in 2021, with the goal to provide orientation and input for developing a strategic 6G research plan. The topics selected for the workshops are aspects of 6G that we expect will have a significant impact on other industries and on society: - 6G as both a communication infrastructure and a sensing infrastructure - The extensive use of artificial intelligence in 6G - The security and resilience of 6G This paper does not go into the technical details of how to develop and implement 6G. Rather, it provides input from experts from both the wireless industry as well as from other sectors about (mostly) non-technical topics that will need to be addressed in parallel with the technical developments, such as new use cases, regulation, communication with the public, and cross-industry cooperation. We have identified six areas that will have a significant impact on the development and use of 6G, and that organizations must consider as they begin their plans and designs for 6G. Based on these six impact areas and on the discussion in the workshops, we compiled a list of the top 10 recommendations for specific areas where organizations should place their focus when developing their strategic plan for 6G. In addition, for our readers who are involved in 6G research, be it at a university, at a research institute or in industrial research, we also included a summary of the top 10 areas that require additional research, again based on the input received in the workshops. A version of this paper is also available at www.thinknet-6g.de. If you had a copy of the preview version of this paper, the text is exactly the same. Only the layout and graphics have changed. △ Less","20 May, 2022",https://arxiv.org/pdf/2203.13094
Kratt: Developing an Automatic Subject Indexing Tool for The National Library of Estonia,Marit Asula;Jane Makke;Linda Freienthal;Hele-Andra Kuulmets;Raul Sirel,"Manual subject indexing in libraries is a time-consuming and costly process and the quality of the assigned subjects is affected by the cataloguer's knowledge on the specific topics contained in the book. Trying to solve these issues, we exploited the opportunities arising from artificial intelligence to develop Kratt: a prototype of an automatic subject indexing tool. Kratt is able to subject index a book independent of its extent and genre with a set of keywords present in the Estonian Subject Thesaurus. It takes Kratt approximately 1 minute to subject index a book, outperforming humans 10-15 times. Although the resulting keywords were not considered satisfactory by the cataloguers, the ratings of a small sample of regular library users showed more promise. We also argue that the results can be enhanced by including a bigger corpus for training the model and applying more careful preprocessing techniques. △ Less","24 March, 2022",https://arxiv.org/pdf/2203.12998
Millimeter-wave Foresight Sensing for Safety and Resilience in Autonomous Operations,Daniel Mitchell;Jamie Blanche;Sam T. Harper;Theodore Lim;Valentin Robu;Ikuo Yamamoto;David Flynn,"Robotic platforms are highly programmable, scalable and versatile to complete several tasks including Inspection, Maintenance and Repair (IMR). Mobile robotics offer reduced restrictions in operating environments, resulting in greater flexibility; operation at height, dangerous areas and repetitive tasks. Cyber physical infrastructures have been identified by the UK Robotics Growth Partnership as a key enabler in how we utilize and interact with sensors and machines via the virtual and physical worlds. Cyber Physical Systems (CPS) allow for robotics and artificial intelligence to adapt and repurpose at pace, allowing for the addressment of new challenges in CPS. A challenge exists within robotics to secure an effective partnership in a wide range of areas which include shared workspaces and Beyond Visual Line of Sight (BVLOS). Robotic manipulation abilities have improved a robots accessibility via the ability to open doorways, however, challenges exist in how a robot decides if it is safe to move into a new workspace. Current sensing methods are limited to line of sight and are unable to capture data beyond doorways or walls, therefore, a robot is unable to sense if it is safe to open a door. Another limitation exists as robots are unable to detect if a human is within a shared workspace. Therefore, if a human is detected, extended safety precautions can be taken to ensure the safe autonomous operation of a robot. These challenges are represented as safety, trust and resilience, inhibiting the successful advancement of CPS. This paper evaluates the use of frequency modulated continuous wave radar sensing for human detection and through-wall detection to increase situational awareness. The results validate the use of the sensor to detect the difference between a person and infrastructure, and increased situational awareness for navigation via foresight monitoring through walls. △ Less","24 March, 2022",https://arxiv.org/pdf/2203.12987
Onto4MAT: A Swarm Shepherding Ontology for Generalised Multi-Agent Teaming,Adam J. Hepworth;Daniel P. Baxter;Hussein A. Abbass,"Research in multi-agent teaming has increased substantially over recent years, with knowledge-based systems to support teaming processes typically focused on delivering functional (communicative) solutions for a team to act meaningfully in response to direction. Enabling humans to effectively interact and team with a swarm of autonomous cognitive agents is an open research challenge in Human-Swarm Teaming research, partially due to the focus on developing the enabling architectures to support these systems. Typically, bi-directional transparency and shared semantic understanding between agents has not prioritised a designed mechanism in Human-Swarm Teaming, potentially limiting how a human and a swarm team can share understanding and information\textemdash data through concepts and contexts\textemdash to achieve a goal. To address this, we provide a formal knowledge representation design that enables the swarm Artificial Intelligence to reason about its environment and system, ultimately achieving a shared goal. We propose the Ontology for Generalised Multi-Agent Teaming, Onto4MAT, to enable more effective teaming between humans and teams through the biologically-inspired approach of shepherding. △ Less","24 March, 2022",https://arxiv.org/pdf/2203.12955
Direct evaluation of progression or regression of disease burden in brain metastatic disease with Deep Neuroevolution,Joseph Stember;Robert Young;Hrithwik Shalu,"Purpose: A core component of advancing cancer treatment research is assessing response to therapy. Doing so by hand, for example as per RECIST or RANO criteria, is tedious, time-consuming, and can miss important tumor response information; most notably, they exclude non-target lesions. We wish to assess change in a holistic fashion that includes all lesions, obtaining simple, informative, and automated assessments of tumor progression or regression. Due to often low patient enrolments in clinical trials, we wish to make response assessments with small training sets. Deep neuroevolution (DNE) can produce radiology artificial intelligence (AI) that performs well on small training sets. Here we use DNE for function approximation that predicts progression versus regression of metastatic brain disease. Methods: We analyzed 50 pairs of MRI contrast-enhanced images as our training set. Half of these pairs, separated in time, qualified as disease progression, while the other 25 images constituted regression. We trained the parameters of a relatively small CNN via mutations that consisted of random CNN weight adjustments and mutation fitness. We then incorporated the best mutations into the next generations CNN, repeating this process for approximately 50,000 generations. We applied the CNNs to our training set, as well as a separate testing set with the same class balance of 25 progression and 25 regression images. Results: DNE achieved monotonic convergence to 100% training set accuracy. DNE also converged monotonically to 100% testing set accuracy. Conclusion: DNE can accurately classify brain-metastatic disease progression versus regression. Future work will extend the input from 2D image slices to full 3D volumes, and include the category of no change. We believe that an approach such as our could ultimately provide a useful adjunct to RANO/RECIST assessment. △ Less","24 March, 2022",https://arxiv.org/pdf/2203.12853
Functional mimicry of Ruffini receptors with Fiber Bragg Gratings and Deep Neural Networks enables a bio-inspired large-area tactile sensitive skin,Luca Massari;Giulia Fransvea;Jessica D'Abbraccio;Mariangela Filosa;Giuseppe Terruso;Andrea Aliperta;Giacomo D'Alesio;Martina Zaltieri;Emiliano Schena;Eduardo Palermo;Edoardo Sinibaldi;Calogero Maria Oddo,"Collaborative robots are expected to physically interact with humans in daily living and workplace, including industrial and healthcare settings. A related key enabling technology is tactile sensing, which currently requires addressing the outstanding scientific challenge to simultaneously detect contact location and intensity by means of soft conformable artificial skins adapting over large areas to the complex curved geometries of robot embodiments. In this work, the development of a large-area sensitive soft skin with a curved geometry is presented, allowing for robot total-body coverage through modular patches. The biomimetic skin consists of a soft polymeric matrix, resembling a human forearm, embedded with photonic Fiber Bragg Grating (FBG) transducers, which partially mimics Ruffini mechanoreceptor functionality with diffuse, overlapping receptive fields. A Convolutional Neural Network deep learning algorithm and a multigrid Neuron Integration Process were implemented to decode the FBG sensor outputs for inferring contact force magnitude and localization through the skin surface. Results achieved 35 mN (IQR = 56 mN) and 3.2 mm (IQR = 2.3 mm) median errors, for force and localization predictions, respectively. Demonstrations with an anthropomorphic arm pave the way towards AI-based integrated skins enabling safe human-robot cooperation via machine intelligence. △ Less","23 March, 2022",https://arxiv.org/pdf/2203.12752
An interactive music infilling interface for pop music composition,Rui Guo,"Artificial intelligence (AI) has been widely applied to music generation topics such as continuation, melody/harmony generation, genre transfer and music infilling application. Although with the burst interest to apply AI to music, there are still few interfaces for the musicians to take advantage of the latest progress of the AI technology. This makes those tools less valuable in practice and harder to find its advantage/drawbacks without utilizing them in the real scenario. This work builds a max patch for interactive music infilling application with different levels of control, including track density/polyphony/occupation rate and bar tonal tension control. The user can select the melody/bass/harmony track as the infilling content up to 16 bars. The infilling algorithm is based on the author's previous work, and the interface sends/receives messages to the AI system hosted in the cloud. This interface lowers the barrier of AI technology and can generate different variations of the selected content. Those results can give several alternatives to the musicians' composition, and the interactive process realizes the value of the AI infilling system. △ Less","23 March, 2022",https://arxiv.org/pdf/2203.12736
Reclaiming saliency: rhythmic precision-modulated action and perception,Ajith Anil Meera;Filip Novicky;Thomas Parr;Karl Friston;Pablo Lanillos;Noor Sajid,"Computational models of visual attention in artificial intelligence and robotics have been inspired by the concept of a saliency map. These models account for the mutual information between the (current) visual information and its estimated causes. However, they fail to consider the circular causality between perception and action. In other words, they do not consider where to sample next, given current beliefs. Here, we reclaim salience as an active inference process that relies on two basic principles: uncertainty minimisation and rhythmic scheduling. For this, we make a distinction between attention and salience. Briefly, we associate attention with precision control, i.e., the confidence with which beliefs can be updated given sampled sensory data, and salience with uncertainty minimisation that underwrites the selection of future sensory data. Using this, we propose a new account of attention based on rhythmic precision-modulation and discuss its potential in robotics, providing numerical experiments that showcase advantages of precision-modulation for state and noise estimation, system identification and action selection for informative path planning. △ Less","23 March, 2022",https://arxiv.org/pdf/2203.12652
Applications of physics informed neural operators,Shawn G. Rosofsky;Hani Al Majed;E. A. Huerta,"We present an end-to-end framework to learn partial differential equations that brings together initial data production, selection of boundary conditions, and the use of physics-informed neural operators to solve partial differential equations that are ubiquitous in the study and modeling of physics phenomena. We first demonstrate that our methods reproduce the accuracy and performance of other neural operators published elsewhere in the literature to learn the 1D wave equation and the 1D Burgers equation. Thereafter, we apply our physics-informed neural operators to learn new types of equations, including the 2D Burgers equation in the scalar, inviscid and vector types. Finally, we show that our approach is also applicable to learn the physics of the 2D linear and nonlinear shallow water equations, which involve three coupled partial differential equations. We release our artificial intelligence surrogates and scientific software to produce initial data and boundary conditions to study a broad range of physically motivated scenarios. We provide the source code, an interactive website to visualize the predictions of our physics informed neural operators, and a tutorial for their use at the Data and Learning Hub for Science. △ Less","8 December, 2022",https://arxiv.org/pdf/2203.12634
The state-of-the-art review on resource allocation problem using artificial intelligence methods on various computing paradigms,Javad Hassannataj Joloudari;Sanaz Mojrian;Hamid Saadatfar;Issa Nodehi;Fatemeh Fazl;Sahar Khanjani shirkharkolaie;Roohallah Alizadehsani;H M Dipu Kabir;Ru-San Tan;U Rajendra Acharya,"With the increasing growth of information through smart devices, increasing the quality level of human life requires various computational paradigms presentation including the Internet of Things, fog, and cloud. Between these three paradigms, the cloud computing paradigm as an emerging technology adds cloud layer services to the edge of the network so that resource allocation operations occur close to the end-user to reduce resource processing time and network traffic overhead. Hence, the resource allocation problem for its providers in terms of presenting a suitable platform, by using computational paradigms is considered a challenge. In general, resource allocation approaches are divided into two methods, including auction-based methods(goal, increase profits for service providers-increase user satisfaction and usability) and optimization-based methods(energy, cost, network exploitation, Runtime, reduction of time delay). In this paper, according to the latest scientific achievements, a comprehensive literature study (CLS) on artificial intelligence methods based on resource allocation optimization without considering auction-based methods in various computing environments are provided such as cloud computing, Vehicular Fog Computing, wireless, IoT, vehicular networks, 5G networks, vehicular cloud architecture,machine-to-machine communication(M2M),Train-to-Train(T2T) communication network, Peer-to-Peer(P2P) network. Since deep learning methods based on artificial intelligence are used as the most important methods in resource allocation problems; Therefore, in this paper, resource allocation approaches based on deep learning are also used in the mentioned computational environments such as deep reinforcement learning, Q-learning technique, reinforcement learning, online learning, and also Classical learning methods such as Bayesian learning, Cummins clustering, Markov decision process. △ Less","4 November, 2022",https://arxiv.org/pdf/2203.12315
Converse: A Tree-Based Modular Task-Oriented Dialogue System,Tian Xie;Xinyi Yang;Angela S. Lin;Feihong Wu;Kazuma Hashimoto;Jin Qu;Young Mo Kang;Wenpeng Yin;Huan Wang;Semih Yavuz;Gang Wu;Michael Jones;Richard Socher;Yingbo Zhou;Wenhao Liu;Caiming Xiong,"Creating a system that can have meaningful conversations with humans to help accomplish tasks is one of the ultimate goals of Artificial Intelligence (AI). It has defined the meaning of AI since the beginning. A lot has been accomplished in this area recently, with voice assistant products entering our daily lives and chat bot systems becoming commonplace in customer service. At first glance there seems to be no shortage of options for dialogue systems. However, the frequently deployed dialogue systems today seem to all struggle with a critical weakness - they are hard to build and harder to maintain. At the core of the struggle is the need to script every single turn of interactions between the bot and the human user. This makes the dialogue systems more difficult to maintain as the tasks become more complex and more tasks are added to the system. In this paper, we propose Converse, a flexible tree-based modular task-oriented dialogue system. Converse uses an and-or tree structure to represent tasks and offers powerful multi-task dialogue management. Converse supports task dependency and task switching, which are unique features compared to other open-source dialogue frameworks. At the same time, Converse aims to make the bot building process easy and simple, for both professional and non-professional software developers. The code is available at https://github.com/salesforce/Converse. △ Less","9 May, 2022",https://arxiv.org/pdf/2203.12187
A Theoretically Grounded Benchmark for Evaluating Machine Commonsense,Henrique Santos;Ke Shen;Alice M. Mulvehill;Yasaman Razeghi;Deborah L. McGuinness;Mayank Kejriwal,"Programming machines with commonsense reasoning (CSR) abilities is a longstanding challenge in the Artificial Intelligence community. Current CSR benchmarks use multiple-choice (and in relatively fewer cases, generative) question-answering instances to evaluate machine commonsense. Recent progress in transformer-based language representation models suggest that considerable progress has been made on existing benchmarks. However, although tens of CSR benchmarks currently exist, and are growing, it is not evident that the full suite of commonsense capabilities have been systematically evaluated. Furthermore, there are doubts about whether language models are 'fitting' to a benchmark dataset's training partition by picking up on subtle, but normatively irrelevant (at least for CSR), statistical features to achieve good performance on the testing partition. To address these challenges, we propose a benchmark called Theoretically-Grounded Commonsense Reasoning (TG-CSR) that is also based on discriminative question answering, but with questions designed to evaluate diverse aspects of commonsense, such as space, time, and world states. TG-CSR is based on a subset of commonsense categories first proposed as a viable theory of commonsense by Gordon and Hobbs. The benchmark is also designed to be few-shot (and in the future, zero-shot), with only a few training and validation examples provided. This report discusses the structure and construction of the benchmark. Preliminary results suggest that the benchmark is challenging even for advanced language representation models designed for discriminative CSR question answering tasks. Benchmark access and leaderboard: https://codalab.lisn.upsaclay.fr/competitions/3080 Benchmark website: https://usc-isi-i2.github.io/TGCSR/ △ Less","14 July, 2022",https://arxiv.org/pdf/2203.12184
"Out of Distribution Detection, Generalization, and Robustness Triangle with Maximum Probability Theorem",Amir Emad Marvasti;Ehsan Emad Marvasti;Ulas Bagci,"Maximum Probability Framework, powered by Maximum Probability Theorem, is a recent theoretical development in artificial intelligence, aiming to formally define probabilistic models, guiding development of objective functions, and regularization of probabilistic models. MPT uses the probability distribution that the models assume on random variables to provide an upper bound on the probability of the model. We apply MPT to challenging out-of-distribution (OOD) detection problems in computer vision by incorporating MPT as a regularization scheme in the training of CNNs and their energy-based variants. We demonstrate the effectiveness of the proposed method on 1080 trained models, with varying hyperparameters, and conclude that the MPT-based regularization strategy stabilizes and improves the generalization and robustness of base models in addition to enhanced OOD performance on CIFAR10, CIFAR100, and MNIST datasets. △ Less","6 September, 2022",https://arxiv.org/pdf/2203.12145
Enabling faster and more reliable sonographic assessment of gestational age through machine learning,Chace Lee;Angelica Willis;Christina Chen;Marcin Sieniek;Akib Uddin;Jonny Wong;Rory Pilgrim;Katherine Chou;Daniel Tse;Shravya Shetty;Ryan G. Gomes,"Fetal ultrasounds are an essential part of prenatal care and can be used to estimate gestational age (GA). Accurate GA assessment is important for providing appropriate prenatal care throughout pregnancy and identifying complications such as fetal growth disorders. Since derivation of GA from manual fetal biometry measurements (head, abdomen, femur) are operator-dependent and time-consuming, there have been a number of research efforts focused on using artificial intelligence (AI) models to estimate GA using standard biometry images, but there is still room to improve the accuracy and reliability of these AI systems for widescale adoption. To improve GA estimates, without significant change to provider workflows, we leverage AI to interpret standard plane ultrasound images as well as 'fly-to' ultrasound videos, which are 5-10s videos automatically recorded as part of the standard of care before the still image is captured. We developed and validated three AI models: an image model using standard plane images, a video model using fly-to videos, and an ensemble model (combining both image and video). All three were statistically superior to standard fetal biometry-based GA estimates derived by expert sonographers, the ensemble model has the lowest mean absolute error (MAE) compared to the clinical standard fetal biometry (mean difference: -1.51 \pm 3.96 days, 95% CI [-1.9, -1.1]) on a test set that consisted of 404 participants. We showed that our models outperform standard biometry by a more substantial margin on fetuses that were small for GA. Our AI models have the potential to empower trained operators to estimate GA with higher accuracy while reducing the amount of time required and user variability in measurement acquisition. △ Less","22 March, 2022",https://arxiv.org/pdf/2203.11903
Performance Evaluation of Machine Learning-based Algorithm and Taguchi Algorithm for the Determination of the Hardness Value of the Friction Stir Welded AA 6262 Joints at a Nugget Zone,Akshansh Mishra;Eyob Messele Sefene;Gopikrishna Nidigonda;Assefa Asmare Tsegaw,"Nowadays, industry 4.0 plays a tremendous role in the manufacturing industries for increasing the amount of data and accuracy in modern manufacturing systems. Thanks to artificial intelligence, particularly machine learning, big data analytics have dramatically amended, and manufacturers easily exploit organized and unorganized data. This study utilized hybrid optimization algorithms to find friction stir welding and optimal hardness value at the nugget zone. A similar AA 6262 material was used and welded in a butt joint configuration. Tool rotational speed (RPM), tool traverse speed (mm/min), and the plane depth (mm) are used as controllable parameters and optimized using Taguchi L9, Random Forest, and XG Boost machine learning tools. Analysis of variance was also conducted at a 95% confidence interval for identifying the significant parameters. The result indicated that the coefficient of determination from Taguchi L9 orthogonal array is 0.91 obtained while Random Forest and XG Boost algorithm imparted 0.62 and 0.65, respectively. △ Less","22 March, 2022",https://arxiv.org/pdf/2203.11649
Explainability in reinforcement learning: perspective and position,Agneza Krajna;Mario Brcic;Tomislav Lipic;Juraj Doncevic,"Artificial intelligence (AI) has been embedded into many aspects of people's daily lives and it has become normal for people to have AI make decisions for them. Reinforcement learning (RL) models increase the space of solvable problems with respect to other machine learning paradigms. Some of the most interesting applications are in situations with non-differentiable expected reward function, operating in unknown or underdefined environment, as well as for algorithmic discovery that surpasses performance of any teacher, whereby agent learns from experimental experience through simple feedback. The range of applications and their social impact is vast, just to name a few: genomics, game-playing (chess, Go, etc.), general optimization, financial investment, governmental policies, self-driving cars, recommendation systems, etc. It is therefore essential to improve the trust and transparency of RL-based systems through explanations. Most articles dealing with explainability in artificial intelligence provide methods that concern supervised learning and there are very few articles dealing with this in the area of RL. The reasons for this are the credit assignment problem, delayed rewards, and the inability to assume that data is independently and identically distributed (i.i.d.). This position paper attempts to give a systematic overview of existing methods in the explainable RL area and propose a novel unified taxonomy, building and expanding on the existing ones. The position section describes pragmatic aspects of how explainability can be observed. The gap between the parties receiving and generating the explanation is especially emphasized. To reduce the gap and achieve honesty and truthfulness of explanations, we set up three pillars: proactivity, risk attitudes, and epistemological constraints. To this end, we illustrate our proposal on simple variants of the shortest path problem. △ Less","22 March, 2022",https://arxiv.org/pdf/2203.11547
SSD-KD: A Self-supervised Diverse Knowledge Distillation Method for Lightweight Skin Lesion Classification Using Dermoscopic Images,Yongwei Wang;Yuheng Wang;Tim K. Lee;Chunyan Miao;Z. Jane Wang,"Skin cancer is one of the most common types of malignancy, affecting a large population and causing a heavy economic burden worldwide. Over the last few years, computer-aided diagnosis has been rapidly developed and make great progress in healthcare and medical practices due to the advances in artificial intelligence. However, most studies in skin cancer detection keep pursuing high prediction accuracies without considering the limitation of computing resources on portable devices. In this case, knowledge distillation (KD) has been proven as an efficient tool to help improve the adaptability of lightweight models under limited resources, meanwhile keeping a high-level representation capability. To bridge the gap, this study specifically proposes a novel method, termed SSD-KD, that unifies diverse knowledge into a generic KD framework for skin diseases classification. Our method models an intra-instance relational feature representation and integrates it with existing KD research. A dual relational knowledge distillation architecture is self-supervisedly trained while the weighted softened outputs are also exploited to enable the student model to capture richer knowledge from the teacher model. To demonstrate the effectiveness of our method, we conduct experiments on ISIC 2019, a large-scale open-accessed benchmark of skin diseases dermoscopic images. Experiments show that our distilled lightweight model can achieve an accuracy as high as 85% for the classification tasks of 8 different skin diseases with minimal parameters and computing requirements. Ablation studies confirm the effectiveness of our intra- and inter-instance relational knowledge integration strategy. Compared with state-of-the-art knowledge distillation techniques, the proposed method demonstrates improved performances for multi-diseases classification on the large-scale dermoscopy database. △ Less","29 March, 2022",https://arxiv.org/pdf/2203.11490
Optimizing Binary Decision Diagrams with MaxSAT for classification,Hao Hu;Marie-José Huguet;Mohamed Siala,"The growing interest in explainable artificial intelligence (XAI) for critical decision making motivates the need for interpretable machine learning (ML) models. In fact, due to their structure (especially with small sizes), these models are inherently understandable by humans. Recently, several exact methods for computing such models are proposed to overcome weaknesses of traditional heuristic methods by providing more compact models or better prediction quality. Despite their compressed representation of Boolean functions, Binary decision diagrams (BDDs) did not gain enough interest as other interpretable ML models. In this paper, we first propose SAT-based models for learning optimal BDDs (in terms of the number of features) that classify all input examples. Then, we lift the encoding to a MaxSAT model to learn optimal BDDs in limited depths, that maximize the number of examples correctly classified. Finally, we tackle the fragmentation problem by introducing a method to merge compatible subtrees for the BDDs found via the MaxSAT model. Our empirical study shows clear benefits of the proposed approach in terms of prediction quality and intrepretability (i.e., lighter size) compared to the state-of-the-art approaches. △ Less","21 March, 2022",https://arxiv.org/pdf/2203.11386
EEG based Emotion Recognition: A Tutorial and Review,Xiang Li;Yazhou Zhang;Prayag Tiwari;Dawei Song;Bin Hu;Meihong Yang;Zhigang Zhao;Neeraj Kumar;Pekka Marttinen,"Emotion recognition technology through analyzing the EEG signal is currently an essential concept in Artificial Intelligence and holds great potential in emotional health care, human-computer interaction, multimedia content recommendation, etc. Though there have been several works devoted to reviewing EEG-based emotion recognition, the content of these reviews needs to be updated. In addition, those works are either fragmented in content or only focus on specific techniques adopted in this area but neglect the holistic perspective of the entire technical routes. Hence, in this paper, we review from the perspective of researchers who try to take the first step on this topic. We review the recent representative works in the EEG-based emotion recognition research and provide a tutorial to guide the researchers to start from the beginning. The scientific basis of EEG-based emotion recognition in the psychological and physiological levels is introduced. Further, we categorize these reviewed works into different technical routes and illustrate the theoretical basis and the research motivation, which will help the readers better understand why those techniques are studied and employed. At last, existing challenges and future investigations are also discussed in this paper, which guides the researchers to decide potential future research directions. △ Less","16 March, 2022",https://arxiv.org/pdf/2203.11279
"Automated Clinical Coding: What, Why, and Where We Are?",Hang Dong;Matúš Falis;William Whiteley;Beatrice Alex;Joshua Matterson;Shaoxiong Ji;Jiaoyan Chen;Honghan Wu,"Clinical coding is the task of transforming medical information in a patient's health records into structured codes so that they can be used for statistical analysis. This is a cognitive and time-consuming task that follows a standard process in order to achieve a high level of consistency. Clinical coding could potentially be supported by an automated system to improve the efficiency and accuracy of the process. We introduce the idea of automated clinical coding and summarise its challenges from the perspective of Artificial Intelligence (AI) and Natural Language Processing (NLP), based on the literature, our project experience over the past two and half years (late 2019 - early 2022), and discussions with clinical coding experts in Scotland and the UK. Our research reveals the gaps between the current deep learning-based approach applied to clinical coding and the need for explainability and consistency in real-world practice. Knowledge-based methods that represent and reason the standard, explainable process of a task may need to be incorporated into deep learning-based methods for clinical coding. Automated clinical coding is a promising task for AI, despite the technical and organisational challenges. Coders are needed to be involved in the development process. There is much to achieve to develop and deploy an AI-based automated system to support coding in the next five years and beyond. △ Less","9 October, 2022",https://arxiv.org/pdf/2203.11092
Filter Drug-induced Liver Injury Literature with Natural Language Processing and Ensemble Learning,Xianghao Zhan;Fanjin Wang;Olivier Gevaert,"Drug-induced liver injury (DILI) describes the adverse effects of drugs that damage liver. Life-threatening results including liver failure or death were also reported in severe DILI cases. Therefore, DILI-related events are strictly monitored for all approved drugs and the liver toxicity became important assessments for new drug candidates. These DILI-related reports are documented in hospital records, in clinical trial results, and also in research papers that contain preliminary in vitro and in vivo experiments. Conventionally, data extraction from previous publications relies heavily on resource-demanding manual labelling, which considerably decreased the efficiency of the information extraction process. The recent development of artificial intelligence, particularly, the rise of natural language processing (NLP) techniques, enabled the automatic processing of biomedical texts. In this study, based on around 28,000 papers (titles and abstracts) provided by the Critical Assessment of Massive Data Analysis (CAMDA) challenge, we benchmarked model performances on filtering out DILI literature. Among four word vectorization techniques, the model using term frequency-inverse document frequency (TF-IDF) and logistic regression outperformed others with an accuracy of 0.957 with our in-house test set. Furthermore, an ensemble model with similar overall performances was implemented and was fine-tuned to lower the false-negative cases to avoid neglecting potential DILI reports. The ensemble model achieved a high accuracy of 0.954 and an F1 score of 0.955 in the hold-out validation data provided by the CAMDA committee. Moreover, important words in positive/negative predictions were identified via model interpretation. Overall, the ensemble model reached satisfactory classification results, which can be further used by researchers to rapidly filter DILI-related literature. △ Less","9 March, 2022",https://arxiv.org/pdf/2203.11015
An integrated Auto Encoder-Block Switching defense approach to prevent adversarial attacks,Anirudh Yadav;Ashutosh Upadhyay;S. Sharanya,"According to recent studies, the vulnerability of state-of-the-art Neural Networks to adversarial input samples has increased drastically. A neural network is an intermediate path or technique by which a computer learns to perform tasks using Machine learning algorithms. Machine Learning and Artificial Intelligence model has become a fundamental aspect of life, such as self-driving cars [1], smart home devices, so any vulnerability is a significant concern. The smallest input deviations can fool these extremely literal systems and deceive their users as well as administrator into precarious situations. This article proposes a defense algorithm that utilizes the combination of an auto-encoder [3] and block-switching architecture. Auto-coder is intended to remove any perturbations found in input images whereas the block switching method is used to make it more robust against White-box attacks. The attack is planned using FGSM [9] model, and the subsequent counter-attack by the proposed architecture will take place thereby demonstrating the feasibility and security delivered by the algorithm. △ Less","11 March, 2022",https://arxiv.org/pdf/2203.10930
TCM-SD: A Benchmark for Probing Syndrome Differentiation via Natural Language Processing,Mucheng Ren;Heyan Huang;Yuxiang Zhou;Qianwen Cao;Yuan Bu;Yang Gao,"Traditional Chinese Medicine (TCM) is a natural, safe, and effective therapy that has spread and been applied worldwide. The unique TCM diagnosis and treatment system requires a comprehensive analysis of a patient's symptoms hidden in the clinical record written in free text. Prior studies have shown that this system can be informationized and intelligentized with the aid of artificial intelligence (AI) technology, such as natural language processing (NLP). However, existing datasets are not of sufficient quality nor quantity to support the further development of data-driven AI technology in TCM. Therefore, in this paper, we focus on the core task of the TCM diagnosis and treatment system -- syndrome differentiation (SD) -- and we introduce the first public large-scale dataset for SD, called TCM-SD. Our dataset contains 54,152 real-world clinical records covering 148 syndromes. Furthermore, we collect a large-scale unlabelled textual corpus in the field of TCM and propose a domain-specific pre-trained language model, called ZY-BERT. We conducted experiments using deep neural networks to establish a strong performance baseline, reveal various challenges in SD, and prove the potential of domain-specific pre-trained language model. Our study and analysis reveal opportunities for incorporating computer science and linguistics knowledge to explore the empirical validity of TCM theories. △ Less","2 August, 2022",https://arxiv.org/pdf/2203.10839
Human-Centric Artificial Intelligence Architecture for Industry 5.0 Applications,Jože M. Rožanec;Inna Novalija;Patrik Zajec;Klemen Kenda;Hooman Tavakoli;Sungho Suh;Entso Veliou;Dimitrios Papamartzivanos;Thanassis Giannetsos;Sofia Anna Menesidou;Ruben Alonso;Nino Cauli;Antonello Meloni;Diego Reforgiato Recupero;Dimosthenis Kyriazis;Georgios Sofianidis;Spyros Theodoropoulos;Blaž Fortuna;Dunja Mladenić;John Soldatos,"Human-centricity is the core value behind the evolution of manufacturing towards Industry 5.0. Nevertheless, there is a lack of architecture that considers safety, trustworthiness, and human-centricity at its core. Therefore, we propose an architecture that integrates Artificial Intelligence (Active Learning, Forecasting, Explainable Artificial Intelligence), simulated reality, decision-making, and users' feedback, focusing on synergies between humans and machines. Furthermore, we align the proposed architecture with the Big Data Value Association Reference Architecture Model. Finally, we validate it on three use cases from real-world case studies. △ Less","19 October, 2022",https://arxiv.org/pdf/2203.10794
Intelligent control of a single-link flexible manipulator using sliding modes and artificial neural networks,Gabriel da Silva Lima;Diego Rolim Porto;Adilson Jose de Oliveira;Wallace Moreira Bessa,"This letter presents a new intelligent control scheme for the accurate trajectory tracking of flexible link manipulators. The proposed approach is mainly based on a sliding mode controller for underactuated systems with an embedded artificial neural network to deal with modeling inaccuracies. The adopted neural network only needs a single input and one hidden layer, which drastically reduces the computational complexity of the control law and allows its implementation in low-power microcontrollers. Online learning, rather than supervised offline training, is chosen to allow the weights of the neural network to be adjusted in real time during the tracking. Therefore, the resulting controller is able to cope with the underactuating issues and to adapt itself by learning from experience, which grants the capacity to deal with plant dynamics properly. The boundedness and convergence properties of the tracking error are proved by evoking Barbalat's lemma in a Lyapunov-like stability analysis. Experimental results obtained with a small single-link flexible manipulator show the efficacy of the proposed control scheme, even in the presence of a high level of uncertainty and noisy signals. △ Less","21 March, 2022",https://arxiv.org/pdf/2203.10771
STCGAT: A Spatio-temporal Causal Graph Attention Network for traffic flow prediction in Intelligent Transportation Systems,Wei Zhao;Shiqi Zhang;Bing Zhou;Bei Wang,"Air pollution and carbon emissions caused by modern transportation are closely related to global climate change. With the help of next-generation information technology such as Internet of Things (IoT) and Artificial Intelligence (AI), accurate traffic flow prediction can effectively solve problems such as traffic congestion and mitigate environmental pollution and climate change. It further promotes the development of Intelligent Transportation Systems (ITS) and smart cities. However, the strong spatial and temporal correlation of traffic data makes the task of accurate traffic forecasting a significant challenge. Existing methods are usually based on graph neural networks using predefined spatial adjacency graphs of traffic networks to model spatial dependencies, ignoring the dynamic correlation of relationships between road nodes. In addition, they usually use independent Spatio-temporal components to capture Spatio-temporal dependencies and do not effectively model global Spatio-temporal dependencies. This paper proposes a new Spatio-temporal Causal Graph Attention Network (STCGAT) for traffic prediction to address the above challenges. In STCGAT, we use a node embedding approach that can adaptively generate spatial adjacency subgraphs at each time step without a priori geographic knowledge and fine-grained modeling of the topology of dynamically generated graphs for different time steps. Meanwhile, we propose an efficient causal temporal correlation component that contains node adaptive learning, graph convolution, and local and global causal temporal convolution modules to learn local and global Spatio-temporal dependencies jointly. Extensive experiments on four real, large traffic datasets show that our model consistently outperforms all baseline models. △ Less","29 September, 2022",https://arxiv.org/pdf/2203.10749
A Policy Driven AI-Assisted PoW Framework,Trisha Chakraborty;Shaswata Mitra;Sudip Mittal;Maxwell Young,"Proof of Work (PoW) based cyberdefense systems require incoming network requests to expend effort solving an arbitrary mathematical puzzle. Current state of the art is unable to differentiate between trustworthy and untrustworthy connections, requiring all to solve complex puzzles. In this paper, we introduce an Artificial Intelligence (AI)-assisted PoW framework that utilizes IP traffic based features to inform an adaptive issuer which can then generate puzzles with varying hardness. The modular framework uses these capabilities to ensure that untrustworthy clients solve harder puzzles thereby incurring longer latency than authentic requests to receive a response from the server. Our preliminary findings reveal our approach effectively throttles untrustworthy traffic. △ Less","20 March, 2022",https://arxiv.org/pdf/2203.10698
Recognising the importance of preference change: A call for a coordinated multidisciplinary research effort in the age of AI,Matija Franklin;Hal Ashton;Rebecca Gorman;Stuart Armstrong,"As artificial intelligence becomes more powerful and a ubiquitous presence in daily life, it is imperative to understand and manage the impact of AI systems on our lives and decisions. Modern ML systems often change user behavior (e.g. personalized recommender systems learn user preferences to deliver recommendations that change online behavior). An externality of behavior change is preference change. This article argues for the establishment of a multidisciplinary endeavor focused on understanding how AI systems change preference: Preference Science. We operationalize preference to incorporate concepts from various disciplines, outlining the importance of meta-preferences and preference-change preferences, and proposing a preliminary framework for how preferences change. We draw a distinction between preference change, permissible preference change, and outright preference manipulation. A diversity of disciplines contribute unique insights to this framework. △ Less","30 March, 2022",https://arxiv.org/pdf/2203.10525
Federated Spatial Reuse Optimization in Next-Generation Decentralized IEEE 802.11 WLANs,Francesc Wilhelmi;Jernej Hribar;Selim F. Yilmaz;Emre Ozfatura;Kerem Ozfatura;Ozlem Yildiz;Deniz Gündüz;Hao Chen;Xiaoying Ye;Lizhao You;Yulin Shao;Paolo Dini;Boris Bellalta,"As wireless standards evolve, more complex functionalities are introduced to address the increasing requirements in terms of throughput, latency, security, and efficiency. To unleash the potential of such new features, artificial intelligence (AI) and machine learning (ML) are currently being exploited for deriving models and protocols from data, rather than by hand-programming. In this paper, we explore the feasibility of applying ML in next-generation wireless local area networks (WLANs). More specifically, we focus on the IEEE 802.11ax spatial reuse (SR) problem and predict its performance through federated learning (FL) models. The set of FL solutions overviewed in this work is part of the 2021 International Telecommunication Union (ITU) AI for 5G Challenge. △ Less","7 June, 2022",https://arxiv.org/pdf/2203.10472
AI system for fetal ultrasound in low-resource settings,Ryan G. Gomes;Bellington Vwalika;Chace Lee;Angelica Willis;Marcin Sieniek;Joan T. Price;Christina Chen;Margaret P. Kasaro;James A. Taylor;Elizabeth M. Stringer;Scott Mayer McKinney;Ntazana Sindano;George E. Dahl;William Goodnight III;Justin Gilmer;Benjamin H. Chi;Charles Lau;Terry Spitz;T Saensuksopa;Kris Liu;Jonny Wong;Rory Pilgrim;Akib Uddin;Greg Corrado;Lily Peng,"Despite considerable progress in maternal healthcare, maternal and perinatal deaths remain high in low-to-middle income countries. Fetal ultrasound is an important component of antenatal care, but shortage of adequately trained healthcare workers has limited its adoption. We developed and validated an artificial intelligence (AI) system that uses novice-acquired ""blind sweep"" ultrasound videos to estimate gestational age (GA) and fetal malpresentation. We further addressed obstacles that may be encountered in low-resourced settings. Using a simplified sweep protocol with real-time AI feedback on sweep quality, we have demonstrated the generalization of model performance to minimally trained novice ultrasound operators using low cost ultrasound devices with on-device AI integration. The GA model was non-inferior to standard fetal biometry estimates with as few as two sweeps, and the fetal malpresentation model had high AUC-ROCs across operators and devices. Our AI models have the potential to assist in upleveling the capabilities of lightly trained ultrasound operators in low resource settings. △ Less","18 March, 2022",https://arxiv.org/pdf/2203.10139
BIOS: An Algorithmically Generated Biomedical Knowledge Graph,Sheng Yu;Zheng Yuan;Jun Xia;Shengxuan Luo;Huaiyuan Ying;Sihang Zeng;Jingyi Ren;Hongyi Yuan;Zhengyun Zhao;Yucong Lin;Keming Lu;Jing Wang;Yutao Xie;Heung-Yeung Shum,"Biomedical knowledge graphs (BioMedKGs) are essential infrastructures for biomedical and healthcare big data and artificial intelligence (AI), facilitating natural language processing, model development, and data exchange. For decades, these knowledge graphs have been developed via expert curation; however, this method can no longer keep up with today's AI development, and a transition to algorithmically generated BioMedKGs is necessary. In this work, we introduce the Biomedical Informatics Ontology System (BIOS), the first large-scale publicly available BioMedKG generated completely by machine learning algorithms. BIOS currently contains 4.1 million concepts, 7.4 million terms in two languages, and 7.3 million relation triplets. We present the methodology for developing BIOS, including the curation of raw biomedical terms, computational identification of synonymous terms and aggregation of these terms to create concept nodes, semantic type classification of the concepts, relation identification, and biomedical machine translation. We provide statistics on the current BIOS content and perform preliminary assessments of term quality, synonym grouping, and relation extraction. The results suggest that machine learning-based BioMedKG development is a viable alternative to traditional expert curation. △ Less","24 April, 2022",https://arxiv.org/pdf/2203.09975
Why we need biased AI -- How including cognitive and ethical machine biases can enhance AI systems,Sarah Fabi;Thilo Hagendorff,"This paper stresses the importance of biases in the field of artificial intelligence (AI) in two regards. First, in order to foster efficient algorithmic decision-making in complex, unstable, and uncertain real-world environments, we argue for the structurewise implementation of human cognitive biases in learning algorithms. Secondly, we argue that in order to achieve ethical machine behavior, filter mechanisms have to be applied for selecting biased training stimuli that represent social or behavioral traits that are ethically desirable. We use insights from cognitive science as well as ethics and apply them to the AI field, combining theoretical considerations with seven case studies depicting tangible bias implementation scenarios. Ultimately, this paper is the first tentative step to explicitly pursue the idea of a re-evaluation of the ethical significance of machine biases, as well as putting the idea forth to implement cognitive biases into machines. △ Less","18 March, 2022",https://arxiv.org/pdf/2203.09911
Blockchain for the Metaverse: A Review,Thippa Reddy Gadekallu;Thien Huynh-The;Weizheng Wang;Gokul Yenduri;Pasika Ranaweera;Quoc-Viet Pham;Daniel Benevides da Costa;Madhusanka Liyanage,"Since Facebook officially changed its name to Metaverse in Oct. 2021, the metaverse has become a new norm of social networks and three-dimensional (3D) virtual worlds. The metaverse aims to bring 3D immersive and personalized experiences to users by leveraging many pertinent technologies. Despite great attention and benefits, a natural question in the metaverse is how to secure its users' digital content and data. In this regard, blockchain is a promising solution owing to its distinct features of decentralization, immutability, and transparency. To better understand the role of blockchain in the metaverse, we aim to provide an extensive survey on the applications of blockchain for the metaverse. We first present a preliminary to blockchain and the metaverse and highlight the motivations behind the use of blockchain for the metaverse. Next, we extensively discuss blockchain-based methods for the metaverse from technical perspectives, such as data acquisition, data storage, data sharing, data interoperability, and data privacy preservation. For each perspective, we first discuss the technical challenges of the metaverse and then highlight how blockchain can help. Moreover, we investigate the impact of blockchain on key-enabling technologies in the metaverse, including Internet-of-Things, digital twins, multi-sensory and immersive applications, artificial intelligence, and big data. We also present some major projects to showcase the role of blockchain in metaverse applications and services. Finally, we present some promising directions to drive further research innovations and developments towards the use of blockchain in the metaverse in the future. △ Less","21 March, 2022",https://arxiv.org/pdf/2203.09738
Federated Learning for Privacy Preservation in Smart Healthcare Systems: A Comprehensive Survey,Mansoor Ali;Faisal Naeem;Muhammad Tariq;Geroges Kaddoum,"Recent advances in electronic devices and communication infrastructure have revolutionized the traditional healthcare system into a smart healthcare system by using IoMT devices. However, due to the centralized training approach of artificial intelligence (AI), the use of mobile and wearable IoMT devices raises privacy concerns with respect to the information that has been communicated between hospitals and end users. The information conveyed by the IoMT devices is highly confidential and can be exposed to adversaries. In this regard, federated learning (FL), a distributive AI paradigm has opened up new opportunities for privacy-preservation in IoMT without accessing the confidential data of the participants. Further, FL provides privacy to end users as only gradients are shared during training. For these specific properties of FL, in this paper we present privacy related issues in IoMT. Afterwards, we present the role of FL in IoMT networks for privacy preservation and introduce some advanced FL architectures incorporating deep reinforcement learning (DRL), digital twin, and generative adversarial networks (GANs) for detecting privacy threats. Subsequently, we present some practical opportunities of FL in smart healthcare systems. At the end, we conclude this survey by providing open research challenges for FL that can be used in future smart healthcare systems △ Less","17 March, 2022",https://arxiv.org/pdf/2203.09702
Emerging Artificial Intelligence Applications in Spatial Transcriptomics Analysis,Yijun Li;Stefan Stanojevic;Lana X. Garmire,Spatial transcriptomics (ST) has advanced significantly in the last few years. Such advancement comes with the urgent need for novel computational methods to handle the unique challenges of ST data analysis. Many artificial intelligence (AI) methods have been developed to utilize various machine learning and deep learning techniques for computational ST analysis. This review provides a comprehensive and up-to-date survey of current AI methods for ST analysis. △ Less,"17 March, 2022",https://arxiv.org/pdf/2203.09664
Symmetry-Based Representations for Artificial and Biological General Intelligence,Irina Higgins;Sébastien Racanière;Danilo Rezende,"Biological intelligence is remarkable in its ability to produce complex behaviour in many diverse situations through data efficient, generalisable and transferable skill acquisition. It is believed that learning ""good"" sensory representations is important for enabling this, however there is little agreement as to what a good representation should look like. In this review article we are going to argue that symmetry transformations are a fundamental principle that can guide our search for what makes a good representation. The idea that there exist transformations (symmetries) that affect some aspects of the system but not others, and their relationship to conserved quantities has become central in modern physics, resulting in a more unified theoretical framework and even ability to predict the existence of new particles. Recently, symmetries have started to gain prominence in machine learning too, resulting in more data efficient and generalisable algorithms that can mimic some of the complex behaviours produced by biological intelligence. Finally, first demonstrations of the importance of symmetry transformations for representation learning in the brain are starting to arise in neuroscience. Taken together, the overwhelming positive effect that symmetries bring to these disciplines suggest that they may be an important general framework that determines the structure of the universe, constrains the nature of natural tasks and consequently shapes both biological and artificial intelligence. △ Less","17 March, 2022",https://arxiv.org/pdf/2203.09250
Blockchain as privacy and security solution for smart environments: A Survey,Maad Ebrahim;Abdelhakim Hafid;Etienne Elie,"Blockchain was always associated with Bitcoin, cryptocurrencies, and digital asset trading. However, its benefits are far beyond that. It supports technologies like the Internet-of-Things (IoT) to pave the way for futuristic smart environments, like smart homes, smart transportation, smart energy trading, smart industries, smart supply chains, and more. To enable these environments, IoT devices, machines, appliances, and vehicles, need to intercommunicate without the need for centralized trusted parties. Blockchain replaces these trusted parties in such trustless environments. It provides security enforcement, privacy assurance, authentication, and other key features to IoT ecosystems. Besides IoT-Blockchain integration, other technologies add more benefits that attract the research community. Software-Defined Networking (SDN), Fog, Edge, and Cloud Computing technologies, for example, play a key role in enabling realistic IoT applications. Moreover, the integration of Artificial Intelligence (AI) provides smart, dynamic, and autonomous decision-making capabilities for IoT devices in smart environments. To push the research further in this domain, we provide in this paper a comprehensive survey that includes state-of-the-art technological integration, challenges, and solutions for smart environments, and the role of these technologies as the building blocks of such smart environments. We also demonstrate how the level of integration between these technologies has increased over the years, which brings us closer to the futuristic view of smart environments. We further discuss the current need to provide general-purpose Blockchain platforms that can adapt to unique design requirements of different applications and solutions. Finally, we provide a simplified architecture of futuristic smart environments that integrate these technologies, showing the advantage of such integration. △ Less","16 March, 2022",https://arxiv.org/pdf/2203.08901
Multimodal Learning on Graphs for Disease Relation Extraction,Yucong Lin;Keming Lu;Sheng Yu;Tianxi Cai;Marinka Zitnik,"Objective: Disease knowledge graphs are a way to connect, organize, and access disparate information about diseases with numerous benefits for artificial intelligence (AI). To create knowledge graphs, it is necessary to extract knowledge from multimodal datasets in the form of relationships between disease concepts and normalize both concepts and relationship types. Methods: We introduce REMAP, a multimodal approach for disease relation extraction and classification. The REMAP machine learning approach jointly embeds a partial, incomplete knowledge graph and a medical language dataset into a compact latent vector space, followed by aligning the multimodal embeddings for optimal disease relation extraction. Results: We apply REMAP approach to a disease knowledge graph with 96,913 relations and a text dataset of 1.24 million sentences. On a dataset annotated by human experts, REMAP improves text-based disease relation extraction by 10.0% (accuracy) and 17.2% (F1-score) by fusing disease knowledge graphs with text information. Further, REMAP leverages text information to recommend new relationships in the knowledge graph, outperforming graph-based methods by 8.4% (accuracy) and 10.4% (F1-score). Conclusion: REMAP is a multimodal approach for extracting and classifying disease relationships by fusing structured knowledge and text information. REMAP provides a flexible neural architecture to easily find, access, and validate AI-driven relationships between disease concepts. △ Less","30 August, 2022",https://arxiv.org/pdf/2203.08893
The Mathematics of Artificial Intelligence,Gitta Kutyniok,"We currently witness the spectacular success of artificial intelligence in both science and public life. However, the development of a rigorous mathematical foundation is still at an early stage. In this survey article, which is based on an invited lecture at the International Congress of Mathematicians 2022, we will in particular focus on the current ""workhorse"" of artificial intelligence, namely deep neural networks. We will present the main theoretical directions along with several exemplary results and discuss key open problems. △ Less","16 March, 2022",https://arxiv.org/pdf/2203.08890
Self-Supervised Deep Learning to Enhance Breast Cancer Detection on Screening Mammography,John D. Miller;Vignesh A. Arasu;Albert X. Pu;Laurie R. Margolies;Weiva Sieh;Li Shen,"A major limitation in applying deep learning to artificial intelligence (AI) systems is the scarcity of high-quality curated datasets. We investigate strong augmentation based self-supervised learning (SSL) techniques to address this problem. Using breast cancer detection as an example, we first identify a mammogram-specific transformation paradigm and then systematically compare four recent SSL methods representing a diversity of approaches. We develop a method to convert a pretrained model from making predictions on uniformly tiled patches to whole images, and an attention-based pooling method that improves the classification performance. We found that the best SSL model substantially outperformed the baseline supervised model. The best SSL model also improved the data efficiency of sample labeling by nearly 4-fold and was highly transferrable from one dataset to another. SSL represents a major breakthrough in computer vision and may help the AI for medical imaging field to shift away from supervised learning and dependency on scarce labels. △ Less","15 March, 2022",https://arxiv.org/pdf/2203.08812
"Disparities in Dermatology AI Performance on a Diverse, Curated Clinical Image Set",Roxana Daneshjou;Kailas Vodrahalli;Roberto A Novoa;Melissa Jenkins;Weixin Liang;Veronica Rotemberg;Justin Ko;Susan M Swetter;Elizabeth E Bailey;Olivier Gevaert;Pritam Mukherjee;Michelle Phung;Kiana Yekrang;Bradley Fong;Rachna Sahasrabudhe;Johan A. C. Allerup;Utako Okata-Karigane;James Zou;Albert Chiou,"Access to dermatological care is a major issue, with an estimated 3 billion people lacking access to care globally. Artificial intelligence (AI) may aid in triaging skin diseases. However, most AI models have not been rigorously assessed on images of diverse skin tones or uncommon diseases. To ascertain potential biases in algorithm performance in this context, we curated the Diverse Dermatology Images (DDI) dataset-the first publicly available, expertly curated, and pathologically confirmed image dataset with diverse skin tones. Using this dataset of 656 images, we show that state-of-the-art dermatology AI models perform substantially worse on DDI, with receiver operator curve area under the curve (ROC-AUC) dropping by 27-36 percent compared to the models' original test results. All the models performed worse on dark skin tones and uncommon diseases, which are represented in the DDI dataset. Additionally, we find that dermatologists, who typically provide visual labels for AI training and test datasets, also perform worse on images of dark skin tones and uncommon diseases compared to ground truth biopsy annotations. Finally, fine-tuning AI models on the well-characterized and diverse DDI images closed the performance gap between light and dark skin tones. Moreover, algorithms fine-tuned on diverse skin tones outperformed dermatologists on identifying malignancy on images of dark skin tones. Our findings identify important weaknesses and biases in dermatology AI that need to be addressed to ensure reliable application to diverse patients and diseases. △ Less","15 March, 2022",https://arxiv.org/pdf/2203.08807
Artificial Intelligence Enables Real-Time and Intuitive Control of Prostheses via Nerve Interface,Diu Khue Luu;Anh Tuan Nguyen;Ming Jiang;Markus W. Drealan;Jian Xu;Tong Wu;Wing-kin Tam;Wenfeng Zhao;Brian Z. H. Lim;Cynthia K. Overstreet;Qi Zhao;Jonathan Cheng;Edward W. Keefer;Zhi Yang,"Objective: The next generation prosthetic hand that moves and feels like a real hand requires a robust neural interconnection between the human minds and machines. Methods: Here we present a neuroprosthetic system to demonstrate that principle by employing an artificial intelligence (AI) agent to translate the amputee's movement intent through a peripheral nerve interface. The AI agent is designed based on the recurrent neural network (RNN) and could simultaneously decode six degree-of-freedom (DOF) from multichannel nerve data in real-time. The decoder's performance is characterized in motor decoding experiments with three human amputees. Results: First, we show the AI agent enables amputees to intuitively control a prosthetic hand with individual finger and wrist movements up to 97-98% accuracy. Second, we demonstrate the AI agent's real-time performance by measuring the reaction time and information throughput in a hand gesture matching task. Third, we investigate the AI agent's long-term uses and show the decoder's robust predictive performance over a 16-month implant duration. Conclusion & significance: Our study demonstrates the potential of AI-enabled nerve technology, underling the next generation of dexterous and intuitive prosthetic hands. △ Less","16 March, 2022",https://arxiv.org/pdf/2203.08648
Building AI Innovation Labs together with Companies,Jens Heidrich;Andreas Jedlitschka;Adam Trendowicz;Anna Maria Vollmer,"In the future, most companies will be confronted with the topic of Artificial Intelligence (AI) and will have to decide on their strategy in this regards. Currently, a lot of companies are thinking about whether and how AI and the usage of data will impact their business model and what potential use cases could look like. One of the biggest challenges lies in coming up with innovative solution ideas with a clear business value. This requires business competencies on the one hand and technical competencies in AI and data analytics on the other hand. In this article, we present the concept of AI innovation labs and demonstrate a comprehensive framework, from coming up with the right ideas to incrementally implementing and evaluating them regarding their business value and their feasibility based on a company's capabilities. The concept is the result of nine years of working on data-driven innovations with companies from various domains. Furthermore, we share some lessons learned from its practical applications. Even though a lot of technical publications can be found in the literature regarding the development of AI models and many consultancy companies provide corresponding services for building AI innovations, we found very few publications sharing details about what an end-to-end framework could look like. △ Less","16 March, 2022",https://arxiv.org/pdf/2203.08465
A Survey of Machine Learning Algorithms for 6G Wireless Networks,Anita Patil;Sridhar Iyer;Rahul Jashvantbhai Pandya,"The primary focus of Artificial Intelligence/Machine Learning (AI/ML) integration within the wireless technology is to reduce capital expenditures, optimize network performance, and build new revenue streams. Replacing traditional algorithms with deep learning AI techniques have dramatically reduced the power consumption and improved the system performance. Further, implementation of ML algorithms also enables the wireless network service providers to (i) offer high automation levels from distributed AI/ML architectures applicable at the network edge, (ii) implement application-based traffic steering across the access networks, (iii) enable dynamic network slicing for addressing different scenarios with varying quality of service requirements, and (iv) enable ubiquitous connectivity across the various 6G communication platforms. In this chapter, we review/survey the ML techniques which are applicable to the 6G wireless networks. and also list the open problems of research which require timely solutions. △ Less","16 March, 2022",https://arxiv.org/pdf/2203.08429
Reducing Flipping Errors in Deep Neural Networks,Xiang Deng;Yun Xiao;Bo Long;Zhongfei Zhang,"Deep neural networks (DNNs) have been widely applied in various domains in artificial intelligence including computer vision and natural language processing. A DNN is typically trained for many epochs and then a validation dataset is used to select the DNN in an epoch (we simply call this epoch ""the last epoch"") as the final model for making predictions on unseen samples, while it usually cannot achieve a perfect accuracy on unseen samples. An interesting question is ""how many test (unseen) samples that a DNN misclassifies in the last epoch were ever correctly classified by the DNN before the last epoch?"". In this paper, we empirically study this question and find on several benchmark datasets that the vast majority of the misclassified samples in the last epoch were ever classified correctly before the last epoch, which means that the predictions for these samples were flipped from ""correct"" to ""wrong"". Motivated by this observation, we propose to restrict the behavior changes of a DNN on the correctly-classified samples so that the correct local boundaries can be maintained and the flipping error on unseen samples can be largely reduced. Extensive experiments on different benchmark datasets with different modern network architectures demonstrate that the proposed flipping error reduction (FER) approach can substantially improve the generalization, the robustness, and the transferability of DNNs without introducing any additional network parameters or inference cost, only with a negligible training overhead. △ Less","16 March, 2022",https://arxiv.org/pdf/2203.08390
Beyond Explaining: Opportunities and Challenges of XAI-Based Model Improvement,Leander Weber;Sebastian Lapuschkin;Alexander Binder;Wojciech Samek,"Explainable Artificial Intelligence (XAI) is an emerging research field bringing transparency to highly complex and opaque machine learning (ML) models. Despite the development of a multitude of methods to explain the decisions of black-box classifiers in recent years, these tools are seldomly used beyond visualization purposes. Only recently, researchers have started to employ explanations in practice to actually improve models. This paper offers a comprehensive overview over techniques that apply XAI practically for improving various properties of ML models, and systematically categorizes these approaches, comparing their respective strengths and weaknesses. We provide a theoretical perspective on these methods, and show empirically through experiments on toy and realistic settings how explanations can help improve properties such as model generalization ability or reasoning, among others. We further discuss potential caveats and drawbacks of these methods. We conclude that while model improvement based on XAI can have significant beneficial effects even on complex and not easily quantifyable model properties, these methods need to be applied carefully, since their success can vary depending on a multitude of factors, such as the model and dataset used, or the employed explanation method. △ Less","15 March, 2022",https://arxiv.org/pdf/2203.08008
Data Smells in Public Datasets,Arumoy Shome;Luis Cruz;Arie van Deursen,"The adoption of Artificial Intelligence (AI) in high-stakes domains such as healthcare, wildlife preservation, autonomous driving and criminal justice system calls for a data-centric approach to AI. Data scientists spend the majority of their time studying and wrangling the data, yet tools to aid them with data analysis are lacking. This study identifies the recurrent data quality issues in public datasets. Analogous to code smells, we introduce a novel catalogue of data smells that can be used to indicate early signs of problems or technical debt in machine learning systems. To understand the prevalence of data quality issues in datasets, we analyse 25 public datasets and identify 14 data smells. △ Less","25 March, 2022",https://arxiv.org/pdf/2203.08007
MSCET: A Multi-Scenario Offloading Schedule for Biomedical Data Processing and Analysis in Cloud-Edge-Terminal Collaborative Vehicular Networks,Zhichen Ni;Honglong Chen;Zhe Li;Xiaomeng Wang;Na Yan;Weifeng Liu;Feng Xia,"With the rapid development of Artificial Intelligence (AI) and Internet of Things (IoTs), an increasing number of computation intensive or delay sensitive biomedical data processing and analysis tasks are produced in vehicles, bringing more and more challenges to the biometric monitoring of drivers. Edge computing is a new paradigm to solve these challenges by offloading tasks from the resource-limited vehicles to Edge Servers (ESs) in Road Side Units (RSUs). However, most of the traditional offloading schedules for vehicular networks concentrate on the edge, while some tasks may be too complex for ESs to process. To this end, we consider a collaborative vehicular network in which the cloud, edge and terminal can cooperate with each other to accomplish the tasks. The vehicles can offload the computation intensive tasks to the cloud to save the resource of edge. We further construct the virtual resource pool which can integrate the resource of multiple ESs since some regions may be covered by multiple RSUs. In this paper, we propose a Multi-Scenario offloading schedule for biomedical data processing and analysis in Cloud-Edge-Terminal collaborative vehicular networks called MSCET. The parameters of the proposed MSCET are optimized to maximize the system utility. We also conduct extensive simulations to evaluate the proposed MSCET and the results illustrate that MSCET outperforms other existing schedules. △ Less","16 February, 2022",https://arxiv.org/pdf/2203.07999
MOBDrone: a Drone Video Dataset for Man OverBoard Rescue,Donato Cafarelli;Luca Ciampi;Lucia Vadicamo;Claudio Gennaro;Andrea Berton;Marco Paterni;Chiara Benvenuti;Mirko Passera;Fabrizio Falchi,"Modern Unmanned Aerial Vehicles (UAV) equipped with cameras can play an essential role in speeding up the identification and rescue of people who have fallen overboard, i.e., man overboard (MOB). To this end, Artificial Intelligence techniques can be leveraged for the automatic understanding of visual data acquired from drones. However, detecting people at sea in aerial imagery is challenging primarily due to the lack of specialized annotated datasets for training and testing detectors for this task. To fill this gap, we introduce and publicly release the MOBDrone benchmark, a collection of more than 125K drone-view images in a marine environment under several conditions, such as different altitudes, camera shooting angles, and illumination. We manually annotated more than 180K objects, of which about 113K man overboard, precisely localizing them with bounding boxes. Moreover, we conduct a thorough performance analysis of several state-of-the-art object detectors on the MOBDrone data, serving as baselines for further research. △ Less","15 March, 2022",https://arxiv.org/pdf/2203.07973
SmartValidator: A Framework for Automatic Identification and Classification of Cyber Threat Data,Chadni Islam;M. Ali Babar;Roland Croft;Helge Janicke,"A wide variety of Cyber Threat Information (CTI) is used by Security Operation Centres (SOCs) to perform validation of security incidents and alerts. Security experts manually define different types of rules and scripts based on CTI to perform validation tasks. These rules and scripts need to be updated continuously due to evolving threats, changing SOCs' requirements and dynamic nature of CTI. The manual process of updating rules and scripts delays the response to attacks. To reduce the burden of human experts and accelerate response, we propose a novel Artificial Intelligence (AI) based framework, SmartValidator. SmartValidator leverages Machine Learning (ML) techniques to enable automated validation of alerts. It consists of three layers to perform the tasks of data collection, model building and alert validation. It projects the validation task as a classification problem. Instead of building and saving models for all possible requirements, we propose to automatically construct the validation models based on SOC's requirements and CTI. We built a Proof of Concept (PoC) system with eight ML algorithms, two feature engineering techniques and 18 requirements to investigate the effectiveness and efficiency of SmartValidator. The evaluation results showed that when prediction models were built automatically for classifying cyber threat data, the F1-score of 75\% of the models were above 0.8, which indicates adequate performance of the PoC for use in a real-world organization. The results further showed that dynamic construction of prediction models required 99\% less models to be built than pre-building models for all possible requirements. The framework can be followed by various industries to accelerate and automate the validation of alerts and incidents based on their CTI and SOC's preferences. △ Less","14 March, 2022",https://arxiv.org/pdf/2203.07603
Unsupervised Clustering of Roman Potsherds via Variational Autoencoders,Simone Parisotto;Ninetta Leone;Carola-Bibiane Schönlieb;Alessandro Launaro,"In this paper we propose an artificial intelligence imaging solution to support archaeologists in the classification task of Roman commonware potsherds. Usually, each potsherd is represented by its sectional profile as a two dimensional black-white image and printed in archaeological books related to specific archaeological excavations. The partiality and handcrafted variance of the fragments make their matching a challenging problem: we propose to pair similar profiles via the unsupervised hierarchical clustering of non-linear features learned in the latent space of a deep convolutional Variational Autoencoder (VAE) network. Our contribution also include the creation of a ROman COmmonware POTtery (ROCOPOT) database, with more than 4000 potsherds profiles extracted from 25 Roman pottery corpora, and a MATLAB GUI software for the easy inspection of shape similarities. Results are commented both from a mathematical and archaeological perspective so as to unlock new research directions in both communities. △ Less","14 March, 2022",https://arxiv.org/pdf/2203.07437
Physico-chemical properties extraction from the fluorescence spectrum with 1D-convolutional neural networks: application to olive oil,Francesca Venturini;Michela Sperti;Umberto Michelucci;Arnaud Gucciardi;Vanessa M. Martose;Marco A. Deriu,"The olive oil sector produces a substantial impact in the Mediterranean's economy and lifestyle. Many studies exist which try to optimize the different steps in the olive oil's production process. One of the main challenges for olive oil producers is the ability to asses and control the quality during the production cycle. For this purpose, several parameters need to be determined, such as the acidity, the UV absorption or the ethyl esters content. To achieve this, samples must be sent to an approved laboratory for chemical analysis. This approach is expensive and cannot be performed very frequently, making quality control of olive oil a real challenge. This work explores a new approach based on fluorescence spectroscopy and artificial intelligence (namely, 1-D convolutional neural networks) to predict the five chemical quality indicators of olive oil (acidity, peroxide value, UV spectroscopic parameters K_{270} and K_{232}, and ethyl esters) from simple fluorescence spectra. Fluorescence spectroscopy is a very attractive optical technique since it does not require sample preparation, is non destructive, and, as shown in this work, can be easily implemented in small and cost-effective sensors. The results indicate that the proposed approach gives exceptional results in the quality determination and would make the continuous quality control of olive oil during and after the production process a reality. Additionally, this novel methodology presents potential applications as a support for quality specifications of olive oil, as defined by the European regulation. △ Less","9 April, 2022",https://arxiv.org/pdf/2203.07229
Toward Ethical AIED,Kaska Porayska-Pomsta;Wayne Holmes,"This paper presents the key conclusions to the forthcoming edited book on The Ethics of Artificial Intelligence in Education: Practices, Challenges and Debates (August 2022, Routlege). As well as highlighting the key contributions to the book, it discusses the key questions and the grand challenges for the field of AI in Education (AIED)in the context of ethics and ethical practices within the field. The book itself presents diverse perspectives from outside and from within the AIED as a way of achieving a broad perspective in the key ethical issues for AIED and a deep understanding of work conducted to date by the AIED community. △ Less","11 March, 2022",https://arxiv.org/pdf/2203.07067
Fairness Evaluation in Deepfake Detection Models using Metamorphic Testing,Muxin Pu;Meng Yi Kuan;Nyee Thoang Lim;Chun Yong Chong;Mei Kuan Lim,"Fairness of deepfake detectors in the presence of anomalies are not well investigated, especially if those anomalies are more prominent in either male or female subjects. The primary motivation for this work is to evaluate how deepfake detection model behaves under such anomalies. However, due to the black-box nature of deep learning (DL) and artificial intelligence (AI) systems, it is hard to predict the performance of a model when the input data is modified. Crucially, if this defect is not addressed properly, it will adversely affect the fairness of the model and result in discrimination of certain sub-population unintentionally. Therefore, the objective of this work is to adopt metamorphic testing to examine the reliability of the selected deepfake detection model, and how the transformation of input variation places influence on the output. We have chosen MesoInception-4, a state-of-the-art deepfake detection model, as the target model and makeup as the anomalies. Makeups are applied through utilizing the Dlib library to obtain the 68 facial landmarks prior to filling in the RGB values. Metamorphic relations are derived based on the notion that realistic perturbations of the input images, such as makeup, involving eyeliners, eyeshadows, blushes, and lipsticks (which are common cosmetic appearance) applied to male and female images, should not alter the output of the model by a huge margin. Furthermore, we narrow down the scope to focus on revealing potential gender biases in DL and AI systems. Specifically, we are interested to examine whether MesoInception-4 model produces unfair decisions, which should be considered as a consequence of robustness issues. The findings from our work have the potential to pave the way for new research directions in the quality assurance and fairness in DL and AI systems. △ Less","13 March, 2022",https://arxiv.org/pdf/2203.06825
Towards On-Device AI and Blockchain for 6G enabled Agricultural Supply-chain Management,Muhammad Zawish;Nouman Ashraf;Rafay Iqbal Ansari;Steven Davy;Hassan Khaliq Qureshi;Nauman Aslam;Syed Ali Hassan,"6G envisions artificial intelligence (AI) powered solutions for enhancing the quality-of-service (QoS) in the network and to ensure optimal utilization of resources. In this work, we propose an architecture based on the combination of unmanned aerial vehicles (UAVs), AI and blockchain for agricultural supply-chain management with the purpose of ensuring traceability, transparency, tracking inventories and contracts. We propose a solution to facilitate on-device AI by generating a roadmap of models with various resource-accuracy trade-offs. A fully convolutional neural network (FCN) model is used for biomass estimation through images captured by the UAV. Instead of a single compressed FCN model for deployment on UAV, we motivate the idea of iterative pruning to provide multiple task-specific models with various complexities and accuracy. To alleviate the impact of flight failure in a 6G enabled dynamic UAV network, the proposed model selection strategy will assist UAVs to update the model based on the runtime resource requirements. △ Less","12 March, 2022",https://arxiv.org/pdf/2203.06465
Predatory Medicine: Exploring and Measuring the Vulnerability of Medical AI to Predatory Science,Shalini Saini;Nitesh Saxena,"Medical Artificial Intelligence (MedAI) for diagnosis, treatment options, and drug development represents the new age of healthcare. The security, integrity, and credibility of MedAI tools are paramount issues because human lives are at stake. MedAI solutions are often heavily dependent on scientific medical research literature as a primary data source that draws the attacker's attention as a potential target. We present a first study of how the output of MedAI can be polluted with Predatory Publications Presence (PPP). We study two MedAI systems: mediKanren (disease independent) and CancerMine (Disease-specific), which use research literature as primary data input from the research repository PubMed, PubMed derived database SemMedDB, and NIH translational Knowledge Graphs (KGs). Our study has a three-pronged focus: (1) identifying the PPP in PubMed; (2) verifying the PPP in SemMedDB and the KGs; (3) demonstrating the existing vulnerability of PPP traversing to the MedAI output. Our contribution lies in identifying the existing PPP in the MedAI inputs and demonstrating how predatory science can jeopardize the credibility of MedAI solutions, making their real-life deployment questionable. △ Less","16 March, 2022",https://arxiv.org/pdf/2203.06245
Human-Like Navigation Behavior: A Statistical Evaluation Framework,Ian Colbert;Mehdi Saeedi,"Recent advancements in deep reinforcement learning have brought forth an impressive display of highly skilled artificial agents capable of complex intelligent behavior. In video games, these artificial agents are increasingly deployed as non-playable characters (NPCs) designed to enhance the experience of human players. However, while it has been shown that the convincing human-like behavior of NPCs leads to increased engagement in video games, the believability of an artificial agent's behavior is most often measured solely by its proficiency at a given task. Recent work has hinted that proficiency alone is not sufficient to discern human-like behavior. Motivated by this, we build a non-parametric two-sample hypothesis test designed to compare the behaviors of artificial agents to those of human players. We show that the resulting p-value not only aligns with anonymous human judgment of human-like behavior, but also that it can be used as a measure of similarity. △ Less","9 March, 2022",https://arxiv.org/pdf/2203.05965
Deep Binary Reinforcement Learning for Scalable Verification,Christopher Lazarus;Mykel J. Kochenderfer,"The use of neural networks as function approximators has enabled many advances in reinforcement learning (RL). The generalization power of neural networks combined with advances in RL algorithms has reignited the field of artificial intelligence. Despite their power, neural networks are considered black boxes, and their use in safety-critical settings remains a challenge. Recently, neural network verification has emerged as a way to certify safety properties of networks. Verification is a hard problem, and it is difficult to scale to large networks such as the ones used in deep reinforcement learning. We provide an approach to train RL policies that are more easily verifiable. We use binarized neural networks (BNNs), a type of network with mostly binary parameters. We present an RL algorithm tailored specifically for BNNs. After training BNNs for the Atari environments, we verify robustness properties. △ Less","10 March, 2022",https://arxiv.org/pdf/2203.05704
Deep Learning-Based Perceptual Stimulus Encoder for Bionic Vision,Lucas Relic;Bowen Zhang;Yi-Lin Tuan;Michael Beyeler,"Retinal implants have the potential to treat incurable blindness, yet the quality of the artificial vision they produce is still rudimentary. An outstanding challenge is identifying electrode activation patterns that lead to intelligible visual percepts (phosphenes). Here we propose a PSE based on CNN that is trained in an end-to-end fashion to predict the electrode activation patterns required to produce a desired visual percept. We demonstrate the effectiveness of the encoder on MNIST using a psychophysically validated phosphene model tailored to individual retinal implant users. The present work constitutes an essential first step towards improving the quality of the artificial vision provided by retinal implants. △ Less","10 March, 2022",https://arxiv.org/pdf/2203.05604
Artificial Intelligence Solution for Effective Treatment Planning for Glioblastoma Patients,Vikram Goddla,"Glioblastomas are the most common malignant brain tumors in adults. Approximately 200000 people die each year from Glioblastoma in the world. Glioblastoma patients have a median survival of 12 months with optimal therapy and about 4 months without treatment. Glioblastomas appear as heterogeneous necrotic masses with irregular peripheral enhancement, surrounded by vasogenic edema. The current standard of care includes surgical resection, radiotherapy and chemotherapy, which require accurate segmentation of brain tumor subregions. For effective treatment planning, it is vital to identify the methylation status of the promoter of Methylguanine Methyltransferase (MGMT), a positive prognostic factor for chemotherapy. However, current methods for brain tumor segmentation are tedious, subjective and not scalable, and current techniques to determine the methylation status of MGMT promoter involve surgically invasive procedures, which are expensive and time consuming. Hence there is a pressing need to develop automated tools to segment brain tumors and non-invasive methods to predict methylation status of MGMT promoter, to facilitate better treatment planning and improve survival rate. I created an integrated diagnostics solution powered by Artificial Intelligence to automatically segment brain tumor subregions and predict MGMT promoter methylation status, using brain MRI scans. My AI solution is proven on large datasets with performance exceeding current standards and field tested with data from teaching files of local neuroradiologists. With my solution, physicians can submit brain MRI images, and get segmentation and methylation predictions in minutes, and guide brain tumor patients with effective treatment planning and ultimately improve survival time. △ Less","9 March, 2022",https://arxiv.org/pdf/2203.05563
"A Full Dive into Realizing the Edge-enabled Metaverse: Visions, Enabling Technologies,and Challenges",Minrui Xu;Wei Chong Ng;Wei Yang Bryan Lim;Jiawen Kang;Zehui Xiong;Dusit Niyato;Qiang Yang;Xuemin Sherman Shen;Chunyan Miao,"Dubbed ""the successor to the mobile Internet"", the concept of the Metaverse has grown in popularity. While there exist lite versions of the Metaverse today, they are still far from realizing the full vision of an immersive, embodied, and interoperable Metaverse. Without addressing the issues of implementation from the communication and networking, as well as computation perspectives, the Metaverse is difficult to succeed the Internet, especially in terms of its accessibility to billions of users today. In this survey, we focus on the edge-enabled Metaverse to realize its ultimate vision. We first provide readers with a succinct tutorial of the Metaverse, an introduction to the architecture, as well as current developments. To enable ubiquitous, seamless, and embodied access to the Metaverse, we discuss the communication and networking challenges and survey cutting-edge solutions and concepts that leverage next-generation communication systems for users to immerse as and interact with embodied avatars in the Metaverse. Moreover, given the high computation costs required, e.g., to render 3D virtual worlds and run data-hungry artificial intelligence-driven avatars, we discuss the computation challenges and cloud-edge-end computation framework-driven solutions to realize the Metaverse on resource-constrained edge devices. Next, we explore how blockchain technologies can aid in the interoperable development of the Metaverse, not just in terms of empowering the economic circulation of virtual user-generated content but also to manage physical edge resources in a decentralized, transparent, and immutable manner. Finally, we discuss the future research directions towards realizing the true vision of the edge-enabled Metaverse. △ Less","20 August, 2022",https://arxiv.org/pdf/2203.05471
Artificial Intelligence in Vehicular Wireless Networks: A Case Study Using ns-3,Matteo Drago;Tommaso Zugno;Federico Mason;Marco Giordani;Mate Boban;Michele Zorzi,"Artificial intelligence (AI) techniques have emerged as a powerful approach to make wireless networks more efficient and adaptable. In this paper we present an ns-3 simulation framework, able to implement AI algorithms for the optimization of wireless networks. Our pipeline consists of: (i) a new geometry-based mobility-dependent channel model for V2X; (ii) all the layers of a 5G-NR-compliant protocol stack, based on the ns3-mmwave module; (iii) a new application to simulate V2X data transmission, and (iv) a new intelligent entity for the control of the network via AI. Thanks to its flexible and modular design, researchers can use this tool to implement, train, and evaluate their own algorithms in a realistic and controlled environment. We test the behavior of our framework in a Predictive Quality of Service (PQoS) scenario, where AI functionalities are implemented using Reinforcement Learning (RL), and demonstrate that it promotes better network optimization compared to baseline solutions that do not implement AI. △ Less","10 March, 2022",https://arxiv.org/pdf/2203.05449
A Systematic Literature Review on Blockchain Enabled Federated Learning Framework for Internet of Vehicles,Mustain Billah;Sk. Tanzir Mehedi;Adnan Anwar;Ziaur Rahman;Rafiqul Islam,"While the convergence of Artificial Intelligence (AI) techniques with improved information technology systems ensured enormous benefits to the Internet of Vehicles (IoVs) systems, it also introduced an increased amount of security and privacy threats. To ensure the security of IoVs data, privacy preservation methodologies have gained significant attention in the literature. However, these strategies also need specific adjustments and modifications to cope with the advances in IoVs design. In the interim, Federated Learning (FL) has been proven as an emerging idea to protect IoVs data privacy and security. On the other hand, Blockchain technology is showing prominent possibilities with secured, dispersed, and auditable data recording and sharing schemes. In this paper, we present a comprehensive survey on the application and implementation of Blockchain-Enabled Federated Learning frameworks for IoVs. Besides, probable issues, challenges, solutions, and future research directions for BC-Enabled FL frameworks for IoVs are also presented. This survey can further be used as the basis for developing modern BC-Enabled FL solutions to resolve different data privacy issues and scenarios of IoVs. △ Less","10 March, 2022",https://arxiv.org/pdf/2203.05192
Evaluating Proposed Fairness Models for Face Recognition Algorithms,John J. Howard;Eli J. Laird;Yevgeniy B. Sirotin;Rebecca E. Rubin;Jerry L. Tipton;Arun R. Vemury,"The development of face recognition algorithms by academic and commercial organizations is growing rapidly due to the onset of deep learning and the widespread availability of training data. Though tests of face recognition algorithm performance indicate yearly performance gains, error rates for many of these systems differ based on the demographic composition of the test set. These ""demographic differentials"" in algorithm performance can contribute to unequal or unfair outcomes for certain groups of people, raising concerns with increased worldwide adoption of face recognition systems. Consequently, regulatory bodies in both the United States and Europe have proposed new rules requiring audits of biometric systems for ""discriminatory impacts"" (European Union Artificial Intelligence Act) and ""fairness"" (U.S. Federal Trade Commission). However, no standard for measuring fairness in biometric systems yet exists. This paper characterizes two proposed measures of face recognition algorithm fairness (fairness measures) from scientists in the U.S. and Europe. We find that both proposed methods are challenging to interpret when applied to disaggregated face recognition error rates as they are commonly experienced in practice. To address this, we propose a set of interpretability criteria, termed the Functional Fairness Measure Criteria (FFMC), that outlines a set of properties desirable in a face recognition algorithm fairness measure. We further develop a new fairness measure, the Gini Aggregation Rate for Biometric Equitability (GARBE), and show how, in conjunction with the Pareto optimization, this measure can be used to select among alternative algorithms based on the accuracy/fairness trade-space. Finally, we have open-sourced our dataset of machine-readable, demographically disaggregated error rates. We believe this is currently the largest open-source dataset of its kind. △ Less","9 March, 2022",https://arxiv.org/pdf/2203.05051
Explainable Machine Learning for Predicting Homicide Clearance in the United States,Gian Maria Campedelli,"Purpose: To explore the potential of Explainable Machine Learning in the prediction and detection of drivers of cleared homicides at the national- and state-levels in the United States. Methods: First, nine algorithmic approaches are compared to assess the best performance in predicting cleared homicides country-wise, using data from the Murder Accountability Project. The most accurate algorithm among all (XGBoost) is then used for predicting clearance outcomes state-wise. Second, SHAP, a framework for Explainable Artificial Intelligence, is employed to capture the most important features in explaining clearance patterns both at the national and state levels. Results: At the national level, XGBoost demonstrates to achieve the best performance overall. Substantial predictive variability is detected state-wise. In terms of explainability, SHAP highlights the relevance of several features in consistently predicting investigation outcomes. These include homicide circumstances, weapons, victims' sex and race, as well as number of involved offenders and victims. Conclusions: Explainable Machine Learning demonstrates to be a helpful framework for predicting homicide clearance. SHAP outcomes suggest a more organic integration of the two theoretical perspectives emerged in the literature. Furthermore, jurisdictional heterogeneity highlights the importance of developing ad hoc state-level strategies to improve police performance in clearing homicides. △ Less","9 March, 2022",https://arxiv.org/pdf/2203.04768
System Cards for AI-Based Decision-Making for Public Policy,Furkan Gursoy;Ioannis A. Kakadiaris,"Decisions impacting human lives are increasingly being made or assisted by automated decision-making algorithms. Many of these algorithms process personal data for predicting recidivism, credit risk analysis, identifying individuals using face recognition, and more. While potentially improving efficiency and effectiveness, such algorithms are not inherently free from bias, opaqueness, lack of explainability, maleficence, and the like. Given that the outcomes of these algorithms have a significant impact on individuals and society and are open to analysis and contestation after deployment, such issues must be accounted for before deployment. Formal audits are a way of ensuring algorithms meet the appropriate accountability standards. This work, based on an extensive analysis of the literature and an expert focus group study, proposes a unifying framework for a system accountability benchmark for formal audits of artificial intelligence-based decision-aiding systems. This work also proposes system cards to serve as scorecards presenting the outcomes of such audits. It consists of 56 criteria organized within a four-by-four matrix composed of rows focused on (i) data, (ii) model, (iii) code, (iv) system, and columns focused on (a) development, (b) assessment, (c) mitigation, and (d) assurance. The proposed system accountability benchmark reflects the state-of-the-art developments for accountable systems, serves as a checklist for algorithm audits, and paves the way for sequential work in future research. △ Less","31 August, 2022",https://arxiv.org/pdf/2203.04754
Mapping global dynamics of benchmark creation and saturation in artificial intelligence,Simon Ott;Adriano Barbosa-Silva;Kathrin Blagec;Jan Brauner;Matthias Samwald,"Benchmarks are crucial to measuring and steering progress in artificial intelligence (AI). However, recent studies raised concerns over the state of AI benchmarking, reporting issues such as benchmark overfitting, benchmark saturation and increasing centralization of benchmark dataset creation. To facilitate monitoring of the health of the AI benchmarking ecosystem, we introduce methodologies for creating condensed maps of the global dynamics of benchmark creation and saturation. We curated data for 3765 benchmarks covering the entire domains of computer vision and natural language processing, and show that a large fraction of benchmarks quickly trended towards near-saturation, that many benchmarks fail to find widespread utilization, and that benchmark performance gains for different AI tasks were prone to unforeseen bursts. We analyze attributes associated with benchmark popularity, and conclude that future benchmarks should emphasize versatility, breadth and real-world utility. △ Less","7 October, 2022",https://arxiv.org/pdf/2203.04592
Design of Detectors at the Electron Ion Collider with Artificial Intelligence,Cristiano Fanelli,"Artificial Intelligence (AI) for design is a relatively new but active area of research across many disciplines. Surprisingly when it comes to designing detectors with AI this is an area at its infancy. The Electron Ion Collider is the ultimate machine to study the strong force. The EIC is a large-scale experiment with an integrated detector that extends for about \pm35 meters to include the central, far-forward, and far-backward regions. The design of the central detector is made by multiple sub-detectors, each in principle characterized by a multidimensional design space and multiple design criteria also called objectives. Simulations with Geant4 are typically compute intensive, and the optimization of the detector design may include non-differentiable terms as well as noisy objectives. In this context, AI can offer state of the art solutions to solve complex combinatorial problems in an efficient way. In particular, one of the proto-collaborations, ECCE, has explored during the detector proposal the possibility of using multi-objective optimization to design the tracking system of the EIC detector. This document provides an overview of these techniques and recent progress made during the EIC detector proposal. Future high energy nuclear physics experiments can leverage AI-based strategies to design more efficient detectors by optimizing their performance driven by physics criteria and minimizing costs for their realization. △ Less","14 March, 2022",https://arxiv.org/pdf/2203.04530
Update Compression for Deep Neural Networks on the Edge,Bo Chen;Ali Bakhshi;Gustavo Batista;Brian Ng;Tat-Jun Chin,"An increasing number of artificial intelligence (AI) applications involve the execution of deep neural networks (DNNs) on edge devices. Many practical reasons motivate the need to update the DNN model on the edge device post-deployment, such as refining the model, concept drift, or outright change in the learning task. In this paper, we consider the scenario where retraining can be done on the server side based on a copy of the DNN model, with only the necessary data transmitted to the edge to update the deployed model. However, due to bandwidth constraints, we want to minimise the transmission required to achieve the update. We develop a simple approach based on matrix factorisation to compress the model update -- this differs from compressing the model itself. The key idea is to preserve existing knowledge in the current model and optimise only small additional parameters for the update which can be used to reconstitute the model on the edge. We compared our method to similar techniques used in federated learning; our method usually requires less than half of the update size of existing methods to achieve the same accuracy. △ Less","21 April, 2022",https://arxiv.org/pdf/2203.04516
Part-level Action Parsing via a Pose-guided Coarse-to-Fine Framework,Xiaodong Chen;Xinchen Liu;Wu Liu;Kun Liu;Dong Wu;Yongdong Zhang;Tao Mei,"Action recognition from videos, i.e., classifying a video into one of the pre-defined action types, has been a popular topic in the communities of artificial intelligence, multimedia, and signal processing. However, existing methods usually consider an input video as a whole and learn models, e.g., Convolutional Neural Networks (CNNs), with coarse video-level class labels. These methods can only output an action class for the video, but cannot provide fine-grained and explainable cues to answer why the video shows a specific action. Therefore, researchers start to focus on a new task, Part-level Action Parsing (PAP), which aims to not only predict the video-level action but also recognize the frame-level fine-grained actions or interactions of body parts for each person in the video. To this end, we propose a coarse-to-fine framework for this challenging task. In particular, our framework first predicts the video-level class of the input video, then localizes the body parts and predicts the part-level action. Moreover, to balance the accuracy and computation in part-level action parsing, we propose to recognize the part-level actions by segment-level features. Furthermore, to overcome the ambiguity of body parts, we propose a pose-guided positional embedding method to accurately localize body parts. Through comprehensive experiments on a large-scale dataset, i.e., Kinetics-TPS, our framework achieves state-of-the-art performance and outperforms existing methods over a 31.10% ROC score. △ Less","1 September, 2022",https://arxiv.org/pdf/2203.04476
Estimating the Uncertainty in Emotion Class Labels with Utterance-Specific Dirichlet Priors,Wen Wu;Chao Zhang;Xixin Wu;Philip C. Woodland,"Emotion recognition is a key attribute for artificial intelligence systems that need to naturally interact with humans. However, the task definition is still an open problem due to the inherent ambiguity of emotions. In this paper, a novel Bayesian training loss based on per-utterance Dirichlet prior distributions is proposed for verbal emotion recognition, which models the uncertainty in one-hot labels created when human annotators assign the same utterance to different emotion classes. An additional metric is used to evaluate the performance by detection test utterances with high labelling uncertainty. This removes a major limitation that emotion classification systems only consider utterances with labels where the majority of annotators agree on the emotion class. Furthermore, a frequentist approach is studied to leverage the continuous-valued ""soft"" labels obtained by averaging the one-hot labels. We propose a two-branch model structure for emotion classification on a per-utterance basis, which achieves state-of-the-art classification results on the widely used IEMOCAP dataset. Based on this, uncertainty estimation experiments were performed. The best performance in terms of the area under the precision-recall curve when detecting utterances with high uncertainty was achieved by interpolating the Bayesian training loss with the Kullback-Leibler divergence training loss for the soft labels. The generality of the proposed approach was verified using the MSP-Podcast dataset which yielded the same pattern of results. △ Less","17 November, 2022",https://arxiv.org/pdf/2203.04443
OpenGridGym: An Open-Source AI-Friendly Toolkit for Distribution Market Simulation,Rayan El Helou;Kiyeob Lee;Dongqi Wu;Le Xie;Srinivas Shakkottai;Vijay Subramanian,"This paper presents OpenGridGym, an open-source Python-based package that allows for seamless integration of distribution market simulation with state-of-the-art artificial intelligence (AI) decision-making algorithms. We present the architecture and design choice for the proposed framework, elaborate on how users interact with OpenGridGym, and highlight its value by providing multiple cases to demonstrate its use. Four modules are used in any simulation: (1) the physical grid, (2) market mechanisms, (3) a set of trainable agents which interact with the former two modules, and (4) environment module that connects and coordinates the above three. We provide templates for each of those four, but they are easily interchangeable with custom alternatives. Several case studies are presented to illustrate the capability and potential of this toolkit in helping researchers address key design and operational questions in distribution electricity markets. △ Less","6 March, 2022",https://arxiv.org/pdf/2203.04410
Breast cancer detection using artificial intelligence techniques: A systematic literature review,Ali Bou Nassif;Manar Abu Talib;Qassim Nasir;Yaman Afadar;Omar Elgendy,"Cancer is one of the most dangerous diseases to humans, and yet no permanent cure has been developed for it. Breast cancer is one of the most common cancer types. According to the National Breast Cancer foundation, in 2020 alone, more than 276,000 new cases of invasive breast cancer and more than 48,000 non-invasive cases were diagnosed in the US. To put these figures in perspective, 64% of these cases are diagnosed early in the disease's cycle, giving patients a 99% chance of survival. Artificial intelligence and machine learning have been used effectively in detection and treatment of several dangerous diseases, helping in early diagnosis and treatment, and thus increasing the patient's chance of survival. Deep learning has been designed to analyze the most important features affecting detection and treatment of serious diseases. For example, breast cancer can be detected using genes or histopathological imaging. Analysis at the genetic level is very expensive, so histopathological imaging is the most common approach used to detect breast cancer. In this research work, we systematically reviewed previous work done on detection and treatment of breast cancer using genetic sequencing or histopathological imaging with the help of deep learning and machine learning. We also provide recommendations to researchers who will work in this field △ Less","8 March, 2022",https://arxiv.org/pdf/2203.04308
AI for Next Generation Computing: Emerging Trends and Future Directions,Sukhpal Singh Gill;Minxian Xu;Carlo Ottaviani;Panos Patros;Rami Bahsoon;Arash Shaghaghi;Muhammed Golec;Vlado Stankovski;Huaming Wu;Ajith Abraham;Manmeet Singh;Harshit Mehta;Soumya K. Ghosh;Thar Baker;Ajith Kumar Parlikad;Hanan Lutfiyya;Salil S. Kanhere;Rizos Sakellariou;Schahram Dustdar;Omer Rana;Ivona Brandic;Steve Uhlig,"Autonomic computing investigates how systems can achieve (user) specified control outcomes on their own, without the intervention of a human operator. Autonomic computing fundamentals have been substantially influenced by those of control theory for closed and open-loop systems. In practice, complex systems may exhibit a number of concurrent and inter-dependent control loops. Despite research into autonomic models for managing computer resources, ranging from individual resources (e.g., web servers) to a resource ensemble (e.g., multiple resources within a data center), research into integrating Artificial Intelligence (AI) and Machine Learning (ML) to improve resource autonomy and performance at scale continues to be a fundamental challenge. The integration of AI/ML to achieve such autonomic and self-management of systems can be achieved at different levels of granularity, from full to human-in-the-loop automation. In this article, leading academics, researchers, practitioners, engineers, and scientists in the fields of cloud computing, AI/ML, and quantum computing join to discuss current research and potential future directions for these fields. Further, we discuss challenges and opportunities for leveraging AI and ML in next generation computing for emerging computing paradigms, including cloud, fog, edge, serverless and quantum computing environments. △ Less","5 March, 2022",https://arxiv.org/pdf/2203.04159
Trust in AI and Implications for the AEC Research: A Literature Analysis,Newsha Emaminejad;Alexa Maria North;Reza Akhavian,"Engendering trust in technically acceptable and psychologically embraceable systems requires domain-specific research to capture unique characteristics of the field of application. The architecture, engineering, and construction (AEC) research community has been recently harnessing advanced solutions offered by artificial intelligence (AI) to improve project workflows. Despite the unique characteristics of work, workers, and workplaces in the AEC industry, the concept of trust in AI has received very little attention in the literature. This paper presents a comprehensive analysis of the academic literature in two main areas of trust in AI and AI in the AEC, to explore the interplay between AEC projects unique aspects and the sociotechnical concepts that lead to trust in AI. A total of 490 peer-reviewed scholarly articles are analyzed in this study. The main constituents of human trust in AI are identified from the literature and are characterized within the AEC project types, processes, and technologies. △ Less","7 March, 2022",https://arxiv.org/pdf/2203.03847
A New Era: Intelligent Tutoring Systems Will Transform Online Learning for Millions,Francois St-Hilaire;Dung Do Vu;Antoine Frau;Nathan Burns;Farid Faraji;Joseph Potochny;Stephane Robert;Arnaud Roussel;Selene Zheng;Taylor Glazier;Junfel Vincent Romano;Robert Belfer;Muhammad Shayan;Ariella Smofsky;Tommy Delarosbil;Seulmin Ahn;Simon Eden-Walker;Kritika Sony;Ansona Onyi Ching;Sabina Elkins;Anush Stepanyan;Adela Matajova;Victor Chen;Hossein Sahraei;Robert Larson,"Despite artificial intelligence (AI) having transformed major aspects of our society, less than a fraction of its potential has been explored, let alone deployed, for education. AI-powered learning can provide millions of learners with a highly personalized, active and practical learning experience, which is key to successful learning. This is especially relevant in the context of online learning platforms. In this paper, we present the results of a comparative head-to-head study on learning outcomes for two popular online learning platforms (n=199 participants): A MOOC platform following a traditional model delivering content using lecture videos and multiple-choice quizzes, and the Korbit learning platform providing a highly personalized, active and practical learning experience. We observe a huge and statistically significant increase in the learning outcomes, with students on the Korbit platform providing full feedback resulting in higher course completion rates and achieving learning gains 2 to 2.5 times higher than both students on the MOOC platform and students in a control group who don't receive personalized feedback on the Korbit platform. The results demonstrate the tremendous impact that can be achieved with a personalized, active learning AI-powered system. Making this technology and learning experience available to millions of learners around the world will represent a significant leap forward towards the democratization of education. △ Less","3 March, 2022",https://arxiv.org/pdf/2203.03724
Needs and Artificial Intelligence,Soheil Human;Ryan Watkins,"Throughout their history, homo sapiens have used technologies to better satisfy their needs. The relation between needs and technology is so fundamental that the US National Research Council defined the distinguishing characteristic of technology as its goal ""to make modifications in the world to meet human needs"". Artificial intelligence (AI) is one of the most promising emerging technologies of our time. Similar to other technologies, AI is expected ""to meet [human] needs"". In this article, we reflect on the relationship between needs and AI, and call for the realisation of needs-aware AI systems. We argue that re-thinking needs for, through, and by AI can be a very useful means towards the development of realistic approaches for Sustainable, Human-centric, Accountable, Lawful, and Ethical (HALE) AI systems. We discuss some of the most critical gaps, barriers, enablers, and drivers of co-creating future AI-based socio-technical systems in which [human] needs are well considered and met. Finally, we provide an overview of potential threats and HALE considerations that should be carefully taken into account, and call for joint, immediate, and interdisciplinary efforts and collaborations. △ Less","18 February, 2022",https://arxiv.org/pdf/2203.03715
Trusted Data Forever: Is AI the Answer?,Emanuele Frontoni;Marina Paolanti;Tracey P. Lauriault;Michael Stiber;Luciana Duranti;Abdul-Mageed Muhammad,"Archival institutions and programs worldwide work to ensure that the records of governments, organizations, communities, and individuals are preserved for future generations as cultural heritage, as sources of rights, and as vehicles for holding the past accountable and to inform the future. This commitment is guaranteed through the adoption of strategic and technical measures for the long-term preservation of digital assets in any medium and form - textual, visual, or aural. Public and private archives are the largest providers of data big and small in the world and collectively host yottabytes of trusted data, to be preserved forever. Several aspects of retention and preservation, arrangement and description, management and administrations, and access and use are still open to improvement. In particular, recent advances in Artificial Intelligence (AI) open the discussion as to whether AI can support the ongoing availability and accessibility of trustworthy public records. This paper presents preliminary results of the InterPARES Trust AI (I Trust AI) international research partnership, which aims to (1) identify and develop specific AI technologies to address critical records and archives challenges; (2) determine the benefits and risks of employing AI technologies on records and archives; (3) ensure that archival concepts and principles inform the development of responsible AI; and (4) validate outcomes through a conglomerate of case studies and demonstrations. △ Less","14 March, 2022",https://arxiv.org/pdf/2203.03712
Clustering and classification of low-dimensional data in explicit feature map domain: intraoperative pixel-wise diagnosis of adenocarcinoma of a colon in a liver,Dario Sitnik;Ivica Kopriva,"Application of artificial intelligence in medicine brings in highly accurate predictions achieved by complex models, the reasoning of which is hard to interpret. Their generalization ability can be reduced because of the lack of pixel wise annotated images that occurs in frozen section tissue analysis. To partially overcome this gap, this paper explores the approximate explicit feature map (aEFM) transform of low-dimensional data into a low-dimensional subspace in Hilbert space. There, with a modest increase in computational complexity, linear algorithms yield improved performance and keep interpretability. They remain amenable to incremental learning that is not a trivial issue for some nonlinear algorithms. We demonstrate proposed methodology on a very large-scale problem related to intraoperative pixel-wise semantic segmentation and clustering of adenocarcinoma of a colon in a liver. Compared to the results in the input space, logistic classifier achieved statistically significant performance improvements in micro balanced accuracy and F1 score in the amounts of 12.04% and 12.58%, respectively. Support vector machine classifier yielded the increase of 8.04% and 9.41%. For clustering, increases of 0.79% and 0.85% are obtained with ultra large-scale spectral clustering algorithm. Results are supported by a discussion of interpretability using Shapely additive explanation values for predictions of linear classifier in input space and aEFM induced space. △ Less","7 March, 2022",https://arxiv.org/pdf/2203.03636
Mammograms Classification: A Review,Marawan Elbatel,"An advanced reliable low-cost form of screening method, Digital mammography has been used as an effective imaging method for breast cancer detection. With an increased focus on technologies to aid healthcare, Mammogram images have been utilized in developing computer-aided diagnosis systems that will potentially help in clinical diagnosis. Researchers have proved that artificial intelligence with its emerging technologies can be used in the early detection of the disease and improve radiologists' performance in assessing breast cancer. In this paper, we review the methods developed for mammogram mass classification in two categories. The first one is classifying manually provided cropped region of interests (ROI) as either malignant or benign, and the second one is the classification of automatically segmented ROIs as either malignant or benign. We also provide an overview of datasets and evaluation metrics used in the classification task. Finally, we compare and discuss the deep learning approach to classical image processing and learning approach in this domain. △ Less","4 March, 2022",https://arxiv.org/pdf/2203.03618
GatorTron: A Large Clinical Language Model to Unlock Patient Information from Unstructured Electronic Health Records,Xi Yang;Aokun Chen;Nima PourNejatian;Hoo Chang Shin;Kaleb E Smith;Christopher Parisien;Colin Compas;Cheryl Martin;Mona G Flores;Ying Zhang;Tanja Magoc;Christopher A Harle;Gloria Lipori;Duane A Mitchell;William R Hogan;Elizabeth A Shenkman;Jiang Bian;Yonghui Wu,"There is an increasing interest in developing artificial intelligence (AI) systems to process and interpret electronic health records (EHRs). Natural language processing (NLP) powered by pretrained language models is the key technology for medical AI systems utilizing clinical narratives. However, there are few clinical language models, the largest of which trained in the clinical domain is comparatively small at 110 million parameters (compared with billions of parameters in the general domain). It is not clear how large clinical language models with billions of parameters can help medical AI systems utilize unstructured EHRs. In this study, we develop from scratch a large clinical language model - GatorTron - using >90 billion words of text (including >82 billion words of de-identified clinical text) and systematically evaluate it on 5 clinical NLP tasks including clinical concept extraction, medical relation extraction, semantic textual similarity, natural language inference (NLI), and medical question answering (MQA). We examine how (1) scaling up the number of parameters and (2) scaling up the size of the training data could benefit these NLP tasks. GatorTron models scale up the clinical language model from 110 million to 8.9 billion parameters and improve 5 clinical NLP tasks (e.g., 9.6% and 9.5% improvement in accuracy for NLI and MQA), which can be applied to medical AI systems to improve healthcare delivery. The GatorTron models are publicly available at: https://catalog.ngc.nvidia.com/orgs/nvidia/teams/clara/models/gatortron_og. △ Less","16 December, 2022",https://arxiv.org/pdf/2203.03540
Machine Learning based Anomaly Detection for Smart Shirt: A Systematic Review,E. C. Nunes,"In recent years, the popularity and use of Artificial Intelligence (AI) and large investments on theInternet of Medical Things (IoMT) will be common to use products such as smart socks, smartpants, and smart shirts. These products are known as Smart Textile or E-textile, which has theability to monitor and collect signals that our body emits. These signals make it possible to extractanomalous components using Machine Learning (ML) techniques that play an essential role in thisarea. This study presents a Systematic Review of the Literature (SLR) on Anomaly Detection usingML techniques in Smart Shirt. The objectives of the SLR are: (i) to identify what type of anomalythe smart shirt; (ii) what ML techniques are being used; (iii) which datasets are being used; (iv)identify smart shirt or signal acquisition devices; (v) list the performance metrics used to evaluatethe ML model; (vi) the results of the techniques in general; (vii) types of ML algorithms are beingapplied.The SLR selected 11 primary studies published between 2017-2021. The results showed that6 types of anomalies were identified, with the Fall anomaly being the most cited. The Support VectorMachines (SVM) algorithm is most used. Most of the primary studies used public or private datasets.The Hexoskin smart shirt was most cited. The most used metric performance was Accuracy. Onaverage, almost all primary studies presented a result above 90%, and all primary studies used theSupervisioned type of ML. △ Less","7 March, 2022",https://arxiv.org/pdf/2203.03300
Piloting Diversity and Inclusion Workshops in Artificial Intelligence and Robotics for Children,Antonio Badillo-Perez;Donato Badillo-Perez;Diego Coyotzi-Molina;Dago Cruz;Rocio Montenegro;Leticia Vazquez;Miguel Xochicale,"In this paper, we present preliminary work from a pilot workshop that aimed to promote diversity and inclusion for fundamentals of Artificial Intelligence and Robotics for Children (air4children) in the context of developing countries. Considering the scarcity of funding and the little to none availability of specialised professionals to teach AI and robotics in developing countries, we present resources based on free open-source hardware and software, open educational resources, and alternative education programs. That said, the contribution of this work is the pilot workshop of four lessons that promote diversity and inclusion on teaching AI and Robotics for children to a small gender-balanced sample of 14 children of an average age of 7.64 years old. We conclude that participant, instructors, coordinators and parents engaged well in the pilot workshop noting the various challenges of having the right resources for the workshops in developing countries and posing future work. The resources to reproduce this work are available at https://github.com/air4children/hri2022. △ Less","10 March, 2022",https://arxiv.org/pdf/2203.03204
"Reconfigurable Intelligent Surfaces for Wireless Communications: Overview of Hardware Designs, Channel Models, and Estimation Techniques",Mengnan Jian;George C. Alexandropoulos;Ertugrul Basar;Chongwen Huang;Ruiqi Liu;Yuanwei Liu;Chau Yuen,"The demanding objectives for the future sixth generation (6G) of wireless communication networks have spurred recent research efforts on novel materials and radio-frequency front-end architectures for wireless connectivity, as well as revolutionary communication and computing paradigms. Among the pioneering candidate technologies for 6G belong the reconfigurable intelligent surfaces (RISs), which are artificial planar structures with integrated electronic circuits that can be programmed to manipulate the incoming electromagnetic field in a wide variety of functionalities. Incorporating RISs in wireless networks has been recently advocated as a revolutionary means to transform any wireless signal propagation environment to a dynamically programmable one, intended for various networking objectives, such as coverage extension and capacity boosting, spatiotemporal focusing with benefits in energy efficiency and secrecy, and low electromagnetic field exposure. Motivated by the recent increasing interests in the field of RISs and the consequent pioneering concept of the RIS-enabled smart wireless environments, in this paper, we overview and taxonomize the latest advances in RIS hardware architectures as well as the most recent developments in the modeling of RIS unit elements and RIS-empowered wireless signal propagation. We also present a thorough overview of the channel estimation approaches for RIS-empowered communications systems, which constitute a prerequisite step for the optimized incorporation of RISs in future wireless networks. Finally, we discuss the relevance of the RIS technology in the latest wireless communication standards, and highlight the current and future standardization activities for the RIS technology and the consequent RIS-empowered wireless networking approaches. △ Less","7 March, 2022",https://arxiv.org/pdf/2203.03176
Virtual vs. Reality: External Validation of COVID-19 Classifiers using XCAT Phantoms for Chest Computed Tomography,Fakrul Islam Tushar;Ehsan Abadi;Saman Sotoudeh-Paima;Rafael B. Fricks;Maciej A. Mazurowski;W. Paul Segars;Ehsan Samei;Joseph Y. Lo,"Research studies of artificial intelligence models in medical imaging have been hampered by poor generalization. This problem has been especially concerning over the last year with numerous applications of deep learning for COVID-19 diagnosis. Virtual imaging trials (VITs) could provide a solution for objective evaluation of these models. In this work utilizing the VITs, we created the CVIT-COVID dataset including 180 virtually imaged computed tomography (CT) images from simulated COVID-19 and normal phantom models under different COVID-19 morphology and imaging properties. We evaluated the performance of an open-source, deep-learning model from the University of Waterloo trained with multi-institutional data and an in-house model trained with the open clinical dataset called MosMed. We further validated the model's performance against open clinical data of 305 CT images to understand virtual vs. real clinical data performance. The open-source model was published with nearly perfect performance on the original Waterloo dataset but showed a consistent performance drop in external testing on another clinical dataset (AUC=0.77) and our simulated CVIT-COVID dataset (AUC=0.55). The in-house model achieved an AUC of 0.87 while testing on the internal test set (MosMed test set). However, performance dropped to an AUC of 0.65 and 0.69 when evaluated on clinical and our simulated CVIT-COVID dataset. The VIT framework offered control over imaging conditions, allowing us to show there was no change in performance as CT exposure was changed from 28.5 to 57 mAs. The VIT framework also provided voxel-level ground truth, revealing that performance of in-house model was much higher at AUC=0.87 for diffuse COVID-19 infection size >2.65% lung volume versus AUC=0.52 for focal disease with <2.65% volume. The virtual imaging framework enabled these uniquely rigorous analyses of model performance. △ Less","6 March, 2022",https://arxiv.org/pdf/2203.03074
Diversifying Agent's Behaviors in Interactive Decision Models,Yinghui Pan;Hanyi Zhang;Yifeng Zeng;Biyang Ma;Jing Tang;Zhong Ming,"Modelling other agents' behaviors plays an important role in decision models for interactions among multiple agents. To optimise its own decisions, a subject agent needs to model what other agents act simultaneously in an uncertain environment. However, modelling insufficiency occurs when the agents are competitive and the subject agent can not get full knowledge about other agents. Even when the agents are collaborative, they may not share their true behaviors due to their privacy concerns. In this article, we investigate into diversifying behaviors of other agents in the subject agent's decision model prior to their interactions. Starting with prior knowledge about other agents' behaviors, we use a linear reduction technique to extract representative behavioral features from the known behaviors. We subsequently generate their new behaviors by expanding the features and propose two diversity measurements to select top-K behaviors. We demonstrate the performance of the new techniques in two well-studied problem domains. This research will contribute to intelligent systems dealing with unknown unknowns in an open artificial intelligence world. △ Less","6 March, 2022",https://arxiv.org/pdf/2203.03068
A Perspective on Robotic Telepresence and Teleoperation using Cognition: Are we there yet?,Hrishav Bakul Barua;Ashis Sau;Ruddra dev Roychoudhury,"Telepresence and teleoperation robotics have attracted a great amount of attention in the last 10 years. With the Artificial Intelligence (AI) revolution already being started, we can see a wide range of robotic applications being realized. Intelligent robotic systems are being deployed both in industrial and domestic environments. Telepresence is the idea of being present in a remote location virtually or via robotic avatars. Similarly, the idea of operating a robot from a remote location for various tasks is called teleoperation. These technologies find significant application in health care, education, surveillance, disaster recovery, and corporate/government sectors. But question still remains about their maturity, security and safety levels. We also need to think about enhancing the user experience and trust in such technologies going into the next generation of computing. △ Less","6 March, 2022",https://arxiv.org/pdf/2203.02959
Towards a Responsible AI Development Lifecycle: Lessons From Information Security,Erick Galinkin,"Legislation and public sentiment throughout the world have promoted fairness metrics, explainability, and interpretability as prescriptions for the responsible development of ethical artificial intelligence systems. Despite the importance of these three pillars in the foundation of the field, they can be challenging to operationalize and attempts to solve the problems in production environments often feel Sisyphean. This difficulty stems from a number of factors: fairness metrics are computationally difficult to incorporate into training and rarely alleviate all of the harms perpetrated by these systems. Interpretability and explainability can be gamed to appear fair, may inadvertently reduce the privacy of personal information contained in training data, and increase user confidence in predictions -- even when the explanations are wrong. In this work, we propose a framework for responsibly developing artificial intelligence systems by incorporating lessons from the field of information security and the secure development lifecycle to overcome challenges associated with protecting users in adversarial settings. In particular, we propose leveraging the concepts of threat modeling, design review, penetration testing, and incident response in the context of developing AI systems as ways to resolve shortcomings in the aforementioned methods. △ Less","6 March, 2022",https://arxiv.org/pdf/2203.02958
What does it mean to represent? Mental representations as falsifiable memory patterns,Eloy Parra-Barrero;Yulia Sandamirskaya,"Representation is a key notion in neuroscience and artificial intelligence (AI). However, a longstanding philosophical debate highlights that specifying what counts as representation is trickier than it seems. With this brief opinion paper we would like to bring the philosophical problem of representation into attention and provide an implementable solution. We note that causal and teleological approaches often assumed by neuroscientists and engineers fail to provide a satisfactory account of representation. We sketch an alternative according to which representations correspond to inferred latent structures in the world, identified on the basis of conditional patterns of activation. These structures are assumed to have certain properties objectively, which allows for planning, prediction, and detection of unexpected events. We illustrate our proposal with the simulation of a simple neural network model. We believe this stronger notion of representation could inform future research in neuroscience and AI. △ Less","20 April, 2022",https://arxiv.org/pdf/2203.02956
"A Survey on Metaverse: Fundamentals, Security, and Privacy",Yuntao Wang;Zhou Su;Ning Zhang;Rui Xing;Dongxiao Liu;Tom H. Luan;Xuemin Shen,"Metaverse, as an evolving paradigm of the next-generation Internet, aims to build a fully immersive, hyper spatiotemporal, and self-sustaining virtual shared space for humans to play, work, and socialize. Driven by recent advances in emerging technologies such as extended reality, artificial intelligence, and blockchain, metaverse is stepping from science fiction to an upcoming reality. However, severe privacy invasions and security breaches (inherited from underlying technologies or emerged in the new digital ecology) of metaverse can impede its wide deployment. At the same time, a series of fundamental challenges (e.g., scalability and interoperability) can arise in metaverse security provisioning owing to the intrinsic characteristics of metaverse, such as immersive realism, hyper spatiotemporality, sustainability, and heterogeneity. In this paper, we present a comprehensive survey of the fundamentals, security, and privacy of metaverse. Specifically, we first investigate a novel distributed metaverse architecture and its key characteristics with ternary-world interactions. Then, we discuss the security and privacy threats, present the critical challenges of metaverse systems, and review the state-of-the-art countermeasures. Finally, we draw open research directions for building future metaverse systems. △ Less","8 September, 2022",https://arxiv.org/pdf/2203.02662
"AutoMO-Mixer: An automated multi-objective Mixer model for balanced, safe and robust prediction in medicine",Xi Chen;Jiahuan Lv;Dehua Feng;Xuanqin Mou;Ling Bai;Shu Zhang;Zhiguo Zhou,"Accurately identifying patient's status through medical images plays an important role in diagnosis and treatment. Artificial intelligence (AI), especially the deep learning, has achieved great success in many fields. However, more reliable AI model is needed in image guided diagnosis and therapy. To achieve this goal, developing a balanced, safe and robust model with a unified framework is desirable. In this study, a new unified model termed as automated multi-objective Mixer (AutoMO-Mixer) model was developed, which utilized a recent developed multiple layer perceptron Mixer (MLP-Mixer) as base. To build a balanced model, sensitivity and specificity were considered as the objective functions simultaneously in training stage. Meanwhile, a new evidential reasoning based on entropy was developed to achieve a safe and robust model in testing stage. The experiment on an optical coherence tomography dataset demonstrated that AutoMO-Mixer can obtain safer, more balanced, and robust results compared with MLP-Mixer and other available models. △ Less","4 March, 2022",https://arxiv.org/pdf/2203.02384
"A Comprehensive Review of Computer Vision in Sports: Open Issues, Future Trends and Research Directions",Banoth Thulasya Naik;Mohammad Farukh Hashmi;Neeraj Dhanraj Bokde,"Recent developments in video analysis of sports and computer vision techniques have achieved significant improvements to enable a variety of critical operations. To provide enhanced information, such as detailed complex analysis in sports like soccer, basketball, cricket, badminton, etc., studies have focused mainly on computer vision techniques employed to carry out different tasks. This paper presents a comprehensive review of sports video analysis for various applications high-level analysis such as detection and classification of players, tracking player or ball in sports and predicting the trajectories of player or ball, recognizing the teams strategies, classifying various events in sports. The paper further discusses published works in a variety of application-specific tasks related to sports and the present researchers views regarding them. Since there is a wide research scope in sports for deploying computer vision techniques in various sports, some of the publicly available datasets related to a particular sport have been provided. This work reviews a detailed discussion on some of the artificial intelligence(AI)applications in sports vision, GPU-based work stations, and embedded platforms. Finally, this review identifies the research directions, probable challenges, and future trends in the area of visual recognition in sports. △ Less","23 March, 2022",https://arxiv.org/pdf/2203.02281
Compressed Predictive Information Coding,Rui Meng;Tianyi Luo;Kristofer Bouchard,"Unsupervised learning plays an important role in many fields, such as artificial intelligence, machine learning, and neuroscience. Compared to static data, methods for extracting low-dimensional structure for dynamic data are lagging. We developed a novel information-theoretic framework, Compressed Predictive Information Coding (CPIC), to extract useful representations from dynamic data. CPIC selectively projects the past (input) into a linear subspace that is predictive about the compressed data projected from the future (output). The key insight of our framework is to learn representations by minimizing the compression complexity and maximizing the predictive information in latent space. We derive variational bounds of the CPIC loss which induces the latent space to capture information that is maximally predictive. Our variational bounds are tractable by leveraging bounds of mutual information. We find that introducing stochasticity in the encoder robustly contributes to better representation. Furthermore, variational approaches perform better in mutual information estimation compared with estimates under a Gaussian assumption. We demonstrate that CPIC is able to recover the latent space of noisy dynamical systems with low signal-to-noise ratios, and extracts features predictive of exogenous variables in neuroscience data. △ Less","3 March, 2022",https://arxiv.org/pdf/2203.02051
DIME: Fine-grained Interpretations of Multimodal Models via Disentangled Local Explanations,Yiwei Lyu;Paul Pu Liang;Zihao Deng;Ruslan Salakhutdinov;Louis-Philippe Morency,"The ability for a human to understand an Artificial Intelligence (AI) model's decision-making process is critical in enabling stakeholders to visualize model behavior, perform model debugging, promote trust in AI models, and assist in collaborative human-AI decision-making. As a result, the research fields of interpretable and explainable AI have gained traction within AI communities as well as interdisciplinary scientists seeking to apply AI in their subject areas. In this paper, we focus on advancing the state-of-the-art in interpreting multimodal models - a class of machine learning methods that tackle core challenges in representing and capturing interactions between heterogeneous data sources such as images, text, audio, and time-series data. Multimodal models have proliferated numerous real-world applications across healthcare, robotics, multimedia, affective computing, and human-computer interaction. By performing model disentanglement into unimodal contributions (UC) and multimodal interactions (MI), our proposed approach, DIME, enables accurate and fine-grained analysis of multimodal models while maintaining generality across arbitrary modalities, model architectures, and tasks. Through a comprehensive suite of experiments on both synthetic and real-world multimodal tasks, we show that DIME generates accurate disentangled explanations, helps users of multimodal models gain a deeper understanding of model behavior, and presents a step towards debugging and improving these models for real-world deployment. Code for our experiments can be found at https://github.com/lvyiwei1/DIME. △ Less","3 March, 2022",https://arxiv.org/pdf/2203.02013
Fairness-aware Adversarial Perturbation Towards Bias Mitigation for Deployed Deep Models,Zhibo Wang;Xiaowei Dong;Henry Xue;Zhifei Zhang;Weifeng Chiu;Tao Wei;Kui Ren,"Prioritizing fairness is of central importance in artificial intelligence (AI) systems, especially for those societal applications, e.g., hiring systems should recommend applicants equally from different demographic groups, and risk assessment systems must eliminate racism in criminal justice. Existing efforts towards the ethical development of AI systems have leveraged data science to mitigate biases in the training set or introduced fairness principles into the training process. For a deployed AI system, however, it may not allow for retraining or tuning in practice. By contrast, we propose a more flexible approach, i.e., fairness-aware adversarial perturbation (FAAP), which learns to perturb input data to blind deployed models on fairness-related features, e.g., gender and ethnicity. The key advantage is that FAAP does not modify deployed models in terms of parameters and structures. To achieve this, we design a discriminator to distinguish fairness-related attributes based on latent representations from deployed models. Meanwhile, a perturbation generator is trained against the discriminator, such that no fairness-related features could be extracted from perturbed inputs. Exhaustive experimental evaluation demonstrates the effectiveness and superior performance of the proposed FAAP. In addition, FAAP is validated on real-world commercial deployments (inaccessible to model parameters), which shows the transferability of FAAP, foreseeing the potential of black-box adaptation. △ Less","3 March, 2022",https://arxiv.org/pdf/2203.01584
DareFightingICE Competition: A Fighting Game Sound Design and AI Competition,Ibrahim Khan;Thai Van Nguyen;Xincheng Dai;Ruck Thawonmas,"This paper presents a new competition -- at the 2022 IEEE Conference on Games (CoG) -- called DareFightingICE Competition. The competition has two tracks: a sound design track and an AI track. The game platform for this competition is also called DareFightingICE, a fighting game platform. DareFightingICE is a sound-design-enhanced version of FightingICE, used earlier in a competition at CoG until 2021 to promote artificial intelligence (AI) research in fighting games. In the sound design track, participants compete for the best sound design, given the default sound design of DareFightingICE as a sample, where we define a sound design as a set of sound effects combined with the source code that implements their timing-control algorithm. Participants of the AI track are asked to develop their AI algorithm that controls a character given only sound as the input (blind AI) to fight against their opponent; a sample deep-learning blind AI will be provided by us. Our means to maximize the synergy between the two tracks are also described. This competition serves to come up with effective sound designs for visually impaired players, a group in the gaming community which has been mostly ignored. To the best of our knowledge, DareFightingICE Competition is the first of its kind within and outside of CoG. △ Less","15 June, 2022",https://arxiv.org/pdf/2203.01556
"Recent, rapid advancement in visual question answering architecture: a review",Venkat Kodali;Daniel Berleant,"Understanding visual question answering is going to be crucial for numerous human activities. However, it presents major challenges at the heart of the artificial intelligence endeavor. This paper presents an update on the rapid advancements in visual question answering using images that have occurred in the last couple of years. Tremendous growth in research on improving visual question answering system architecture has been published recently, showing the importance of multimodal architectures. Several points on the benefits of visual question answering are mentioned in the review paper by Manmadhan et al. (2020), on which the present article builds, including subsequent updates in the field. △ Less","9 July, 2022",https://arxiv.org/pdf/2203.01322
Artificial Concepts of Artificial Intelligence: Institutional Compliance and Resistance in AI Startups,Amy A. Winecoff;Elizabeth Anne Watkins,"Scholars and industry practitioners have debated how to best develop interventions for ethical artificial intelligence (AI). Such interventions recommend that companies building and using AI tools change their technical practices, but fail to wrangle with critical questions about the organizational and institutional context in which AI is developed. In this paper, we contribute descriptive research around the life of ""AI"" as a discursive concept and organizational practice in an understudied sphere--emerging AI startups--and with a focus on extra-organizational pressures faced by entrepreneurs. Leveraging a theoretical lens for how organizations change, we conducted semi-structured interviews with 23 entrepreneurs working at early-stage AI startups. We find that actors within startups both conform to and resist institutional pressures. Our analysis identifies a central tension for AI entrepreneurs: they often valued scientific integrity and methodological rigor; however, influential external stakeholders either lacked the technical knowledge to appreciate entrepreneurs' emphasis on rigor or were more focused on business priorities. As a result, entrepreneurs adopted hyped marketing messages about AI that diverged from their scientific values, but attempted to preserve their legitimacy internally. Institutional pressures and organizational constraints also influenced entrepreneurs' modeling practices and their response to actual or impending regulation. We conclude with a discussion for how such pressures could be used as leverage for effective interventions towards building ethical AI. △ Less","14 June, 2022",https://arxiv.org/pdf/2203.01157
Engineering the Neural Automatic Passenger Counter,Nico Jahn;Michael Siebert,"Automatic passenger counting (APC) in public transportation has been approached with various machine learning and artificial intelligence methods since its introduction in the 1970s. While equivalence testing is becoming more popular than difference detection (Student's t-test), the former is much more difficult to pass to ensure low user risk. On the other hand, recent developments in artificial intelligence have led to algorithms that promise much higher counting quality (lower bias). However, gradient-based methods (including Deep Learning) have one limitation: they typically run into local optima. In this work, we explore and exploit various aspects of machine learning to increase reliability, performance, and counting quality. We perform a grid search with several fundamental parameters: the selection and size of the training set, which is similar to cross-validation, and the initial network weights and randomness during the training process. Using this experiment, we show how aggregation techniques such as ensemble quantiles can reduce bias, and we give an idea of the overall spread of the results. We utilize the test success chance, a simulative metric based on the empirical distribution. We also employ a post-training Monte Carlo quantization approach and introduce cumulative summation to turn counting into a stationary method and allow unbounded counts. △ Less","9 March, 2022",https://arxiv.org/pdf/2203.01156
Satellite Image and Machine Learning based Knowledge Extraction in the Poverty and Welfare Domain,Ola Hall;Mattias Ohlsson;Thortseinn Rögnvaldsson,"Recent advances in artificial intelligence and machine learning have created a step change in how to measure human development indicators, in particular asset based poverty. The combination of satellite imagery and machine learning has the capability to estimate poverty at a level similar to what is achieved with workhorse methods such as face-to-face interviews and household surveys. An increasingly important issue beyond static estimations is whether this technology can contribute to scientific discovery and consequently new knowledge in the poverty and welfare domain. A foundation for achieving scientific insights is domain knowledge, which in turn translates into explainability and scientific consistency. We review the literature focusing on three core elements relevant in this context: transparency, interpretability, and explainability and investigate how they relates to the poverty, machine learning and satellite imagery nexus. Our review of the field shows that the status of the three core elements of explainable machine learning (transparency, interpretability and domain knowledge) is varied and does not completely fulfill the requirements set up for scientific insights and discoveries. We argue that explainability is essential to support wider dissemination and acceptance of this research, and explainability means more than just interpretability. △ Less","2 March, 2022",https://arxiv.org/pdf/2203.01068
On the Configuration of More and Less Expressive Logic Programs,Carmine Dodaro;Marco Maratea;Mauro Vallati,"The decoupling between the representation of a certain problem, i.e., its knowledge model, and the reasoning side is one of main strong points of model-based Artificial Intelligence (AI). This allows, e.g. to focus on improving the reasoning side by having advantages on the whole solving process. Further, it is also well-known that many solvers are very sensitive to even syntactic changes in the input. In this paper, we focus on improving the reasoning side by taking advantages of such sensitivity. We consider two well-known model-based AI methodologies, SAT and ASP, define a number of syntactic features that may characterise their inputs, and use automated configuration tools to reformulate the input formula or program. Results of a wide experimental analysis involving SAT and ASP domains, taken from respective competitions, show the different advantages that can be obtained by using input reformulation and configuration. Under consideration in Theory and Practice of Logic Programming (TPLP). △ Less","2 March, 2022",https://arxiv.org/pdf/2203.01024
Machine learning based lens-free imaging technique for field-portable cytometry,Rajkumar Vaghashiya;Sanghoon Shin;Varun Chauhan;Kaushal Kapadiya;Smit Sanghavi;Sungkyu Seo;Mohendra Roy,"Lens-free Shadow Imaging Technique (LSIT) is a well-established technique for the characterization of microparticles and biological cells. Due to its simplicity and cost-effectiveness, various low-cost solutions have been evolved, such as automatic analysis of complete blood count (CBC), cell viability, 2D cell morphology, 3D cell tomography, etc. The developed auto characterization algorithm so far for this custom-developed LSIT cytometer was based on the hand-crafted features of the cell diffraction patterns from the LSIT cytometer, that were determined from our empirical findings on thousands of samples of individual cell types, which limit the system in terms of induction of a new cell type for auto classification or characterization. Further, its performance is suffering from poor image (cell diffraction pattern) signatures due to its small signal or background noise. In this work, we address these issues by leveraging the artificial intelligence-powered auto signal enhancing scheme such as denoising autoencoder and adaptive cell characterization technique based on the transfer of learning in deep neural networks. The performance of our proposed method shows an increase in accuracy >98% along with the signal enhancement of >5 dB for most of the cell types, such as Red Blood Cell (RBC) and White Blood Cell (WBC). Furthermore, the model is adaptive to learn new type of samples within a few learning iterations and able to successfully classify the newly introduced sample along with the existing other sample types. △ Less","2 March, 2022",https://arxiv.org/pdf/2203.00899
A Unifying Framework for Some Directed Distances in Statistics,Michel Broniatowski;Wolfgang Stummer,"Density-based directed distances -- particularly known as divergences -- between probability distributions are widely used in statistics as well as in the adjacent research fields of information theory, artificial intelligence and machine learning. Prominent examples are the Kullback-Leibler information distance (relative entropy) which e.g. is closely connected to the omnipresent maximum likelihood estimation method, and Pearson's chisquare-distance which e.g. is used for the celebrated chisquare goodness-of-fit test. Another line of statistical inference is built upon distribution-function-based divergences such as e.g. the prominent (weighted versions of) Cramer-von Mises test statistics respectively Anderson-Darling test statistics which are frequently applied for goodness-of-fit investigations; some more recent methods deal with (other kinds of) cumulative paired divergences and closely related concepts. In this paper, we provide a general framework which covers in particular both the above-mentioned density-based and distribution-function-based divergence approaches; the dissimilarity of quantiles respectively of other statistical functionals will be included as well. From this framework, we structurally extract numerous classical and also state-of-the-art (including new) procedures. Furthermore, we deduce new concepts of dependence between random variables, as alternatives to the celebrated mutual information. Some variational representations are discussed, too. △ Less","1 March, 2022",https://arxiv.org/pdf/2203.00863
Conceptual Modeling of Events Based on One-Category Ontology,Sabah Al-Fedaghi,"In previous works, we proposed a one-category (entitled thimac) conceptual model called a thinging machine (TM), which integrates staticity (e.g., objects) and dynamism (e.g., events) without losing valuable aspects of diagrammatic intuition in conceptual modeling. We proposed applying TM to conceptual modeling in software engineering (e.g., on or above the level of UML as a conceptual modeling language). In this paper, to show such an application in software engineering, we first present a complete high-level description of a library service system to demonstrate the TM s applicability. Furthermore, we explore the TM s features, emphasizing the realization of thimacs as events. The purpose is to develop better understanding of the TM notions by contrasting them with their uses in related fields. The notion of an event plays a prominent role in many fields of study, including philosophy, linguistics, literary theory, probability theory, artificial intelligence, physics, and history. A TM event is a static thimac with a time breath (time subthimac) that infuses dynamism into the thimac. It arises from how the TM static region is infected with time. Such a view is contrasted with some philosophical and linguistics definitions of an event (e.g., unit of experience of Whitehead). We also raise interesting issues (e.g., event movement) in this study. △ Less","1 March, 2022",https://arxiv.org/pdf/2203.00850
Learning Robust Real-Time Cultural Transmission without Human Data,Cultural General Intelligence Team;Avishkar Bhoopchand;Bethanie Brownfield;Adrian Collister;Agustin Dal Lago;Ashley Edwards;Richard Everett;Alexandre Frechette;Yanko Gitahy Oliveira;Edward Hughes;Kory W. Mathewson;Piermaria Mendolicchio;Julia Pawar;Miruna Pislar;Alex Platonov;Evan Senter;Sukhdeep Singh;Alexander Zacherl;Lei M. Zhang,"Cultural transmission is the domain-general social skill that allows agents to acquire and use information from each other in real-time with high fidelity and recall. In humans, it is the inheritance process that powers cumulative cultural evolution, expanding our skills, tools and knowledge across generations. We provide a method for generating zero-shot, high recall cultural transmission in artificially intelligent agents. Our agents succeed at real-time cultural transmission from humans in novel contexts without using any pre-collected human data. We identify a surprisingly simple set of ingredients sufficient for generating cultural transmission and develop an evaluation methodology for rigorously assessing it. This paves the way for cultural evolution as an algorithm for developing artificial general intelligence. △ Less","1 March, 2022",https://arxiv.org/pdf/2203.00715
Path sampling of recurrent neural networks by incorporating known physics,Sun-Ting Tsai;Eric Fields;Yijia Xu;En-Jui Kuo;Pratyush Tiwary,"Recurrent neural networks have seen widespread use in modeling dynamical systems in varied domains such as weather prediction, text prediction and several others. Often one wishes to supplement the experimentally observed dynamics with prior knowledge or intuition about the system. While the recurrent nature of these networks allows them to model arbitrarily long memories in the time series used in training, it makes it harder to impose prior knowledge or intuition through generic constraints. In this work, we present a path sampling approach based on principle of Maximum Caliber that allows us to include generic thermodynamic or kinetic constraints into recurrent neural networks. We show the method here for a widely used type of recurrent neural network known as long short-term memory network in the context of supplementing time series collected from different application domains. These include classical Molecular Dynamics of a protein and Monte Carlo simulations of an open quantum system continuously losing photons to the environment and displaying Rabi oscillations. Our method can be easily generalized to other generative artificial intelligence models and to generic time series in different areas of physical and social sciences, where one wishes to supplement limited data with intuition or theory based corrections. △ Less","20 April, 2022",https://arxiv.org/pdf/2203.00597
Towards deep learning-powered IVF: A large public benchmark for morphokinetic parameter prediction,Tristan Gomez;Magalie Feyeux;Nicolas Normand;Laurent David;Perrine Paul-Gilloteaux;Thomas Fréour;Harold Mouchère,"An important limitation to the development of Artificial Intelligence (AI)-based solutions for In Vitro Fertilization (IVF) is the absence of a public reference benchmark to train and evaluate deep learning (DL) models. In this work, we describe a fully annotated dataset of 704 videos of developing embryos, for a total of 337k images. We applied ResNet, LSTM, and ResNet-3D architectures to our dataset and demonstrate that they overperform algorithmic approaches to automatically annotate stage development phases. Altogether, we propose the first public benchmark that will allow the community to evaluate morphokinetic models. This is the first step towards deep learning-powered IVF. Of note, we propose highly detailed annotations with 16 different development phases, including early cell division phases, but also late cell divisions, phases after morulation, and very early phases, which have never been used before. We postulate that this original approach will help improve the overall performance of deep learning approaches on time-lapse videos of embryo development, ultimately benefiting infertile patients with improved clinical success rates (Code and data are available at https://gitlab.univ-nantes.fr/E144069X/bench_mk_pred.git). △ Less","13 May, 2022",https://arxiv.org/pdf/2203.00531
Social Network Extraction Unsupervised,Mahyuddin K. M. Nasution;Rahmad Syah,"In the era of information technology, the two developing sides are data science and artificial intelligence. In terms of scientific data, one of the tasks is the extraction of social networks from information sources that have the nature of big data. Meanwhile, in terms of artificial intelligence, the presence of contradictory methods has an impact on knowledge. This article describes an unsupervised as a stream of methods for extracting social networks from information sources. There are a variety of possible approaches and strategies to superficial methods as a starting concept. Each method has its advantages, but in general, it contributes to the integration of each other, namely simplifying, enriching, and emphasizing the results. △ Less","24 February, 2022",https://arxiv.org/pdf/2203.00515
Compliance Challenges in Forensic Image Analysis Under the Artificial Intelligence Act,Benedikt Lorch;Nicole Scheler;Christian Riess,"In many applications of forensic image analysis, state-of-the-art results are nowadays achieved with machine learning methods. However, concerns about their reliability and opaqueness raise the question whether such methods can be used in criminal investigations. So far, this question of legal compliance has hardly been discussed, also because legal regulations for machine learning methods were not defined explicitly. To this end, the European Commission recently proposed the artificial intelligence (AI) act, a regulatory framework for the trustworthy use of AI. Under the draft AI act, high-risk AI systems for use in law enforcement are permitted but subject to compliance with mandatory requirements. In this paper, we review why the use of machine learning in forensic image analysis is classified as high-risk. We then summarize the mandatory requirements for high-risk AI systems and discuss these requirements in light of two forensic applications, license plate recognition and deep fake detection. The goal of this paper is to raise awareness of the upcoming legal requirements and to point out avenues for future research. △ Less","1 March, 2022",https://arxiv.org/pdf/2203.00469
Explaining a Deep Reinforcement Learning Docking Agent Using Linear Model Trees with User Adapted Visualization,Vilde B. Gjærum;Inga Strümke;Ole Andreas Alsos;Anastasios M. Lekkas,"Deep neural networks (DNNs) can be useful within the marine robotics field, but their utility value is restricted by their black-box nature. Explainable artificial intelligence methods attempt to understand how such black-boxes make their decisions. In this work, linear model trees (LMTs) are used to approximate the DNN controlling an autonomous surface vessel (ASV) in a simulated environment and then run in parallel with the DNN to give explanations in the form of feature attributions in real-time. How well a model can be understood depends not only on the explanation itself, but also on how well it is presented and adapted to the receiver of said explanation. Different end-users may need both different types of explanations, as well as different representations of these. The main contributions of this work are (1) significantly improving both the accuracy and the build time of a greedy approach for building LMTs by introducing ordering of features in the splitting of the tree, (2) giving an overview of the characteristics of the seafarer/operator and the developer as two different end-users of the agent and receiver of the explanations, and (3) suggesting a visualization of the docking agent, the environment, and the feature attributions given by the LMT for when the developer is the end-user of the system, and another visualization for when the seafarer or operator is the end-user, based on their different characteristics. △ Less","1 March, 2022",https://arxiv.org/pdf/2203.00368
"\text{T}^3
OMVP: A Transformer-based Time and Team Reinforcement Learning Scheme for Observation-constrained Multi-Vehicle Pursuit in Urban Area",Zheng Yuan;Tianhao Wu;Qinwen Wang;Yiying Yang;Lei Li;Lin Zhang,"Smart Internet of Vehicles (IoVs) combined with Artificial Intelligence (AI) will contribute to vehicle decision-making in the Intelligent Transportation System (ITS). Multi-Vehicle Pursuit games (MVP), a multi-vehicle cooperative ability to capture mobile targets, is becoming a hot research topic gradually. Although there are some achievements in the field of MVP in the open space environment, the urban area brings complicated road structures and restricted moving spaces as challenges to the resolution of MVP games. We define an Observation-constrained MVP (OMVP) problem in this paper and propose a Transformer-based Time and Team Reinforcement Learning scheme ( \text{T}^3 OMVP) to address the problem. First, a new multi-vehicle pursuit model is constructed based on decentralized partially observed Markov decision processes (Dec-POMDP) to instantiate this problem. Second, by introducing and modifying the transformer-based observation sequence, QMIX is redefined to adapt to the complicated road structure, restricted moving spaces and constrained observations, so as to control vehicles to pursue the target combining the vehicle's observations. Third, a multi-intersection urban environment is built to verify the proposed scheme. Extensive experimental results demonstrate that the proposed \text{T}^3 OMVP scheme achieves significant improvements relative to state-of-the-art QMIX approaches by 9.66%~106.25%. Code is available at https://github.com/pipihaiziguai/T3OMVP. △ Less","3 March, 2022",https://arxiv.org/pdf/2203.00183
AI-based approach for improving the detection of blood doping in sports,Maxx Richard Rahman;Jacob Bejder;Thomas Christian Bonne;Andreas Breenfeldt Andersen;Jesús Rodríguez Huertas;Reid Aikin;Nikolai Baastrup Nordsborg;Wolfgang Maaß,"Sports officials around the world are facing incredible challenges due to the unfair means of practices performed by the athletes to improve their performance in the game. It includes the intake of hormonal based drugs or transfusion of blood to increase their strength and the result of their training. However, the current direct test of detection of these cases includes the laboratory-based method, which is limited because of the cost factors, availability of medical experts, etc. This leads us to seek for indirect tests. With the growing interest of Artificial Intelligence in healthcare, it is important to propose an algorithm based on blood parameters to improve decision making. In this paper, we proposed a statistical and machine learning-based approach to identify the presence of doping substance rhEPO in blood samples. △ Less","9 February, 2022",https://arxiv.org/pdf/2203.00001
The dangers in algorithms learning humans' values and irrationalities,Rebecca Gorman;Stuart Armstrong,"For an artificial intelligence (AI) to be aligned with human values (or human preferences), it must first learn those values. AI systems that are trained on human behavior, risk miscategorising human irrationalities as human values -- and then optimising for these irrationalities. Simply learning human values still carries risks: AI learning them will inevitably also gain information on human irrationalities and human behaviour/policy. Both of these can be dangerous: knowing human policy allows an AI to become generically more powerful (whether it is partially aligned or not aligned at all), while learning human irrationalities allows it to exploit humans without needing to provide value in return. This paper analyses the danger in developing artificial intelligence that learns about human irrationalities and human policy, and constructs a model recommendation system with various levels of information about human biases, human policy, and human values. It concludes that, whatever the power and knowledge of the AI, it is more dangerous for it to know human irrationalities than human values. Thus it is better for the AI to learn human values directly, rather than learning human biases and then deducing values from behaviour. △ Less","1 March, 2022",https://arxiv.org/pdf/2202.13985
Leveraging Channel Noise for Sampling and Privacy via Quantized Federated Langevin Monte Carlo,Yunchuan Zhang;Dongzhu Liu;Osvaldo Simeone,"For engineering applications of artificial intelligence, Bayesian learning holds significant advantages over standard frequentist learning, including the capacity to quantify uncertainty. Langevin Monte Carlo (LMC) is an efficient gradient-based approximate Bayesian learning strategy that aims at producing samples drawn from the posterior distribution of the model parameters. Prior work focused on a distributed implementation of LMC over a multi-access wireless channel via analog modulation. In contrast, this paper proposes quantized federated LMC (FLMC), which integrates one-bit stochastic quantization of the local gradients with channel-driven sampling. Channel-driven sampling leverages channel noise for the purpose of contributing to Monte Carlo sampling, while also serving the role of privacy mechanism. Analog and digital implementations of wireless LMC are compared as a function of differential privacy (DP) requirements, revealing the advantages of the latter at sufficiently high signal-to-noise ratio. △ Less","28 February, 2022",https://arxiv.org/pdf/2202.13932
Time Series Analysis of Blockchain-Based Cryptocurrency Price Changes,Jacques Fleischer;Gregor von Laszewski;Carlos Theran;Yohn Jairo Parra Bautista,"In this paper we apply neural networks and Artificial Intelligence (AI) to historical records of high-risk cryptocurrency coins to train a prediction model that guesses their price. This paper's code contains Jupyter notebooks, one of which outputs a timeseries graph of any cryptocurrency price once a CSV file of the historical data is inputted into the program. Another Jupyter notebook trains an LSTM, or a long short-term memory model, to predict a cryptocurrency's closing price. The LSTM is fed the close price, which is the price that the currency has at the end of the day, so it can learn from those values. The notebook creates two sets: a training set and a test set to assess the accuracy of the results. The data is then normalized using manual min-max scaling so that the model does not experience any bias; this also enhances the performance of the model. Then, the model is trained using three layers -- an LSTM, dropout, and dense layer-minimizing the loss through 50 epochs of training; from this training, a recurrent neural network (RNN) is produced and fitted to the training set. Additionally, a graph of the loss over each epoch is produced, with the loss minimizing over time. Finally, the notebook plots a line graph of the actual currency price in red and the predicted price in blue. The process is then repeated for several more cryptocurrencies to compare prediction models. The parameters for the LSTM, such as number of epochs and batch size, are tweaked to try and minimize the root mean square error. △ Less","18 February, 2022",https://arxiv.org/pdf/2202.13874
Curb Your Self-Modifying Code,Patrik Christen,"Self-modifying code has many intriguing applications in a broad range of fields including software security, artificial general intelligence, and open-ended evolution. Having control over self-modifying code, however, is still an open challenge since it is a balancing act between providing as much freedom as possible so as not to limit possible solutions, while at the same time imposing restriction to avoid security issues and invalid code or solutions. In the present study, I provide a prototype implementation of how one might curb self-modifying code by introducing control mechanisms for code modifications within specific regions and for specific transitions between code and data. I show that this is possible to achieve with the so-called allagmatic method - a framework to formalise, model, implement, and interpret complex systems inspired by Gilbert Simondon's philosophy of individuation and Alfred North Whitehead's philosophy of organism. Thereby, the allagmatic method serves as guidance for self-modification based on concepts defined in a metaphysical framework. I conclude that the allagmatic method seems to be a suitable framework for control mechanisms in self-modifying code and that there are intriguing analogies between the presented control mechanisms and gene regulation. △ Less","29 July, 2022",https://arxiv.org/pdf/2202.13830
Quality Monitoring and Assessment of Deployed Deep Learning Models for Network AIOps,Lixuan Yang;Dario Rossi,"Artificial Intelligence (AI) has recently attracted a lot of attention, transitioning from research labs to a wide range of successful deployments in many fields, which is particularly true for Deep Learning (DL) techniques. Ultimately, DL models being software artifacts, they need to be regularly maintained and updated: AIOps is the logical extension of the DevOps software development practices to AI-software applied to network operation and management. In the lifecycle of a DL model deployment, it is important to assess the quality of deployed models, to detect ""stale"" models and prioritize their update. In this article, we cover the issue in the context of network management, proposing simple yet effective techniques for (i) quality assessment of individual inference, and for (ii) overall model quality tracking over multiple inferences, that we apply to two use cases, representative of the network management and image recognition fields. △ Less","28 February, 2022",https://arxiv.org/pdf/2202.13642
Towards A Device-Independent Deep Learning Approach for the Automated Segmentation of Sonographic Fetal Brain Structures: A Multi-Center and Multi-Device Validation,Abhi Lad;Adithya Narayan;Hari Shankar;Shefali Jain;Pooja Punjani Vyas;Divya Singh;Nivedita Hegde;Jagruthi Atada;Jens Thang;Saw Shier Nee;Arunkumar Govindarajan;Roopa PS;Muralidhar V Pai;Akhila Vasudeva;Prathima Radhakrishnan;Sripad Krishna Devalla,"Quality assessment of prenatal ultrasonography is essential for the screening of fetal central nervous system (CNS) anomalies. The interpretation of fetal brain structures is highly subjective, expertise-driven, and requires years of training experience, limiting quality prenatal care for all pregnant mothers. With recent advancement in Artificial Intelligence (AI), specifically deep learning (DL), assistance in precise anatomy identification through semantic segmentation essential for the reliable assessment of growth and neurodevelopment, and detection of structural abnormalities have been proposed. However, existing works only identify certain structures (e.g., cavum septum pellucidum, lateral ventricles, cerebellum) from either of the axial views (transventricular, transcerebellar), limiting the scope for a thorough anatomical assessment as per practice guidelines necessary for the screening of CNS anomalies. Further, existing works do not analyze the generalizability of these DL algorithms across images from multiple ultrasound devices and centers, thus, limiting their real-world clinical impact. In this study, we propose a DL based segmentation framework for the automated segmentation of 10 key fetal brain structures from 2 axial planes from fetal brain USG images (2D). We developed a custom U-Net variant that uses inceptionv4 block as a feature extractor and leverages custom domain-specific data augmentation. Quantitatively, the mean (10 structures; test sets 1/2/3/4) Dice-coefficients were: 0.827, 0.802, 0.731, 0.783. Irrespective of the USG device/center, the DL segmentations were qualitatively comparable to their manual segmentations. The proposed DL system offered a promising and generalizable performance (multi-centers, multi-device) and also presents evidence in support of device-induced variation in image quality (a challenge to generalizibility) by using UMAP analysis. △ Less","28 February, 2022",https://arxiv.org/pdf/2202.13553
Machine Learning Empowered Intelligent Data Center Networking: A Survey,Bo Li;Ting Wang;Peng Yang;Mingsong Chen;Shui Yu;Mounir Hamdi,"To support the needs of ever-growing cloud-based services, the number of servers and network devices in data centers is increasing exponentially, which in turn results in high complexities and difficulties in network optimization. To address these challenges, both academia and industry turn to artificial intelligence technology to realize network intelligence. To this end, a considerable number of novel and creative machine learning-based (ML-based) research works have been put forward in recent few years. Nevertheless, there are still enormous challenges faced by the intelligent optimization of data center networks (DCNs), especially in the scenario of online real-time dynamic processing of massive heterogeneous services and traffic data. To best of our knowledge, there is a lack of systematic and original comprehensively investigations with in-depth analysis on intelligent DCN. To this end, in this paper, we comprehensively investigate the application of machine learning to data center networking, and provide a general overview and in-depth analysis of the recent works, covering flow prediction, flow classification, load balancing, resource management, routing optimization, and congestion control. In order to provide a multi-dimensional and multi-perspective comparison of various solutions, we design a quality assessment criteria called REBEL-3S to impartially measure the strengths and weaknesses of these research works. Moreover, we also present unique insights into the technology evolution of the fusion of data center network and machine learning, together with some challenges and potential future research opportunities. △ Less","28 February, 2022",https://arxiv.org/pdf/2202.13549
The Quest for a Common Model of the Intelligent Decision Maker,Richard S. Sutton,"The premise of the Multi-disciplinary Conference on Reinforcement Learning and Decision Making is that multiple disciplines share an interest in goal-directed decision making over time. The idea of this paper is to sharpen and deepen this premise by proposing a perspective on the decision maker that is substantive and widely held across psychology, artificial intelligence, economics, control theory, and neuroscience, which I call the ""common model of the intelligent agent"". The common model does not include anything specific to any organism, world, or application domain. The common model does include aspects of the decision maker's interaction with its world (there must be input and output, and a goal) and internal components of the decision maker (for perception, decision-making, internal evaluation, and a world model). I identify these aspects and components, note that they are given different names in different disciplines but refer essentially to the same ideas, and discuss the challenges and benefits of devising a neutral terminology that can be used across disciplines. It is time to recognize and build on the convergence of multiple diverse disciplines on a substantive common model of the intelligent agent. △ Less","5 June, 2022",https://arxiv.org/pdf/2202.13252
Quantum Algorithms for solving Hard Constrained Optimisation Problems,Parfait Atchade-Adelomou,"The thesis deals with Quantum Algorithms for solving Hard Constrained Optimization Problems. It shows how quantum computers can solve difficult everyday problems such as finding the best schedule for social workers or the path of a robot picking and batching in a warehouse. The path to the solution has led to the definition of a new artificial intelligence paradigm with quantum computing, quantum Case-Based Reasoning (qCBR) and to a proof of concept to integrate the capacity of quantum computing within mobile robotics using a Raspberry Pi 4 as a processor (qRobot), capable of operating with leading technology players such as IBMQ, Amazon Braket (D-Wave) and Pennylane. To improve the execution time of variational algorithms in this NISQ era and the next, we have proposed EVA: a quantum Exponential Value Approximation algorithm that speeds up the VQE, and that is, to date, the flagship of the quantum computation. To improve the execution time of variational algorithms in this NISQ era and the next, we have proposed EVA: a quantum Exponential Value Approximation algorithm that speeds up the VQE, and that is, to date, the flagship of the quantum computation. △ Less","26 February, 2022",https://arxiv.org/pdf/2202.13125
Integrated multimodal artificial intelligence framework for healthcare applications,Luis R. Soenksen;Yu Ma;Cynthia Zeng;Leonard D. J. Boussioux;Kimberly Villalobos Carballo;Liangyuan Na;Holly M. Wiberg;Michael L. Li;Ignacio Fuentes;Dimitris Bertsimas,"Artificial intelligence (AI) systems hold great promise to improve healthcare over the next decades. Specifically, AI systems leveraging multiple data sources and input modalities are poised to become a viable method to deliver more accurate results and deployable pipelines across a wide range of applications. In this work, we propose and evaluate a unified Holistic AI in Medicine (HAIM) framework to facilitate the generation and testing of AI systems that leverage multimodal inputs. Our approach uses generalizable data pre-processing and machine learning modeling stages that can be readily adapted for research and deployment in healthcare environments. We evaluate our HAIM framework by training and characterizing 14,324 independent models based on HAIM-MIMIC-MM, a multimodal clinical database (N=34,537 samples) containing 7,279 unique hospitalizations and 6,485 patients, spanning all possible input combinations of 4 data modalities (i.e., tabular, time-series, text, and images), 11 unique data sources and 12 predictive tasks. We show that this framework can consistently and robustly produce models that outperform similar single-source approaches across various healthcare demonstrations (by 6-33%), including 10 distinct chest pathology diagnoses, along with length-of-stay and 48-hour mortality predictions. We also quantify the contribution of each modality and data source using Shapley values, which demonstrates the heterogeneity in data modality importance and the necessity of multimodal inputs across different healthcare-relevant tasks. The generalizable properties and flexibility of our Holistic AI in Medicine (HAIM) framework could offer a promising pathway for future multimodal predictive systems in clinical and operational healthcare settings. △ Less","26 September, 2022",https://arxiv.org/pdf/2202.12998
Wearable uBrain: Fabric Based-Spiking Neural Network,Frances Cleary;Witawas Srisa-an;Beatriz Gil;Jaideep Kesavan;Tobias Engel;David C. Henshall;Sasitharan Balasubramaniam,"On garment intelligence influenced by artificial neural networks and neuromorphic computing is emerging as a research direction in the e-textile sector. In particular, bio inspired Spiking Neural Networks mimicking the workings of the brain show promise in recent ICT research applications. Taking such technological advancements and new research directions driving forward the next generation of e-textiles and smart materials, we present a wearable micro Brain capable of event driven artificial spiking neural network computation in a fabric based environment. We demonstrate a wearable Brain SNN prototype with multi-layer computation, enabling scalability and flexibility in terms of modifications for hidden layers to be augmented to the network. The wearable micro Brain provides a low size, weight and power artificial on-garment intelligent wearable solution with embedded functionality enabling offline adaptive learning through the provision of interchangeable resistor synaptic weightings. The prototype has been evaluated for fault tolerance, where we have determine the robustness of the circuit when certain parts are damaged. Validations were also conducted for movements to determine if the circuit can still perform accurate computation. △ Less","25 February, 2022",https://arxiv.org/pdf/2202.12984
Photonic reinforcement learning based on optoelectronic reservoir computing,Kazutaka Kanno;Atsushi Uchida,"Reinforcement learning has been intensively investigated and developed in artificial intelligence in the absence of training data, such as autonomous driving vehicles, robot control, internet advertising, and elastic optical networks. However, the computational cost of reinforcement learning with deep neural networks is extremely high and reducing the learning cost is a challenging issue. We propose a photonic on-line implementation of reinforcement learning using optoelectronic delay-based reservoir computing, both experimentally and numerically. In the proposed scheme, we accelerate reinforcement learning at a rate of several megahertz because there is no required learning process for the internal connection weights in reservoir computing. We perform two benchmark tasks, CartPole-v0 and MountanCar-v0 tasks, to evaluate the proposed scheme. Our results represent the first hardware implementation of reinforcement learning based on photonic reservoir computing and pave the way for fast and efficient reinforcement learning as a novel photonic accelerator. △ Less","25 February, 2022",https://arxiv.org/pdf/2202.12896
A Systematic Literature Review about Idea Mining: The Use of Machine-driven Analytics to Generate Ideas,Workneh Y. Ayele;Gustaf Juell-Skielse,"Idea generation is the core activity of innovation. Digital data sources, which are sources of innovation, such as patents, publications, social media, websites, etc., are increasingly growing at unprecedented volume. Manual idea generation is time-consuming and is affected by the subjectivity of the individuals involved. Therefore, the use machine-driven data analytics techniques to analyze data to generate ideas and support idea generation by serving users is useful. The objective of this study is to study state-of the-art machine-driven analytics for idea generation and data sources, hence the result of this study will generally server as a guideline for choosing techniques and data sources. A systematic literature review is conducted to identify relevant scholarly literature from IEEE, Scopus, Web of Science and Google Scholar. We selected a total of 71 articles and analyzed them thematically. The results of this study indicate that idea generation through machine-driven analytics applies text mining, information retrieval (IR), artificial intelligence (AI), deep learning, machine learning, statistical techniques, natural language processing (NLP), NLP-based morphological analysis, network analysis, and bibliometric to support idea generation. The results include a list of techniques and procedures in idea generation through machine-driven idea analytics. Additionally, characterization and heuristics used in idea generation are summarized. For the future, tools designed to generate ideas could be explored. △ Less","30 January, 2022",https://arxiv.org/pdf/2202.12826
Brain Principles Programming,Evgenii Vityaev;Anton Kolonin;Andrey Kurpatov;Artem Molchanov,"In the monograph, STRONG ARTIFICIAL INTELLIGENCE. On the Approaches to Superintelligence, published by Sberbank, provides a cross-disciplinary review of general artificial intelligence. As an anthropomorphic direction of research, it considers Brain Principles Programming, BPP) the formalization of universal mechanisms (principles) of the brain's work with information, which are implemented at all levels of the organization of nervous tissue. This monograph provides a formalization of these principles in terms of the category theory. However, this formalization is not enough to develop algorithms for working with information. In this paper, for the description and modeling of Brain Principles Programming, it is proposed to apply mathematical models and algorithms developed by us earlier that model cognitive functions, which are based on well-known physiological, psychological and other natural science theories. The paper uses mathematical models and algorithms of the following theories: P.K.Anokhin's Theory of Functional Brain Systems, Eleonor Rosh's prototypical categorization theory, Bob Rehter's theory of causal models and natural classification. As a result, the formalization of the BPP is obtained and computer examples are given that demonstrate the algorithm's operation. △ Less","3 April, 2022",https://arxiv.org/pdf/2202.12710
"Bridging the Urban-Rural Connectivity Gap through Intelligent Space, Air, and Ground Networks",Fares Fourati;Saeed Hamood Alsamhi;Mohamed-Slim Alouini,"Connectivity in rural areas is one of the main challenges of communication networks. To overcome this challenge, a variety of solutions for different situations are required. Optimizing the current networking paradigms is therefore mandatory. The high costs of infrastructure and the low revenue of cell sites in rural areas compared with urban areas are especially unattractive for telecommunication operators. Therefore, space, air, and ground networks should all be optimized for achieving connectivity in rural areas. We highlight the latest works on rural connectivity, discuss the solutions for terrestrial networks, and study the potential benefits of nonterrestrial networks. Furthermore, we present an overview of artificial intelligence (AI) techniques for improving space, air, and ground networks, hence improving connectivity in rural areas. AI enables intelligent communications and can integrate space, air, and ground networks for rural connectivity. We discuss the rural connectivity challenges and highlight the latest projects and research and the empowerment of networks using AI. Finally, we discuss the potential positive impacts of providing connectivity to rural communities. △ Less","25 February, 2022",https://arxiv.org/pdf/2202.12683
"Deep Learning, Natural Language Processing, and Explainable Artificial Intelligence in the Biomedical Domain",Milad Moradi;Matthias Samwald,"In this article, we first give an introduction to artificial intelligence and its applications in biology and medicine in Section 1. Deep learning methods are then described in Section 2. We narrow down the focus of the study on textual data in Section 3, where natural language processing and its applications in the biomedical domain are described. In Section 4, we give an introduction to explainable artificial intelligence and discuss the importance of explainability of artificial intelligence systems, especially in the biomedical domain. △ Less","7 March, 2022",https://arxiv.org/pdf/2202.12678
Oscillatory Neural Network as Hetero-Associative Memory for Image Edge Detection,Madeleine Abernot;Thierry Gil;Aida Todri-Sanial,"The increasing amount of data to be processed on edge devices, such as cameras, has motivated Artificial Intelligence (AI) integration at the edge. Typical image processing methods performed at the edge, such as feature extraction or edge detection, use convolutional filters that are energy, computation, and memory hungry algorithms. But edge devices and cameras have scarce computational resources, bandwidth, and power and are limited due to privacy constraints to send data over to the cloud. Thus, there is a need to process image data at the edge. Over the years, this need has incited a lot of interest in implementing neuromorphic computing at the edge. Neuromorphic systems aim to emulate the biological neural functions to achieve energy-efficient computing. Recently, Oscillatory Neural Networks (ONN) present a novel brain-inspired computing approach by emulating brain oscillations to perform autoassociative memory types of applications. To speed up image edge detection and reduce its power consumption, we perform an in-depth investigation with ONNs. We propose a novel image processing method by using ONNs as a hetero-associative memory (HAM) for image edge detection. We simulate our ONN-HAM solution using first, a Matlab emulator, and then a fully digital ONN design. We show results on gray scale square evaluation maps, also on black and white and gray scale 28x28 MNIST images and finally on black and white 512x512 standard test images. We compare our solution with standard edge detection filters such as Sobel and Canny. Finally, using the fully digital design simulation results, we report on timing and resource characteristics, and evaluate its feasibility for real-time image processing applications. Our digital ONN-HAM solution can process images with up to 120x120 pixels (166 MHz system frequency) respecting real-time camera constraints. This work is the first to explore ONNs as hetero-associative memory for image processing applications. △ Less","25 February, 2022",https://arxiv.org/pdf/2202.12541
Matching Papers and Reviewers at Large Conferences,Kevin Leyton-Brown;Mausam;Yatin Nandwani;Hedayat Zarkoob;Chris Cameron;Neil Newman;Dinesh Raghu,"Peer-reviewed conferences, the main publication venues in CS, rely critically on matching highly qualified reviewers for each paper. Because of the growing scale of these conferences, the tight timelines on which they operate, and a recent surge in explicitly dishonest behavior, there is now no alternative to performing this matching in an automated way. This paper studies a novel reviewer-paper matching approach that was recently deployed in the 35th AAAI Conference on Artificial Intelligence (AAAI 2021), and has since been adopted (wholly or partially) by other conferences including ICML 2022, AAAI 2022, and IJCAI 2022. This approach has three main elements: (1) collecting and processing input data to identify problematic matches and generate reviewer-paper scores; (2) formulating and solving an optimization problem to find good reviewer-paper matchings; and (3) a two-phase reviewing process that shifts reviewing resources away from papers likely to be rejected and towards papers closer to the decision boundary. This paper also describes an evaluation of these innovations based on an extensive post-hoc analysis on real data -- including a comparison with the matching algorithm used in AAAI's previous (2020) iteration -- and supplements this with additional numerical experimentation. △ Less","5 August, 2022",https://arxiv.org/pdf/2202.12273
Is Neuro-Symbolic AI Meeting its Promise in Natural Language Processing? A Structured Review,Kyle Hamilton;Aparna Nayak;Bojan Božić;Luca Longo,"Advocates for Neuro-Symbolic Artificial Intelligence (NeSy) assert that combining deep learning with symbolic reasoning will lead to stronger AI than either paradigm on its own. As successful as deep learning has been, it is generally accepted that even our best deep learning systems are not very good at abstract reasoning. And since reasoning is inextricably linked to language, it makes intuitive sense that Natural Language Processing (NLP), would be a particularly well-suited candidate for NeSy. We conduct a structured review of studies implementing NeSy for NLP, with the aim of answering the question of whether NeSy is indeed meeting its promises: reasoning, out-of-distribution generalization, interpretability, learning and reasoning from small data, and transferability to new domains. We examine the impact of knowledge representation, such as rules and semantic networks, language structure and relational structure, and whether implicit or explicit reasoning contributes to higher promise scores. We find that systems where logic is compiled into the neural network lead to the most NeSy goals being satisfied, while other factors such as knowledge representation, or type of neural architecture do not exhibit a clear correlation with goals being met. We find many discrepancies in how reasoning is defined, specifically in relation to human level reasoning, which impact decisions about model architectures and drive conclusions which are not always consistent across studies. Hence we advocate for a more methodical approach to the application of theories of human reasoning as well as the development of appropriate benchmarks, which we hope can lead to a better understanding of progress in the field. We make our data and code available on github for further analysis. △ Less","30 June, 2022",https://arxiv.org/pdf/2202.12205
Exploring the Unfairness of DP-SGD Across Settings,Frederik Noe;Rasmus Herskind;Anders Søgaard,"End users and regulators require private and fair artificial intelligence models, but previous work suggests these objectives may be at odds. We use the CivilComments to evaluate the impact of applying the {\em de facto} standard approach to privacy, DP-SGD, across several fairness metrics. We evaluate three implementations of DP-SGD: for dimensionality reduction (PCA), linear classification (logistic regression), and robust deep learning (Group-DRO). We establish a negative, logarithmic correlation between privacy and fairness in the case of linear classification and robust deep learning. DP-SGD had no significant impact on fairness for PCA, but upon inspection, also did not seem to lead to private representations. △ Less","24 February, 2022",https://arxiv.org/pdf/2202.12058
Computer Aided Diagnosis and Out-of-Distribution Detection in Glaucoma Screening Using Color Fundus Photography,Satoshi Kondo;Satoshi Kasai;Kosuke Hirasawa,"Artificial Intelligence for RObust Glaucoma Screening (AIROGS) Challenge is held for developing solutions for glaucoma screening from color fundus photography that are robust to real-world scenarios. This report describes our method submitted to the AIROGS challenge. Our method employs convolutional neural networks to classify input images to ""referable glaucoma"" or ""no referable glaucoma"". In addition, we introduce an inference-time out-of-distribution (OOD) detection method to identify ungradable images. Our OOD detection is based on an energy-based method combined with activation rectification. △ Less","24 February, 2022",https://arxiv.org/pdf/2202.11944
Towards Tailored Models on Private AIoT Devices: Federated Direct Neural Architecture Search,Chunhui Zhang;Xiaoming Yuan;Qianyun Zhang;Guangxu Zhu;Lei Cheng;Ning Zhang,"Neural networks often encounter various stringent resource constraints while deploying on edge devices. To tackle these problems with less human efforts, automated machine learning becomes popular in finding various neural architectures that fit diverse Artificial Intelligence of Things (AIoT) scenarios. Recently, to prevent the leakage of private information while enable automated machine intelligence, there is an emerging trend to integrate federated learning and neural architecture search (NAS). Although promising as it may seem, the coupling of difficulties from both tenets makes the algorithm development quite challenging. In particular, how to efficiently search the optimal neural architecture directly from massive non-independent and identically distributed (non-IID) data among AIoT devices in a federated manner is a hard nut to crack. In this paper, to tackle this challenge, by leveraging the advances in ProxylessNAS, we propose a Federated Direct Neural Architecture Search (FDNAS) framework that allows for hardware-friendly NAS from non- IID data across devices. To further adapt to both various data distributions and different types of devices with heterogeneous embedded hardware platforms, inspired by meta-learning, a Cluster Federated Direct Neural Architecture Search (CFDNAS) framework is proposed to achieve device-aware NAS, in the sense that each device can learn a tailored deep learning model for its particular data distribution and hardware constraint. Extensive experiments on non-IID datasets have shown the state-of-the-art accuracy-efficiency trade-offs achieved by the proposed solution in the presence of both data and device heterogeneity. △ Less","23 February, 2022",https://arxiv.org/pdf/2202.11490
Deep Learning Reproducibility and Explainable AI (XAI),A. -M. Leventi-Peetz;T. Östreich,"The nondeterminism of Deep Learning (DL) training algorithms and its influence on the explainability of neural network (NN) models are investigated in this work with the help of image classification examples. To discuss the issue, two convolutional neural networks (CNN) have been trained and their results compared. The comparison serves the exploration of the feasibility of creating deterministic, robust DL models and deterministic explainable artificial intelligence (XAI) in practice. Successes and limitation of all here carried out efforts are described in detail. The source code of the attained deterministic models has been listed in this work. Reproducibility is indexed as a development-phase-component of the Model Governance Framework, proposed by the EU within their excellence in AI approach. Furthermore, reproducibility is a requirement for establishing causality for the interpretation of model results and building of trust towards the overwhelming expansion of AI systems applications. Problems that have to be solved on the way to reproducibility and ways to deal with some of them, are examined in this work. △ Less","2 March, 2022",https://arxiv.org/pdf/2202.11452
Neural Network based Successor Representations of Space and Language,Paul Stoewer;Christian Schlieker;Achim Schilling;Claus Metzner;Andreas Maier;Patrick Krauss,"How does the mind organize thoughts? The hippocampal-entorhinal complex is thought to support domain-general representation and processing of structural knowledge of arbitrary state, feature and concept spaces. In particular, it enables the formation of cognitive maps, and navigation on these maps, thereby broadly contributing to cognition. It has been proposed that the concept of multi-scale successor representations provides an explanation of the underlying computations performed by place and grid cells. Here, we present a neural network based approach to learn such representations, and its application to different scenarios: a spatial exploration task based on supervised learning, a spatial navigation task based on reinforcement learning, and a non-spatial task where linguistic constructions have to be inferred by observing sample sentences. In all scenarios, the neural network correctly learns and approximates the underlying structure by building successor representations. Furthermore, the resulting neural firing patterns are strikingly similar to experimentally observed place and grid cell firing patterns. We conclude that cognitive maps and neural network-based successor representations of structured knowledge provide a promising way to overcome some of the short comings of deep learning towards artificial general intelligence. △ Less","22 February, 2022",https://arxiv.org/pdf/2202.11190
Enabling Reproducibility and Meta-learning Through a Lifelong Database of Experiments (LDE),Jason Tsay;Andrea Bartezzaghi;Aleke Nolte;Cristiano Malossi,"Artificial Intelligence (AI) development is inherently iterative and experimental. Over the course of normal development, especially with the advent of automated AI, hundreds or thousands of experiments are generated and are often lost or never examined again. There is a lost opportunity to document these experiments and learn from them at scale, but the complexity of tracking and reproducing these experiments is often prohibitive to data scientists. We present the Lifelong Database of Experiments (LDE) that automatically extracts and stores linked metadata from experiment artifacts and provides features to reproduce these artifacts and perform meta-learning across them. We store context from multiple stages of the AI development lifecycle including datasets, pipelines, how each is configured, and training runs with information about their runtime environment. The standardized nature of the stored metadata allows for querying and aggregation, especially in terms of ranking artifacts by performance metrics. We exhibit the capabilities of the LDE by reproducing an existing meta-learning study and storing the reproduced metadata in our system. Then, we perform two experiments on this metadata: 1) examining the reproducibility and variability of the performance metrics and 2) implementing a number of meta-learning algorithms on top of the data and examining how variability in experimental results impacts recommendation performance. The experimental results suggest significant variation in performance, especially depending on dataset configurations; this variation carries over when meta-learning is built on top of the results, with performance improving when using aggregated results. This suggests that a system that automatically collects and aggregates results such as the LDE not only assists in implementing meta-learning but may also improve its performance. △ Less","23 February, 2022",https://arxiv.org/pdf/2202.10979
Recognizing Concepts and Recognizing Musical Themes. A Quantum Semantic Analysis,Maria Luisa Dalla Chiara;Roberto Giuntini;Eleonora Negri;Giuseppe Sergioli,"How are abstract concepts and musical themes recognized on the basis of some previous experience? It is interesting to compare the different behaviors of human and of artificial intelligences with respect to this problem. Generally, a human mind that abstracts a concept (say, table) from a given set of known examples creates a table-Gestalt: a kind of vague and out of focus image that does not fully correspond to a particular table with well determined features. A similar situation arises in the case of musical themes. Can the construction of a gestaltic pattern, which is so natural for human minds, be taught to an intelligent machine? This problem can be successfully discussed in the framework of a quantum approach to pattern recognition and to machine learning. The basic idea is replacing classical data sets with quantum data sets, where either objects or musical themes can be formally represented as pieces of quantum information, involving the uncertainties and the ambiguities that characterize the quantum world. In this framework, the intuitive concept of Gestalt can be simulated by the mathematical concept of positive centroid of a given quantum data set. Accordingly, the crucial problem ""how can we classify a new object or a new musical theme (we have listened to) on the basis of a previous experience?"" can be dealt with in terms of some special quantum similarity-relations. Although recognition procedures are different for human and for artificial intelligences, there is a common method of ""facing the problems"" that seems to work in both cases. △ Less","17 February, 2022",https://arxiv.org/pdf/2202.10941
Social Computational Design Method for Generating Product Shapes with GAN and Transformer Models,Maolin Yang;Pingyu Jiang,"A social computational design method is established, aiming at taking advantages of the fast-developing artificial intelligence technologies for intelligent product design. Supported with multi-agent system, shape grammar, Generative adversarial network, Bayesian network, Transformer, etc., the method is able to define the design solution space, prepare training samples, and eventually acquire an intelligent model that can recommend design solutions according to incomplete solutions for given design tasks. Product shape design is used as entry point to demonstrate the method, however, the method can be applied to tasks rather than shape design when the solutions can be properly coded. △ Less","22 February, 2022",https://arxiv.org/pdf/2202.10774
Adversarial Attacks on Speech Recognition Systems for Mission-Critical Applications: A Survey,Ngoc Dung Huynh;Mohamed Reda Bouadjenek;Imran Razzak;Kevin Lee;Chetan Arora;Ali Hassani;Arkady Zaslavsky,"A Machine-Critical Application is a system that is fundamentally necessary to the success of specific and sensitive operations such as search and recovery, rescue, military, and emergency management actions. Recent advances in Machine Learning, Natural Language Processing, voice recognition, and speech processing technologies have naturally allowed the development and deployment of speech-based conversational interfaces to interact with various machine-critical applications. While these conversational interfaces have allowed users to give voice commands to carry out strategic and critical activities, their robustness to adversarial attacks remains uncertain and unclear. Indeed, Adversarial Artificial Intelligence (AI) which refers to a set of techniques that attempt to fool machine learning models with deceptive data, is a growing threat in the AI and machine learning research community, in particular for machine-critical applications. The most common reason of adversarial attacks is to cause a malfunction in a machine learning model. An adversarial attack might entail presenting a model with inaccurate or fabricated samples as it's training data, or introducing maliciously designed data to deceive an already trained model. While focusing on speech recognition for machine-critical applications, in this paper, we first review existing speech recognition techniques, then, we investigate the effectiveness of adversarial attacks and defenses against these systems, before outlining research challenges, defense recommendations, and future work. This paper is expected to serve researchers and practitioners as a reference to help them in understanding the challenges, position themselves and, ultimately, help them to improve existing models of speech recognition for mission-critical applications. Keywords: Mission-Critical Applications, Adversarial AI, Speech Recognition Systems. △ Less","21 February, 2022",https://arxiv.org/pdf/2202.10594
Human-in-the-loop Machine Learning: A Macro-Micro Perspective,Jiangtao Wang;Bin Guo;Liming Chen,"Though technical advance of artificial intelligence and machine learning has enabled many promising intelligent systems, many computing tasks are still not able to be fully accomplished by machine intelligence. Motivated by the complementary nature of human and machine intelligence, an emerging trend is to involve humans in the loop of machine learning and decision-making. In this paper, we provide a macro-micro review of human-in-the-loop machine learning. We first describe major machine learning challenges which can be addressed by human intervention in the loop. Then we examine closely the latest research and findings of introducing humans into each step of the lifecycle of machine learning. Finally, we analyze current research gaps and point out future research directions. △ Less","21 February, 2022",https://arxiv.org/pdf/2202.10564
Guidelines and Evaluation of Clinical Explainable AI in Medical Image Analysis,Weina Jin;Xiaoxiao Li;Mostafa Fatehi;Ghassan Hamarneh,"Explainable artificial intelligence (XAI) is essential for enabling clinical users to get informed decision support from AI and comply with evidence-based medical practice. Applying XAI in clinical settings requires proper evaluation criteria to ensure the explanation technique is both technically sound and clinically useful, but specific support is lacking to achieve this goal. To bridge the research gap, we propose the Clinical XAI Guidelines that consist of five criteria a clinical XAI needs to be optimized for. The guidelines recommend choosing an explanation form based on Guideline 1 (G1) Understandability and G2 Clinical relevance. For the chosen explanation form, its specific XAI technique should be optimized for G3 Truthfulness, G4 Informative plausibility, and G5 Computational efficiency. Following the guidelines, we conducted a systematic evaluation on a novel problem of multi-modal medical image explanation with two clinical tasks, and proposed new evaluation metrics accordingly. Sixteen commonly-used heatmap XAI techniques were evaluated and found to be insufficient for clinical use due to their failure in G3 and G4. Our evaluation demonstrated the use of Clinical XAI Guidelines to support the design and evaluation of clinically viable XAI. △ Less","8 December, 2022",https://arxiv.org/pdf/2202.10553
"Towards technological adaptation of advanced farming through AI, IoT, and Robotics: A Comprehensive overview",Md. Mahadi Hasan;Muhammad Usama Islam;Muhammad Jafar Sadeq,"The population explosion of the 21st century has adversely affected the natural resources with restricted availability of cultivable land, increased average temperatures due to global warming, and carbon footprint resulting in a drastic increase in floods as well as droughts thus making food security significant anxiety for most countries. The traditional methods were no longer sufficient which paved the way for technological ascents such as a substantial rise in Artificial Intelligence (AI), Internet of Things (IoT), as well as Robotics that provides high productivity, functional efficiency, flexibility, cost-effectiveness in the domain of agriculture. AI, IoT, and Robotics-based devices and methods have produced new paradigms and opportunities in agriculture. AI's existing approaches are soil management, crop diseases identification, weed identification, and management in collaboration with IoT devices. IoT has utilized automatic agricultural operations and real-time monitoring with few personnel employed in real-time. The major existing applications of agricultural robotics are for the function of soil preparation, planting, monitoring, harvesting, and storage. In this paper, researchers have explored a comprehensive overview of recent implementation, scopes, opportunities, challenges, limitations, and future research instructions of AI, IoT, and Robotics based methodology in the agriculture sector. △ Less","21 February, 2022",https://arxiv.org/pdf/2202.10459
A Tutorial on Adversarial Learning Attacks and Countermeasures,Cato Pauling;Michael Gimson;Muhammed Qaid;Ahmad Kida;Basel Halak,"Machine learning algorithms are used to construct a mathematical model for a system based on training data. Such a model is capable of making highly accurate predictions without being explicitly programmed to do so. These techniques have a great many applications in all areas of the modern digital economy and artificial intelligence. More importantly, these methods are essential for a rapidly increasing number of safety-critical applications such as autonomous vehicles and intelligent defense systems. However, emerging adversarial learning attacks pose a serious security threat that greatly undermines further such systems. The latter are classified into four types, evasion (manipulating data to avoid detection), poisoning (injection malicious training samples to disrupt retraining), model stealing (extraction), and inference (leveraging over-generalization on training data). Understanding this type of attacks is a crucial first step for the development of effective countermeasures. The paper provides a detailed tutorial on the principles of adversarial machining learning, explains the different attack scenarios, and gives an in-depth insight into the state-of-art defense mechanisms against this rising threat . △ Less","21 February, 2022",https://arxiv.org/pdf/2202.10377
Cyber-Physical Defense in the Quantum Era,Michel Barbeau;Joaquin Garcia-Alfaro,"Networked-Control Systems (NCSs), a type of cyber-physical systems, consist of tightly integrated computing, communication and control technologies. While being very flexible environments, they are vulnerable to computing and networking attacks. Recent NCSs hacking incidents had major impact. They call for more research on cyber-physical security. Fears about the use of quantum computing to break current cryptosystems make matters worse. While the quantum threat motivated the creation of new disciplines to handle the issue, such as post-quantum cryptography, other fields have overlooked the existence of quantum-enabled adversaries. This is the case of cyber-physical defense research, a distinct but complementary discipline to cyber-physical protection. Cyber-physical defense refers to the capability to detect and react in response to cyber-physical attacks. Concretely, it involves the integration of mechanisms to identify adverse events and prepare response plans, during and after incidents occur. In this paper, we make the assumption that the eventually available quantum computer will provide an advantage to adversaries against defenders, unless they also adopt this technology. We envision the necessity for a paradigm shift, where an increase of adversarial resources because of quantum supremacy does not translate into higher likelihood of disruptions. Consistently with current system design practices in other areas, such as the use of artificial intelligence for the reinforcement of attack detection tools, we outline a vision for next generation cyber-physical defense layers leveraging ideas from quantum computing and machine learning. Through an example, we show that defenders of NCSs can learn and improve their strategies to anticipate and recover from attacks. △ Less","21 February, 2022",https://arxiv.org/pdf/2202.10354
Artificial Intelligence for the Metaverse: A Survey,Thien Huynh-The;Quoc-Viet Pham;Xuan-Qui Pham;Thanh Thi Nguyen;Zhu Han;Dong-Seong Kim,"Along with the massive growth of the Internet from the 1990s until now, various innovative technologies have been created to bring users breathtaking experiences with more virtual interactions in cyberspace. Many virtual environments with thousands of services and applications, from social networks to virtual gaming worlds, have been developed with immersive experience and digital transformation, but most are incoherent instead of being integrated into a platform. In this context, metaverse, a term formed by combining meta and universe, has been introduced as a shared virtual world that is fueled by many emerging technologies, such as fifth-generation networks and beyond, virtual reality, and artificial intelligence (AI). Among such technologies, AI has shown the great importance of processing big data to enhance immersive experience and enable human-like intelligence of virtual agents. In this survey, we make a beneficial effort to explore the role of AI in the foundation and development of the metaverse. We first deliver a preliminary of AI, including machine learning algorithms and deep learning architectures, and its role in the metaverse. We then convey a comprehensive investigation of AI-based methods concerning six technical aspects that have potentials for the metaverse: natural language processing, machine vision, blockchain, networking, digital twin, and neural interface, and being potential for the metaverse. Subsequently, several AI-aided applications, such as healthcare, manufacturing, smart cities, and gaming, are studied to be deployed in the virtual worlds. Finally, we conclude the key contribution of this survey and open some future research directions in AI for the metaverse. △ Less","14 February, 2022",https://arxiv.org/pdf/2202.10336
OpenRAN Gym: An Open Toolbox for Data Collection and Experimentation with AI in O-RAN,Leonardo Bonati;Michele Polese;Salvatore D'Oro;Stefano Basagni;Tommaso Melodia,"Open Radio Access Network (RAN) architectures will enable interoperability, openness, and programmatic data-driven control in next generation cellular networks. However, developing scalable and efficient data-driven algorithms that can generalize across diverse deployments and optimize RAN performance is a complex feat, largely unaddressed as of today. Specifically, the ability to design efficient data-driven algorithms for network control and inference requires at a minimum (i) access to large, rich, and heterogeneous datasets; (ii) testing at scale in controlled but realistic environments, and (iii) software pipelines to automate data collection and experimentation. To facilitate these tasks, in this paper we propose OpenRAN Gym, a practical, open, experimental toolbox that provides end-to-end design, data collection, and testing workflows for intelligent control in next generation Open RAN systems. OpenRAN Gym builds on software frameworks for the collection of large datasets and RAN control, and on a lightweight O-RAN environment for experimental wireless platforms. We first provide an overview of OpenRAN Gym and then describe how it can be used to collect data, to design and train artificial intelligence and machine learning-based O-RAN applications (xApps), and to test xApps on a softwarized RAN. Then, we provide an example of two xApps designed with OpenRAN Gym and used to control a large-scale network with 7 base stations and 42 users deployed on the Colosseum testbed. OpenRAN Gym and its software components are open source and publicly-available to the research community. △ Less","21 February, 2022",https://arxiv.org/pdf/2202.10318
Machine Learning Operations: A Survey on MLOps Tool Support,Nipuni Hewage;Dulani Meedeniya,"Machine Learning (ML) has become a fast-growing, trending approach in solution development in practice. Deep Learning (DL) which is a subset of ML, learns using deep neural networks to simulate the human brain. It trains machines to learn techniques and processes individually using computer algorithms, which is also considered to be a role of Artificial Intelligence (AI). In this paper, we study current technical issues related to software development and delivery in organizations that work on ML projects. Therefore, the importance of the Machine Learning Operations (MLOps) concept, which can deliver appropriate solutions for such concerns, is discussed. We investigate commercially available MLOps tool support in software development. The comparison between MLOps tools analyzes the performance of each system and its use cases. Moreover, we examine the features and usability of MLOps tools to identify the most appropriate tool support for given scenarios. Finally, we recognize that there is a shortage in the availability of a fully functional MLOps platform on which processes can be automated by reducing human intervention. △ Less","22 February, 2022",https://arxiv.org/pdf/2202.10169
Applications of blockchain and artificial intelligence technologies for enabling prosumers in smart grids: A review,Weiqi Hua;Ying Chen;Meysam Qadrdan;Jing Jiang;Hongjian Sun;Jianzhong Wu,"Governments' net zero emission target aims at increasing the share of renewable energy sources as well as influencing the behaviours of consumers to support the cost-effective balancing of energy supply and demand. These will be achieved by the advanced information and control infrastructures of smart grids which allow the interoperability among various stakeholders. Under this circumstance, increasing number of consumers produce, store, and consume energy, giving them a new role of prosumers. The integration of prosumers and accommodation of incurred bidirectional flows of energy and information rely on two key factors: flexible structures of energy markets and intelligent operations of power systems. The blockchain and artificial intelligence (AI) are innovative technologies to fulfil these two factors, by which the blockchain provides decentralised trading platforms for energy markets and the AI supports the optimal operational control of power systems. This paper attempts to address how to incorporate the blockchain and AI in the smart grids for facilitating prosumers to participate in energy markets. To achieve this objective, first, this paper reviews how policy designs price carbon emissions caused by the fossil-fuel based generation so as to facilitate the integration of prosumers with renewable energy sources. Second, the potential structures of energy markets with the support of the blockchain technologies are discussed. Last, how to apply the AI for enhancing the state monitoring and decision making during the operations of power systems is introduced. △ Less","21 February, 2022",https://arxiv.org/pdf/2202.10098
Two-Stage Auction Mechanism for Long-Term Participation in Crowdsourcing,Timothy Shin Heng Mak;Albert Y. S. Lam,"Crowdsourcing has become an important tool to collect data for various artificial intelligence applications and auction can be an effective way to allocate work and determine reward in a crowdsourcing platform. In this paper, we focus on the crowdsourcing of small tasks such as image labelling and voice recording where we face a number of challenges. First, workers have different limits on the amount of work they would be willing to do, and they may also misreport these limits in their bid for work. Secondly, if the auction is repeated over time, unsuccessful workers may drop out of the system, reducing competition and diversity. To tackle these issues, we first extend the results of the celebrated Myerson's optimal auction mechanism for a single-parameter bid to the case where the bid consists of the unit cost of work, the maximum amount of work one is willing to do, and the actual work completed. We show that a simple payment mechanism is sufficient to ensure a dominant strategy from the workers, and that this dominant strategy is robust to the true utility function of the workers. Secondly, we propose a novel, flexible work allocation mechanism, which allows the requester to balance between cost efficiency and equality. While cost minimization is obviously important, encouraging equality in the allocation of work increases the diversity of the workforce as well as promotes long-term participation on the crowdsourcing platform. Our main results are proved analytically and validated through simulations. △ Less","21 February, 2022",https://arxiv.org/pdf/2202.10064
Cooperative Artificial Intelligence,Tobias Baumann,"In the future, artificial learning agents are likely to become increasingly widespread in our society. They will interact with both other learning agents and humans in a variety of complex settings including social dilemmas. We argue that there is a need for research on the intersection between game theory and artificial intelligence, with the goal of achieving cooperative artificial intelligence that can navigate social dilemmas well. We consider the problem of how an external agent can promote cooperation between artificial learners by distributing additional rewards and punishments based on observing the actions of the learners. We propose a rule for automatically learning how to create the right incentives by considering the anticipated parameter updates of each agent. Using this learning rule leads to cooperation with high social welfare in matrix games in which the agents would otherwise learn to defect with high probability. We show that the resulting cooperative outcome is stable in certain games even if the planning agent is turned off after a given number of episodes, while other games require ongoing intervention to maintain mutual cooperation. Finally, we reflect on what the goals of multi-agent reinforcement learning should be in the first place, and discuss the necessary building blocks towards the goal of building cooperative AI. △ Less","20 February, 2022",https://arxiv.org/pdf/2202.09859
Automated Reasoning in Non-classical Logics in the TPTP World,Alexander Steen;David Fuenmayor;Tobias Gleißner;Geoff Sutcliffe;Christoph Benzmüller,"Non-classical logics are used in a wide spectrum of disciplines, including artificial intelligence, computer science, mathematics, and philosophy. The de-facto standard infrastructure for automated theorem proving, the TPTP World, currently supports only classical logics. Similar standards for non-classical logic reasoning do not exist (yet). This hampers practical development of reasoning systems, and limits their interoperability and application. This paper describes the latest extension of the TPTP World, which provides languages and infrastructure for reasoning in non-classical logics. The extensions integrate seamlessly with the existing TPTP World. △ Less","20 February, 2022",https://arxiv.org/pdf/2202.09836
"Attacks, Defenses, And Tools: A Framework To Facilitate Robust AI/ML Systems",Mohamad Fazelnia;Igor Khokhlov;Mehdi Mirakhorli,"Software systems are increasingly relying on Artificial Intelligence (AI) and Machine Learning (ML) components. The emerging popularity of AI techniques in various application domains attracts malicious actors and adversaries. Therefore, the developers of AI-enabled software systems need to take into account various novel cyber-attacks and vulnerabilities that these systems may be susceptible to. This paper presents a framework to characterize attacks and weaknesses associated with AI-enabled systems and provide mitigation techniques and defense strategies. This framework aims to support software designers in taking proactive measures in developing AI-enabled software, understanding the attack surface of such systems, and developing products that are resilient to various emerging attacks associated with ML. The developed framework covers a broad spectrum of attacks, mitigation techniques, and defensive and offensive tools. In this paper, we demonstrate the framework architecture and its major components, describe their attributes, and discuss the long-term goals of this research. △ Less","18 February, 2022",https://arxiv.org/pdf/2202.09465
System Safety and Artificial Intelligence,Roel I. J. Dobbe,"This chapter formulates seven lessons for preventing harm in artificial intelligence (AI) systems based on insights from the field of system safety for software-based automation in safety-critical domains. New applications of AI across societal domains and public organizations and infrastructures come with new hazards, which lead to new forms of harm, both grave and pernicious. The text addresses the lack of consensus for diagnosing and eliminating new AI system hazards. For decades, the field of system safety has dealt with accidents and harm in safety-critical systems governed by varying degrees of software-based automation and decision-making. This field embraces the core assumption of systems and control that AI systems cannot be safeguarded by technical design choices on the model or algorithm alone, instead requiring an end-to-end hazard analysis and design frame that includes the context of use, impacted stakeholders and the formal and informal institutional environment in which the system operates. Safety and other values are then inherently socio-technical and emergent system properties that require design and control measures to instantiate these across the technical, social and institutional components of a system. This chapter honors system safety pioneer Nancy Leveson, by situating her core lessons for today's AI system safety challenges. For every lesson, concrete tools are offered for rethinking and reorganizing the safety management of AI systems, both in design and governance. This history tells us that effective AI safety management requires transdisciplinary approaches and a shared language that allows involvement of all levels of society. △ Less","18 February, 2022",https://arxiv.org/pdf/2202.09292
REFUGE2 Challenge: A Treasure Trove for Multi-Dimension Analysis and Evaluation in Glaucoma Screening,Huihui Fang;Fei Li;Junde Wu;Huazhu Fu;Xu Sun;Jaemin Son;Shuang Yu;Menglu Zhang;Chenglang Yuan;Cheng Bian;Baiying Lei;Benjian Zhao;Xinxing Xu;Shaohua Li;Francisco Fumero;José Sigut;Haidar Almubarak;Yakoub Bazi;Yuanhao Guo;Yating Zhou;Ujjwal Baid;Shubham Innani;Tianjiao Guo;Jie Yang;José Ignacio Orlando,"With the rapid development of artificial intelligence (AI) in medical image processing, deep learning in color fundus photography (CFP) analysis is also evolving. Although there are some open-source, labeled datasets of CFPs in the ophthalmology community, large-scale datasets for screening only have labels of disease categories, and datasets with annotations of fundus structures are usually small in size. In addition, labeling standards are not uniform across datasets, and there is no clear information on the acquisition device. Here we release a multi-annotation, multi-quality, and multi-device color fundus image dataset for glaucoma analysis on an original challenge -- Retinal Fundus Glaucoma Challenge 2nd Edition (REFUGE2). The REFUGE2 dataset contains 2000 color fundus images with annotations of glaucoma classification, optic disc/cup segmentation, as well as fovea localization. Meanwhile, the REFUGE2 challenge sets three sub-tasks of automatic glaucoma diagnosis and fundus structure analysis and provides an online evaluation framework. Based on the characteristics of multi-device and multi-quality data, some methods with strong generalizations are provided in the challenge to make the predictions more robust. This shows that REFUGE2 brings attention to the characteristics of real-world multi-domain data, bridging the gap between scientific research and clinical application. △ Less","29 December, 2022",https://arxiv.org/pdf/2202.08994
Multimodal Emotion Recognition using Transfer Learning from Speaker Recognition and BERT-based models,Sarala Padi;Seyed Omid Sadjadi;Dinesh Manocha;Ram D. Sriram,"Automatic emotion recognition plays a key role in computer-human interaction as it has the potential to enrich the next-generation artificial intelligence with emotional intelligence. It finds applications in customer and/or representative behavior analysis in call centers, gaming, personal assistants, and social robots, to mention a few. Therefore, there has been an increasing demand to develop robust automatic methods to analyze and recognize the various emotions. In this paper, we propose a neural network-based emotion recognition framework that uses a late fusion of transfer-learned and fine-tuned models from speech and text modalities. More specifically, we i) adapt a residual network (ResNet) based model trained on a large-scale speaker recognition task using transfer learning along with a spectrogram augmentation approach to recognize emotions from speech, and ii) use a fine-tuned bidirectional encoder representations from transformers (BERT) based model to represent and recognize emotions from the text. The proposed system then combines the ResNet and BERT-based model scores using a late fusion strategy to further improve the emotion recognition performance. The proposed multimodal solution addresses the data scarcity limitation in emotion recognition using transfer learning, data augmentation, and fine-tuning, thereby improving the generalization performance of the emotion recognition models. We evaluate the effectiveness of our proposed multimodal approach on the interactive emotional dyadic motion capture (IEMOCAP) dataset. Experimental results indicate that both audio and text-based models improve the emotion recognition performance and that the proposed multimodal solution achieves state-of-the-art results on the IEMOCAP benchmark. △ Less","15 February, 2022",https://arxiv.org/pdf/2202.08974
Transformer for Graphs: An Overview from Architecture Perspective,Erxue Min;Runfa Chen;Yatao Bian;Tingyang Xu;Kangfei Zhao;Wenbing Huang;Peilin Zhao;Junzhou Huang;Sophia Ananiadou;Yu Rong,"Recently, Transformer model, which has achieved great success in many artificial intelligence fields, has demonstrated its great potential in modeling graph-structured data. Till now, a great variety of Transformers has been proposed to adapt to the graph-structured data. However, a comprehensive literature review and systematical evaluation of these Transformer variants for graphs are still unavailable. It's imperative to sort out the existing Transformer models for graphs and systematically investigate their effectiveness on various graph tasks. In this survey, we provide a comprehensive review of various Graph Transformer models from the architectural design perspective. We first disassemble the existing models and conclude three typical ways to incorporate the graph information into the vanilla Transformer: 1) GNNs as Auxiliary Modules, 2) Improved Positional Embedding from Graphs, and 3) Improved Attention Matrix from Graphs. Furthermore, we implement the representative components in three groups and conduct a comprehensive comparison on various kinds of famous graph data benchmarks to investigate the real performance gain of each component. Our experiments confirm the benefits of current graph-specific modules on Transformer and reveal their advantages on different kinds of graph tasks. △ Less","17 February, 2022",https://arxiv.org/pdf/2202.08455
The Quarks of Attention,Pierre Baldi;Roman Vershynin,"Attention plays a fundamental role in both natural and artificial intelligence systems. In deep learning, attention-based neural architectures, such as transformer architectures, are widely used to tackle problems in natural language processing and beyond. Here we investigate the fundamental building blocks of attention and their computational properties. Within the standard model of deep learning, we classify all possible fundamental building blocks of attention in terms of their source, target, and computational mechanism. We identify and study three most important mechanisms: additive activation attention, multiplicative output attention (output gating), and multiplicative synaptic attention (synaptic gating). The gating mechanisms correspond to multiplicative extensions of the standard model and are used across all current attention-based deep learning architectures. We study their functional properties and estimate the capacity of several attentional building blocks in the case of linear and polynomial threshold gates. Surprisingly, additive activation attention plays a central role in the proofs of the lower bounds. Attention mechanisms reduce the depth of certain basic circuits and leverage the power of quadratic activations without incurring their full cost. △ Less","15 February, 2022",https://arxiv.org/pdf/2202.08371
XAI in the context of Predictive Process Monitoring: Too much to Reveal,Ghada Elkhawaga;Mervat Abuelkheir;Manfred Reichert,"Predictive Process Monitoring (PPM) has been integrated into process mining tools as a value-adding task. PPM provides useful predictions on the further execution of the running business processes. To this end, machine learning-based techniques are widely employed in the context of PPM. In order to gain stakeholders trust and advocacy of PPM predictions, eXplainable Artificial Intelligence (XAI) methods are employed in order to compensate for the lack of transparency of most efficient predictive models. Even when employed under the same settings regarding data, preprocessing techniques, and ML models, explanations generated by multiple XAI methods differ profoundly. A comparison is missing to distinguish XAI characteristics or underlying conditions that are deterministic to an explanation. To address this gap, we provide a framework to enable studying the effect of different PPM-related settings and ML model-related choices on characteristics and expressiveness of resulting explanations. In addition, we compare how different explainability methods characteristics can shape resulting explanations and enable reflecting underlying model reasoning process △ Less","16 February, 2022",https://arxiv.org/pdf/2202.08265
The Adversarial Security Mitigations of mmWave Beamforming Prediction Models using Defensive Distillation and Adversarial Retraining,Murat Kuzlu;Ferhat Ozgur Catak;Umit Cali;Evren Catak;Ozgur Guler,"The design of a security scheme for beamforming prediction is critical for next-generation wireless networks (5G, 6G, and beyond). However, there is no consensus about protecting the beamforming prediction using deep learning algorithms in these networks. This paper presents the security vulnerabilities in deep learning for beamforming prediction using deep neural networks (DNNs) in 6G wireless networks, which treats the beamforming prediction as a multi-output regression problem. It is indicated that the initial DNN model is vulnerable against adversarial attacks, such as Fast Gradient Sign Method (FGSM), Basic Iterative Method (BIM), Projected Gradient Descent (PGD), and Momentum Iterative Method (MIM), because the initial DNN model is sensitive to the perturbations of the adversarial samples of the training data. This study also offers two mitigation methods, such as adversarial training and defensive distillation, for adversarial attacks against artificial intelligence (AI)-based models used in the millimeter-wave (mmWave) beamforming prediction. Furthermore, the proposed scheme can be used in situations where the data are corrupted due to the adversarial examples in the training data. Experimental results show that the proposed methods effectively defend the DNN models against adversarial attacks in next-generation wireless networks. △ Less","16 February, 2022",https://arxiv.org/pdf/2202.08185
Bias and unfairness in machine learning models: a systematic literature review,Tiago Palma Pagano;Rafael Bessa Loureiro;Fernanda Vitória Nascimento Lisboa;Gustavo Oliveira Ramos Cruz;Rodrigo Matos Peixoto;Guilherme Aragão de Sousa Guimarães;Lucas Lisboa dos Santos;Maira Matos Araujo;Marco Cruz;Ewerton Lopes Silva de Oliveira;Ingrid Winkler;Erick Giovani Sperandio Nascimento,"One of the difficulties of artificial intelligence is to ensure that model decisions are fair and free of bias. In research, datasets, metrics, techniques, and tools are applied to detect and mitigate algorithmic unfairness and bias. This study aims to examine existing knowledge on bias and unfairness in Machine Learning models, identifying mitigation methods, fairness metrics, and supporting tools. A Systematic Literature Review found 40 eligible articles published between 2017 and 2022 in the Scopus, IEEE Xplore, Web of Science, and Google Scholar knowledge bases. The results show numerous bias and unfairness detection and mitigation approaches for ML technologies, with clearly defined metrics in the literature, and varied metrics can be highlighted. We recommend further research to define the techniques and metrics that should be employed in each case to standardize and ensure the impartiality of the machine learning model, thus, allowing the most appropriate metric to detect bias and unfairness in a given context. △ Less","3 November, 2022",https://arxiv.org/pdf/2202.08176
Explainability of Predictive Process Monitoring Results: Can You See My Data Issues?,Ghada Elkhawaga;Mervat Abuelkheir;Manfred Reichert,"Predictive business process monitoring (PPM) has been around for several years as a use case of process mining. PPM enables foreseeing the future of a business process through predicting relevant information about how a running process instance might end, related performance indicators, and other predictable aspects. A big share of PPM approaches adopts a Machine Learning (ML) technique to address a prediction task, especially non-process-aware PPM approaches. Consequently, PPM inherits the challenges faced by ML approaches. One of these challenges concerns the need to gain user trust in the predictions generated. The field of explainable artificial intelligence (XAI) addresses this issue. However, the choices made, and the techniques employed in a PPM task, in addition to ML model characteristics, influence resulting explanations. A comparison of the influence of different settings on the generated explanations is missing. To address this gap, we investigate the effect of different PPM settings on resulting data fed into an ML model and consequently to a XAI method. We study how differences in resulting explanations may indicate several issues in underlying data. We construct a framework for our experiments including different settings at each stage of PPM with XAI integrated as a fundamental part. Our experiments reveal several inconsistencies, as well as agreements, between data characteristics (and hence expectations about these data), important data used by the ML model as a result of querying it, and explanations of predictions of the investigated ML model. △ Less","16 February, 2022",https://arxiv.org/pdf/2202.08041
TimeREISE: Time-series Randomized Evolving Input Sample Explanation,Dominique Mercier;Andreas Dengel;Sheraz Ahmed,"Deep neural networks are one of the most successful classifiers across different domains. However, due to their limitations concerning interpretability their use is limited in safety critical context. The research field of explainable artificial intelligence addresses this problem. However, most of the interpretability methods are aligned to the image modality by design. The paper introduces TimeREISE a model agnostic attribution method specifically aligned to success in the context of time series classification. The method shows superior performance compared to existing approaches concerning different well-established measurements. TimeREISE is applicable to any time series classification network, its runtime does not scale in a linear manner concerning the input shape and it does not rely on prior data knowledge. △ Less","27 May, 2022",https://arxiv.org/pdf/2202.07952
CycleGAN for Undamaged-to-Damaged Domain Translation for Structural Health Monitoring and Damage Detection,Furkan Luleci;F. Necati Catbas;Onur Avci,"The recent advances in the data science field in the last few decades have benefitted many other fields including Structural Health Monitoring (SHM). Particularly, Artificial Intelligence (AI) such as Machine Learning (ML) and Deep Learning (DL) methods for vibration-based damage diagnostics of civil structures has been utilized extensively due to the observed high performances in learning from data. Along with diagnostics, damage prognostics is also vitally important for estimating the remaining useful life of civil structures. Currently, AI-based data-driven methods used for damage diagnostics and prognostics centered on historical data of the structures and require a substantial amount of data for prediction models. Although some of these methods are generative-based models, they are used to perform ML or DL tasks such as classification, regression, clustering, etc. after learning the distribution of the data. In this study, a variant of Generative Adversarial Networks (GAN), Cycle-Consistent Wasserstein Deep Convolutional GAN with Gradient Penalty (CycleWDCGAN-GP) model is developed to investigate the ""transition of structural dynamic signature from an undamaged-to-damaged state"" and ""if this transition can be employed for predictive damage detection"". The outcomes of this study demonstrate that the proposed model can accurately generate damaged responses from undamaged responses or vice versa. In other words, it will be possible to understand the damaged condition while the structure is still in a healthy (undamaged) condition or vice versa with the proposed methodology. This will enable a more proactive approach in overseeing the life-cycle performance as well as in predicting the remaining useful life of structures. △ Less","6 March, 2022",https://arxiv.org/pdf/2202.07831
Wireless Resource Management in Intelligent Semantic Communication Networks,Le Xia;Yao Sun;Xiaoqian Li;Gang Feng;Muhammad Ali Imran,"The prosperity of artificial intelligence (AI) has laid a promising paradigm of communication system, i.e., intelligent semantic communication (ISC), where semantic contents, instead of traditional bit sequences, are coded by AI models for efficient communication. Due to the unique demand of background knowledge for semantic recovery, wireless resource management faces new challenges in ISC. In this paper, we address the user association (UA) and bandwidth allocation (BA) problems in an ISC-enabled heterogeneous network (ISC-HetNet). We first introduce the auxiliary knowledge base (KB) into the system model, and develop a new performance metric for the ISC-HetNet, named system throughput in message (STM). Joint optimization of UA and BA is then formulated with the aim of STM maximization subject to KB matching and wireless bandwidth constraints. To this end, we propose a two-stage solution, including a stochastic programming method in the first stage to obtain a deterministic objective with semantic confidence, and a heuristic algorithm in the second stage to reach the optimality of UA and BA. Numerical results show great superiority and reliability of our proposed solution on the STM performance when compared with two baseline algorithms. △ Less","15 February, 2022",https://arxiv.org/pdf/2202.07632
Improving the repeatability of deep learning models with Monte Carlo dropout,Andreanne Lemay;Katharina Hoebel;Christopher P. Bridge;Brian Befano;Silvia De Sanjosé;Diden Egemen;Ana Cecilia Rodriguez;Mark Schiffman;John Peter Campbell;Jayashree Kalpathy-Cramer,"The integration of artificial intelligence into clinical workflows requires reliable and robust models. Repeatability is a key attribute of model robustness. Repeatable models output predictions with low variation during independent tests carried out under similar conditions. During model development and evaluation, much attention is given to classification performance while model repeatability is rarely assessed, leading to the development of models that are unusable in clinical practice. In this work, we evaluate the repeatability of four model types (binary classification, multi-class classification, ordinal classification, and regression) on images that were acquired from the same patient during the same visit. We study the performance of binary, multi-class, ordinal, and regression models on four medical image classification tasks from public and private datasets: knee osteoarthritis, cervical cancer screening, breast density estimation, and retinopathy of prematurity. Repeatability is measured and compared on ResNet and DenseNet architectures. Moreover, we assess the impact of sampling Monte Carlo dropout predictions at test time on classification performance and repeatability. Leveraging Monte Carlo predictions significantly increased repeatability for all tasks on the binary, multi-class, and ordinal models leading to an average reduction of the 95\% limits of agreement by 16% points and of the disagreement rate by 7% points. The classification accuracy improved in most settings along with the repeatability. Our results suggest that beyond about 20 Monte Carlo iterations, there is no further gain in repeatability. In addition to the higher test-retest agreement, Monte Carlo predictions were better calibrated which leads to output probabilities reflecting more accurately the true likelihood of being correctly classified. △ Less","15 February, 2022",https://arxiv.org/pdf/2202.07562
A Real-time System for Detecting Landslide Reports on Social Media using Artificial Intelligence,Ferda Ofli;Umair Qazi;Muhammad Imran;Julien Roch;Catherine Pennington;Vanessa Banks;Remy Bossu,"This paper presents an online system that leverages social media data in real time to identify landslide-related information automatically using state-of-the-art artificial intelligence techniques. The designed system can (i) reduce the information overload by eliminating duplicate and irrelevant content, (ii) identify landslide images, (iii) infer geolocation of the images, and (iv) categorize the user type (organization or person) of the account sharing the information. The system was deployed in February 2020 online at https://landslide-aidr.qcri.org/landslide_system.php to monitor live Twitter data stream and has been running continuously since then to provide time-critical information to partners such as British Geological Survey and European Mediterranean Seismological Centre. We trust this system can both contribute to harvesting of global landslide data for further research and support global landslide maps to facilitate emergency response and decision making. △ Less","14 February, 2022",https://arxiv.org/pdf/2202.07475
Trustworthy Autonomous Systems (TAS): Engaging TAS experts in curriculum design,Mohammad Naiseh;Caitlin Bentley;Sarvapali D. Ramchurn,"Recent advances in artificial intelligence, specifically machine learning, contributed positively to enhancing the autonomous systems industry, along with introducing social, technical, legal and ethical challenges to make them trustworthy. Although Trustworthy Autonomous Systems (TAS) is an established and growing research direction that has been discussed in multiple disciplines, e.g., Artificial Intelligence, Human-Computer Interaction, Law, and Psychology. The impact of TAS on education curricula and required skills for future TAS engineers has rarely been discussed in the literature. This study brings together the collective insights from a number of TAS leading experts to highlight significant challenges for curriculum design and potential TAS required skills posed by the rapid emergence of TAS. Our analysis is of interest not only to the TAS education community but also to other researchers, as it offers ways to guide future research toward operationalising TAS education. △ Less","10 March, 2022",https://arxiv.org/pdf/2202.07447
Relational Artificial Intelligence,Virginia Dignum,"The impact of Artificial Intelligence does not depend only on fundamental research and technological developments, but for a large part on how these systems are introduced into society and used in everyday situations. Even though AI is traditionally associated with rational decision making, understanding and shaping the societal impact of AI in all its facets requires a relational perspective. A rational approach to AI, where computational algorithms drive decision making independent of human intervention, insights and emotions, has shown to result in bias and exclusion, laying bare societal vulnerabilities and insecurities. A relational approach, that focus on the relational nature of things, is needed to deal with the ethical, legal, societal, cultural, and environmental implications of AI. A relational approach to AI recognises that objective and rational reasoning cannot does not always result in the 'right' way to proceed because what is 'right' depends on the dynamics of the situation in which the decision is taken, and that rather than solving ethical problems the focus of design and use of AI must be on asking the ethical question. In this position paper, I start with a general discussion of current conceptualisations of AI followed by an overview of existing approaches to governance and responsible development and use of AI. Then, I reflect over what should be the bases of a social paradigm for AI and how this should be embedded in relational, feminist and non-Western philosophies, in particular the Ubuntu philosophy. △ Less","4 February, 2022",https://arxiv.org/pdf/2202.07446
The potential of artificial intelligence for achieving healthy and sustainable societies,B. Sirmacek;S. Gupta;F. Mallor;H. Azizpour;Y. Ban;H. Eivazi;H. Fang;F. Golzar;I. Leite;G. I. Melsion;K. Smith;F. Fuso Nerini;R. Vinuesa,"In this chapter we extend earlier work (Vinuesa et al., Nature Communications 11, 2020) on the potential of artificial intelligence (AI) to achieve the 17 Sustainable Development Goals (SDGs) proposed by the United Nations (UN) for the 2030 Agenda. The present contribution focuses on three SDGs related to healthy and sustainable societies, i.e. SDG 3 (on good health), SDG 11 (on sustainable cities) and SDG 13 (on climate action). This chapter extends the previous study within those three goals, and goes beyond the 2030 targets. These SDGs are selected because they are closely related to the coronavirus disease 19 (COVID-19) pandemic, and also to crises like climate change, which constitute important challenges to our society. △ Less","11 February, 2022",https://arxiv.org/pdf/2202.07424
"IF-City: Intelligible Fair City Planning to Measure, Explain and Mitigate Inequality",Yan Lyu;Hangxin Lu;Min Kyung Lee;Gerhard Schmitt;Brian Y. Lim,"With the increasing pervasiveness of Artificial Intelligence (AI), many visual analytics tools have been proposed to examine fairness, but they mostly focus on data scientist users. Instead, tackling fairness must be inclusive and involve domain experts with specialized tools and workflows. Thus, domain-specific visualizations are needed for algorithmic fairness. Furthermore, while much work on AI fairness has focused on predictive decisions, less has been done for fair allocation and planning, which require human expertise and iterative design to integrate myriad constraints. We propose the Intelligible Fair Allocation (IF-Alloc) Framework that leverages explanations of causal attribution (Why), contrastive (Why Not) and counterfactual reasoning (What If, How To) to aid domain experts to assess and alleviate unfairness in allocation problems. We apply the framework to fair urban planning for designing cities that provide equal access to amenities and benefits for diverse resident types. Specifically, we propose an interactive visual tool, Intelligible Fair City Planner (IF-City), to help urban planners to perceive inequality across groups, identify and attribute sources of inequality, and mitigate inequality with automatic allocation simulations and constraint-satisfying recommendations. We demonstrate and evaluate the usage and usefulness of IF-City on a real neighborhood in New York City, US, with practicing urban planners from multiple countries, and discuss generalizing our findings, application, and framework to other use cases and applications of fair allocation. △ Less","15 February, 2022",https://arxiv.org/pdf/2202.07349
A Survey of Neural Trojan Attacks and Defenses in Deep Learning,Jie Wang;Ghulam Mubashar Hassan;Naveed Akhtar,"Artificial Intelligence (AI) relies heavily on deep learning - a technology that is becoming increasingly popular in real-life applications of AI, even in the safety-critical and high-risk domains. However, it is recently discovered that deep learning can be manipulated by embedding Trojans inside it. Unfortunately, pragmatic solutions to circumvent the computational requirements of deep learning, e.g. outsourcing model training or data annotation to third parties, further add to model susceptibility to the Trojan attacks. Due to the key importance of the topic in deep learning, recent literature has seen many contributions in this direction. We conduct a comprehensive review of the techniques that devise Trojan attacks for deep learning and explore their defenses. Our informative survey systematically organizes the recent literature and discusses the key concepts of the methods while assuming minimal knowledge of the domain on the readers part. It provides a comprehensible gateway to the broader community to understand the recent developments in Neural Trojans. △ Less","14 February, 2022",https://arxiv.org/pdf/2202.07183
Gaze-Guided Class Activation Mapping: Leveraging Human Attention for Network Attention in Chest X-rays Classification,Hongzhi Zhu;Septimiu Salcudean;Robert Rohling,"The increased availability and accuracy of eye-gaze tracking technology has sparked attention-related research in psychology, neuroscience, and, more recently, computer vision and artificial intelligence. The attention mechanism in artificial neural networks is known to improve learning tasks. However, no previous research has combined the network attention and human attention. This paper describes a gaze-guided class activation mapping (GG-CAM) method to directly regulate the formation of network attention based on expert radiologists' visual attention for the chest X-ray pathology classification problem, which remains challenging due to the complex and often nuanced differences among images. GG-CAM is a lightweight (3 additional trainable parameters for regulating the learning process) and generic extension that can be easily applied to most classification convolutional neural networks (CNN). GG-CAM-modified CNNs do not require human attention as an input when fully trained. Comparative experiments suggest that two standard CNNs with the GG-CAM extension achieve significantly greater classification performance. The median area under the curve (AUC) metrics for ResNet50 increases from 0.721 to 0.776. For EfficientNetv2 (s), the median AUC increases from 0.723 to 0.801. The GG-CAM also brings better interpretability of the network that facilitates the weakly-supervised pathology localization and analysis. △ Less","14 February, 2022",https://arxiv.org/pdf/2202.07107
Artificial Intelligence-Based Smart Grid Vulnerabilities and Potential Solutions for Fake-Normal Attacks: A Short Review,J. D. Ndibwile,"Smart grid systems are critical to the power industry, however their sophisticated architectural design and operations expose them to a number of cybersecurity threats, such as data tampering, data eavesdropping, and Denial of Service, among others. Artificial Intelligence (AI)-based technologies are becoming increasingly popular for detecting cyber assaults in a variety of computer settings, and several efforts have been made to secure various systems. The present AI systems are being exposed and vanquished because of the recent emergence of sophisticated adversarial systems such as Generative Adversarial Networks (GAN). The purpose of this short review is to outline some of the initiatives to protect smart grid systems, their obstacles, and what might be a potential future AI research direction △ Less","14 February, 2022",https://arxiv.org/pdf/2202.07050
Online-updated High-order Collaborative Networks for Single Image Deraining,Cong Wang;Jinshan Pan;Xiao-Ming Wu,"Single image deraining is an important and challenging task for some downstream artificial intelligence applications such as video surveillance and self-driving systems. Most of the existing deep-learning-based methods constrain the network to generate derained images but few of them explore features from intermediate layers, different levels, and different modules which are beneficial for rain streaks removal. In this paper, we propose a high-order collaborative network with multi-scale compact constraints and a bidirectional scale-content similarity mining module to exploit features from deep networks externally and internally for rain streaks removal. Externally, we design a deraining framework with three sub-networks trained in a collaborative manner, where the bottom network transmits intermediate features to the middle network which also receives shallower rainy features from the top network and sends back features to the bottom network. Internally, we enforce multi-scale compact constraints on the intermediate layers of deep networks to learn useful features via a Laplacian pyramid. Further, we develop a bidirectional scale-content similarity mining module to explore features at different scales in a down-to-up and up-to-down manner. To improve the model performance on real-world images, we propose an online-update learning approach, which uses real-world rainy images to fine-tune the network and update the deraining results in a self-supervised manner. Extensive experiments demonstrate that our proposed method performs favorably against eleven state-of-the-art methods on five public synthetic datasets and one real-world dataset. The source code will be available at \url{https://supercong94.wixsite.com/supercong94}. △ Less","14 February, 2022",https://arxiv.org/pdf/2202.06568
A Survey on Machine Learning Approaches for Modelling Intuitive Physics,Jiafei Duan;Arijit Dasgupta;Jason Fischer;Cheston Tan,"Research in cognitive science has provided extensive evidence of human cognitive ability in performing physical reasoning of objects from noisy perceptual inputs. Such a cognitive ability is commonly known as intuitive physics. With advancements in deep learning, there is an increasing interest in building intelligent systems that are capable of performing physical reasoning from a given scene for the purpose of building better AI systems. As a result, many contemporary approaches in modelling intuitive physics for machine cognition have been inspired by literature from cognitive science. Despite the wide range of work in physical reasoning for machine cognition, there is a scarcity of reviews that organize and group these deep learning approaches. Especially at the intersection of intuitive physics and artificial intelligence, there is a need to make sense of the diverse range of ideas and approaches. Therefore, this paper presents a comprehensive survey of recent advances and techniques in intuitive physics-inspired deep learning approaches for physical reasoning. The survey will first categorize existing deep learning approaches into three facets of physical reasoning before organizing them into three general technical approaches and propose six categorical tasks of the field. Finally, we highlight the challenges of the current field and present some future research directions. △ Less","27 April, 2022",https://arxiv.org/pdf/2202.06481
Semantic Communication Meets Edge Intelligence,Wanting Yang;Zi Qin Liew;Wei Yang Bryan Lim;Zehui Xiong;Dusit Niyato;Xuefen Chi;Xianbin Cao;Khaled B. Letaief,"The development of emerging applications, such as autonomous transportation systems, are expected to result in an explosive growth in mobile data traffic. As the available spectrum resource becomes more and more scarce, there is a growing need for a paradigm shift from Shannon's Classical Information Theory (CIT) to semantic communication (SemCom). Specifically, the former adopts a ""transmit-before-understanding"" approach while the latter leverages artificial intelligence (AI) techniques to ""understand-before-transmit"", thereby alleviating bandwidth pressure by reducing the amount of data to be exchanged without negating the semantic effectiveness of the transmitted symbols. However, the semantic extraction (SE) procedure incurs costly computation and storage overheads. In this article, we introduce an edge-driven training, maintenance, and execution of SE. We further investigate how edge intelligence can be enhanced with SemCom through improving the generalization capabilities of intelligent agents at lower computation overheads and reducing the communication overhead of information exchange. Finally, we present a case study involving semantic-aware resource optimization for the wireless powered Internet of Things (IoT). △ Less","13 February, 2022",https://arxiv.org/pdf/2202.06471
A multi-task semi-supervised framework for Text2Graph & Graph2Text,Oriol Domingo;Marta R. Costa-jussà;Carlos Escolano,"The Artificial Intelligence industry regularly develops applications that mostly rely on Knowledge Bases, a data repository about specific, or general, domains, usually represented in a graph shape. Similar to other databases, they face two main challenges: information ingestion and information retrieval. We approach these challenges by jointly learning graph extraction from text and text generation from graphs. The proposed solution, a T5 architecture, is trained in a multi-task semi-supervised environment, with our collected non-parallel data, following a cycle training regime. Experiments on WebNLG dataset show that our approach surpasses unsupervised state-of-the-art results in text-to-graph and graph-to-text. More relevantly, our framework is more consistent across seen and unseen domains than supervised models. The resulting model can be easily trained in any new domain with non-parallel data, by simply adding text and graphs about it, in our cycle framework. △ Less","18 February, 2022",https://arxiv.org/pdf/2202.06041
Formalization of a Stochastic Approximation Theorem,Koundinya Vajjha;Barry Trager;Avraham Shinnar;Vasily Pestun,"Stochastic approximation algorithms are iterative procedures which are used to approximate a target value in an environment where the target is unknown and direct observations are corrupted by noise. These algorithms are useful, for instance, for root-finding and function minimization when the target function or model is not directly known. Originally introduced in a 1951 paper by Robbins and Monro, the field of Stochastic approximation has grown enormously and has come to influence application domains from adaptive signal processing to artificial intelligence. As an example, the Stochastic Gradient Descent algorithm which is ubiquitous in various subdomains of Machine Learning is based on stochastic approximation theory. In this paper, we give a formal proof (in the Coq proof assistant) of a general convergence theorem due to Aryeh Dvoretzky, which implies the convergence of important classical methods such as the Robbins-Monro and the Kiefer-Wolfowitz algorithms. In the process, we build a comprehensive Coq library of measure-theoretic probability theory and stochastic processes. △ Less","8 August, 2022",https://arxiv.org/pdf/2202.05959
Confident AI,Jim Davis,"In this paper, we propose ""Confident AI"" as a means to designing Artificial Intelligence (AI) and Machine Learning (ML) systems with both algorithm and user confidence in model predictions and reported results. The 4 basic tenets of Confident AI are Repeatability, Believability, Sufficiency, and Adaptability. Each of the tenets is used to explore fundamental issues in current AI/ML systems and together provide an overall approach to Confident AI. △ Less","11 February, 2022",https://arxiv.org/pdf/2202.05957
Artificial Intelligence and Auction Design,Martino Banchio;Andrzej Skrzypacz,"Motivated by online advertising auctions, we study auction design in repeated auctions played by simple Artificial Intelligence algorithms (Q-learning). We find that first-price auctions with no additional feedback lead to tacit-collusive outcomes (bids lower than values), while second-price auctions do not. We show that the difference is driven by the incentive in first-price auctions to outbid opponents by just one bid increment. This facilitates re-coordination on low bids after a phase of experimentation. We also show that providing information about lowest bid to win, as introduced by Google at the time of switch to first-price auctions, increases competitiveness of auctions. △ Less","11 February, 2022",https://arxiv.org/pdf/2202.05947
"Choices, Risks, and Reward Reports: Charting Public Policy for Reinforcement Learning Systems",Thomas Krendl Gilbert;Sarah Dean;Tom Zick;Nathan Lambert,"In the long term, reinforcement learning (RL) is considered by many AI theorists to be the most promising path to artificial general intelligence. This places RL practitioners in a position to design systems that have never existed before and lack prior documentation in law and policy. Public agencies could intervene on complex dynamics that were previously too opaque to deliberate about, and long-held policy ambitions would finally be made tractable. In this whitepaper we illustrate this potential and how it might be technically enacted in the domains of energy infrastructure, social media recommender systems, and transportation. Alongside these unprecedented interventions come new forms of risk that exacerbate the harms already generated by standard machine learning tools. We correspondingly present a new typology of risks arising from RL design choices, falling under four categories: scoping the horizon, defining rewards, pruning information, and training multiple agents. Rather than allowing RL systems to unilaterally reshape human domains, policymakers need new mechanisms for the rule of reason, foreseeability, and interoperability that match the risks these systems pose. We argue that criteria for these choices may be drawn from emerging subfields within antitrust, tort, and administrative law. It will then be possible for courts, federal and state agencies, and non-governmental organizations to play more active roles in RL specification and evaluation. Building on the ""model cards"" and ""datasheets"" frameworks proposed by Mitchell et al. and Gebru et al., we argue the need for Reward Reports for AI systems. Reward Reports are living documents for proposed RL deployments that demarcate design choices. △ Less","11 February, 2022",https://arxiv.org/pdf/2202.05716
Semi-Supervised GCN for learning Molecular Structure-Activity Relationships,Alessio Ragno;Dylan Savoia;Roberto Capobianco,"Since the introduction of artificial intelligence in medicinal chemistry, the necessity has emerged to analyse how molecular property variation is modulated by either single atoms or chemical groups. In this paper, we propose to train graph-to-graph neural network using semi-supervised learning for attributing structure-property relationships. As initial case studies we apply the method to solubility and molecular acidity while checking its consistency in comparison with known experimental chemical data. As final goal, our approach could represent a valuable tool to deal with problems such as activity cliffs, lead optimization and de-novo drug design. △ Less","25 January, 2022",https://arxiv.org/pdf/2202.05704
HaT5: Hate Language Identification using Text-to-Text Transfer Transformer,Sana Sabah Sabry;Tosin Adewumi;Nosheen Abid;György Kovacs;Foteini Liwicki;Marcus Liwicki,"We investigate the performance of a state-of-the art (SoTA) architecture T5 (available on the SuperGLUE) and compare with it 3 other previous SoTA architectures across 5 different tasks from 2 relatively diverse datasets. The datasets are diverse in terms of the number and types of tasks they have. To improve performance, we augment the training data by using an autoregressive model. We achieve near-SoTA results on a couple of the tasks - macro F1 scores of 81.66% for task A of the OLID 2019 dataset and 82.54% for task A of the hate speech and offensive content (HASOC) 2021 dataset, where SoTA are 82.9% and 83.05%, respectively. We perform error analysis and explain why one of the models (Bi-LSTM) makes the predictions it does by using a publicly available algorithm: Integrated Gradient (IG). This is because explainable artificial intelligence (XAI) is essential for earning the trust of users. The main contributions of this work are the implementation method of T5, which is discussed; the data augmentation using a new conversational AI model checkpoint, which brought performance improvements; and the revelation on the shortcomings of HASOC 2021 dataset. It reveals the difficulties of poor data annotation by using a small set of examples where the T5 model made the correct predictions, even when the ground truth of the test set were incorrect (in our opinion). We also provide our model checkpoints on the HuggingFace hub1 to foster transparency. △ Less","11 February, 2022",https://arxiv.org/pdf/2202.05690
Explainable Machine Learning for Breakdown Prediction in High Gradient RF Cavities,Christoph Obermair;Thomas Cartier-Michaud;Andrea Apollonio;William Millar;Lukas Felsberger;Lorenz Fischl;Holger Severin Bovbjerg;Daniel Wollmann;Walter Wuensch;Nuria Catalan-Lasheras;Marçà Boronat;Franz Pernkopf;Graeme Burt,"The occurrence of vacuum arcs or radio frequency (rf) breakdowns is one of the most prevalent factors limiting the high-gradient performance of normal conducting rf cavities in particle accelerators. In this paper, we search for the existence of previously unrecognized features related to the incidence of rf breakdowns by applying a machine learning strategy to high-gradient cavity data from CERN's test stand for the Compact Linear Collider (CLIC). By interpreting the parameters of the learned models with explainable artificial intelligence (AI), we reverse-engineer physical properties for deriving fast, reliable, and simple rule-based models. Based on 6 months of historical data and dedicated experiments, our models show fractions of data with a high influence on the occurrence of breakdowns. Specifically, it is shown that the field emitted current following an initial breakdown is closely related to the probability of another breakdown occurring shortly thereafter. Results also indicate that the cavity pressure should be monitored with increased temporal resolution in future experiments, to further explore the vacuum activity associated with breakdowns. △ Less","8 December, 2022",https://arxiv.org/pdf/2202.05610
Privacy-preserving Generative Framework Against Membership Inference Attacks,Ruikang Yang;Jianfeng Ma;Yinbin Miao;Xindi Ma,"Artificial intelligence and machine learning have been integrated into all aspects of our lives and the privacy of personal data has attracted more and more attention. Since the generation of the model needs to extract the effective information of the training data, the model has the risk of leaking the privacy of the training data. Membership inference attacks can measure the model leakage of source data to a certain degree. In this paper, we design a privacy-preserving generative framework against membership inference attacks, through the information extraction and data generation capabilities of the generative model variational autoencoder (VAE) to generate synthetic data that meets the needs of differential privacy. Instead of adding noise to the model output or tampering with the training process of the target model, we directly process the original data. We first map the source data to the latent space through the VAE model to get the latent code, then perform noise process satisfying metric privacy on the latent code, and finally use the VAE model to reconstruct the synthetic data. Our experimental evaluation demonstrates that the machine learning model trained with newly generated synthetic data can effectively resist membership inference attacks and still maintain high utility. △ Less","11 February, 2022",https://arxiv.org/pdf/2202.05469
"Give me a knee radiograph, I will tell you where the knee joint area is: a deep convolutional neural network adventure",Shi Yan;Taghi Ramazanian;Elham Sagheb;Walter K. Kremers;Vipin Chaudhary;Michael Taunton;Hilal Maradit Kremers;Ahmad P. Tafti,"Knee pain is undoubtedly the most common musculoskeletal symptom that impairs quality of life, confines mobility and functionality across all ages. Knee pain is clinically evaluated by routine radiographs, where the widespread adoption of radiographic images and their availability at low cost, make them the principle component in the assessment of knee pain and knee pathologies, such as arthritis, trauma, and sport injuries. However, interpretation of the knee radiographs is still highly subjective, and overlapping structures within the radiographs and the large volume of images needing to be analyzed on a daily basis, make interpretation challenging for both naive and experienced practitioners. There is thus a need to implement an artificial intelligence strategy to objectively and automatically interpret knee radiographs, facilitating triage of abnormal radiographs in a timely fashion. The current work proposes an accurate and effective pipeline for autonomous detection, localization, and classification of knee joint area in plain radiographs combining the You Only Look Once (YOLO v3) deep convolutional neural network with a large and fully-annotated knee radiographs dataset. The present work is expected to stimulate more interest from the deep learning computer vision community to this pragmatic and clinical application. △ Less","10 February, 2022",https://arxiv.org/pdf/2202.05382
"Accountability in an Algorithmic Society: Relationality, Responsibility, and Robustness in Machine Learning",A. Feder Cooper;Emanuel Moss;Benjamin Laufer;Helen Nissenbaum,"In 1996, Accountability in a Computerized Society [95] issued a clarion call concerning the erosion of accountability in society due to the ubiquitous delegation of consequential functions to computerized systems. Nissenbaum [95] described four barriers to accountability that computerization presented, which we revisit in relation to the ascendance of data-driven algorithmic systems--i.e., machine learning or artificial intelligence--to uncover new challenges for accountability that these systems present. Nissenbaum's original paper grounded discussion of the barriers in moral philosophy; we bring this analysis together with recent scholarship on relational accountability frameworks and discuss how the barriers present difficulties for instantiating a unified moral, relational framework in practice for data-driven algorithmic systems. We conclude by discussing ways of weakening the barriers in order to do so. △ Less","13 May, 2022",https://arxiv.org/pdf/2202.05338
Describing image focused in cognitive and visual details for visually impaired people: An approach to generating inclusive paragraphs,Daniel Louzada Fernandes;Marcos Henrique Fonseca Ribeiro;Fabio Ribeiro Cerqueira;Michel Melo Silva,"Several services for people with visual disabilities have emerged recently due to achievements in Assistive Technologies and Artificial Intelligence areas. Despite the growth in assistive systems availability, there is a lack of services that support specific tasks, such as understanding the image context presented in online content, e.g., webinars. Image captioning techniques and their variants are limited as Assistive Technologies as they do not match the needs of visually impaired people when generating specific descriptions. We propose an approach for generating context of webinar images combining a dense captioning technique with a set of filters, to fit the captions in our domain, and a language model for the abstractive summary task. The results demonstrated that we can produce descriptions with higher interpretability and focused on the relevant information for that group of people by combining image analysis methods and neural language models. △ Less","15 February, 2022",https://arxiv.org/pdf/2202.05331
"Trust in AI: Interpretability is not necessary or sufficient, while black-box interaction is necessary and sufficient",Max W. Shen,"The problem of human trust in artificial intelligence is one of the most fundamental problems in applied machine learning. Our processes for evaluating AI trustworthiness have substantial ramifications for ML's impact on science, health, and humanity, yet confusion surrounds foundational concepts. What does it mean to trust an AI, and how do humans assess AI trustworthiness? What are the mechanisms for building trustworthy AI? And what is the role of interpretable ML in trust? Here, we draw from statistical learning theory and sociological lenses on human-automation trust to motivate an AI-as-tool framework, which distinguishes human-AI trust from human-AI-human trust. Evaluating an AI's contractual trustworthiness involves predicting future model behavior using behavior certificates (BCs) that aggregate behavioral evidence from diverse sources including empirical out-of-distribution and out-of-task evaluation and theoretical proofs linking model architecture to behavior. We clarify the role of interpretability in trust with a ladder of model access. Interpretability (level 3) is not necessary or even sufficient for trust, while the ability to run a black-box model at-will (level 2) is necessary and sufficient. While interpretability can offer benefits for trust, it can also incur costs. We clarify ways interpretability can contribute to trust, while questioning the perceived centrality of interpretability to trust in popular discourse. How can we empower people with tools to evaluate trust? Instead of trying to understand how a model works, we argue for understanding how a model behaves. Instead of opening up black boxes, we should create more behavior certificates that are more correct, relevant, and understandable. We discuss how to build trusted and trustworthy AI responsibly. △ Less","10 February, 2022",https://arxiv.org/pdf/2202.05302
Towards a Guideline for Evaluation Metrics in Medical Image Segmentation,Dominik Müller;Iñaki Soto-Rey;Frank Kramer,"In the last decade, research on artificial intelligence has seen rapid growth with deep learning models, especially in the field of medical image segmentation. Various studies demonstrated that these models have powerful prediction capabilities and achieved similar results as clinicians. However, recent studies revealed that the evaluation in image segmentation studies lacks reliable model performance assessment and showed statistical bias by incorrect metric implementation or usage. Thus, this work provides an overview and interpretation guide on the following metrics for medical image segmentation evaluation in binary as well as multi-class problems: Dice similarity coefficient, Jaccard, Sensitivity, Specificity, Rand index, ROC curves, Cohen's Kappa, and Hausdorff distance. As a summary, we propose a guideline for standardized medical image segmentation evaluation to improve evaluation quality, reproducibility, and comparability in the research field. △ Less","10 February, 2022",https://arxiv.org/pdf/2202.05273
Zero Shot Learning for Predicting Energy Usage of Buildings in Sustainable Design,Arun Zachariah;Praveen Rao;Brian Corn;Dominique Davison,"The 2030 Challenge is aimed at making all new buildings and major renovations carbon neutral by 2030. One of the potential solutions to meet this challenge is through innovative sustainable design strategies. For developing such strategies it is important to understand how the various building factors contribute to energy usage of a building, right at design time. The growth of artificial intelligence (AI) in recent years provides an unprecedented opportunity to advance sustainable design by learning complex relationships between building factors from available data. However, rich training datasets are needed for AI-based solutions to achieve good prediction accuracy. Unfortunately, obtaining training datasets are time consuming and expensive in many real-world applications. Motivated by these reasons, we address the problem of accurately predicting the energy usage of new or unknown building types, i.e., those building types that do not have any training data. We propose a novel approach based on zero-shot learning (ZSL) to solve this problem. Our approach uses side information from building energy modeling experts to predict the closest building types for a given new/unknown building type. We then obtain the predicted energy usage for the k-closest building types using the models learned during training and combine the predicted values using a weighted averaging function. We evaluated our approach on a dataset containing five building types generated using BuildSimHub, a popular platform for building energy modeling. Our approach achieved better average accuracy than a regression model (based on XGBoost) trained on the entire dataset of known building types. △ Less","10 February, 2022",https://arxiv.org/pdf/2202.05206
Needs-aware Artificial Intelligence: AI that 'serves [human] needs',Ryan Watkins;Soheil Human,"By defining the current limits (and thereby the frontiers), many boundaries are shaping, and will continue to shape, the future of Artificial Intelligence (AI). We push on these boundaries in order to make further progress into what were yesterday's frontiers. They are both pliable and resilient - always creating new boundaries of what AI can (or should) achieve. Among these are technical boundaries (such as processing capacity), psychological boundaries (such as human trust in AI systems), ethical boundaries (such as with AI weapons), and conceptual boundaries (such as the AI people can imagine). It is within this final category while it can play a fundamental role in all other boundaries} that we find the construct of needs and the limitations that our current concept of need places on the future AI. △ Less","26 May, 2022",https://arxiv.org/pdf/2202.04977
A Survey on Artificial Intelligence for Source Code: A Dialogue Systems Perspective,Erfan Al-Hossami;Samira Shaikh,"In this survey paper, we overview major deep learning methods used in Natural Language Processing (NLP) and source code over the last 35 years. Next, we present a survey of the applications of Artificial Intelligence (AI) for source code, also known as Code Intelligence (CI) and Programming Language Processing (PLP). We survey over 287 publications and present a software-engineering centered taxonomy for CI placing each of the works into one category describing how it best assists the software development cycle. Then, we overview the field of conversational assistants and their applications in software engineering and education. Lastly, we highlight research opportunities at the intersection of AI for code and conversational assistants and provide future directions for researching conversational assistants with CI capabilities. △ Less","10 February, 2022",https://arxiv.org/pdf/2202.04847
FCM-DNN: diagnosing coronary artery disease by deep accuracy Fuzzy C-Means clustering model,Javad Hassannataj Joloudari;Hamid Saadatfar;Mohammad GhasemiGol;Roohallah Alizadehsani;Zahra Alizadeh Sani;Fereshteh Hasanzadeh;Edris Hassannataj;Danial Sharifrazi;Zulkefli Mansor,"Cardiovascular disease is one of the most challenging diseases in middle-aged and older people, which causes high mortality. Coronary artery disease (CAD) is known as a common cardiovascular disease. A standard clinical tool for diagnosing CAD is angiography. The main challenges are dangerous side effects and high angiography costs. Today, the development of artificial intelligence-based methods is a valuable achievement for diagnosing disease. Hence, in this paper, artificial intelligence methods such as neural network (NN), deep neural network (DNN), and Fuzzy C-Means clustering combined with deep neural network (FCM-DNN) are developed for diagnosing CAD on a cardiac magnetic resonance imaging (CMRI) dataset. The original dataset is used in two different approaches. First, the labeled dataset is applied to the NN and DNN to create the NN and DNN models. Second, the labels are removed, and the unlabeled dataset is clustered via the FCM method, and then, the clustered dataset is fed to the DNN to create the FCM-DNN model. By utilizing the second clustering and modeling, the training process is improved, and consequently, the accuracy is increased. As a result, the proposed FCM-DNN model achieves the best performance with a 99.91% accuracy specifying 10 clusters, i.e., 5 clusters for healthy subjects and 5 clusters for sick subjects, through the 10-fold cross-validation technique compared to the NN and DNN models reaching the accuracies of 92.18% and 99.63%, respectively. To the best of our knowledge, no study has been conducted for CAD diagnosis on the CMRI dataset using artificial intelligence methods. The results confirm that the proposed FCM-DNN model can be helpful for scientific and research centers. △ Less","28 February, 2022",https://arxiv.org/pdf/2202.04645
Optimal Hyperparameters and Structure Setting of Multi-Objective Robust CNN Systems via Generalized Taguchi Method and Objective Vector Norm,Sheng-Guo Wang;Shanshan Jiang,"Recently, Machine Learning (ML), Artificial Intelligence (AI), and Convolutional Neural Network (CNN) have made huge progress with broad applications, where their systems have deep learning structures and a large number of hyperparameters that determine the quality and performance of the CNNs and AI systems. These systems may have multi-objective ML and AI performance needs. There is a key requirement to find the optimal hyperparameters and structures for multi-objective robust optimal CNN systems. This paper proposes a generalized Taguchi approach to effectively determine the optimal hyperparameters and structure for the multi-objective robust optimal CNN systems via their objective performance vector norm. The proposed approach and methods are applied to a CNN classification system with the original ResNet for CIFAR-10 dataset as a demonstration and validation, which shows the proposed methods are highly effective to achieve an optimal accuracy rate of the original ResNet on CIFAR-10. △ Less","10 February, 2022",https://arxiv.org/pdf/2202.04567
Precision Radiotherapy via Information Integration of Expert Human Knowledge and AI Recommendation to Optimize Clinical Decision Making,Wenbo Sun;Dipesh Niraula;Issam El Naqa;Randall K Ten Haken;Ivo D Dinov;Kyle Cuneo;Judy Jin,"In the precision medicine era, there is a growing need for precision radiotherapy where the planned radiation dose needs to be optimally determined by considering a myriad of patient-specific information in order to ensure treatment efficacy. Existing artificial-intelligence (AI) methods can recommend radiation dose prescriptions within the scope of this available information. However, treating physicians may not fully entrust the AI's recommended prescriptions due to known limitations or when the AI recommendation may go beyond physicians' current knowledge. This paper lays out a systematic method to integrate expert human knowledge with AI recommendations for optimizing clinical decision making. Towards this goal, Gaussian process (GP) models are integrated with deep neural networks (DNNs) to quantify the uncertainty of the treatment outcomes given by physicians and AI recommendations, respectively, which are further used as a guideline to educate clinical physicians and improve AI models performance. The proposed method is demonstrated in a comprehensive dataset where patient-specific information and treatment outcomes are prospectively collected during radiotherapy of 67 non-small cell lung cancer patients and retrospectively analyzed. △ Less","9 February, 2022",https://arxiv.org/pdf/2202.04565
"TinyM^2
Net: A Flexible System Algorithm Co-designed Multimodal Learning Framework for Tiny Devices",Hasib-Al Rashid;Pretom Roy Ovi;Carl Busart;Aryya Gangopadhyay;Tinoosh Mohsenin,"With the emergence of Artificial Intelligence (AI), new attention has been given to implement AI algorithms on resource constrained tiny devices to expand the application domain of IoT. Multimodal Learning has recently become very popular with the classification task due to its impressive performance for both image and audio event classification. This paper presents TinyM^2Net -- a flexible system algorithm co-designed multimodal learning framework for resource constrained tiny devices. The framework was designed to be evaluated on two different case-studies: COVID-19 detection from multimodal audio recordings and battle field object detection from multimodal images and audios. In order to compress the model to implement on tiny devices, substantial network architecture optimization and mixed precision quantization were performed (mixed 8-bit and 4-bit). TinyM^2Net shows that even a tiny multimodal learning model can improve the classification performance than that of any unimodal frameworks. The most compressed TinyM^2Net achieves 88.4% COVID-19 detection accuracy (14.5% improvement from unimodal base model) and 96.8% battle field object detection accuracy (3.9% improvement from unimodal base model). Finally, we test our TinyM^2Net models on a Raspberry Pi 4 to see how they perform when deployed to a resource constrained tiny device. △ Less","19 April, 2022",https://arxiv.org/pdf/2202.04303
Financial Vision Based Reinforcement Learning Trading Strategy,Yun-Cheng Tsai;Fu-Min Szu;Jun-Hao Chen;Samuel Yen-Chi Chen,"Recent advances in artificial intelligence (AI) for quantitative trading have led to its general superhuman performance in significant trading performance. However, the potential risk of AI trading is a ""black box"" decision. Some AI computing mechanisms are complex and challenging to understand. If we use AI without proper supervision, AI may lead to wrong choices and make huge losses. Hence, we need to ask about the AI ""black box"", including why did AI decide to do this or not? Why can people trust AI or not? How can people fix their mistakes? These problems also highlight the challenges that AI technology can explain in the trading field. △ Less","2 February, 2022",https://arxiv.org/pdf/2202.04115
"The EMory BrEast imaging Dataset (EMBED): A Racially Diverse, Granular Dataset of 3.5M Screening and Diagnostic Mammograms",Jiwoong J. Jeong;Brianna L. Vey;Ananth Reddy;Thomas Kim;Thiago Santos;Ramon Correa;Raman Dutt;Marina Mosunjac;Gabriela Oprea-Ilies;Geoffrey Smith;Minjae Woo;Christopher R. McAdams;Mary S. Newell;Imon Banerjee;Judy Gichoya;Hari Trivedi,"Developing and validating artificial intelligence models in medical imaging requires datasets that are large, granular, and diverse. To date, the majority of publicly available breast imaging datasets lack in one or more of these areas. Models trained on these data may therefore underperform on patient populations or pathologies that have not previously been encountered. The EMory BrEast imaging Dataset (EMBED) addresses these gaps by providing 3650,000 2D and DBT screening and diagnostic mammograms for 116,000 women divided equally between White and African American patients. The dataset also contains 40,000 annotated lesions linked to structured imaging descriptors and 61 ground truth pathologic outcomes grouped into six severity classes. Our goal is to share this dataset with research partners to aid in development and validation of breast AI models that will serve all patients fairly and help decrease bias in medical AI. △ Less","8 February, 2022",https://arxiv.org/pdf/2202.04073
Latent gaze information in highly dynamic decision-tasks,Benedikt Hosp,"Digitization is penetrating more and more areas of life. Tasks are increasingly being completed digitally, and are therefore not only fulfilled faster, more efficiently but also more purposefully and successfully. The rapid developments in the field of artificial intelligence in recent years have played a major role in this, as they brought up many helpful approaches to build on. At the same time, the eyes, their movements, and the meaning of these movements are being progressively researched. The combination of these developments has led to exciting approaches. In this dissertation, I present some of these approaches which I worked on during my Ph.D. First, I provide insight into the development of models that use artificial intelligence to connect eye movements with visual expertise. This is demonstrated for two domains or rather groups of people: athletes in decision-making actions and surgeons in arthroscopic procedures. The resulting models can be considered as digital diagnostic models for automatic expertise recognition. Furthermore, I show approaches that investigate the transferability of eye movement patterns to different expertise domains and subsequently, important aspects of techniques for generalization. Finally, I address the temporal detection of confusion based on eye movement data. The results suggest the use of the resulting model as a clock signal for possible digital assistance options in the training of young professionals. An interesting aspect of my research is that I was able to draw on very valuable data from DFB youth elite athletes as well as on long-standing experts in arthroscopy. In particular, the work with the DFB data attracted the interest of radio and print media, namely DeutschlandFunk Nova and SWR DasDing. All resulting articles presented here have been published in internationally renowned journals or at conferences. △ Less","8 February, 2022",https://arxiv.org/pdf/2202.04072
Learning MAX-SAT from Contextual Examples for Combinatorial Optimisation,Mohit Kumar;Samuel Kolb;Stefano Teso;Luc De Raedt,"Combinatorial optimisation problems are ubiquitous in artificial intelligence. Designing the underlying models, however, requires substantial expertise, which is a limiting factor in practice. The models typically consist of hard and soft constraints, or combine hard constraints with an objective function. We introduce a novel setting for learning combinatorial optimisation problems from contextual examples. These positive and negative examples show - in a particular context - whether the solutions are good enough or not. We develop our framework using the MAX-SAT formalism as it is simple yet powerful setting having these features. We study the learnability of MAX-SAT models. Our theoretical results show that high-quality MAX-SAT models can be learned from contextual examples in the realisable and agnostic settings, as long as the data satisfies an intuitive ""representativeness"" condition. We also contribute two implementations based on our theoretical results: one leverages ideas from syntax-guided synthesis while the other makes use of stochastic local search techniques. The two implementations are evaluated by recovering synthetic and benchmark models from contextual examples. The experimental results support our theoretical analysis, showing that MAX-SAT models can be learned from contextual examples. Among the two implementations, the stochastic local search learner scales much better than the syntax-guided implementation while providing comparable or better models. △ Less","8 February, 2022",https://arxiv.org/pdf/2202.03888
Investigating the fidelity of explainable artificial intelligence methods for applications of convolutional neural networks in geoscience,Antonios Mamalakis;Elizabeth A. Barnes;Imme Ebert-Uphoff,"Convolutional neural networks (CNNs) have recently attracted great attention in geoscience due to their ability to capture non-linear system behavior and extract predictive spatiotemporal patterns. Given their black-box nature however, and the importance of prediction explainability, methods of explainable artificial intelligence (XAI) are gaining popularity as a means to explain the CNN decision-making strategy. Here, we establish an intercomparison of some of the most popular XAI methods and investigate their fidelity in explaining CNN decisions for geoscientific applications. Our goal is to raise awareness of the theoretical limitations of these methods and gain insight into the relative strengths and weaknesses to help guide best practices. The considered XAI methods are first applied to an idealized attribution benchmark, where the ground truth of explanation of the network is known a priori, to help objectively assess their performance. Secondly, we apply XAI to a climate-related prediction setting, namely to explain a CNN that is trained to predict the number of atmospheric rivers in daily snapshots of climate simulations. Our results highlight several important issues of XAI methods (e.g., gradient shattering, inability to distinguish the sign of attribution, ignorance to zero input) that have previously been overlooked in our field and, if not considered cautiously, may lead to a distorted picture of the CNN decision-making strategy. We envision that our analysis will motivate further investigation into XAI fidelity and will help towards a cautious implementation of XAI in geoscience, which can lead to further exploitation of CNNs and deep learning for prediction problems. △ Less","5 September, 2022",https://arxiv.org/pdf/2202.03407
Link Prediction of Artificial Intelligence Concepts using Low Computational Power,Francisco Valente,"This paper presents an approach proposed for the Science4cast 2021 competition, organized by the Institute of Advanced Research in Artificial Intelligence, whose main goal was to predict the likelihood of future associations between machine learning concepts in a semantic network. The developed methodology corresponds to a solution for a scenario of availability of low computational power only, exploiting the extraction of low order topological features and its incorporation in an optimized classifier to estimate the degree of future connections between the nodes. The reasons that motivated the developed methodologies will be discussed, as well as some results, limitations and suggestions of improvements. △ Less","7 February, 2022",https://arxiv.org/pdf/2202.03393
"AI-based artistic representation of emotions from EEG signals: a discussion on fairness, inclusion, and aesthetics",Piera Riccio;Kristin Bergaust;Boel Christensen-Scheel;Juan-Carlos De Martin;Maria A. Zuluaga;Stefano Nichele,"While Artificial Intelligence (AI) technologies are being progressively developed, artists and researchers are investigating their role in artistic practices. In this work, we present an AI-based Brain-Computer Interface (BCI) in which humans and machines interact to express feelings artistically. This system and its production of images give opportunities to reflect on the complexities and range of human emotions and their expressions. In this discussion, we seek to understand the dynamics of this interaction to reach better co-existence in fairness, inclusion, and aesthetics. △ Less","7 February, 2022",https://arxiv.org/pdf/2202.03246
AI Research Associate for Early-Stage Scientific Discovery,Morad Behandish;John Maxwell III;Johan de Kleer,"Artificial intelligence (AI) has been increasingly applied in scientific activities for decades; however, it is still far from an insightful and trustworthy collaborator in the scientific process. Most existing AI methods are either too simplistic to be useful in real problems faced by scientists or too domain-specialized (even dogmatized), stifling transformative discoveries or paradigm shifts. We present an AI research associate for early-stage scientific discovery based on (a) a novel minimally-biased ontology for physics-based modeling that is context-aware, interpretable, and generalizable across classical and relativistic physics; (b) automatic search for viable and parsimonious hypotheses, represented at a high-level (via domain-agnostic constructs) with built-in invariants, e.g., postulated forms of conservation principles implied by a presupposed spacetime topology; and (c) automatic compilation of the enumerated hypotheses to domain-specific, interpretable, and trainable/testable tensor-based computation graphs to learn phenomenological relations, e.g., constitutive or material laws, from sparse (and possibly noisy) data sets. △ Less","2 February, 2022",https://arxiv.org/pdf/2202.03199
Knowledge-Integrated Informed AI for National Security,Anu K. Myne;Kevin J. Leahy;Ryan J. Soklaski,"The state of artificial intelligence technology has a rich history that dates back decades and includes two fall-outs before the explosive resurgence of today, which is credited largely to data-driven techniques. While AI technology has and continues to become increasingly mainstream with impact across domains and industries, it's not without several drawbacks, weaknesses, and potential to cause undesired effects. AI techniques are numerous with many approaches and variants, but they can be classified simply based on the degree of knowledge they capture and how much data they require; two broad categories emerge as prominent across AI to date: (1) techniques that are primarily, and often solely, data-driven while leveraging little to no knowledge and (2) techniques that primarily leverage knowledge and depend less on data. Now, a third category is starting to emerge that leverages both data and knowledge, that some refer to as ""informed AI."" This third category can be a game changer within the national security domain where there is ample scientific and domain-specific knowledge that stands ready to be leveraged, and where purely data-driven AI can lead to serious unwanted consequences. This report shares findings from a thorough exploration of AI approaches that exploit data as well as principled and/or practical knowledge, which we refer to as ""knowledge-integrated informed AI."" Specifically, we review illuminating examples of knowledge integrated in deep learning and reinforcement learning pipelines, taking note of the performance gains they provide. We also discuss an apparent trade space across variants of knowledge-integrated informed AI, along with observed and prominent issues that suggest worthwhile future research directions. Most importantly, this report suggests how the advantages of knowledge-integrated informed AI stand to benefit the national security domain. △ Less","4 February, 2022",https://arxiv.org/pdf/2202.03188
The 6-Ds of Creating AI-Enabled Systems,John Piorkowski,"We are entering our tenth year of the current Artificial Intelligence (AI) spring, and, as with previous AI hype cycles, the threat of an AI winter looms. AI winters occurred because of ineffective approaches towards navigating the technology valley of death. The 6-D framework provides an end-to-end framework to successfully navigate this challenge. The 6-D framework starts with problem decomposition to identify potential AI solutions, and ends with considerations for deployment of AI-enabled systems. Each component of the 6-D framework and a precision medicine use case is described in this paper. △ Less","4 February, 2022",https://arxiv.org/pdf/2202.03172
Existence and perception as the basis of AGI (Artificial General Intelligence),Victor V. Senkevich,"As is known, AGI (Artificial General Intelligence), unlike AI, should operate with meanings. And that's what distinguishes it from AI. Any successful AI implementations (playing chess, unmanned driving, face recognition etc.) do not operate with the meanings of the processed objects in any way and do not recognize the meaning. And they don't need to. But for AGI, which emulates human thinking, this ability is crucial. Numerous attempts to define the concept of ""meaning"" have one very significant drawback - all such definitions are not strict and formalized, so they cannot be programmed. The meaning search procedure should use a formalized description of its existence and possible forms of its perception. For the practical implementation of AGI, it is necessary to develop such ""ready-to-code"" descriptions in the context of their use for processing the related cognitive concepts of ""meaning"" and ""knowledge"". An attempt to formalize the definition of such concepts is made in this article. △ Less","30 January, 2022",https://arxiv.org/pdf/2202.03155
Think-Aloud Verbalizations for Identifying User Experience Problems: Effects of Language Proficiency with Chinese Non-Native English Speakers,Mingming Fan;Lingyun Zhu,"Subtle patterns in users' think-aloud (TA) verbalizations (i.e., utterances) are shown to be telltale signs of user experience (UX) problems and used to build artificial intelligence (AI) models or AI-assisted tools to help UX evaluators identify UX problems automatically or semi-automatically. Despite the potential of such verbalization patterns, they were uncovered with native English speakers. As most people who speak English are non-native speakers, it is important to investigate whether similar patterns exist in non-native English speakers' TA verbalizations. As a first step to answer this question, we conducted think-aloud usability testing with Chinese non-native English speakers and native English speakers using three common TA protocols. We compared their verbalizations and UX problems that they encountered to understand the effects of language and TA protocols. Our findings show that both language groups had similar amounts and proportions of verbalization categories, encountered similar problems, and had similar verbalization patterns that indicate UX problems. Furthermore, TA protocols did not significantly affect the correlations between verbalizations and problems. Based on the findings, we present three design implications for UX practitioners and the design of AI-assisted analysis tools. △ Less","7 February, 2022",https://arxiv.org/pdf/2202.02970
An Empirical Analysis of AI Contributions to Sustainable Cities (SDG11),Shivam Gupta;Auriol Degbelo,"Artificial Intelligence (AI) presents opportunities to develop tools and techniques for addressing some of the major global challenges and deliver solutions with significant social and economic impacts. The application of AI has far-reaching implications for the 17 Sustainable Development Goals (SDGs) in general, and sustainable urban development in particular. However, existing attempts to understand and use the opportunities offered by AI for SDG 11 have been explored sparsely, and the shortage of empirical evidence about the practical application of AI remains. In this chapter, we analyze the contribution of AI to support the progress of SDG 11 (Sustainable Cities and Communities). We address the knowledge gap by empirically analyzing the AI systems (N = 29) from the AIxSDG database and the Community Research and Development Information Service (CORDIS) database. Our analysis revealed that AI systems have indeed contributed to advancing sustainable cities in several ways (e.g., waste management, air quality monitoring, disaster response management, transportation management), but many projects are still working for citizens and not with them. This snapshot of AI's impact on SDG11 is inherently partial, yet useful to advance our understanding as we move towards more mature systems and research on the impact of AI systems for social good. △ Less","6 February, 2022",https://arxiv.org/pdf/2202.02879
"Human rights, democracy, and the rule of law assurance framework for AI systems: A proposal",David Leslie;Christopher Burr;Mhairi Aitken;Michael Katell;Morgan Briggs;Cami Rincon,"Following on from the publication of its Feasibility Study in December 2020, the Council of Europe's Ad Hoc Committee on Artificial Intelligence (CAHAI) and its subgroups initiated efforts to formulate and draft its Possible Elements of a Legal Framework on Artificial Intelligence, based on the Council of Europe's standards on human rights, democracy, and the rule of law. This document was ultimately adopted by the CAHAI plenary in December 2021. To support this effort, The Alan Turing Institute undertook a programme of research that explored the governance processes and practical tools needed to operationalise the integration of human right due diligence with the assurance of trustworthy AI innovation practices. The resulting framework was completed and submitted to the Council of Europe in September 2021. It presents an end-to-end approach to the assurance of AI project lifecycles that integrates context-based risk analysis and appropriate stakeholder engagement with comprehensive impact assessment, and transparent risk management, impact mitigation, and innovation assurance practices. Taken together, these interlocking processes constitute a Human Rights, Democracy and the Rule of Law Assurance Framework (HUDERAF). The HUDERAF combines the procedural requirements for principles-based human rights due diligence with the governance mechanisms needed to set up technical and socio-technical guardrails for responsible and trustworthy AI innovation practices. Its purpose is to provide an accessible and user-friendly set of mechanisms for facilitating compliance with a binding legal framework on artificial intelligence, based on the Council of Europe's standards on human rights, democracy, and the rule of law, and to ensure that AI innovation projects are carried out with appropriate levels of public accountability, transparency, and democratic governance. △ Less","6 February, 2022",https://arxiv.org/pdf/2202.02776
The Self-Driving Car: Crossroads at the Bleeding Edge of Artificial Intelligence and Law,Scott McLachlan;Evangelia Kyrimi;Kudakwashe Dube;Norman Fenton;Burkhard Schafer,"Artificial intelligence (AI) features are increasingly being embedded in cars and are central to the operation of self-driving cars (SDC). There is little or no effort expended towards understanding and assessing the broad legal and regulatory impact of the decisions made by AI in cars. A comprehensive literature review was conducted to determine the perceived barriers, benefits and facilitating factors of SDC in order to help us understand the suitability and limitations of existing and proposed law and regulation. (1) existing and proposed laws are largely based on claimed benefits of SDV that are still mostly speculative and untested; (2) while publicly presented as issues of assigning blame and identifying who pays where the SDC is involved in an accident, the barriers broadly intersect with almost every area of society, laws and regulations; and (3) new law and regulation are most frequently identified as the primary factor for enabling SDC. Research on assessing the impact of AI in SDC needs to be broadened beyond negligence and liability to encompass barriers, benefits and facilitating factors identified in this paper. Results of this paper are significant in that they point to the need for deeper comprehension of the broad impact of all existing law and regulations on the introduction of SDC technology, with a focus on identifying only those areas truly requiring ongoing legislative attention. △ Less","6 February, 2022",https://arxiv.org/pdf/2202.02734
Classification on Sentence Embeddings for Legal Assistance,Arka Mitra,"Legal proceedings take plenty of time and also cost a lot. The lawyers have to do a lot of work in order to identify the different sections of prior cases and statutes. The paper tries to solve the first tasks in AILA2021 (Artificial Intelligence for Legal Assistance) that will be held in FIRE2021 (Forum for Information Retrieval Evaluation). The task is to semantically segment the document into different assigned one of the 7 predefined labels or ""rhetorical roles."" The paper uses BERT to obtain the sentence embeddings from a sentence, and then a linear classifier is used to output the final prediction. The experiments show that when more weightage is assigned to the class with the highest frequency, the results are better than those when more weightage is given to the class with a lower frequency. In task 1, the team legalNLP obtained a F1 score of 0.22. △ Less","5 February, 2022",https://arxiv.org/pdf/2202.02639
Layer-wise Regularized Adversarial Training using Layers Sustainability Analysis (LSA) framework,Mohammad Khalooei;Mohammad Mehdi Homayounpour;Maryam Amirmazlaghani,"Deep neural network models are used today in various applications of artificial intelligence, the strengthening of which, in the face of adversarial attacks is of particular importance. An appropriate solution to adversarial attacks is adversarial training, which reaches a trade-off between robustness and generalization. This paper introduces a novel framework (Layer Sustainability Analysis (LSA)) for the analysis of layer vulnerability in an arbitrary neural network in the scenario of adversarial attacks. LSA can be a helpful toolkit to assess deep neural networks and to extend the adversarial training approaches towards improving the sustainability of model layers via layer monitoring and analysis. The LSA framework identifies a list of Most Vulnerable Layers (MVL list) of the given network. The relative error, as a comparison measure, is used to evaluate representation sustainability of each layer against adversarial inputs. The proposed approach for obtaining robust neural networks to fend off adversarial attacks is based on a layer-wise regularization (LR) over LSA proposal(s) for adversarial training (AT); i.e. the AT-LR procedure. AT-LR could be used with any benchmark adversarial attack to reduce the vulnerability of network layers and to improve conventional adversarial training approaches. The proposed idea performs well theoretically and experimentally for state-of-the-art multilayer perceptron and convolutional neural network architectures. Compared with the AT-LR and its corresponding base adversarial training, the classification accuracy of more significant perturbations increased by 16.35%, 21.79%, and 10.730% on Moon, MNIST, and CIFAR-10 benchmark datasets, respectively. The LSA framework is available and published at https://github.com/khalooei/LSA. △ Less","15 February, 2022",https://arxiv.org/pdf/2202.02626
Science Facing Interoperability as a Necessary Condition of Success and Evil,Remy Demichelis,"Artificial intelligence (AI) systems, such as machine learning algorithms, have allowed scientists, marketers and governments to shed light on correlations that remained invisible until now. Beforehand, the dots that we had to connect in order to imagine a new knowledge were either too numerous, too sparse or not even detected. Sometimes, the information was not stored in the same data lake or format and was not able to communicate. But in creating new bridges with AI, many problems appeared such as bias reproduction, unfair inferences or mass surveillance. Our aim is to show that, on one hand, the AI's deep ethical problem lays essentially in these new connections made possible by systems interoperability. In connecting the spheres of our life, these systems undermine the notion of justice particular to each of them, because the new interactions create dominances of social goods from a sphere to another. These systems make therefore spheres permeable to one another and, in doing so, they open to progress as well as to tyranny. On another hand, however, we would like to emphasize that the act to connect what used to seem a priori disjoint is a necessary move of knowledge and scientific progress. △ Less","5 February, 2022",https://arxiv.org/pdf/2202.02540
A Survey on Poisoning Attacks Against Supervised Machine Learning,Wenjun Qiu,"With the rise of artificial intelligence and machine learning in modern computing, one of the major concerns regarding such techniques is to provide privacy and security against adversaries. We present this survey paper to cover the most representative papers in poisoning attacks against supervised machine learning models. We first provide a taxonomy to categorize existing studies and then present detailed summaries for selected papers. We summarize and compare the methodology and limitations of existing literature. We conclude this paper with potential improvements and future directions to further exploit and prevent poisoning attacks on supervised models. We propose several unanswered research questions to encourage and inspire researchers for future work. △ Less","7 February, 2022",https://arxiv.org/pdf/2202.02510
Space-Air-Ground Integrated Multi-domain Network Resource Orchestration based on Virtual Network Architecture: a DRL Method,Peiying Zhang;Chao Wang;Neeraj Kumar;Lei Liu,"Traditional ground wireless communication networks cannot provide high-quality services for artificial intelligence (AI) applications such as intelligent transportation systems (ITS) due to deployment, coverage and capacity issues. The space-air-ground integrated network (SAGIN) has become a research focus in the industry. Compared with traditional wireless communication networks, SAGIN is more flexible and reliable, and it has wider coverage and higher quality of seamless connection. However, due to its inherent heterogeneity, time-varying and self-organizing characteristics, the deployment and use of SAGIN still faces huge challenges, among which the orchestration of heterogeneous resources is a key issue. Based on virtual network architecture and deep reinforcement learning (DRL), we model SAGIN's heterogeneous resource orchestration as a multi-domain virtual network embedding (VNE) problem, and propose a SAGIN cross-domain VNE algorithm. We model the different network segments of SAGIN, and set the network attributes according to the actual situation of SAGIN and user needs. In DRL, the agent is acted by a five-layer policy network. We build a feature matrix based on network attributes extracted from SAGIN and use it as the agent training environment. Through training, the probability of each underlying node being embedded can be derived. In test phase, we complete the embedding process of virtual nodes and links in turn based on this probability. Finally, we verify the effectiveness of the algorithm from both training and testing. △ Less","2 February, 2022",https://arxiv.org/pdf/2202.02459
Towards Training Reproducible Deep Learning Models,Boyuan Chen;Mingzhi Wen;Yong Shi;Dayi Lin;Gopi Krishnan Rajbahadur;Zhen Ming;Jiang,"Reproducibility is an increasing concern in Artificial Intelligence (AI), particularly in the area of Deep Learning (DL). Being able to reproduce DL models is crucial for AI-based systems, as it is closely tied to various tasks like training, testing, debugging, and auditing. However, DL models are challenging to be reproduced due to issues like randomness in the software (e.g., DL algorithms) and non-determinism in the hardware (e.g., GPU). There are various practices to mitigate some of the aforementioned issues. However, many of them are either too intrusive or can only work for a specific usage context. In this paper, we propose a systematic approach to training reproducible DL models. Our approach includes three main parts: (1) a set of general criteria to thoroughly evaluate the reproducibility of DL models for two different domains, (2) a unified framework which leverages a record-and-replay technique to mitigate software-related randomness and a profile-and-patch technique to control hardware-related non-determinism, and (3) a reproducibility guideline which explains the rationales and the mitigation strategies on conducting a reproducible training process for DL models. Case study results show our approach can successfully reproduce six open source and one commercial DL models. △ Less","4 February, 2022",https://arxiv.org/pdf/2202.02326
Towards a consistent interpretation of AIOps models,Yingzhe Lyu;Gopi Krishnan Rajbahadur;Dayi Lin;Boyuan Chen;Zhen Ming;Jiang,"Artificial Intelligence for IT Operations (AIOps) has been adopted in organizations in various tasks, including interpreting models to identify indicators of service failures. To avoid misleading practitioners, AIOps model interpretations should be consistent (i.e., different AIOps models on the same task agree with one another on feature importance). However, many AIOps studies violate established practices in the machine learning community when deriving interpretations, such as interpreting models with suboptimal performance, though the impact of such violations on the interpretation consistency has not been studied. In this paper, we investigate the consistency of AIOps model interpretation along three dimensions: internal consistency, external consistency, and time consistency. We conduct a case study on two AIOps tasks: predicting Google cluster job failures, and Backblaze hard drive failures. We find that the randomness from learners, hyperparameter tuning, and data sampling should be controlled to generate consistent interpretations. AIOps models with AUCs greater than 0.75 yield more consistent interpretation compared to low-performing models. Finally, AIOps models that are constructed with the Sliding Window or Full History approaches have the most consistent interpretation with the trends presented in the entire datasets. Our study provides valuable guidelines for practitioners to derive consistent AIOps model interpretation. △ Less","4 February, 2022",https://arxiv.org/pdf/2202.02298
Choosing an Appropriate Platform and Workflow for Processing Camera Trap Data using Artificial Intelligence,Juliana Vélez;Paula J. Castiblanco-Camacho;Michael A. Tabak;Carl Chalmers;Paul Fergus;John Fieberg,"Camera traps have transformed how ecologists study wildlife species distributions, activity patterns, and interspecific interactions. Although camera traps provide a cost-effective method for monitoring species, the time required for data processing can limit survey efficiency. Thus, the potential of Artificial Intelligence (AI), specifically Deep Learning (DL), to process camera-trap data has gained considerable attention. Using DL for these applications involves training algorithms, such as Convolutional Neural Networks (CNNs), to automatically detect objects and classify species. To overcome technical challenges associated with training CNNs, several research communities have recently developed platforms that incorporate DL in easy-to-use interfaces. We review key characteristics of four AI-powered platforms -- Wildlife Insights (WI), MegaDetector (MD), Machine Learning for Wildlife Image Classification (MLWIC2), and Conservation AI -- including data management tools and AI features. We also provide R code in an open-source GitBook, to demonstrate how users can evaluate model performance, and incorporate AI output in semi-automated workflows. We found that species classifications from WI and MLWIC2 generally had low recall values (animals that were present in the images often were not classified to the correct species). Yet, the precision of WI and MLWIC2 classifications for some species was high (i.e., when classifications were made, they were generally accurate). MD, which classifies images using broader categories (e.g., ""blank"" or ""animal""), also performed well. Thus, we conclude that, although species classifiers were not accurate enough to automate image processing, DL could be used to improve efficiencies by accepting classifications with high confidence values for certain species or by filtering images containing blanks. △ Less","4 February, 2022",https://arxiv.org/pdf/2202.02283
The Ecological Footprint of Neural Machine Translation Systems,Dimitar Shterionov;Eva Vanmassenhove,"Over the past decade, deep learning (DL) has led to significant advancements in various fields of artificial intelligence, including machine translation (MT). These advancements would not be possible without the ever-growing volumes of data and the hardware that allows large DL models to be trained efficiently. Due to the large amount of computing cores as well as dedicated memory, graphics processing units (GPUs) are a more effective hardware solution for training and inference with DL models than central processing units (CPUs). However, the former is very power demanding. The electrical power consumption has economical as well as ecological implications. This chapter focuses on the ecological footprint of neural MT systems. It starts from the power drain during the training of and the inference with neural MT models and moves towards the environment impact, in terms of carbon dioxide emissions. Different architectures (RNN and Transformer) and different GPUs (consumer-grate NVidia 1080Ti and workstation-grade NVidia P100) are compared. Then, the overall CO2 offload is calculated for Ireland and the Netherlands. The NMT models and their ecological impact are compared to common household appliances to draw a more clear picture. The last part of this chapter analyses quantization, a technique for reducing the size and complexity of models, as a way to reduce power consumption. As quantized models can run on CPUs, they present a power-efficient inference solution without depending on a GPU. △ Less","4 February, 2022",https://arxiv.org/pdf/2202.02170
Multi Objective Resource Optimization of Wireless Network Based on Cross Domain Virtual Network Embedding,Chao Wang;Tao Dong;Youxiang Duan;Qifeng Sun;Peiying Zhang,"The rapid development of virtual network architecture makes it possible for wireless network to be widely used. With the popularity of artificial intelligence (AI) industry in daily life, efficient resource allocation of wireless network has become a problem. Especially when network users request wireless network resources from different management domains, they still face many practical problems. From the perspective of virtual network embedding (VNE), this paper designs and implements a multi-objective optimization VNE algorithm for wireless network resource allocation. Resource allocation in virtual network is essentially a problem of allocating underlying resources for virtual network requests (VNRs). According to the proposed objective formula, we consider the optimization mapping cost, network delay and VNR acceptance rate. VNE is completed by node mapping and link mapping. In the experiment and simulation stage, it is compared with other VNE algorithms, the cross domain VNE algorithm proposed in this paper is optimal in the above three indicators. This shows the effectiveness of the algorithm in wireless network resource allocation. △ Less","3 February, 2022",https://arxiv.org/pdf/2202.02139
Interpretability methods of machine learning algorithms with applications in breast cancer diagnosis,Panagiota Karatza;Kalliopi V. Dalakleidi;Maria Athanasiou;Konstantina S. Nikita,"Early detection of breast cancer is a powerful tool towards decreasing its socioeconomic burden. Although, artificial intelligence (AI) methods have shown remarkable results towards this goal, their ""black box"" nature hinders their wide adoption in clinical practice. To address the need for AI guided breast cancer diagnosis, interpretability methods can be utilized. In this study, we used AI methods, i.e., Random Forests (RF), Neural Networks (NN) and Ensembles of Neural Networks (ENN), towards this goal and explained and optimized their performance through interpretability techniques, such as the Global Surrogate (GS) method, the Individual Conditional Expectation (ICE) plots and the Shapley values (SV). The Wisconsin Diagnostic Breast Cancer (WDBC) dataset of the open UCI repository was used for the training and evaluation of the AI algorithms. The best performance for breast cancer diagnosis was achieved by the proposed ENN (96.6% accuracy and 0.96 area under the ROC curve), and its predictions were explained by ICE plots, proving that its decisions were compliant with current medical knowledge and can be further utilized to gain new insights in the pathophysiological mechanisms of breast cancer. Feature selection based on features' importance according to the GS model improved the performance of the RF (leading the accuracy from 96.49% to 97.18% and the area under the ROC curve from 0.96 to 0.97) and feature selection based on features' importance according to SV improved the performance of the NN (leading the accuracy from 94.6% to 95.53% and the area under the ROC curve from 0.94 to 0.95). Compared to other approaches on the same dataset, our proposed models demonstrated state of the art performance while being interpretable. △ Less","4 February, 2022",https://arxiv.org/pdf/2202.02131
Artificial Intelligence Powered Material Search Engine,Mohendra Roy,"Many data-driven applications in material science have been made possible because of recent breakthroughs in artificial intelligence(AI). The use of AI in material engineering is becoming more viable as the number of material data such as X-Ray diffraction, various spectroscopy, and microscope data grows. In this work, we have reported a material search engine that uses the interatomic space (d value) from X-ray diffraction to provide material information. We have investigated various techniques for predicting prospective material using X-ray diffraction data. We used the Random Forest, Naive Bayes (Gaussian), and Neural Network algorithms to achieve this. These algorithms have an average accuracy of 88.50\%, 100.0\%, and 88.89\%, respectively. Finally, we combined all these techniques into an ensemble approach to make the prediction more generic. This ensemble method has a ~100\% accuracy rate. Furthermore, we are designing a graph neural network (GNN)-based architecture to improve interpretability and accuracy. Thus, we want to solve the computational and time complexity of traditional dictionary-based and metadata-based material search engines and to provide a more generic prediction. △ Less","19 January, 2022",https://arxiv.org/pdf/2202.01916
QoS-SLA-Aware Adaptive Genetic Algorithm for Multi-Request Offloading in Integrated Edge-Cloud Computing in Internet of Vehicles,Leila Ismail;Huned Materwala;Hossam S. Hassanein,"The Internet of Vehicles over Vehicular Ad-hoc Networks is an emerging technology enabling the development of smart city applications focused on improving traffic safety, traffic efficiency, and the overall driving experience. These applications have stringent requirements detailed in Service Level Agreement. Since vehicles have limited computational and storage capabilities, applications requests are offloaded onto an integrated edge-cloud computing system. Existing offloading solutions focus on optimizing the application's Quality of Service (QoS) in terms of execution time, and respecting a single SLA constraint. They do not consider the impact of overlapped multi-requests processing nor the vehicle's varying speed. This paper proposes a novel Artificial Intelligence QoS-SLA-aware adaptive genetic algorithm (QoS-SLA-AGA) to optimize the application's execution time for multi-request offloading in a heterogeneous edge-cloud computing system, which considers the impact of processing multi-requests overlapping and dynamic vehicle speed. The proposed genetic algorithm integrates an adaptive penalty function to assimilate the SLA constraints regarding latency, processing time, deadline, CPU, and memory requirements. Numerical experiments and analysis compare our QoS-SLA-AGA to random offloading, and baseline genetic-based approaches. Results show QoS-SLA-AGA executes the requests 1.22 times faster on average compared to the random offloading approach and with 59.9% fewer SLA violations. In contrast, the baseline genetic-based approach increases the requests' performance by 1.14 times, with 19.8% more SLA violations. △ Less","1 March, 2022",https://arxiv.org/pdf/2202.01696
Computer sciences and synthesis: retrospective and perspective,Vladislav Dorofeev;Petro Trokhimchuk,"The problem of synthesis in computer sciences, including cybernetics, artificial intelligence and system analysis, is analyzed. Main methods of realization this problem are discussed. Ways of search universal method of creation universal synthetic science are represented. As example of such universal method polymetric analysis is given. Perspective of further development of this research, including application polymetric method for the resolution main problems of computer sciences, is analyzed too. △ Less","25 January, 2022",https://arxiv.org/pdf/2202.01291
An Experience Report of Executive-Level Artificial Intelligence Education in the United Arab Emirates,David Johnson;Mohammad Alsharid;Rasheed El-Bouri;Nigel Mehdi;Farah Shamout;Alexandre Szenicer;David Toman;Saqr Binghalib,"Teaching artificial intelligence (AI) is challenging. It is a fast moving field and therefore difficult to keep people updated with the state-of-the-art. Educational offerings for students are ever increasing, beyond university degree programs where AI education traditionally lay. In this paper, we present an experience report of teaching an AI course to business executives in the United Arab Emirates (UAE). Rather than focusing only on theoretical and technical aspects, we developed a course that teaches AI with a view to enabling students to understand how to incorporate it into existing business processes. We present an overview of our course, curriculum and teaching methods, and we discuss our reflections on teaching adult learners, and to students in the UAE. △ Less","2 February, 2022",https://arxiv.org/pdf/2202.01281
"Understanding O-RAN: Architecture, Interfaces, Algorithms, Security, and Research Challenges",Michele Polese;Leonardo Bonati;Salvatore D'Oro;Stefano Basagni;Tommaso Melodia,"The Open Radio Access Network (RAN) and its embodiment through the O-RAN Alliance specifications are poised to revolutionize the telecom ecosystem. O-RAN promotes virtualized RANs where disaggregated components are connected via open interfaces and optimized by intelligent controllers. The result is a new paradigm for the RAN design, deployment, and operations: O-RAN networks can be built with multi-vendor, interoperable components, and can be programmatically optimized through a centralized abstraction layer and data-driven closed-loop control. Therefore, understanding O-RAN, its architecture, its interfaces, and workflows is key for researchers and practitioners in the wireless community. In this article, we present the first detailed tutorial on O-RAN. We also discuss the main research challenges and review early research results. We provide a deep dive of the O-RAN specifications, describing its architecture, design principles, and the O-RAN interfaces. We then describe how the O-RAN RAN Intelligent Controllers (RICs) can be used to effectively control and manage 3GPP-defined RANs. Based on this, we discuss innovations and challenges of O-RAN networks, including the Artificial Intelligence (AI) and Machine Learning (ML) workflows that the architecture and interfaces enable, security and standardization issues. Finally, we review experimental research platforms that can be used to design and test O-RAN networks, along with recent research results, and we outline future directions for O-RAN development. △ Less","1 August, 2022",https://arxiv.org/pdf/2202.01032
MMSys'22 Grand Challenge on AI-based Video Production for Soccer,Cise Midoglu;Steven A. Hicks;Vajira Thambawita;Tomas Kupka;Pål Halvorsen,"Soccer has a considerable market share of the global sports industry, and the interest in viewing videos from soccer games continues to grow. In this respect, it is important to provide game summaries and highlights of the main game events. However, annotating and producing events and summaries often require expensive equipment and a lot of tedious, cumbersome, manual labor. Therefore, automating the video production pipeline providing fast game highlights at a much lower cost is seen as the ""holy grail"". In this context, recent developments in Artificial Intelligence (AI) technology have shown great potential. Still, state-of-the-art approaches are far from being adequate for practical scenarios that have demanding real-time requirements, as well as strict performance criteria (where at least the detection of official events such as goals and cards must be 100% accurate). In addition, event detection should be thoroughly enhanced by annotation and classification, proper clipping, generating short descriptions, selecting appropriate thumbnails for highlight clips, and finally, combining the event highlights into an overall game summary, similar to what is commonly aired during sports news. Even though the event tagging operation has by far received the most attention, an end-to-end video production pipeline also includes various other operations which serve the overall purpose of automated soccer analysis. This challenge aims to assist the automation of such a production pipeline using AI. In particular, we focus on the enhancement operations that take place after an event has been detected, namely event clipping (Task 1), thumbnail selection (Task 2), and game summarization (Task 3). Challenge website: https://mmsys2022.ie/authors/grand-challenge. △ Less","2 February, 2022",https://arxiv.org/pdf/2202.01031
Machine Intelligence-Driven Classification of Cancer Patients-Derived Extracellular Vesicles using Fluorescence Correlation Spectroscopy: Results from a Pilot Study,Abicumaran Uthamacumaran;Mohamed Abdouh;Kinshuk Sengupta;Zu-hua Gao;Stefano Forte;Thupten Tsering;Julia V Burnier;Goffredo Arena,"Patient-derived extracellular vesicles (EVs) that contains a complex biological cargo is a valuable source of liquid biopsy diagnostics to aid in early detection, cancer screening, and precision nanotherapeutics. In this study, we predicted that coupling cancer patient blood-derived EVs to time-resolved spectroscopy and artificial intelligence (AI) could provide a robust cancer screening and follow-up tools. Methods: Fluorescence correlation spectroscopy (FCS) measurements were performed on 24 blood samples-derived EVs. Blood samples were obtained from 15 cancer patients (presenting 5 different types of cancers), and 9 healthy controls (including patients with benign lesions). The obtained FCS autocorrelation spectra were processed into power spectra using the Fast-Fourier Transform algorithm and subjected to various machine learning algorithms to distinguish cancer spectra from healthy control spectra. Results and Applications: The performance of AdaBoost Random Forest (RF) classifier, support vector machine, and multilayer perceptron, were tested on selected frequencies in the N=118 power spectra. The RF classifier exhibited a 90% classification accuracy and high sensitivity and specificity in distinguishing the FCS power spectra of cancer patients from those of healthy controls. Further, an image convolutional neural network (CNN), ResNet network, and a quantum CNN were assessed on the power spectral images as additional validation tools. All image-based CNNs exhibited a nearly equal classification performance with an accuracy of roughly 82% and reasonably high sensitivity and specificity scores. Our pilot study demonstrates that AI-algorithms coupled to time-resolved FCS power spectra can accurately and differentially classify the complex patient-derived EVs from different cancer samples of distinct tissue subtypes. △ Less","1 February, 2022",https://arxiv.org/pdf/2202.00495
Explainable AI through the Learning of Arguments,Jonas Bei;David Pomerenke;Lukas Schreiner;Sepideh Sharbaf;Pieter Collins;Nico Roos,"Learning arguments is highly relevant to the field of explainable artificial intelligence. It is a family of symbolic machine learning techniques that is particularly human-interpretable. These techniques learn a set of arguments as an intermediate representation. Arguments are small rules with exceptions that can be chained to larger arguments for making predictions or decisions. We investigate the learning of arguments, specifically the learning of arguments from a 'case model' proposed by Verheij [34]. The case model in Verheij's approach are cases or scenarios in a legal setting. The number of cases in a case model are relatively low. Here, we investigate whether Verheij's approach can be used for learning arguments from other types of data sets with a much larger number of instances. We compare the learning of arguments from a case model with the HeRO algorithm [15] and learning a decision tree. △ Less","1 February, 2022",https://arxiv.org/pdf/2202.00383
Quantifying Relevance in Learning and Inference,Matteo Marsili;Yasser Roudi,"Learning is a distinctive feature of intelligent behaviour. High-throughput experimental data and Big Data promise to open new windows on complex systems such as cells, the brain or our societies. Yet, the puzzling success of Artificial Intelligence and Machine Learning shows that we still have a poor conceptual understanding of learning. These applications push statistical inference into uncharted territories where data is high-dimensional and scarce, and prior information on ""true"" models is scant if not totally absent. Here we review recent progress on understanding learning, based on the notion of ""relevance"". The relevance, as we define it here, quantifies the amount of information that a dataset or the internal representation of a learning machine contains on the generative model of the data. This allows us to define maximally informative samples, on one hand, and optimal learning machines on the other. These are ideal limits of samples and of machines, that contain the maximal amount of information about the unknown generative process, at a given resolution (or level of compression). Both ideal limits exhibit critical features in the statistical sense: Maximally informative samples are characterised by a power-law frequency distribution (statistical criticality) and optimal learning machines by an anomalously large susceptibility. The trade-off between resolution (i.e. compression) and relevance distinguishes the regime of noisy representations from that of lossy compression. These are separated by a special point characterised by Zipf's law statistics. This identifies samples obeying Zipf's law as the most compressed loss-less representations that are optimal in the sense of maximal relevance. Criticality in optimal learning machines manifests in an exponential degeneracy of energy levels, that leads to unusual thermodynamic properties. △ Less","1 February, 2022",https://arxiv.org/pdf/2202.00339
Submodularity In Machine Learning and Artificial Intelligence,Jeff Bilmes,"In this manuscript, we offer a gentle review of submodularity and supermodularity and their properties. We offer a plethora of submodular definitions; a full description of a number of example submodular functions and their generalizations; example discrete constraints; a discussion of basic algorithms for maximization, minimization, and other operations; a brief overview of continuous submodular extensions; and some historical applications. We then turn to how submodularity is useful in machine learning and artificial intelligence. This includes summarization, and we offer a complete account of the differences between and commonalities amongst sketching, coresets, extractive and abstractive summarization in NLP, data distillation and condensation, and data subset selection and feature selection. We discuss a variety of ways to produce a submodular function useful for machine learning, including heuristic hand-crafting, learning or approximately learning a submodular function or aspects thereof, and some advantages of the use of a submodular function as a coreset producer. We discuss submodular combinatorial information functions, and how submodularity is useful for clustering, data partitioning, parallel machine learning, active and semi-supervised learning, probabilistic modeling, and structured norms and loss functions. △ Less","4 October, 2022",https://arxiv.org/pdf/2202.00132
"Won't you see my neighbor?: User predictions, mental models, and similarity-based explanations of AI classifiers",Kimberly Glasgow;Jonathan Kopecky;John Gersh;Adam Crego,"Humans should be able work more effectively with artificial intelligence-based systems when they can predict likely failures and form useful mental models of how the systems work. We conducted a study of human's mental models of artificial intelligence systems using a high-performing image classifier, focusing on participants' ability to predict the classification result for a particular image. Participants viewed individual labeled images in one of two classes and then tried to predict whether the classifier would label them correctly. In this experiment we explored the effect of giving participants additional information about an image's nearest neighbors in a space representing the otherwise uninterpretable features extracted by the lower layers of the classifier's neural network. We found that providing this information did increase participants' prediction performance, and that the performance improvement could be related to the neighbor images' similarity to the target image. We also found indications that the presentation of this information may influence people's own classification of the target image -- that is, rather than just anthropomorphizing the system, in some cases the humans become ""mechanomorphized"" in their judgements. △ Less","31 January, 2022",https://arxiv.org/pdf/2201.13301
Network-level Safety Metrics for Overall Traffic Safety Assessment: A Case Study,Xiwen Chen;Hao Wang;Abolfazl Razi;Brendan Russo;Jason Pacheco;John Roberts;Jeffrey Wishart;Larry Head;Alonso Granados Baca,"Driving safety analysis has recently experienced unprecedented improvements thanks to technological advances in precise positioning sensors, artificial intelligence (AI)-based safety features, autonomous driving systems, connected vehicles, high-throughput computing, and edge computing servers. Particularly, deep learning (DL) methods empowered volume video processing to extract safety-related features from massive videos captured by roadside units (RSU). Safety metrics are commonly used measures to investigate crashes and near-conflict events. However, these metrics provide limited insight into the overall network-level traffic management. On the other hand, some safety assessment efforts are devoted to processing crash reports and identifying spatial and temporal patterns of crashes that correlate with road geometry, traffic volume, and weather conditions. This approach relies merely on crash reports and ignores the rich information of traffic videos that can help identify the role of safety violations in crashes. To bridge these two perspectives, we define a new set of network-level safety metrics (NSM) to assess the overall safety profile of traffic flow by processing imagery taken by RSU cameras. Our analysis suggests that NSMs show significant statistical associations with crash rates. This approach is different than simply generalizing the results of individual crash analyses, since all vehicles contribute to calculating NSMs, not only the ones involved in crash incidents. This perspective considers the traffic flow as a complex dynamic system where actions of some nodes can propagate through the network and influence the crash risk for other nodes. We also provide a comprehensive review of surrogate safety metrics (SSM) in the Appendix A. △ Less","13 June, 2022",https://arxiv.org/pdf/2201.13229
Causal Explanations and XAI,Sander Beckers,"Although standard Machine Learning models are optimized for making predictions about observations, more and more they are used for making predictions about the results of actions. An important goal of Explainable Artificial Intelligence (XAI) is to compensate for this mismatch by offering explanations about the predictions of an ML-model which ensure that they are reliably action-guiding. As action-guiding explanations are causal explanations, the literature on this topic is starting to embrace insights from the literature on causal models. Here I take a step further down this path by formally defining the causal notions of sufficient explanations and counterfactual explanations. I show how these notions relate to (and improve upon) existing work, and motivate their adequacy by illustrating how different explanations are action-guiding under different circumstances. Moreover, this work is the first to offer a formal definition of actual causation that is founded entirely in action-guiding explanations. Although the definitions are motivated by a focus on XAI, the analysis of causal explanation and actual causation applies in general. I also touch upon the significance of this work for fairness in AI by showing how actual causation can be used to improve the idea of path-specific counterfactual fairness. △ Less","14 February, 2022",https://arxiv.org/pdf/2201.13169
Computational Complexity of Segmentation,Federico Adolfi;Todd Wareham;Iris van Rooij,"Computational feasibility is a widespread concern that guides the framing and modeling of biological and artificial intelligence. The specification of cognitive system capacities is often shaped by unexamined intuitive assumptions about the search space and complexity of a subcomputation. However, a mistaken intuition might make such initial conceptualizations misleading for what empirical questions appear relevant later on. We undertake here computational-level modeling and complexity analyses of segmentation - a widely hypothesized subcomputation that plays a requisite role in explanations of capacities across domains - as a case study to show how crucial it is to formally assess these assumptions. We mathematically prove two sets of results regarding hardness and search space size that may run counter to intuition, and position their implications with respect to existing views on the subcapacity. △ Less","11 May, 2022",https://arxiv.org/pdf/2201.13106
Computational Metacognition,Michael Cox;Zahiduddin Mohammad;Sravya Kondrakunta;Ventaksamapth Raja Gogineni;Dustin Dannenhauer;Othalia Larue,Computational metacognition represents a cognitive systems perspective on high-order reasoning in integrated artificial systems that seeks to leverage ideas from human metacognition and from metareasoning approaches in artificial intelligence. The key characteristic is to declaratively represent and then monitor traces of cognitive activity in an intelligent system in order to manage the performance of cognition itself. Improvements in cognition then lead to improvements in behavior and thus performance. We illustrate these concepts with an agent implementation in a cognitive architecture called MIDCA and show the value of metacognition in problem-solving. The results illustrate how computational metacognition improves performance by changing cognition through meta-level goal operations and learning. △ Less,"30 January, 2022",https://arxiv.org/pdf/2201.12885
A Safety-Critical Decision Making and Control Framework Combining Machine Learning and Rule-based Algorithms,Andrei Aksjonov;Ville Kyrki,"While artificial-intelligence-based methods suffer from lack of transparency, rule-based methods dominate in safety-critical systems. Yet, the latter cannot compete with the first ones in robustness to multiple requirements, for instance, simultaneously addressing safety, comfort, and efficiency. Hence, to benefit from both methods they must be joined in a single system. This paper proposes a decision making and control framework, which profits from advantages of both the rule- and machine-learning-based techniques while compensating for their disadvantages. The proposed method embodies two controllers operating in parallel, called Safety and Learned. A rule-based switching logic selects one of the actions transmitted from both controllers. The Safety controller is prioritized every time, when the Learned one does not meet the safety constraint, and also directly participates in the safe Learned controller training. Decision making and control in autonomous driving is chosen as the system case study, where an autonomous vehicle learns a multi-task policy to safely cross an unprotected intersection. Multiple requirements (i.e., safety, efficiency, and comfort) are set for vehicle operation. A numerical simulation is performed for the proposed framework validation, where its ability to satisfy the requirements and robustness to changing environment is successfully demonstrated. △ Less","30 January, 2022",https://arxiv.org/pdf/2201.12819
Any-Play: An Intrinsic Augmentation for Zero-Shot Coordination,Keane Lucas;Ross E. Allen,"Cooperative artificial intelligence with human or superhuman proficiency in collaborative tasks stands at the frontier of machine learning research. Prior work has tended to evaluate cooperative AI performance under the restrictive paradigms of self-play (teams composed of agents trained together) and cross-play (teams of agents trained independently but using the same algorithm). Recent work has indicated that AI optimized for these narrow settings may make for undesirable collaborators in the real-world. We formalize an alternative criteria for evaluating cooperative AI, referred to as inter-algorithm cross-play, where agents are evaluated on teaming performance with all other agents within an experiment pool with no assumption of algorithmic similarities between agents. We show that existing state-of-the-art cooperative AI algorithms, such as Other-Play and Off-Belief Learning, under-perform in this paradigm. We propose the Any-Play learning augmentation -- a multi-agent extension of diversity-based intrinsic rewards for zero-shot coordination (ZSC) -- for generalizing self-play-based algorithms to the inter-algorithm cross-play setting. We apply the Any-Play learning augmentation to the Simplified Action Decoder (SAD) and demonstrate state-of-the-art performance in the collaborative card game Hanabi. △ Less","28 January, 2022",https://arxiv.org/pdf/2201.12436
Bioinspired Cortex-based Fast Codebook Generation,Meric Yucel;Serdar Bagis;Ahmet Sertbas;Mehmet Sarikaya;Burak Berk Ustundag,"A major archetype of artificial intelligence is developing algorithms facilitating temporal efficiency and accuracy while boosting the generalization performance. Even with the latest developments in machine learning, a key limitation has been the inefficient feature extraction from the initial data, which is essential in performance optimization. Here, we introduce a feature extraction method inspired by sensory cortical networks in the brain. Dubbed as bioinspired cortex, the algorithm provides convergence to orthogonal features from streaming signals with superior computational efficiency while processing data in compressed form. We demonstrate the performance of the new algorithm using artificially created complex data by comparing it with the commonly used traditional clustering algorithms, such as Birch, GMM, and K-means. While the data processing time is significantly reduced, seconds versus hours, encoding distortions remain essentially the same in the new algorithm providing a basis for better generalization. Although we show herein the superior performance of the cortex model in clustering and vector quantization, it also provides potent implementation opportunities for machine learning fundamental components, such as reasoning, anomaly detection and classification in large scope applications, e.g., finance, cybersecurity, and healthcare. △ Less","28 January, 2022",https://arxiv.org/pdf/2201.12322
Recursive Binding for Similarity-Preserving Hypervector Representations of Sequences,Dmitri A. Rachkovskij;Denis Kleyko,"Hyperdimensional computing (HDC), also known as vector symbolic architectures (VSA), is a computing framework used within artificial intelligence and cognitive computing that operates with distributed vector representations of large fixed dimensionality. A critical step for designing the HDC/VSA solutions is to obtain such representations from the input data. Here, we focus on sequences and propose their transformation to distributed representations that both preserve the similarity of identical sequence elements at nearby positions and are equivariant to the sequence shift. These properties are enabled by forming representations of sequence positions using recursive binding and superposition operations. The proposed transformation was experimentally investigated with symbolic strings used for modeling human perception of word similarity. The obtained results are on a par with more sophisticated approaches from the literature. The proposed transformation was designed for the HDC/VSA model known as Fourier Holographic Reduced Representations. However, it can be adapted to some other HDC/VSA models. △ Less","16 May, 2022",https://arxiv.org/pdf/2201.11691
Density-Aware Hyper-Graph Neural Networks for Graph-based Semi-supervised Node Classification,Jianpeng Liao;Qian Tao;Jun Yan,"Graph-based semi-supervised learning, which can exploit the connectivity relationship between labeled and unlabeled data, has been shown to outperform the state-of-the-art in many artificial intelligence applications. One of the most challenging problems for graph-based semi-supervised node classification is how to use the implicit information among various data to improve the performance of classifying. Traditional studies on graph-based semi-supervised learning have focused on the pairwise connections among data. However, the data correlation in real applications could be beyond pairwise and more complicated. The density information has been demonstrated to be an important clue, but it is rarely explored in depth among existing graph-based semi-supervised node classification methods. To develop a flexible and effective model for graph-based semi-supervised node classification, we propose a novel Density-Aware Hyper-Graph Neural Networks (DA-HGNN). In our proposed approach, hyper-graph is provided to explore the high-order semantic correlation among data, and a density-aware hyper-graph attention network is presented to explore the high-order connection relationship. Extensive experiments are conducted in various benchmark datasets, and the results demonstrate the effectiveness of the proposed approach. △ Less","27 January, 2022",https://arxiv.org/pdf/2201.11511
Human-centered mechanism design with Democratic AI,Raphael Koster;Jan Balaguer;Andrea Tacchetti;Ari Weinstein;Tina Zhu;Oliver Hauser;Duncan Williams;Lucy Campbell-Gillingham;Phoebe Thacker;Matthew Botvinick;Christopher Summerfield,"Building artificial intelligence (AI) that aligns with human values is an unsolved problem. Here, we developed a human-in-the-loop research pipeline called Democratic AI, in which reinforcement learning is used to design a social mechanism that humans prefer by majority. A large group of humans played an online investment game that involved deciding whether to keep a monetary endowment or to share it with others for collective benefit. Shared revenue was returned to players under two different redistribution mechanisms, one designed by the AI and the other by humans. The AI discovered a mechanism that redressed initial wealth imbalance, sanctioned free riders, and successfully won the majority vote. By optimizing for human preferences, Democratic AI may be a promising method for value-aligned policy innovation. △ Less","27 January, 2022",https://arxiv.org/pdf/2201.11441
Epistemic AI platform accelerates innovation by connecting biomedical knowledge,Da Chen Emily Koo;Heather Bowling;Kenneth Ashworth;David J. Heeger;Stefano Pacifico,"Epistemic AI accelerates biomedical discovery by finding hidden connections in the network of biomedical knowledge. The Epistemic AI web-based software platform embodies the concept of knowledge mapping, an interactive process that relies on a knowledge graph in combination with natural language processing (NLP), information retrieval, relevance feedback, and network analysis. Knowledge mapping reduces information overload, prevents costly mistakes, and minimizes missed opportunities in the research process. The platform combines state-of-the-art methods for information extraction with machine learning, artificial intelligence and network analysis. Starting from a single biological entity, such as a gene or disease, users may: a) construct a map of connections to that entity, b) map an entire domain of interest, and c) gain insight into large biological networks of knowledge. Knowledge maps provide clarity and organization, simplifying the day-to-day research processes. △ Less","31 March, 2022",https://arxiv.org/pdf/2201.11331
To what extent should we trust AI models when they extrapolate?,Roozbeh Yousefzadeh;Xuenan Cao,"Many applications affecting human lives rely on models that have come to be known under the umbrella of machine learning and artificial intelligence. These AI models are usually complicated mathematical functions that map from an input space to an output space. Stakeholders are interested to know the rationales behind models' decisions and functional behavior. We study this functional behavior in relation to the data used to create the models. On this topic, scholars have often assumed that models do not extrapolate, i.e., they learn from their training samples and process new input by interpolation. This assumption is questionable: we show that models extrapolate frequently; the extent of extrapolation varies and can be socially consequential. We demonstrate that extrapolation happens for a substantial portion of datasets more than one would consider reasonable. How can we trust models if we do not know whether they are extrapolating? Given a model trained to recommend clinical procedures for patients, can we trust the recommendation when the model considers a patient older or younger than all the samples in the training set? If the training set is mostly Whites, to what extent can we trust its recommendations about Black and Hispanic patients? Which dimension (race, gender, or age) does extrapolation happen? Even if a model is trained on people of all races, it still may extrapolate in significant ways related to race. The leading question is, to what extent can we trust AI models when they process inputs that fall outside their training set? This paper investigates several social applications of AI, showing how models extrapolate without notice. We also look at different sub-spaces of extrapolation for specific individuals subject to AI models and report how these extrapolations can be interpreted, not mathematically, but from a humanistic point of view. △ Less","26 January, 2022",https://arxiv.org/pdf/2201.11260
Artificial Emotional Intelligence in Socially Assistive Robots for Older Adults: A Pilot Study,Hojjat Abdollahi;Mohammad H. Mahoor;Rohola Zandie;Jarid Siewierski;Sara H. Qualls,"This paper presents our recent research on integrating artificial emotional intelligence in a social robot (Ryan) and studies the robot's effectiveness in engaging older adults. Ryan is a socially assistive robot designed to provide companionship for older adults with depression and dementia through conversation. We used two versions of Ryan for our study, empathic and non-empathic. The empathic Ryan utilizes a multimodal emotion recognition algorithm and a multimodal emotion expression system. Using different input modalities for emotion, i.e. facial expression and speech sentiment, the empathic Ryan detects users' emotional state and utilizes an affective dialogue manager to generate a response. On the other hand, the non-empathic Ryan lacks facial expression and uses scripted dialogues that do not factor in the users' emotional state. We studied these two versions of Ryan with 10 older adults living in a senior care facility. The statistically significant improvement in the users' reported face-scale mood measurement indicates an overall positive effect from the interaction with both the empathic and non-empathic versions of Ryan. However, the number of spoken words measurement and the exit survey analysis suggest that the users perceive the empathic Ryan as more engaging and likable. △ Less","26 January, 2022",https://arxiv.org/pdf/2201.11167
Inference-optimized AI and high performance computing for gravitational wave detection at scale,Pranshu Chaturvedi;Asad Khan;Minyang Tian;E. A. Huerta;Huihuo Zheng,"We introduce an ensemble of artificial intelligence models for gravitational wave detection that we trained in the Summit supercomputer using 32 nodes, equivalent to 192 NVIDIA V100 GPUs, within 2 hours. Once fully trained, we optimized these models for accelerated inference using NVIDIA TensorRT. We deployed our inference-optimized AI ensemble in the ThetaGPU supercomputer at Argonne Leadership Computer Facility to conduct distributed inference. Using the entire ThetaGPU supercomputer, consisting of 20 nodes each of which has 8 NVIDIA A100 Tensor Core GPUs and 2 AMD Rome CPUs, our NVIDIA TensorRT-optimized AI ensemble processed an entire month of advanced LIGO data (including Hanford and Livingston data streams) within 50 seconds. Our inference-optimized AI ensemble retains the same sensitivity of traditional AI models, namely, it identifies all known binary black hole mergers previously identified in this advanced LIGO dataset and reports no misclassifications, while also providing a 3X inference speedup compared to traditional artificial intelligence models. We used time slides to quantify the performance of our AI ensemble to process up to 5 years worth of advanced LIGO data. In this synthetically enhanced dataset, our AI ensemble reports an average of one misclassification for every month of searched advanced LIGO data. We also present the receiver operating characteristic curve of our AI ensemble using this 5 year long advanced LIGO dataset. This approach provides the required tools to conduct accelerated, AI-driven gravitational wave detection at scale. △ Less","17 February, 2022",https://arxiv.org/pdf/2201.11133
Physics-informed ConvNet: Learning Physical Field from a Shallow Neural Network,Pengpeng Shi;Zhi Zeng;Tianshou Liang,"Big-data-based artificial intelligence (AI) supports profound evolution in almost all of science and technology. However, modeling and forecasting multi-physical systems remain a challenge due to unavoidable data scarcity and noise. Improving the generalization ability of neural networks by ""teaching"" domain knowledge and developing a new generation of models combined with the physical laws have become promising areas of machine learning research. Different from ""deep"" fully-connected neural networks embedded with physical information (PINN), a novel shallow framework named physics-informed convolutional network (PICN) is recommended from a CNN perspective, in which the physical field is generated by a deconvolution layer and a single convolution layer. The difference fields forming the physical operator are constructed using the pre-trained shallow convolution layer. An efficient linear interpolation network calculates the loss function involving boundary conditions and the physical constraints in irregular geometry domains. The effectiveness of the current development is illustrated through some numerical cases involving the solving (and estimation) of nonlinear physical operator equations and recovering physical information from noisy observations. Its potential advantage in approximating physical fields with multi-frequency components indicates that PICN may become an alternative neural network solver in physics-informed machine learning. △ Less","7 February, 2022",https://arxiv.org/pdf/2201.10967
Task-Oriented Image Semantic Communication Based on Rate-Distortion Theory,Fangfang Liu;Wanjie Tong;Yang Yang;Zhengfen Sun;Caili Guo,"Task-oriented image semantic communication is a new communication paradigm, which aims to transmit semantics for artificial intelligent (AI) tasks while ignoring the reconstruction quality of the images. However, in some applications, such as autonomous driving, both image reconstruction quality and the performance of the followed AI tasks must be simultaneously considered. To tackle this challenge, this paper proposes a task-oriented semantic communication scheme with semantic reconstruction (TOSC-SR). Its main goal is to simultaneously minimize pixel-level and task-relevant semantic-level distortion during communications under a certain rate, which formulates a new rate-distortion optimization problem. To successfully measure the loss at the semantic level, a new form of semantic distortion measured by the mutual information between the semantic-reconstructed images and the task labels is proposed. Then, we derive an analytical solution for the formulated problem, where the self-consistent equations of the problem are obtained to determine the optimal mapping of the source and the semantic-reconstructed images. To implement TOSC-SR, we further obtain an extended form of rate-distortion form based on the variational approximation of mutual information, which is applicable to multiple AI tasks. Experimental results show that the proposed approach outperforms the traditional JPEG, JPEG2000, BPG, VVC-based image communication systems and deep learning based benchmarks in terms of image reconstruction quality, AI task performance, and multi-task generalization ability. △ Less","1 December, 2022",https://arxiv.org/pdf/2201.10929
An Explainable Artificial Intelligence Framework for Quality-Aware IoE Service Delivery,Md. Shirajum Munir;Seong-Bae Park;Choong Seon Hong,"One of the core envisions of the sixth-generation (6G) wireless networks is to accumulate artificial intelligence (AI) for autonomous controlling of the Internet of Everything (IoE). Particularly, the quality of IoE services delivery must be maintained by analyzing contextual metrics of IoE such as people, data, process, and things. However, the challenges incorporate when the AI model conceives a lake of interpretation and intuition to the network service provider. Therefore, this paper provides an explainable artificial intelligence (XAI) framework for quality-aware IoE service delivery that enables both intelligence and interpretation. First, a problem of quality-aware IoE service delivery is formulated by taking into account network dynamics and contextual metrics of IoE, where the objective is to maximize the channel quality index (CQI) of each IoE service user. Second, a regression problem is devised to solve the formulated problem, where explainable coefficients of the contextual matrices are estimated by Shapley value interpretation. Third, the XAI-enabled quality-aware IoE service delivery algorithm is implemented by employing ensemble-based regression models for ensuring the interpretation of contextual relationships among the matrices to reconfigure network parameters. Finally, the experiment results show that the uplink improvement rate becomes 42.43% and 16.32% for the AdaBoost and Extra Trees, respectively, while the downlink improvement rate reaches up to 28.57% and 14.29%. However, the AdaBoost-based approach cannot maintain the CQI of IoE service users. Therefore, the proposed Extra Trees-based regression model shows significant performance gain for mitigating the trade-off between accuracy and interpretability than other baselines. △ Less","26 January, 2022",https://arxiv.org/pdf/2201.10822
"Speed, Quality, and the Optimal Timing of Complex Decisions: Field Evidence",Uwe Sunde;Dainis Zegners;Anthony Strittmatter,"This paper presents an empirical investigation of the relation between decision speed and decision quality for a real-world setting of cognitively-demanding decisions in which the timing of decisions is endogenous: professional chess. Move-by-move data provide exceptionally detailed and precise information about decision times and decision quality, based on a comparison of actual decisions to a computational benchmark of best moves constructed using the artificial intelligence of a chess engine. The results reveal that faster decisions are associated with better performance. The findings are consistent with the predictions of procedural decision models like drift-diffusion-models in which decision makers sequentially acquire information about decision alternatives with uncertain valuations. △ Less","26 January, 2022",https://arxiv.org/pdf/2201.10808
Learning Norms via Natural Language Teachings,Taylor Olson;Ken Forbus,"To interact with humans, artificial intelligence (AI) systems must understand our social world. Within this world norms play an important role in motivating and guiding agents. However, very few computational theories for learning social norms have been proposed. There also exists a long history of debate on the distinction between what is normal (is) and what is normative (ought). Many have argued that being capable of learning both concepts and recognizing the difference is necessary for all social agents. This paper introduces and demonstrates a computational approach to learning norms from natural language text that accounts for both what is normal and what is normative. It provides a foundation for everyday people to train AI systems about social norms. △ Less","20 January, 2022",https://arxiv.org/pdf/2201.10556
Explainability in Music Recommender Systems,Darius Afchar;Alessandro B. Melchiorre;Markus Schedl;Romain Hennequin;Elena V. Epure;Manuel Moussallam,"The most common way to listen to recorded music nowadays is via streaming platforms which provide access to tens of millions of tracks. To assist users in effectively browsing these large catalogs, the integration of Music Recommender Systems (MRSs) has become essential. Current real-world MRSs are often quite complex and optimized for recommendation accuracy. They combine several building blocks based on collaborative filtering and content-based recommendation. This complexity can hinder the ability to explain recommendations to end users, which is particularly important for recommendations perceived as unexpected or inappropriate. While pure recommendation performance often correlates with user satisfaction, explainability has a positive impact on other factors such as trust and forgiveness, which are ultimately essential to maintain user loyalty. In this article, we discuss how explainability can be addressed in the context of MRSs. We provide perspectives on how explainability could improve music recommendation algorithms and enhance user experience. First, we review common dimensions and goals of recommenders' explainability and in general of eXplainable Artificial Intelligence (XAI), and elaborate on the extent to which these apply -- or need to be adapted -- to the specific characteristics of music consumption and recommendation. Then, we show how explainability components can be integrated within a MRS and in what form explanations can be provided. Since the evaluation of explanation quality is decoupled from pure accuracy-based evaluation criteria, we also discuss requirements and strategies for evaluating explanations of music recommendations. Finally, we describe the current challenges for introducing explainability within a large-scale industrial music recommender system and provide research perspectives. △ Less","25 January, 2022",https://arxiv.org/pdf/2201.10528
GIU-GANs: Global Information Utilization for Generative Adversarial Networks,Yongqi Tian;Xueyuan Gong;Jialin Tang;Binghua Su;Xiaoxiang Liu;Xinyuan Zhang,"In recent years, with the rapid development of artificial intelligence, image generation based on deep learning has dramatically advanced. Image generation based on Generative Adversarial Networks (GANs) is a promising study. However, since convolutions are limited by spatial-agnostic and channel-specific, features extracted by traditional GANs based on convolution are constrained. Therefore, GANs are unable to capture any more details per image. On the other hand, straightforwardly stacking of convolutions causes too many parameters and layers in GANs, which will lead to a high risk of overfitting. To overcome the aforementioned limitations, in this paper, we propose a new GANs called Involution Generative Adversarial Networks (GIU-GANs). GIU-GANs leverages a brand new module called the Global Information Utilization (GIU) module, which integrates Squeeze-and-Excitation Networks (SENet) and involution to focus on global information by channel attention mechanism, leading to a higher quality of generated images. Meanwhile, Batch Normalization(BN) inevitably ignores the representation differences among noise sampled by the generator, and thus degrade the generated image quality. Thus we introduce Representative Batch Normalization(RBN) to the GANs architecture for this issue. The CIFAR-10 and CelebA datasets are employed to demonstrate the effectiveness of our proposed model. A large number of experiments prove that our model achieves state-of-the-art competitive performance. △ Less","15 March, 2022",https://arxiv.org/pdf/2201.10471
The First AI4TSP Competition: Learning to Solve Stochastic Routing Problems,Laurens Bliek;Paulo da Costa;Reza Refaei Afshar;Yingqian Zhang;Tom Catshoek;Daniël Vos;Sicco Verwer;Fynn Schmitt-Ulms;André Hottung;Tapan Shah;Meinolf Sellmann;Kevin Tierney;Carl Perreault-Lafleur;Caroline Leboeuf;Federico Bobbio;Justine Pepin;Warley Almeida Silva;Ricardo Gama;Hugo L. Fernandes;Martin Zaefferer;Manuel López-Ibáñez;Ekhine Irurozki,"This paper reports on the first international competition on AI for the traveling salesman problem (TSP) at the International Joint Conference on Artificial Intelligence 2021 (IJCAI-21). The TSP is one of the classical combinatorial optimization problems, with many variants inspired by real-world applications. This first competition asked the participants to develop algorithms to solve a time-dependent orienteering problem with stochastic weights and time windows (TD-OPSWTW). It focused on two types of learning approaches: surrogate-based optimization and deep reinforcement learning. In this paper, we describe the problem, the setup of the competition, the winning methods, and give an overview of the results. The winning methods described in this work have advanced the state-of-the-art in using AI for stochastic routing problems. Overall, by organizing this competition we have introduced routing problems as an interesting problem setting for AI researchers. The simulator of the problem has been made open-source and can be used by other researchers as a benchmark for new AI methods. △ Less","25 January, 2022",https://arxiv.org/pdf/2201.10453
Interspecies Collaboration in the Design of Visual Identity: A Case Study,Bojan Jerbić;Marko Švaco;Filip Šuligoj;Bojan Šekoranja;Josip Vidaković;Marija Turković;Mihaela Lekić;Borjan Pavlek;Bruno Bolfan;Davor Bruketa;Dina Borošić;Barbara Bušić,"Design usually relies on human ingenuity, but the past decade has seen the field's toolbox expanding to Artificial Intelligence (AI) and its adjacent methods, making room for hybrid, algorithmic creations. This article aims to substantiate the concept of interspecies collaboration - that of natural and artificial intelligence - in the active co-creation of a visual identity, describing a case study of the Regional Center of Excellence for Robotic Technology (CRTA) which opened on 750 m2 in June 2021 within the University of Zagreb. The visual identity of the Center comprises three separately devised elements, each representative of the human-AI relationship and embedded in the institution's logo. Firstly, the letter ""C"" (from the CRTA acronym) was created using a Gaussian Mixture Model (GMM) applied to (x, y) coordinates that the neurosurgical robot RONNA, CRTA's flagship innovation, generated when hand-guided by a human operator. The second shape of the letter ""C"" was created by using the same (x, y) coordinates as inputs fed to a neural network whose goal was to output letters in a novel, AI-generated typography. A basic feedforward back-propagating neural network with two hidden layers was chosen for the task. The final and third design element was a trajectory the robot RONNA makes when performing a brain biopsy. As CRTA embodies a state-of-the-art venue for robotics research, the 'interspecies' approach was used to accentuate the importance of human-robot collaboration which is at the core of the newly opened Center, illustrating the potential of reciprocal and amicable relationship that humans could have with technology. △ Less","25 January, 2022",https://arxiv.org/pdf/2201.10393
ADAPT: An Open-Source sUAS Payload for Real-Time Disaster Prediction and Response with AI,Daniel Davila;Joseph VanPelt;Alexander Lynch;Adam Romlein;Peter Webley;Matthew S. Brown,"Small unmanned aircraft systems (sUAS) are becoming prominent components of many humanitarian assistance and disaster response (HADR) operations. Pairing sUAS with onboard artificial intelligence (AI) substantially extends their utility in covering larger areas with fewer support personnel. A variety of missions, such as search and rescue, assessing structural damage, and monitoring forest fires, floods, and chemical spills, can be supported simply by deploying the appropriate AI models. However, adoption by resource-constrained groups, such as local municipalities, regulatory agencies, and researchers, has been hampered by the lack of a cost-effective, readily-accessible baseline platform that can be adapted to their unique missions. To fill this gap, we have developed the free and open-source ADAPT multi-mission payload for deploying real-time AI and computer vision onboard a sUAS during local and beyond-line-of-site missions. We have emphasized a modular design with low-cost, readily-available components, open-source software, and thorough documentation (https://kitware.github.io/adapt/). The system integrates an inertial navigation system, high-resolution color camera, computer, and wireless downlink to process imagery and broadcast georegistered analytics back to a ground station. Our goal is to make it easy for the HADR community to build their own copies of the ADAPT payload and leverage the thousands of hours of engineering we have devoted to developing and testing. In this paper, we detail the development and testing of the ADAPT payload. We demonstrate the example mission of real-time, in-flight ice segmentation to monitor river ice state and provide timely predictions of catastrophic flooding events. We deploy a novel active learning workflow to annotate river ice imagery, train a real-time deep neural network for ice segmentation, and demonstrate operation in the field. △ Less","25 January, 2022",https://arxiv.org/pdf/2201.10366
Roadmap for Cybersecurity in Autonomous Vehicles,Vipin Kumar Kukkala;Sooryaa Vignesh Thiruloga;Sudeep Pasricha,"Autonomous vehicles are on the horizon and will be transforming transportation safety and comfort. These vehicles will be connected to various external systems and utilize advanced embedded systems to perceive their environment and make intelligent decisions. However, this increased connectivity makes these vehicles vulnerable to various cyber-attacks that can have catastrophic effects. Attacks on automotive systems are already on the rise in today's vehicles and are expected to become more commonplace in future autonomous vehicles. Thus, there is a need to strengthen cybersecurity in future autonomous vehicles. In this article, we discuss major automotive cyber-attacks over the past decade and present state-of-the-art solutions that leverage artificial intelligence (AI). We propose a roadmap towards building secure autonomous vehicles and highlight key open challenges that need to be addressed. △ Less","19 January, 2022",https://arxiv.org/pdf/2201.10349
Post-Hoc Explanations Fail to Achieve their Purpose in Adversarial Contexts,Sebastian Bordt;Michèle Finck;Eric Raidl;Ulrike von Luxburg,"Existing and planned legislation stipulates various obligations to provide information about machine learning algorithms and their functioning, often interpreted as obligations to ""explain"". Many researchers suggest using post-hoc explanation algorithms for this purpose. In this paper, we combine legal, philosophical and technical arguments to show that post-hoc explanation algorithms are unsuitable to achieve the law's objectives. Indeed, most situations where explanations are requested are adversarial, meaning that the explanation provider and receiver have opposing interests and incentives, so that the provider might manipulate the explanation for her own ends. We show that this fundamental conflict cannot be resolved because of the high degree of ambiguity of post-hoc explanations in realistic application scenarios. As a consequence, post-hoc explanation algorithms are unsuitable to achieve the transparency objectives inherent to the legal norms. Instead, there is a need to more explicitly discuss the objectives underlying ""explainability"" obligations as these can often be better achieved through other mechanisms. There is an urgent need for a more open and honest discussion regarding the potential and limitations of post-hoc explanations in adversarial contexts, in particular in light of the current negotiations of the European Union's draft Artificial Intelligence Act. △ Less","10 May, 2022",https://arxiv.org/pdf/2201.10295
Maximizing information from chemical engineering data sets: Applications to machine learning,Alexander Thebelt;Johannes Wiebe;Jan Kronqvist;Calvin Tsay;Ruth Misener,"It is well-documented how artificial intelligence can have (and already is having) a big impact on chemical engineering. But classical machine learning approaches may be weak for many chemical engineering applications. This review discusses how challenging data characteristics arise in chemical engineering applications. We identify four characteristics of data arising in chemical engineering applications that make applying classical artificial intelligence approaches difficult: (1) high variance, low volume data, (2) low variance, high volume data, (3) noisy/corrupt/missing data, and (4) restricted data with physics-based limitations. For each of these four data characteristics, we discuss applications where these data characteristics arise and show how current chemical engineering research is extending the fields of data science and machine learning to incorporate these challenges. Finally, we identify several challenges for future research. △ Less","24 January, 2022",https://arxiv.org/pdf/2201.10035
Learning to Act with Affordance-Aware Multimodal Neural SLAM,Zhiwei Jia;Kaixiang Lin;Yizhou Zhao;Qiaozi Gao;Govind Thattai;Gaurav Sukhatme,"Recent years have witnessed an emerging paradigm shift toward embodied artificial intelligence, in which an agent must learn to solve challenging tasks by interacting with its environment. There are several challenges in solving embodied multimodal tasks, including long-horizon planning, vision-and-language grounding, and efficient exploration. We focus on a critical bottleneck, namely the performance of planning and navigation. To tackle this challenge, we propose a Neural SLAM approach that, for the first time, utilizes several modalities for exploration, predicts an affordance-aware semantic map, and plans over it at the same time. This significantly improves exploration efficiency, leads to robust long-horizon planning, and enables effective vision-and-language grounding. With the proposed Affordance-aware Multimodal Neural SLAM (AMSLAM) approach, we obtain more than 40% improvement over prior published work on the ALFRED benchmark and set a new state-of-the-art generalization performance at a success rate of 23.48% on the test unseen scenes. △ Less","24 October, 2022",https://arxiv.org/pdf/2201.09862
Surrogate Neural Network Model for Sensitivity Analysis and Uncertainty Quantification of the Mechanical Behavior in the Optical Lens-Barrel Assembly,Shantanu Shahane;Erman Guleryuz;Diab W Abueidda;Allen Lee;Joe Liu;Xin Yu;Raymond Chiu;Seid Koric;Narayana R Aluru;Placid M Ferreira,"Surrogate neural network-based models have been lately trained and used in a variety of science and engineering applications where the number of evaluations of a target function is limited by execution time. In cell phone camera systems, various errors, such as interferences at the lens-barrel and lens-lens interfaces and axial, radial, and tilt misalignments, accumulate and alter profile of the lenses in a stochastic manner which ultimately changes optical focusing properties. Nonlinear finite element analysis of the stochastic mechanical behavior of lenses due to the interference fits is used on high-performance computing (HPC) to generate sufficient training and testing data for subsequent deep learning. Once properly trained and validated, the surrogate neural network model enabled accurate and almost instant evaluations of millions of function evaluations providing the final lens profiles. This computational model, enhanced by artificial intelligence, enabled us to efficiently perform Monte-Carlo analysis for sensitivity and uncertainty quantification of the final lens profile to various interferences. It can be further coupled with an optical analysis to perform ray tracing and analyze the focal properties of the lens module. Moreover, it can provide a valuable tool for optimizing tolerance design and intelligent components matching for many similar press-fit assembly processes. △ Less","28 June, 2022",https://arxiv.org/pdf/2201.09659
AlphaFold Accelerates Artificial Intelligence Powered Drug Discovery: Efficient Discovery of a Novel Cyclin-dependent Kinase 20 (CDK20) Small Molecule Inhibitor,Feng Ren;Xiao Ding;Min Zheng;Mikhail Korzinkin;Xin Cai;Wei Zhu;Alexey Mantsyzov;Alex Aliper;Vladimir Aladinskiy;Zhongying Cao;Shanshan Kong;Xi Long;Bonnie Hei Man Liu;Yingtao Liu;Vladimir Naumov;Anastasia Shneyderman;Ivan V. Ozerov;Ju Wang;Frank W. Pun;Alan Aspuru-Guzik;Michael Levitt;Alex Zhavoronkov,"The AlphaFold computer program predicted protein structures for the whole human genome, which has been considered as a remarkable breakthrough both in artificial intelligence (AI) application and structural biology. Despite the varying confidence level, these predicted structures still could significantly contribute to structure-based drug design of novel targets, especially the ones with no or limited structural information. In this work, we successfully applied AlphaFold in our end-to-end AI-powered drug discovery engines constituted of a biocomputational platform PandaOmics and a generative chemistry platform Chemistry42, to identify a first-in-class hit molecule of a novel target without an experimental structure starting from target selection towards hit identification in a cost- and time-efficient manner. PandaOmics provided the targets of interest and Chemistry42 generated the molecules based on the AlphaFold predicted structure, and the selected molecules were synthesized and tested in biological assays. Through this approach, we identified a small molecule hit compound for CDK20 with a Kd value of 8.9 +/- 1.6 uM (n = 4) within 30 days from target selection and after only synthesizing 7 compounds. Based on the available data, the second round of AI-powered compound generation was conducted and through which, a more potent hit molecule, ISM042-2 048, was discovered with a Kd value of 210.0 +/- 42.4 nM (n = 2), within 30 days and after synthesizing 6 compounds from the discovery of the first hit ISM042-2-001. To the best of our knowledge, this is the first reported small molecule targeting CDK20 and more importantly, this work is the first demonstration of AlphaFold application in the hit identification process in early drug discovery. △ Less","12 February, 2022",https://arxiv.org/pdf/2201.09647
Communication-Efficient Stochastic Zeroth-Order Optimization for Federated Learning,Wenzhi Fang;Ziyi Yu;Yuning Jiang;Yuanming Shi;Colin N. Jones;Yong Zhou,"Federated learning (FL), as an emerging edge artificial intelligence paradigm, enables many edge devices to collaboratively train a global model without sharing their private data. To enhance the training efficiency of FL, various algorithms have been proposed, ranging from first-order to second-order methods. However, these algorithms cannot be applied in scenarios where the gradient information is not available, e.g., federated black-box attack and federated hyperparameter tuning. To address this issue, in this paper we propose a derivative-free federated zeroth-order optimization (FedZO) algorithm featured by performing multiple local updates based on stochastic gradient estimators in each communication round and enabling partial device participation. Under non-convex settings, we derive the convergence performance of the FedZO algorithm on non-independent and identically distributed data and characterize the impact of the numbers of local iterates and participating edge devices on the convergence. To enable communication-efficient FedZO over wireless networks, we further propose an over-the-air computation (AirComp) assisted FedZO algorithm. With an appropriate transceiver design, we show that the convergence of AirComp-assisted FedZO can still be preserved under certain signal-to-noise ratio conditions. Simulation results demonstrate the effectiveness of the FedZO algorithm and validate the theoretical observations. △ Less","10 October, 2022",https://arxiv.org/pdf/2201.09531
MISeval: a Metric Library for Medical Image Segmentation Evaluation,Dominik Müller;Dennis Hartmann;Philip Meyer;Florian Auer;Iñaki Soto-Rey;Frank Kramer,"Correct performance assessment is crucial for evaluating modern artificial intelligence algorithms in medicine like deep-learning based medical image segmentation models. However, there is no universal metric library in Python for standardized and reproducible evaluation. Thus, we propose our open-source publicly available Python package MISeval: a metric library for Medical Image Segmentation Evaluation. The implemented metrics can be intuitively used and easily integrated into any performance assessment pipeline. The package utilizes modern CI/CD strategies to ensure functionality and stability. MISeval is available from PyPI (miseval) and GitHub: https://github.com/frankkramer-lab/miseval. △ Less","23 January, 2022",https://arxiv.org/pdf/2201.09395
POTHER: Patch-Voted Deep Learning-Based Chest X-ray Bias Analysis for COVID-19 Detection,Tomasz Szczepański;Arkadiusz Sitek;Tomasz Trzciński;Szymon Płotka,"A critical step in the fight against COVID-19, which continues to have a catastrophic impact on peoples lives, is the effective screening of patients presented in the clinics with severe COVID-19 symptoms. Chest radiography is one of the promising screening approaches. Many studies reported detecting COVID-19 in chest X-rays accurately using deep learning. A serious limitation of many published approaches is insufficient attention paid to explaining decisions made by deep learning models. Using explainable artificial intelligence methods, we demonstrate that model decisions may rely on confounding factors rather than medical pathology. After an analysis of potential confounding factors found on chest X-ray images, we propose a novel method to minimise their negative impact. We show that our proposed method is more robust than previous attempts to counter confounding factors such as ECG leads in chest X-rays that often influence model classification decisions. In addition to being robust, our method achieves results comparable to the state-of-the-art. The source code and pre-trained weights are publicly available at (https://github.com/tomek1911/POTHER). △ Less","8 August, 2022",https://arxiv.org/pdf/2201.09360
Artificial Intelligence for Suicide Assessment using Audiovisual Cues: A Review,Sahraoui Dhelim;Liming Chen;Huansheng Ning;Chris Nugent,"Death by suicide is the seventh leading death cause worldwide. The recent advancement in Artificial Intelligence (AI), specifically AI applications in image and voice processing, has created a promising opportunity to revolutionize suicide risk assessment. Subsequently, we have witnessed fast-growing literature of research that applies AI to extract audiovisual non-verbal cues for mental illness assessment. However, the majority of the recent works focus on depression, despite the evident difference between depression symptoms and suicidal behavior and non-verbal cues. This paper reviews recent works that study suicide ideation and suicide behavior detection through audiovisual feature analysis, mainly suicidal voice/speech acoustic features analysis and suicidal visual cues. Automatic suicide assessment is a promising research direction that is still in the early stages. Accordingly, there is a lack of large datasets that can be used to train machine learning and deep learning models proven to be effective in other, similar tasks. △ Less","3 November, 2022",https://arxiv.org/pdf/2201.09130
Towards Sustainable Deep Learning for Wireless Fingerprinting Localization,Anže Pirnat;Blaž Bertalanič;Gregor Cerar;Mihael Mohorčič;Marko Meža;Carolina Fortuna,"Location based services, already popular with end users, are now inevitably becoming part of new wireless infrastructures and emerging business processes. The increasingly popular Deep Learning (DL) artificial intelligence methods perform very well in wireless fingerprinting localization based on extensive indoor radio measurement data. However, with the increasing complexity these methods become computationally very intensive and energy hungry, both for their training and subsequent operation. Considering only mobile users, estimated to exceed 7.4billion by the end of 2025, and assuming that the networks serving these users will need to perform only one localization per user per hour on average, the machine learning models used for the calculation would need to perform 65*10^12 predictions per year. Add to this equation tens of billions of other connected devices and applications that rely heavily on more frequent location updates, and it becomes apparent that localization will contribute significantly to carbon emissions unless more energy-efficient models are developed and used. This motivated our work on a new DL-based architecture for indoor localization that is more energy efficient compared to related state-of-the-art approaches while showing only marginal performance degradation. A detailed performance evaluation shows that the proposed model producesonly 58 % of the carbon footprint while maintaining 98.7 % of the overall performance compared to state of the art model external to our group. Additionally, we elaborate on a methodology to calculate the complexity of the DL model and thus the CO2 footprint during its training and operation. △ Less","22 January, 2022",https://arxiv.org/pdf/2201.09071
A Collaborative Statistical Actor-Critic Learning Approach for 6G Network Slicing Control,Farhad Rezazadeh;Hatim Chergui;Luis Blanco;Luis Alonso;Christos Verikoukis,"Artificial intelligence (AI)-driven zero-touch massive network slicing is envisioned to be a disruptive technology in beyond 5G (B5G)/6G, where tenancy would be extended to the final consumer in the form of advanced digital use-cases. In this paper, we propose a novel model-free deep reinforcement learning (DRL) framework, called collaborative statistical Actor-Critic (CS-AC) that enables a scalable and farsighted slice performance management in a 6G-like RAN scenario that is built upon mobile edge computing (MEC) and massive multiple-input multiple-output (mMIMO). In this intent, the proposed CS-AC targets the optimization of the latency cost under a long-term statistical service-level agreement (SLA). In particular, we consider the Q-th delay percentile SLA metric and enforce some slice-specific preset constraints on it. Moreover, to implement distributed learners, we propose a developed variant of soft Actor-Critic (SAC) with less hyperparameter sensitivity. Finally, we present numerical results to showcase the gain of the adopted approach on our built OpenAI-based network slicing environment and verify the performance in terms of latency, SLA Q-th percentile, and time efficiency. To the best of our knowledge, this is the first work that studies the feasibility of an AI-driven approach for massive network slicing under statistical SLA. △ Less","22 January, 2022",https://arxiv.org/pdf/2201.08990
Actor-Critic-Based Learning for Zero-touch Joint Resource and Energy Control in Network Slicing,Farhad Rezazadeh;Hatim Chergui;Loizos Christofi;Christos Verikoukis,"To harness the full potential of beyond 5G (B5G) communication systems, zero-touch network slicing (NS) is viewed as a promising fully-automated management and orchestration (MANO) system. This paper proposes a novel knowledge plane (KP)-based MANO framework that accommodates and exploits recent NS technologies and is termed KB5G. Specifically, we deliberate on algorithmic innovation and artificial intelligence (AI) in KB5G. We invoke a continuous model-free deep reinforcement learning (DRL) method to minimize energy consumption and virtual network function (VNF) instantiation cost. We present a novel Actor-Critic-based NS approach to stabilize learning called, twin-delayed double-Q soft Actor-Critic (TDSAC) method. The TDSAC enables central unit (CU) to learn continuously to accumulate the knowledge learned in the past to minimize future NS costs. Finally, we present numerical results to showcase the gain of the adopted approach and verify the performance in terms of energy consumption, CPU utilization, and time efficiency. △ Less","22 January, 2022",https://arxiv.org/pdf/2201.08985
Physical Reasoning in an Open World,Zhuoran Zeng;Ernest Davis,"Most work on physical reasoning, both in artificial intelligence and in cognitive science, has focused on closed-world reasoning, in which it is assumed that the problem specification specifies all relevant objects and substance, all their relations in an initial situation, and all exogenous events. However, in many situations, it is important to do open-world reasoning; that is, making valid conclusions from very incomplete information. We have implemented in Prolog an open-world reasoner for a toy microworld of containers that can be loaded, unloaded, sealed, unsealed, carried, and dumped. △ Less","21 January, 2022",https://arxiv.org/pdf/2201.08950
Active Predictive Coding Networks: A Neural Solution to the Problem of Learning Reference Frames and Part-Whole Hierarchies,Dimitrios C. Gklezakos;Rajesh P. N. Rao,"We introduce Active Predictive Coding Networks (APCNs), a new class of neural networks that solve a major problem posed by Hinton and others in the fields of artificial intelligence and brain modeling: how can neural networks learn intrinsic reference frames for objects and parse visual scenes into part-whole hierarchies by dynamically allocating nodes in a parse tree? APCNs address this problem by using a novel combination of ideas: (1) hypernetworks are used for dynamically generating recurrent neural networks that predict parts and their locations within intrinsic reference frames conditioned on higher object-level embedding vectors, and (2) reinforcement learning is used in conjunction with backpropagation for end-to-end learning of model parameters. The APCN architecture lends itself naturally to multi-level hierarchical learning and is closely related to predictive coding models of cortical function. Using the MNIST, Fashion-MNIST and Omniglot datasets, we demonstrate that APCNs can (a) learn to parse images into part-whole hierarchies, (b) learn compositional representations, and (c) transfer their knowledge to unseen classes of objects. With their ability to dynamically generate parse trees with part locations for objects, APCNs offer a new framework for explainable AI that leverages advances in deep learning while retaining interpretability and compositionality. △ Less","14 January, 2022",https://arxiv.org/pdf/2201.08813
AiTLAS: Artificial Intelligence Toolbox for Earth Observation,Ivica Dimitrovski;Ivan Kitanovski;Panče Panov;Nikola Simidjievski;Dragi Kocev,"The AiTLAS toolbox (Artificial Intelligence Toolbox for Earth Observation) includes state-of-the-art machine learning methods for exploratory and predictive analysis of satellite imagery as well as repository of AI-ready Earth Observation (EO) datasets. It can be easily applied for a variety of Earth Observation tasks, such as land use and cover classification, crop type prediction, localization of specific objects (semantic segmentation), etc. The main goal of AiTLAS is to facilitate better usability and adoption of novel AI methods (and models) by EO experts, while offering easy access and standardized format of EO datasets to AI experts which further allows benchmarking of various existing and novel AI methods tailored for EO data. △ Less","21 January, 2022",https://arxiv.org/pdf/2201.08789
"AI Technical Considerations: Data Storage, Cloud usage and AI Pipeline",P. M. A van Ooijen;Erfan Darzidehkalani;Andre Dekker,"Artificial intelligence (AI), especially deep learning, requires vast amounts of data for training, testing, and validation. Collecting these data and the corresponding annotations requires the implementation of imaging biobanks that provide access to these data in a standardized way. This requires careful design and implementation based on the current standards and guidelines and complying with the current legal restrictions. However, the realization of proper imaging data collections is not sufficient to train, validate and deploy AI as resource demands are high and require a careful hybrid implementation of AI pipelines both on-premise and in the cloud. This chapter aims to help the reader when technical considerations have to be made about the AI environment by providing a technical background of different concepts and implementation aspects involved in data storage, cloud usage, and AI pipelines. △ Less","20 January, 2022",https://arxiv.org/pdf/2201.08356
From Psychological Curiosity to Artificial Curiosity: Curiosity-Driven Learning in Artificial Intelligence Tasks,Chenyu Sun;Hangwei Qian;Chunyan Miao,"Psychological curiosity plays a significant role in human intelligence to enhance learning through exploration and information acquisition. In the Artificial Intelligence (AI) community, artificial curiosity provides a natural intrinsic motivation for efficient learning as inspired by human cognitive development; meanwhile, it can bridge the existing gap between AI research and practical application scenarios, such as overfitting, poor generalization, limited training samples, high computational cost, etc. As a result, curiosity-driven learning (CDL) has become increasingly popular, where agents are self-motivated to learn novel knowledge. In this paper, we first present a comprehensive review on the psychological study of curiosity and summarize a unified framework for quantifying curiosity as well as its arousal mechanism. Based on the psychological principle, we further survey the literature of existing CDL methods in the fields of Reinforcement Learning, Recommendation, and Classification, where both advantages and disadvantages as well as future work are discussed. As a result, this work provides fruitful insights for future CDL research and yield possible directions for further improvement. △ Less","20 January, 2022",https://arxiv.org/pdf/2201.08300
Lifelong Learning Metrics,Alexander New;Megan Baker;Eric Nguyen;Gautam Vallabha,"The DARPA Lifelong Learning Machines (L2M) program seeks to yield advances in artificial intelligence (AI) systems so that they are capable of learning (and improving) continuously, leveraging data on one task to improve performance on another, and doing so in a computationally sustainable way. Performers on this program developed systems capable of performing a diverse range of functions, including autonomous driving, real-time strategy, and drone simulation. These systems featured a diverse range of characteristics (e.g., task structure, lifetime duration), and an immediate challenge faced by the program's testing and evaluation team was measuring system performance across these different settings. This document, developed in close collaboration with DARPA and the program performers, outlines a formalism for constructing and characterizing the performance of agents performing lifelong learning scenarios. △ Less","20 January, 2022",https://arxiv.org/pdf/2201.08278
Data-Driven Innovation: What Is It,Jianxi Luo,"The future of innovation processes is anticipated to be more data-driven and empowered by the ubiquitous digitalization, increasing data accessibility and rapid advances in machine learning, artificial intelligence, and computing technologies. While the data-driven innovation (DDI) paradigm is emerging, it has yet been formally defined and theorized and often confused with several other data-related phenomena. This paper defines and crystalizes ""data-driven innovation"" as a formal innovation process paradigm, dissects its value creation, and distinguishes it from data-driven optimization (DDO), data-based innovation (DBI), and the traditional innovation processes that purely rely on human intelligence. With real-world examples and theoretical framing, I elucidate what DDI entails and how it addresses uncertainty and enhance creativity in the innovation process and present a process-based taxonomy of different data-driven innovation approaches. On this basis, I recommend the strategies and actions for innovators, companies, R&D organizations, and governments to enact data-driven innovation. △ Less","7 July, 2022",https://arxiv.org/pdf/2201.08184
"Survey on Federated Learning Threats: concepts, taxonomy on attacks and defences, experimental study and challenges",Nuria Rodríguez-Barroso;Daniel Jiménez López;M. Victoria Luzón;Francisco Herrera;Eugenio Martínez-Cámara,"Federated learning is a machine learning paradigm that emerges as a solution to the privacy-preservation demands in artificial intelligence. As machine learning, federated learning is threatened by adversarial attacks against the integrity of the learning model and the privacy of data via a distributed approach to tackle local and global learning. This weak point is exacerbated by the inaccessibility of data in federated learning, which makes harder the protection against adversarial attacks and evidences the need to furtherance the research on defence methods to make federated learning a real solution for safeguarding data privacy. In this paper, we present an extensive review of the threats of federated learning, as well as as their corresponding countermeasures, attacks versus defences. This survey provides a taxonomy of adversarial attacks and a taxonomy of defence methods that depict a general picture of this vulnerability of federated learning and how to overcome it. Likewise, we expound guidelines for selecting the most adequate defence method according to the category of the adversarial attack. Besides, we carry out an extensive experimental study from which we draw further conclusions about the behaviour of attacks and defences and the guidelines for selecting the most adequate defence method according to the category of the adversarial attack. This study is finished leading to meditated learned lessons and challenges. △ Less","20 January, 2022",https://arxiv.org/pdf/2201.08135
Before and After: Machine learning for perioperative patient care,Iuliia Ganskaia;Stanislav Abaimov,"For centuries nursing has been known as a job that requires complex manual operations, that cannot be automated or replaced by any machinery. All the devices and techniques have been invented only to support, but never fully replace, a person with knowledge and expert intuition. With the rise of Artificial Intelligence and continuously increasing digital data flow in healthcare, new tools have arrived to improve patient care and reduce the labour-intensive work conditions of a nurse. This cross-disciplinary review aims to build a bridge over the gap between computer science and nursing. It outlines and classifies the methods for machine learning and data processing in patient care before and after the operation. It comprises of Process-, Patient-, Operator-, Feedback-, and Technology-centric classifications. The presented classifications are based on the technical aspects of patient case. △ Less","20 January, 2022",https://arxiv.org/pdf/2201.08095
Combining Machine Learning with Knowledge Engineering to detect Fake News in Social Networks-a survey,Sajjad Ahmed;Knut Hinkelmann;Flavio Corradini,"Due to extensive spread of fake news on social and news media it became an emerging research topic now a days that gained attention. In the news media and social media the information is spread highspeed but without accuracy and hence detection mechanism should be able to predict news fast enough to tackle the dissemination of fake news. It has the potential for negative impacts on individuals and society. Therefore, detecting fake news on social media is important and also a technically challenging problem these days. We knew that Machine learning is helpful for building Artificial intelligence systems based on tacit knowledge because it can help us to solve complex problems due to real word data. On the other side we knew that Knowledge engineering is helpful for representing experts knowledge which people aware of that knowledge. Due to this we proposed that integration of Machine learning and knowledge engineering can be helpful in detection of fake news. In this paper we present what is fake news, importance of fake news, overall impact of fake news on different areas, different ways to detect fake news on social media, existing detections algorithms that can help us to overcome the issue, similar application areas and at the end we proposed combination of data driven and engineered knowledge to combat fake news. We studied and compared three different modules text classifiers, stance detection applications and fact checking existing techniques that can help to detect fake news. Furthermore, we investigated the impact of fake news on society. Experimental evaluation of publically available datasets and our proposed fake news detection combination can serve better in detection of fake news. △ Less","20 January, 2022",https://arxiv.org/pdf/2201.08032
Transfer Learning for Fault Diagnosis of Transmission Lines,Fatemeh Mohammadi Shakiba;Milad Shojaee;S. Mohsen Azizi;Mengchu Zhou,"Recent artificial intelligence-based methods have shown great promise in the use of neural networks for real-time sensing and detection of transmission line faults and estimation of their locations. The expansion of power systems including transmission lines with various lengths have made a fault detection, classification, and location estimation process more challenging. Transmission line datasets are stream data which are continuously collected by various sensors and hence, require generalized and fast fault diagnosis approaches. Newly collected datasets including voltages and currents might not have enough and accurate labels (fault and no fault) that are useful to train neural networks. In this paper, a novel transfer learning framework based on a pre-trained LeNet-5 convolutional neural network is proposed. This method is able to diagnose faults for different transmission line lengths and impedances by transferring the knowledge from a source convolutional neural network to predict a dissimilar target dataset. By transferring this knowledge, faults from various transmission lines, without having enough labels, can be diagnosed faster and more efficiently compared to the existing methods. To prove the feasibility and effectiveness of this methodology, seven different datasets that include various lengths of transmission lines are used. The robustness of the proposed methodology against generator voltage fluctuation, variation in fault distance, fault inception angle, fault resistance, and phase difference between the two generators are well shown, thus proving its practical values in the fault diagnosis of transmission lines. △ Less","20 January, 2022",https://arxiv.org/pdf/2201.08018
Caring Without Sharing: A Federated Learning Crowdsensing Framework for Diversifying Representation of Cities,Michael Cho;Afra Mashhadi,"Mobile Crowdsensing has become main stream paradigm for researchers to collect behavioral data from citizens in large scales. This valuable data can be leveraged to create centralized repositories that can be used to train advanced Artificial Intelligent (AI) models for various services that benefit society in all aspects. Although decades of research has explored the viability of Mobile Crowdsensing in terms of incentives and many attempts have been made to reduce the participation barriers, the overshadowing privacy concerns regarding sharing personal data still remain. Recently a new pathway has emerged to enable to shift MCS paradigm towards a more privacy-preserving collaborative learning, namely Federated Learning. In this paper, we posit a first of its kind framework for this emerging paradigm. We demonstrate the functionalities of our framework through a case study of diversifying two vision algorithms through to learn the representation of ordinary sidewalk obstacles as part of enhancing visually impaired navigation. △ Less","19 January, 2022",https://arxiv.org/pdf/2201.07980
Towards deep observation: A systematic survey on artificial intelligence techniques to monitor fetus via Ultrasound Images,Mahmood Alzubaidi;Marco Agus;Khalid Alyafei;Khaled A Althelaya;Uzair Shah;Alaa Abd-Alrazaq;Mohammed Anbar;Michel Makhlouf;Mowafa Househ,"Developing innovative informatics approaches aimed to enhance fetal monitoring is a burgeoning field of study in reproductive medicine. Several reviews have been conducted regarding Artificial intelligence (AI) techniques to improve pregnancy outcomes. They are limited by focusing on specific data such as mother's care during pregnancy. This systematic survey aims to explore how artificial intelligence (AI) can assist with fetal growth monitoring via Ultrasound (US) image. We used eight medical and computer science bibliographic databases, including PubMed, Embase, PsycINFO, ScienceDirect, IEEE explore, ACM Library, Google Scholar, and the Web of Science. We retrieved studies published between 2010 to 2021. Data extracted from studies were synthesized using a narrative approach. Out of 1269 retrieved studies, we included 107 distinct studies from queries that were relevant to the topic in the survey. We found that 2D ultrasound images were more popular (n=88) than 3D and 4D ultrasound images (n=19). Classification is the most used method (n=42), followed by segmentation (n=31), classification integrated with segmentation (n=16) and other miscellaneous such as object-detection, regression and reinforcement learning (n=18). The most common areas within the pregnancy domain were the fetus head (n=43), then fetus body (n=31), fetus heart (n=13), fetus abdomen (n=10), and lastly the fetus face (n=10). In the most recent studies, deep learning techniques were primarily used (n=81), followed by machine learning (n=16), artificial neural network (n=7), and reinforcement learning (n=2). AI techniques played a crucial role in predicting fetal diseases and identifying fetus anatomy structures during pregnancy. More research is required to validate this technology from a physician's perspective, such as pilot studies and randomized controlled trials on AI and its applications in a hospital setting. △ Less","25 August, 2022",https://arxiv.org/pdf/2201.07935
Detection of Correlated Alarms Using Graph Embedding,Hossein Khaleghy;Iman Izadi,"Industrial alarm systems have recently progressed considerably in terms of network complexity and the number of alarms. The increase in complexity and number of alarms presents challenges in these systems that decrease system efficiency and cause distrust of the operator, which might result in widespread damages. One contributing factor in alarm inefficiency is the correlated alarms. These alarms do not contain new information and only confuse the operator. This paper tries to present a novel method for detecting correlated alarms based on artificial intelligence methods to help the operator. The proposed method is based on graph embedding and alarm clustering, resulting in the detection of correlated alarms. To evaluate the proposed method, a case study is conducted on the well-known Tennessee-Eastman process. △ Less","17 January, 2022",https://arxiv.org/pdf/2201.07748
Problem examination for AI methods in product design,Philipp Rosenthal;Oliver Niggemann,"Artificial Intelligence (AI) has significant potential for product design: AI can check technical and non-technical constraints on products, it can support a quick design of new product variants and new AI methods may also support creativity. But currently product design and AI are separate communities fostering different terms and theories. This makes a mapping of AI approaches to product design needs difficult and prevents new solutions. As a solution, this paper first clarifies important terms and concepts for the interdisciplinary domain of AI methods in product design. A key contribution of this paper is a new classification of design problems using the four characteristics decomposability, inter-dependencies, innovation and creativity. Definitions of these concepts are given where they are lacking. Early mappings of these concepts to AI solutions are sketched and verified using design examples. The importance of creativity in product design and a corresponding gap in AI is pointed out for future research. △ Less","19 January, 2022",https://arxiv.org/pdf/2201.07642
Virtual Coil Augmentation Technology for MR Coil Extrapolation via Deep Learning,Cailian Yang;Xianghao Liao;Yuhao Wang;Minghui Zhang;Qiegen Liu,"Magnetic resonance imaging (MRI) is a widely used medical imaging modality. However, due to the limitations in hardware, scan time, and throughput, it is often clinically challenging to obtain high-quality MR images. In this article, we propose a method of using artificial intelligence to expand the channel to achieve the goal of generating the virtual coils. The main characteristic of our work is utilizing dummy variable technology to expand/extrapolate the receive coils in both image and k-space domains. The high-dimensional information formed by channel expansion is used as the prior information to improve the reconstruction effect of parallel imaging. Two main components are incorporated into the network design, namely variable augmentation technology and sum of squares (SOS) objective function. Variable augmentation provides the network with more high-dimensional prior information, which is helpful for the network to extract the deep feature information of the data. The SOS objective function is employed to solve the deficiency of k-space data training while speeding up convergence. Experimental results demonstrated its great potentials in super-resolution of MR images and accelerated parallel imaging reconstruction. △ Less","20 March, 2022",https://arxiv.org/pdf/2201.07540
Fooling MOSS Detection with Pretrained Language Models,Stella Biderman;Edward Raff,"As artificial intelligence (AI) technologies become increasingly powerful and prominent in society, their misuse is a growing concern. In educational settings, AI technologies could be used by students to cheat on assignments and exams. In this paper we explore whether transformers can be used to solve introductory level programming assignments while bypassing commonly used AI tools to detect similarities between pieces of software. We find that a student using GPT-J [Wang and Komatsuzaki, 2021] can complete introductory level programming assignments without triggering suspicion from MOSS [Aiken, 2000], a widely used software similarity and plagiarism detection tool. This holds despite the fact that GPT-J was not trained on the problems in question and is not provided with any examples to work from. We further find that the code written by GPT-J is diverse in structure, lacking any particular tells that future plagiarism detection techniques may use to try to identify algorithmically generated code. We conclude with a discussion of the ethical and educational implications of large language models and directions for future research. △ Less","6 September, 2022",https://arxiv.org/pdf/2201.07406
Automated Theorem Proving in the Classroom,Wolfgang Windsteiger,"We report on several scenarios of using automated theorem proving software in university education. In particular, we focus on using the Theorema system in a software-enhanced logic-course for students in computer science or artificial intelligence. The purpose of using logic-software in our teaching is not to teach students the proper use of a particular piece of software. In contrast, we try to employ certain software in order to spark students' motivation and to support their understanding of logic principles they are supposed to understand after having passed the course. In a sense, we try to let the software act as a logic-tutor, the software is not an additional subject we teach. △ Less","3 January, 2022",https://arxiv.org/pdf/2201.07275
AI-based Carcinoma Detection and Classification Using Histopathological Images: A Systematic Review,Swathi Prabhua;Keerthana Prasada;Antonio Robels-Kelly;Xuequan Lu,"Histopathological image analysis is the gold standard to diagnose cancer. Carcinoma is a subtype of cancer that constitutes more than 80% of all cancer cases. Squamous cell carcinoma and adenocarcinoma are two major subtypes of carcinoma, diagnosed by microscopic study of biopsy slides. However, manual microscopic evaluation is a subjective and time-consuming process. Many researchers have reported methods to automate carcinoma detection and classification. The increasing use of artificial intelligence (AI) in the automation of carcinoma diagnosis also reveals a significant rise in the use of deep network models. In this systematic literature review, we present a comprehensive review of the state-of-the-art approaches reported in carcinoma diagnosis using histopathological images. Studies are selected from well-known databases with strict inclusion/exclusion criteria. We have categorized the articles and recapitulated their methods based on specific organs of carcinoma origin. Further, we have summarized pertinent literature on AI methods, highlighted critical challenges and limitations, and provided insights on future research direction in automated carcinoma diagnosis. Out of 101 articles selected, most of the studies experimented on private datasets with varied image sizes, obtaining accuracy between 63% and 100%. Overall, this review highlights the need for a generalized AI-based carcinoma diagnostic system. Additionally, it is desirable to have accountable approaches to extract microscopic features from images of multiple magnifications that should mimic pathologists' evaluations. △ Less","18 January, 2022",https://arxiv.org/pdf/2201.07231
Benchmark datasets driving artificial intelligence development fail to capture the needs of medical professionals,Kathrin Blagec;Jakob Kraiger;Wolfgang Frühwirt;Matthias Samwald,"Publicly accessible benchmarks that allow for assessing and comparing model performances are important drivers of progress in artificial intelligence (AI). While recent advances in AI capabilities hold the potential to transform medical practice by assisting and augmenting the cognitive processes of healthcare professionals, the coverage of clinically relevant tasks by AI benchmarks is largely unclear. Furthermore, there is a lack of systematized meta-information that allows clinical AI researchers to quickly determine accessibility, scope, content and other characteristics of datasets and benchmark datasets relevant to the clinical domain. To address these issues, we curated and released a comprehensive catalogue of datasets and benchmarks pertaining to the broad domain of clinical and biomedical natural language processing (NLP), based on a systematic review of literature and online resources. A total of 450 NLP datasets were manually systematized and annotated with rich metadata, such as targeted tasks, clinical applicability, data types, performance metrics, accessibility and licensing information, and availability of data splits. We then compared tasks covered by AI benchmark datasets with relevant tasks that medical practitioners reported as highly desirable targets for automation in a previous empirical study. Our analysis indicates that AI benchmarks of direct clinical relevance are scarce and fail to cover most work activities that clinicians want to see addressed. In particular, tasks associated with routine documentation and patient data administration workflows are not represented despite significant associated workloads. Thus, currently available AI benchmarks are improperly aligned with desired targets for AI automation in clinical settings, and novel benchmarks should be created to fill these gaps. △ Less","12 May, 2022",https://arxiv.org/pdf/2201.07040
The Mathematics of Comparing Objects,Marcus Weber;Konstantin Fackeldey,"""After reading two different crime stories, an artificial intelligence concludes that in both stories the police has found the murderer just by random."" -- To what extend and under which assumptions this is a description of a realistic scenario?","30 March, 2022",https://arxiv.org/pdf/2201.07032
TCR-GAN: Predicting tropical cyclone passive microwave rainfall using infrared imagery via generative adversarial networks,Fan Meng;Tao Song;Danya Xu,"Tropical cyclones (TC) generally carry large amounts of water vapor and can cause large-scale extreme rainfall. Passive microwave rainfall (PMR) estimation of TC with high spatial and temporal resolution is crucial for disaster warning of TC, but remains a challenging problem due to the low temporal resolution of microwave sensors. This study attempts to solve this problem by directly forecasting PMR from satellite infrared (IR) images of TC. We develop a generative adversarial network (GAN) to convert IR images into PMR, and establish the mapping relationship between TC cloud-top bright temperature and PMR, the algorithm is named TCR-GAN. Meanwhile, a new dataset that is available as a benchmark, Dataset of Tropical Cyclone IR-to-Rainfall Prediction (TCIRRP) was established, which is expected to advance the development of artificial intelligence in this direction. Experimental results show that the algorithm can effectively extract key features from IR. The end-to-end deep learning approach shows potential as a technique that can be applied globally and provides a new perspective tropical cyclone precipitation prediction via satellite, which is expected to provide important insights for real-time visualization of TC rainfall globally in operations. △ Less","14 January, 2022",https://arxiv.org/pdf/2201.07000
A conceptual framework of Intelligent Management Control System for Higher Education,Helena Dudycz;Marcin Hernes;Zdzislaw Kes;Eunika Mercier-Laurent;Bartlomiej Nita;Krzysztof Nowosielski;Piotr Oleksyk;L. Owoc Mieczyslaw;Rafal Palak;Maciej Pondel;Krystian Wojtkiewicz,"The utilization of management control systems in university management poses a considerable challenge because university's strategic goals are not identical to those applied in profit-oriented management. A university's management control system should take into account the processing of management information for management purposes, allowing for the relationships between different groups of stakeholders. The specificity of the university operation assumes conducting long-term scientific research and educational programmes. Therefore, the controlling approach to university management should considerat long-term performance measurement as well as management in key areas such as research, provision of education to students, and interaction with the tertiary institution's socioeconomic environment.This paper aims to develop a conceptual framework of the Intelligent Management Control System for Higher Education (IMCSHE) based on cognitive agents. The main findings are related to developing the assumption, model, and technological basis including the artificial intelligence method. △ Less","12 January, 2022",https://arxiv.org/pdf/2201.06969
"AI for Closed-Loop Control Systems -- New Opportunities for Modeling, Designing, and Tuning Control Systems",Julius Schöning;Adrian Riechmann;Hans-Jürgen Pfisterer,"Control Systems, particularly closed-loop control systems (CLCS), are frequently used in production machines, vehicles, and robots nowadays. CLCS are needed to actively align actual values of a process to a given reference or set values in real-time with a very high precession. Yet, artificial intelligence (AI) is not used to model, design, optimize, and tune CLCS. This paper will highlight potential AI-empowered and -based control system designs and designing procedures, gathering new opportunities and research direction in the field of control system engineering. Therefore, this paper illustrates which building blocks within the standard block diagram of CLCS can be replaced by AI, i.e., artificial neuronal networks (ANN). Having processes with real-time contains and functional safety in mind, it is discussed if AI-based controller blocks can cope with these demands. By concluding the paper, the pros and cons of AI-empowered as well as -based CLCS designs are discussed, and possible research directions for introducing AI in the domain of control system engineering are given. △ Less","18 January, 2022",https://arxiv.org/pdf/2201.06961
Knowledge Tracing: A Survey,Ghodai Abdelrahman;Qing Wang;Bernardo Pereira Nunes,"Humans ability to transfer knowledge through teaching is one of the essential aspects for human intelligence. A human teacher can track the knowledge of students to customize the teaching on students needs. With the rise of online education platforms, there is a similar need for machines to track the knowledge of students and tailor their learning experience. This is known as the Knowledge Tracing (KT) problem in the literature. Effectively solving the KT problem would unlock the potential of computer-aided education applications such as intelligent tutoring systems, curriculum learning, and learning materials' recommendation. Moreover, from a more general viewpoint, a student may represent any kind of intelligent agents including both human and artificial agents. Thus, the potential of KT can be extended to any machine teaching application scenarios which seek for customizing the learning experience for a student agent (i.e., a machine learning model). In this paper, we provide a comprehensive and systematic review for the KT literature. We cover a broad range of methods starting from the early attempts to the recent state-of-the-art methods using deep learning, while highlighting the theoretical aspects of models and the characteristics of benchmark datasets. Besides these, we shed light on key modelling differences between closely related methods and summarize them in an easy-to-understand format. Finally, we discuss current research gaps in the KT literature and possible future research and application directions. △ Less","8 January, 2022",https://arxiv.org/pdf/2201.06953
Fairness Score and Process Standardization: Framework for Fairness Certification in Artificial Intelligence Systems,Avinash Agarwal;Harsh Agarwal;Nihaarika Agarwal,"Decisions made by various Artificial Intelligence (AI) systems greatly influence our day-to-day lives. With the increasing use of AI systems, it becomes crucial to know that they are fair, identify the underlying biases in their decision-making, and create a standardized framework to ascertain their fairness. In this paper, we propose a novel Fairness Score to measure the fairness of a data-driven AI system and a Standard Operating Procedure (SOP) for issuing Fairness Certification for such systems. Fairness Score and audit process standardization will ensure quality, reduce ambiguity, enable comparison and improve the trustworthiness of the AI systems. It will also provide a framework to operationalise the concept of fairness and facilitate the commercial deployment of such systems. Furthermore, a Fairness Certificate issued by a designated third-party auditing agency following the standardized process would boost the conviction of the organizations in the AI systems that they intend to deploy. The Bias Index proposed in this paper also reveals comparative bias amongst the various protected attributes within the dataset. To substantiate the proposed framework, we iteratively train a model on biased and unbiased data using multiple datasets and check that the Fairness Score and the proposed process correctly identify the biases and judge the fairness. △ Less","10 January, 2022",https://arxiv.org/pdf/2201.06952
AI-Aided Integrated Terrestrial and Non-Terrestrial 6G Solutions for Sustainable Maritime Networking,Salwa Saafi;Olga Vikhrova;Gábor Fodor;Jiri Hosek;Sergey Andreev,"The maritime industry is experiencing a technological revolution that affects shipbuilding, operation of both seagoing and inland vessels, cargo management, and working practices in harbors. This ongoing transformation is driven by the ambition to make the ecosystem more sustainable and cost-efficient. Digitalization and automation help achieve these goals by transforming shipping and cruising into a much more cost- and energy-efficient, and decarbonized industry segment. The key enablers in these processes are always-available connectivity and content delivery services, which can not only aid shipping companies in improving their operational efficiency and reducing carbon emissions but also contribute to enhanced crew welfare and passenger experience. Due to recent advancements in integrating high-capacity and ultra-reliable terrestrial and non-terrestrial networking technologies, ubiquitous maritime connectivity is becoming a reality. To cope with the increased complexity of managing these integrated systems, this article advocates the use of artificial intelligence and machine learning-based approaches to meet the service requirements and energy efficiency targets in various maritime communications scenarios. △ Less","25 January, 2022",https://arxiv.org/pdf/2201.06947
Digital Twin: From Concept to Practice,Ashwin Agrawal;Martin Fischer;Vishal Singh,"Recent technological developments and advances in Artificial Intelligence (AI) have enabled sophisticated capabilities to be a part of Digital Twin (DT), virtually making it possible to introduce automation into all aspects of work processes. Given these possibilities that DT can offer, practitioners are facing increasingly difficult decisions regarding what capabilities to select while deploying a DT in practice. The lack of research in this field has not helped either. It has resulted in the rebranding and reuse of emerging technological capabilities like prediction, simulation, AI, and Machine Learning (ML) as necessary constituents of DT. Inappropriate selection of capabilities in a DT can result in missed opportunities, strategic misalignments, inflated expectations, and risk of it being rejected as just hype by the practitioners. To alleviate this challenge, this paper proposes the digitalization framework, designed and developed by following a Design Science Research (DSR) methodology over a period of 18 months. The framework can help practitioners select an appropriate level of sophistication in a DT by weighing the pros and cons for each level, deciding evaluation criteria for the digital twin system, and assessing the implications of the selected DT on the organizational processes and strategies, and value creation. Three real-life case studies illustrate the application and usefulness of the framework. △ Less","14 January, 2022",https://arxiv.org/pdf/2201.06912
Frequent Itemset-driven Search for Finding Minimum Node Separators in Complex Networks,Yangming Zhou;Xiaze Zhang;Na Geng;Zhibin Jiang;Mengchu Zhou,"Finding an optimal set of critical nodes in a complex network has been a long-standing problem in the fields of both artificial intelligence and operations research. Potential applications include epidemic control, network security, carbon emission monitoring, emergence response, drug design, and vulnerability assessment. In this work, we consider the problem of finding a minimal node separator whose removal separates a graph into multiple different connected components with fewer than a limited number of vertices in each component. To solve it, we propose a frequent itemset-driven search approach, which integrates the concept of frequent itemset mining in data mining into the well-known memetic search framework. Starting from a high-quality population built by the solution construction and population repair procedures, it iteratively employs the frequent itemset recombination operator (to generate promising offspring solution based on itemsets that frequently occur in high-quality solutions), tabu search-based simulated annealing (to find high-quality local optima), population repair procedure (to modify the population), and rank-based population management strategy (to guarantee a healthy population). Extensive evaluations on 50 widely used benchmark instances show that it significantly outperforms state-of-the-art algorithms. In particular, it discovers 29 new upper bounds and matches 18 previous best-known bounds. Finally, experimental analyses are performed to confirm the effectiveness of key algorithmic modules of the proposed method. △ Less","18 January, 2022",https://arxiv.org/pdf/2201.06877
Using Reinforcement Learning for Load Testing of Video Games,Rosalia Tufano;Simone Scalabrino;Luca Pascarella;Emad Aghajani;Rocco Oliveto;Gabriele Bavota,"Different from what happens for most types of software systems, testing video games has largely remained a manual activity performed by human testers. This is mostly due to the continuous and intelligent user interaction video games require. Recently, reinforcement learning (RL) has been exploited to partially automate functional testing. RL enables training smart agents that can even achieve super-human performance in playing games, thus being suitable to explore them looking for bugs. We investigate the possibility of using RL for load testing video games. Indeed, the goal of game testing is not only to identify functional bugs, but also to examine the game's performance, such as its ability to avoid lags and keep a minimum number of frames per second (FPS) when high-demanding 3D scenes are shown on screen. We define a methodology employing RL to train an agent able to play the game as a human while also trying to identify areas of the game resulting in a drop of FPS. We demonstrate the feasibility of our approach on three games. Two of them are used as proof-of-concept, by injecting artificial performance bugs. The third one is an open-source 3D game that we load test using the trained agent showing its potential to identify areas of the game resulting in lower FPS. △ Less","18 January, 2022",https://arxiv.org/pdf/2201.06865
Label-dependent and event-guided interpretable disease risk prediction using EHRs,Shuai Niu;Yunya Song;Qing Yin;Yike Guo;Xian Yang,"Electronic health records (EHRs) contain patients' heterogeneous data that are collected from medical providers involved in the patient's care, including medical notes, clinical events, laboratory test results, symptoms, and diagnoses. In the field of modern healthcare, predicting whether patients would experience any risks based on their EHRs has emerged as a promising research area, in which artificial intelligence (AI) plays a key role. To make AI models practically applicable, it is required that the prediction results should be both accurate and interpretable. To achieve this goal, this paper proposed a label-dependent and event-guided risk prediction model (LERP) to predict the presence of multiple disease risks by mainly extracting information from unstructured medical notes. Our model is featured in the following aspects. First, we adopt a label-dependent mechanism that gives greater attention to words from medical notes that are semantically similar to the names of risk labels. Secondly, as the clinical events (e.g., treatments and drugs) can also indicate the health status of patients, our model utilizes the information from events and uses them to generate an event-guided representation of medical notes. Thirdly, both label-dependent and event-guided representations are integrated to make a robust prediction, in which the interpretability is enabled by the attention weights over words from medical notes. To demonstrate the applicability of the proposed method, we apply it to the MIMIC-III dataset, which contains real-world EHRs collected from hospitals. Our method is evaluated in both quantitative and qualitative ways. △ Less","18 January, 2022",https://arxiv.org/pdf/2201.06783
Label Dependent Attention Model for Disease Risk Prediction Using Multimodal Electronic Health Records,Shuai Niu;Qing Yin;Yunya Song;Yike Guo;Xian Yang,"Disease risk prediction has attracted increasing attention in the field of modern healthcare, especially with the latest advances in artificial intelligence (AI). Electronic health records (EHRs), which contain heterogeneous patient information, are widely used in disease risk prediction tasks. One challenge of applying AI models for risk prediction lies in generating interpretable evidence to support the prediction results while retaining the prediction ability. In order to address this problem, we propose the method of jointly embedding words and labels whereby attention modules learn the weights of words from medical notes according to their relevance to the names of risk prediction labels. This approach boosts interpretability by employing an attention mechanism and including the names of prediction tasks in the model. However, its application is only limited to the handling of textual inputs such as medical notes. In this paper, we propose a label dependent attention model LDAM to 1) improve the interpretability by exploiting Clinical-BERT (a biomedical language model pre-trained on a large clinical corpus) to encode biomedically meaningful features and labels jointly; 2) extend the idea of joint embedding to the processing of time-series data, and develop a multi-modal learning framework for integrating heterogeneous information from medical notes and time-series health status indicators. To demonstrate our method, we apply LDAM to the MIMIC-III dataset to predict different disease risks. We evaluate our method both quantitatively and qualitatively. Specifically, the predictive power of LDAM will be shown, and case studies will be carried out to illustrate its interpretability. △ Less","18 January, 2022",https://arxiv.org/pdf/2201.06779
A Literature Survey of Recent Advances in Chatbots,Guendalina Caldarini;Sardar Jaf;Kenneth McGarry,"Chatbots are intelligent conversational computer systems designed to mimic human conversation to enable automated online guidance and support. The increased benefits of chatbots led to their wide adoption by many industries in order to provide virtual assistance to customers. Chatbots utilise methods and algorithms from two Artificial Intelligence domains: Natural Language Processing and Machine Learning. However, there are many challenges and limitations in their application. In this survey we review recent advances on chatbots, where Artificial Intelligence and Natural Language processing are used. We highlight the main challenges and limitations of current work and make recommendations for future research investigation. △ Less","17 January, 2022",https://arxiv.org/pdf/2201.06657
Chatbot System Architecture,Moataz Mohammed;Mostafa M. Aref,"The conversational agents is one of the most interested topics in computer science field in the recent decade. Which can be composite from more than one subject in this field, which you need to apply Natural Language Processing Concepts and some Artificial Intelligence Techniques such as Deep Learning methods to make decision about how should be the response. This paper is dedicated to discuss the system architecture for the conversational agent and explain each component in details. △ Less","17 January, 2022",https://arxiv.org/pdf/2201.06348
Discourse Analysis for Evaluating Coherence in Video Paragraph Captions,Arjun R Akula;Song-Chun Zhu,"Video paragraph captioning is the task of automatically generating a coherent paragraph description of the actions in a video. Previous linguistic studies have demonstrated that coherence of a natural language text is reflected by its discourse structure and relations. However, existing video captioning methods evaluate the coherence of generated paragraphs by comparing them merely against human paragraph annotations and fail to reason about the underlying discourse structure. At UCLA, we are currently exploring a novel discourse based framework to evaluate the coherence of video paragraphs. Central to our approach is the discourse representation of videos, which helps in modeling coherence of paragraphs conditioned on coherence of videos. We also introduce DisNet, a novel dataset containing the proposed visual discourse annotations of 3000 videos and their paragraphs. Our experiment results have shown that the proposed framework evaluates coherence of video paragraphs significantly better than all the baseline methods. We believe that many other multi-discipline Artificial Intelligence problems such as Visual Dialog and Visual Storytelling would also greatly benefit from the proposed visual discourse framework and the DisNet dataset. △ Less","16 January, 2022",https://arxiv.org/pdf/2201.06207
Data augmentation through multivariate scenario forecasting in Data Centers using Generative Adversarial Networks,Jaime Pérez;Patricia Arroba;José M. Moya,"The Cloud paradigm is at a critical point in which the existing energy-efficiency techniques are reaching a plateau, while the computing resources demand at Data Center facilities continues to increase exponentially. The main challenge in achieving a global energy efficiency strategy based on Artificial Intelligence is that we need massive amounts of data to feed the algorithms. This paper proposes a time-series data augmentation methodology based on synthetic scenario forecasting within the Data Center. For this purpose, we will implement a powerful generative algorithm: Generative Adversarial Networks (GANs). Specifically, our work combines the disciplines of GAN-based data augmentation and scenario forecasting, filling the gap in the generation of synthetic data in DCs. Furthermore, we propose a methodology to increase the variability and heterogeneity of the generated data by introducing on-demand anomalies without additional effort or expert knowledge. We also suggest the use of Kullback-Leibler Divergence and Mean Squared Error as new metrics in the validation of synthetic time series generation, as they provide a better overall comparison of multivariate data distributions. We validate our approach using real data collected in an operating Data Center, successfully generating synthetic data helpful for prediction and optimization models. Our research will help optimize the energy consumed in Data Centers, although the proposed methodology can be employed in any similar time-series-like problem. △ Less","29 March, 2022",https://arxiv.org/pdf/2201.06147
DeepCreativity: Measuring Creativity with Deep Learning Techniques,Giorgio Franceschelli;Mirco Musolesi,"Measuring machine creativity is one of the most fascinating challenges in Artificial Intelligence. This paper explores the possibility of using generative learning techniques for automatic assessment of creativity. The proposed solution does not involve human judgement, it is modular and of general applicability. We introduce a new measure, namely DeepCreativity, based on Margaret Boden's definition of creativity as composed by value, novelty and surprise. We evaluate our methodology (and related measure) considering a case study, i.e., the generation of 19th century American poetry, showing its effectiveness and expressiveness. △ Less","16 January, 2022",https://arxiv.org/pdf/2201.06118
"Towards 6G Communications: Architecture, Challenges, and Future Directions",Purbita Mitra;Rouprita Bhattacharjee;Twinkle Chatterjee;Soumalya De;Raja Karmakar;Arindam Ghosh;Tinku Adhikari,"The cellular network standard is gradually stepping towards the 6th Generation (6G). In 6G, the pioneering and exclusive features, such as creating connectivity even in space and under water, are attracting Governments, organizations and researchers to spend time, money, effort extensively in this area. In the direction of intelligent network management and distributed secured systems, Artificial Intelligence (AI) and blockchain are going to form the backbone of 6G, respectively. However, there is a need for the study of the 6g architecture and technology, such that researchers can identify the scopes of improvement in 6G. Therefore, in this survey, we discuss the primary requirements of 6G along with its overall architecture and technological aspects. We also highlight crucial challenges and future research directions in 6G networks, which can lead to the successful practical implementation of 6G, as per the objective of its introduction in next generation cellular networks. △ Less","16 January, 2022",https://arxiv.org/pdf/2201.06079
Data Science in Perspective,Rogerio Rossi,"Data and Science has stood out in the generation of results, whether in the projects of the scientific domain or business domain. CERN Project, Scientific Institutes, companies like Walmart, Google, Apple, among others, need data to present their results and make predictions in the competitive data world. Data and Science are words that together culminated in a globally recognized term called Data Science. Data Science is in its initial phase, possibly being part of formal sciences and also being presented as part of applied sciences, capable of generating value and supporting decision making. Data Science considers science and, consequently, the scientific method to promote decision making through data intelligence. In many cases, the application of the method (or part of it) is considered in Data Science projects in scientific domain (social sciences, bioinformatics, geospatial projects) or business domain (finance, logistic, retail), among others. In this sense, this article addresses the perspectives of Data Science as a multidisciplinary area, considering science and the scientific method, and its formal structure which integrate Statistics, Computer Science, and Business Science, also taking into account Artificial Intelligence, emphasizing Machine Learning, among others. The article also deals with the perspective of applied Data Science, since Data Science is used for generating value through scientific and business projects. Data Science persona is also discussed in the article, concerning the education of Data Science professionals and its corresponding profiles, since its projection changes the field of data in the world. △ Less","15 January, 2022",https://arxiv.org/pdf/2201.05852
Mixed Diagnostics for Longitudinal Properties of Electron Bunches in a Free-Electron Laser,J. Zhu;N. M. Lockmann;M. K. Czwalinna;H. Schlarb,"Longitudinal properties of electron bunches are critical for the performance of a wide range of scientific facilities. In a free-electron laser, for example, the existing diagnostics only provide very limited longitudinal information of the electron bunch during online tuning and optimization. We leverage the power of artificial intelligence to build a neural network model using experimental data, in order to bring the destructive longitudinal phase space (LPS) diagnostics online virtually and improve the existing current profile online diagnostics which uses a coherent transition radiation (CTR) spectrometer. The model can also serve as a digital twin of the real machine on which algorithms can be tested efficiently and effectively. We demonstrate at the FLASH facility that the encoder-decoder model with more than one decoder can make highly accurate predictions of megapixel LPS images and coherent transition radiation spectra concurrently for electron bunches in a bunch train with broad ranges of LPS shapes and peak currents, which are obtained by scanning all the major control knobs for LPS manipulation. Furthermore, we propose a way to significantly improve the CTR spectrometer online measurement by combining the predicted and measured spectra. Our work showcases how to combine virtual and real diagnostics in order to provide heterogeneous and reliable mixed diagnostics for scientific facilities. △ Less","15 January, 2022",https://arxiv.org/pdf/2201.05769
Predicting Research Trends in Artificial Intelligence with Gradient Boosting Decision Trees and Time-aware Graph Neural Networks,Yichao Lu,"The Science4cast 2021 competition focuses on predicting future edges in an evolving semantic network, where each vertex represents an artificial intelligence concept, and an edge between a pair of vertices denotes that the two concepts have been investigated together in a scientific paper. In this paper, we describe our solution to this competition. We present two distinct approaches: a tree-based gradient boosting approach and a deep learning approach, and demonstrate that both approaches achieve competitive performance. Our final solution, which is based on a blend of the two approaches, achieved the 1st place among all the participating teams. The source code for this paper is available at https://github.com/YichaoLu/Science4cast2021. △ Less","14 January, 2022",https://arxiv.org/pdf/2201.05743
Tools and Practices for Responsible AI Engineering,Ryan Soklaski;Justin Goodwin;Olivia Brown;Michael Yee;Jason Matterer,"Responsible Artificial Intelligence (AI) - the practice of developing, evaluating, and maintaining accurate AI systems that also exhibit essential properties such as robustness and explainability - represents a multifaceted challenge that often stretches standard machine learning tooling, frameworks, and testing methods beyond their limits. In this paper, we present two new software libraries - hydra-zen and the rAI-toolbox - that address critical needs for responsible AI engineering. hydra-zen dramatically simplifies the process of making complex AI applications configurable, and their behaviors reproducible. The rAI-toolbox is designed to enable methods for evaluating and enhancing the robustness of AI-models in a way that is scalable and that composes naturally with other popular ML frameworks. We describe the design principles and methodologies that make these tools effective, including the use of property-based testing to bolster the reliability of the tools themselves. Finally, we demonstrate the composability and flexibility of the tools by showing how various use cases from adversarial robustness and explainable AI can be concisely implemented with familiar APIs. △ Less","14 January, 2022",https://arxiv.org/pdf/2201.05647
Reinforcement Learning based Air Combat Maneuver Generation,Muhammed Murat Ozbek;Emre Koyuncu,"The advent of artificial intelligence technology paved the way of many researches to be made within air combat sector. Academicians and many other researchers did a research on a prominent research direction called autonomous maneuver decision of UAV. Elaborative researches produced some outcomes, but decisions that include Reinforcement Learning(RL) came out to be more efficient. There have been many researches and experiments done to make an agent reach its target in an optimal way, most prominent are Genetic Algorithm(GA) , A star, RRT and other various optimization techniques have been used. But Reinforcement Learning is the well known one for its success. In DARPHA Alpha Dogfight Trials, reinforcement learning prevailed against a real veteran F16 human pilot who was trained by Boeing. This successor model was developed by Heron Systems. After this accomplishment, reinforcement learning bring tremendous attention on itself. In this research we aimed our UAV which has a dubin vehicle dynamic property to move to the target in two dimensional space in an optimal path using Twin Delayed Deep Deterministic Policy Gradients (TD3) and used in experience replay Hindsight Experience Replay(HER).We did tests on two different environments and used simulations. △ Less","14 January, 2022",https://arxiv.org/pdf/2201.05528
Emergence of Machine Language: Towards Symbolic Intelligence with Neural Networks,Yuqi Wang;Xu-Yao Zhang;Cheng-Lin Liu;Zhaoxiang Zhang,"Representation is a core issue in artificial intelligence. Humans use discrete language to communicate and learn from each other, while machines use continuous features (like vector, matrix, or tensor in deep neural networks) to represent cognitive patterns. Discrete symbols are low-dimensional, decoupled, and have strong reasoning ability, while continuous features are high-dimensional, coupled, and have incredible abstracting capabilities. In recent years, deep learning has developed the idea of continuous representation to the extreme, using millions of parameters to achieve high accuracies. Although this is reasonable from the statistical perspective, it has other major problems like lacking interpretability, poor generalization, and is easy to be attacked. Since both paradigms have strengths and weaknesses, a better choice is to seek reconciliation. In this paper, we make an initial attempt towards this direction. Specifically, we propose to combine symbolism and connectionism principles by using neural networks to derive a discrete representation. This process is highly similar to human language, which is a natural combination of discrete symbols and neural systems, where the brain processes continuous signals and represents intelligence via discrete language. To mimic this functionality, we denote our approach as machine language. By designing an interactive environment and task, we demonstrated that machines could generate a spontaneous, flexible, and semantic language through cooperation. Moreover, through experiments we show that discrete language representation has several advantages compared with continuous feature representation, from the aspects of interpretability, generalization, and robustness. △ Less","14 January, 2022",https://arxiv.org/pdf/2201.05489
"Artificial Intelligence in Software Testing : Impact, Problems, Challenges and Prospect",Zubair Khaliq;Sheikh Umar Farooq;Dawood Ashraf Khan,"Artificial Intelligence (AI) is making a significant impact in multiple areas like medical, military, industrial, domestic, law, arts as AI is capable to perform several roles such as managing smart factories, driving autonomous vehicles, creating accurate weather forecasts, detecting cancer and personal assistants, etc. Software testing is the process of putting the software to test for some abnormal behaviour of the software. Software testing is a tedious, laborious and most time-consuming process. Automation tools have been developed that help to automate some activities of the testing process to enhance quality and timely delivery. Over time with the inclusion of continuous integration and continuous delivery (CI/CD) pipeline, automation tools are becoming less effective. The testing community is turning to AI to fill the gap as AI is able to check the code for bugs and errors without any human intervention and in a much faster way than humans. In this study, we aim to recognize the impact of AI technologies on various software testing activities or facets in the STLC. Further, the study aims to recognize and explain some of the biggest challenges software testers face while applying AI to testing. The paper also proposes some key contributions of AI in the future to the domain of software testing. △ Less","14 January, 2022",https://arxiv.org/pdf/2201.05371
Assemble Foundation Models for Automatic Code Summarization,Jian Gu;Pasquale Salza;Harald C. Gall,"Automatic code summarization is beneficial to daily software development since it could help reduce the requirement of manual writing. Currently, artificial intelligence is undergoing a paradigm shift. The foundation models pretrained on massive data and finetuned to downstream tasks surpass specially customized models. This trend inspired us to consider reusing foundation models instead of learning from scratch. Thereby, we propose a flexible and robust approach for automatic code summarization, based on neural models. We assemble available foundation models, such as CodeBERT and GPT-2, into a single neural model named AdaMo. Moreover, we utilize Gaussian noise as the simulation of contextual information to optimize the latent representation. Furthermore, we introduce two adaptive schemes from the perspective of knowledge transfer, namely continuous pretraining and intermediate finetuning, and design intermediate stage tasks for general sequence-to-sequence learning. Finally, we evaluate AdaMo against a benchmark dataset for code summarization, by comparing it with state-of-the-art models. △ Less","11 March, 2022",https://arxiv.org/pdf/2201.05222
Structured access: an emerging paradigm for safe AI deployment,Toby Shevlane,"Structured access is an emerging paradigm for the safe deployment of artificial intelligence (AI). Instead of openly disseminating AI systems, developers facilitate controlled, arm's length interactions with their AI systems. The aim is to prevent dangerous AI capabilities from being widely accessible, whilst preserving access to AI capabilities that can be used safely. The developer must both restrict how the AI system can be used, and prevent the user from circumventing these restrictions through modification or reverse engineering of the AI system. Structured access is most effective when implemented through cloud-based AI services, rather than disseminating AI software that runs locally on users' hardware. Cloud-based interfaces provide the AI developer greater scope for controlling how the AI system is used, and for protecting against unauthorized modifications to the system's design. This chapter expands the discussion of ""publication norms"" in the AI community, which to date has focused on the question of how the informational content of AI research projects should be disseminated (e.g., code and models). Although this is an important question, there are limits to what can be achieved through the control of information flows. Structured access views AI software not only as information that can be shared but also as a tool with which users can have arm's length interactions. There are early examples of structured access being practiced by AI developers, but there is much room for further development, both in the functionality of cloud-based interfaces and in the wider institutional framework. △ Less","11 April, 2022",https://arxiv.org/pdf/2201.05159
Applying Machine Learning and AI Explanations to Analyze Vaccine Hesitancy,Carsten Lange;Jian Lange,"The paper quantifies the impact of race, poverty, politics, and age on COVID-19 vaccination rates in counties in the continental US. Both, OLS regression analysis and Random Forest machine learning algorithms are applied to quantify factors for county-level vaccination hesitancy. The machine learning model considers joint effects of variables (race/ethnicity, partisanship, age, etc.) simultaneously to capture the unique combination of these factors on the vaccination rate. By implementing a state-of-the-art Artificial Intelligence Explanations (AIX) algorithm, it is possible to solve the black box problem with machine learning models and provide answers to the ""how much"" question for each measured impact factor in every county. For most counties, a higher percentage vote for Republicans, a greater African American population share, and a higher poverty rate lower the vaccination rate. While a higher Asian population share increases the predicted vaccination rate. The impact on the vaccination rate from the Hispanic population proportion is positive in the OLS model, but only positive for counties with a high Hispanic population (>65%) in the Random Forest model. Both the proportion of seniors and the one for young people in a county have a significant impact in the OLS model - positive and negative, respectively. In contrast, the impacts are ambiguous in the Random Forest model. Because results vary between geographies and since the AIX algorithm is able to quantify vaccine impacts individually for each county, this research can be tailored to local communities. An interactive online mapping dashboard that identifies impact factors for individual U.S. counties is available at https://www.cpp.edu/~clange/vacmap.html. It is apparent that the influence of impact factors is not universally the same across different geographies. △ Less","7 January, 2022",https://arxiv.org/pdf/2201.05070
Flood Prediction and Analysis on the Relevance of Features using Explainable Artificial Intelligence,Sai Prasanth Kadiyala;Wai Lok Woo,"This paper presents flood prediction models for the state of Kerala in India by analyzing the monthly rainfall data and applying machine learning algorithms including Logistic Regression, K-Nearest Neighbors, Decision Trees, Random Forests, and Support Vector Machine. Although these models have shown high accuracy prediction of the occurrence of flood in a particular year, they do not quantitatively and qualitatively explain the prediction decision. This paper shows how the background features are learned that contributed to the prediction decision and further extended to explain the inner workings with the development of explainable artificial intelligence modules. The obtained results have confirmed the validity of the findings uncovered by the explainer modules basing on the historical flood monthly rainfall data in Kerala. △ Less","13 January, 2022",https://arxiv.org/pdf/2201.05046
Fantastic Data and How to Query Them,Trung-Kien Tran;Anh Le-Tuan;Manh Nguyen-Duc;Jicheng Yuan;Danh Le-Phuoc,"It is commonly acknowledged that the availability of the huge amount of (training) data is one of the most important factors for many recent advances in Artificial Intelligence (AI). However, datasets are often designed for specific tasks in narrow AI sub areas and there is no unified way to manage and access them. This not only creates unnecessary overheads when training or deploying Machine Learning models but also limits the understanding of the data, which is very important for data-centric AI. In this paper, we present our vision about a unified framework for different datasets so that they can be integrated and queried easily, e.g., using standard query languages. We demonstrate this in our ongoing work to create a framework for datasets in Computer Vision and show its advantages in different scenarios. Our demonstration is available at https://vision.semkg.org. △ Less","13 January, 2022",https://arxiv.org/pdf/2201.05026
Trusted Media Challenge Dataset and User Study,Weiling Chen;Sheng Lun Benjamin Chua;Stefan Winkler;See-Kiong Ng,"The development of powerful deep learning technologies has brought about some negative effects to both society and individuals. One such issue is the emergence of fake media. To tackle the issue, we have organized the Trusted Media Challenge (TMC) to explore how Artificial Intelligence (AI) technologies could be leveraged to combat fake media. To enable further research, we are releasing the dataset that we had prepared from the TMC challenge, consisting of 4,380 fake and 2,563 real videos, with various video and/or audio manipulation methods employed to produce different types of fake media. All the videos in the TMC dataset are accompanied with audios and have a minimum resolution of 360p. The videos have various durations, background, illumination, and may contain perturbations that mimic transmission errors and compression. We have also carried out a user study to demonstrate the quality of the TMC dataset and to compare the performance of humans and AI models. The results showed that the TMC dataset can fool human participants in many cases, and the winning AI models of the Trusted Media Challenge outperformed humans. The TMC dataset is available for research purpose upon request via tmc-dataset@aisingapore.org. △ Less","16 August, 2022",https://arxiv.org/pdf/2201.04788
A Survey on Masked Facial Detection Methods and Datasets for Fighting Against COVID-19,Bingshu Wang;Jiangbin Zheng;C. L. Philip Chen,"Coronavirus disease 2019 (COVID-19) continues to pose a great challenge to the world since its outbreak. To fight against the disease, a series of artificial intelligence (AI) techniques are developed and applied to real-world scenarios such as safety monitoring, disease diagnosis, infection risk assessment, lesion segmentation of COVID-19 CT scans,etc. The coronavirus epidemics have forced people wear masks to counteract the transmission of virus, which also brings difficulties to monitor large groups of people wearing masks. In this paper, we primarily focus on the AI techniques of masked facial detection and related datasets. We survey the recent advances, beginning with the descriptions of masked facial detection datasets. Thirteen available datasets are described and discussed in details. Then, the methods are roughly categorized into two classes: conventional methods and neural network-based methods. Conventional methods are usually trained by boosting algorithms with hand-crafted features, which accounts for a small proportion. Neural network-based methods are further classified as three parts according to the number of processing stages. Representative algorithms are described in detail, coupled with some typical techniques that are described briefly. Finally, we summarize the recent benchmarking results, give the discussions on the limitations of datasets and methods, and expand future research directions. To our knowledge, this is the first survey about masked facial detection methods and datasets. Hopefully our survey could provide some help to fight against epidemics. △ Less","12 January, 2022",https://arxiv.org/pdf/2201.04777
Everything You wanted to Know about Smart Agriculture,Alakananda Mitra;Sukrutha L. T. Vangipuram;Anand K. Bapatla;Venkata K. V. V. Bathalapalli;Saraju P. Mohanty;Elias Kougianos;Chittaranjan Ray,"The world population is anticipated to increase by close to 2 billion by 2050 causing a rapid escalation of food demand. A recent projection shows that the world is lagging behind accomplishing the ""Zero Hunger"" goal, in spite of some advancements. Socio-economic and well being fallout will affect the food security. Vulnerable groups of people will suffer malnutrition. To cater to the needs of the increasing population, the agricultural industry needs to be modernized, become smart, and automated. Traditional agriculture can be remade to efficient, sustainable, eco-friendly smart agriculture by adopting existing technologies. In this survey paper the authors present the applications, technological trends, available datasets, networking options, and challenges in smart agriculture. How Agro Cyber Physical Systems are built upon the Internet-of-Agro-Things is discussed through various application fields. Agriculture 4.0 is also discussed as a whole. We focus on the technologies, such as Artificial Intelligence (AI) and Machine Learning (ML) which support the automation, along with the Distributed Ledger Technology (DLT) which provides data integrity and security. After an in-depth study of different architectures, we also present a smart agriculture framework which relies on the location of data processing. We have divided open research problems of smart agriculture as future research work in two groups - from a technological perspective and from a networking perspective. AI, ML, the blockchain as a DLT, and Physical Unclonable Functions (PUF) based hardware security fall under the technology group, whereas any network related attacks, fake data injection and similar threats fall under the network research problem group. △ Less","12 January, 2022",https://arxiv.org/pdf/2201.04754
Multi-echelon Supply Chains with Uncertain Seasonal Demands and Lead Times Using Deep Reinforcement Learning,Julio César Alves;Geraldo Robson Mateus,"We address the problem of production planning and distribution in multi-echelon supply chains. We consider uncertain demands and lead times which makes the problem stochastic and non-linear. A Markov Decision Process formulation and a Non-linear Programming model are presented. As a sequential decision-making problem, Deep Reinforcement Learning (RL) is a possible solution approach. This type of technique has gained a lot of attention from Artificial Intelligence and Optimization communities in recent years. Considering the good results obtained with Deep RL approaches in different areas there is a growing interest in applying them in problems from the Operations Research field. We have used a Deep RL technique, namely Proximal Policy Optimization (PPO2), to solve the problem considering uncertain, regular and seasonal demands and constant or stochastic lead times. Experiments are carried out in different scenarios to better assess the suitability of the algorithm. An agent based on a linearized model is used as a baseline. Experimental results indicate that PPO2 is a competitive and adequate tool for this type of problem. PPO2 agent is better than baseline in all scenarios with stochastic lead times (7.3-11.2%), regardless of whether demands are seasonal or not. In scenarios with constant lead times, the PPO2 agent is better when uncertain demands are non-seasonal (2.2-4.7%). The results show that the greater the uncertainty of the scenario, the greater the viability of this type of approach. △ Less","12 January, 2022",https://arxiv.org/pdf/2201.04651
Get your Foes Fooled: Proximal Gradient Split Learning for Defense against Model Inversion Attacks on IoMT data,Sunder Ali Khowaja;Ik Hyun Lee;Kapal Dev;Muhammad Aslam Jarwar;Nawab Muhammad Faseeh Qureshi,"The past decade has seen a rapid adoption of Artificial Intelligence (AI), specifically the deep learning networks, in Internet of Medical Things (IoMT) ecosystem. However, it has been shown recently that the deep learning networks can be exploited by adversarial attacks that not only make IoMT vulnerable to the data theft but also to the manipulation of medical diagnosis. The existing studies consider adding noise to the raw IoMT data or model parameters which not only reduces the overall performance concerning medical inferences but also is ineffective to the likes of deep leakage from gradients method. In this work, we propose proximal gradient split learning (PSGL) method for defense against the model inversion attacks. The proposed method intentionally attacks the IoMT data when undergoing the deep neural network training process at client side. We propose the use of proximal gradient method to recover gradient maps and a decision-level fusion strategy to improve the recognition performance. Extensive analysis show that the PGSL not only provides effective defense mechanism against the model inversion attacks but also helps in improving the recognition performance on publicly available datasets. We report 14.0\%, 17.9\%, and 36.9\% gains in accuracy over reconstructed and adversarial attacked images, respectively. △ Less","9 August, 2022",https://arxiv.org/pdf/2201.04569
Evolutionary Optimization for Proactive and Dynamic Computing Resource Allocation in Open Radio Access Network,Gan Ruan;Leandro L. Minku;Zhao Xu;Xin Yao,"Intelligent techniques are urged to achieve automatic allocation of the computing resource in Open Radio Access Network (O-RAN), to save computing resource, increase utilization rate of them and decrease the delay. However, the existing problem formulation to solve this resource allocation problem is unsuitable as it defines the capacity utility of resource in an inappropriate way and tends to cause much delay. Moreover, the existing problem has only been attempted to be solved based on greedy search, which is not ideal as it could get stuck into local optima. Considering those, a new formulation that better describes the problem is proposed. In addition, as a well-known global search meta heuristic approach, an evolutionary algorithm (EA) is designed tailored for solving the new problem formulation, to find a resource allocation scheme to proactively and dynamically deploy the computing resource for processing upcoming traffic data. Experimental studies carried out on several real-world datasets and newly generated artificial datasets with more properties beyond the real-world datasets have demonstrated the significant superiority over a baseline greedy algorithm under different parameter settings. Moreover, experimental studies are taken to compare the proposed EA and two variants, to indicate the impact of different algorithm choices. △ Less","12 January, 2022",https://arxiv.org/pdf/2201.04361
Toward Experience-Driven Traffic Management and Orchestration in Digital-Twin-Enabled 6G Networks,Muhammad Tariq;Faisal Naeem;H. Vincent Poor,"The envisioned 6G networks are expected to support extremely high data rates, low-latency, and radically new applications empowered by machine learning. The futuristic 6G networks require a novel framework that can be used to operate, manage, and optimize its underlying services such as ultra-reliable and low-latency communication, and Internet of everything. In recent years, artificial intelligence (AI) has demonstrated significant success in optimizing and designing networks. The AI-enabled traffic orchestration can dynamically organize different network architectures and slices to provide quality of experience considering the dynamic nature of the wireless communication network. In this paper, we propose a digital twin enabled network framework, empowered by AI to cater the variability and complexity of envisioned 6G networks, to provide smart resource management and intelligent service provisioning. Digital twin paves a way for achieving optimizing 6G services by creating a virtual representation of the 6G network along with its associated communication technologies (e.g., intelligent reflecting surfaces, terahertz and millimeter communication), computing systems (e.g., cloud computing and fog computing) with its associated algorithms (e.g., optimization and machine learning). We then discuss and review the existing AI-enabled traffic management and orchestration techniques and highlight future research directions and potential solutions in 6G networks. △ Less","11 January, 2022",https://arxiv.org/pdf/2201.04259
The Turing Trap: The Promise & Peril of Human-Like Artificial Intelligence,Erik Brynjolfsson,"In 1950, Alan Turing proposed an imitation game as the ultimate test of whether a machine was intelligent: could a machine imitate a human so well that its answers to questions indistinguishable from a human. Ever since, creating intelligence that matches human intelligence has implicitly or explicitly been the goal of thousands of researchers, engineers, and entrepreneurs. The benefits of human-like artificial intelligence (HLAI) include soaring productivity, increased leisure, and perhaps most profoundly, a better understanding of our own minds. But not all types of AI are human-like. In fact, many of the most powerful systems are very different from humans. So an excessive focus on developing and deploying HLAI can lead us into a trap. As machines become better substitutes for human labor, workers lose economic and political bargaining power and become increasingly dependent on those who control the technology. In contrast, when AI is focused on augmenting humans rather than mimicking them, then humans retain the power to insist on a share of the value created. Furthermore, augmentation creates new capabilities and new products and services, ultimately generating far more value than merely human-like AI. While both types of AI can be enormously beneficial, there are currently excess incentives for automation rather than augmentation among technologists, business executives, and policymakers. △ Less","11 January, 2022",https://arxiv.org/pdf/2201.04200
Matching-based Service Offloading for Compute-less Driven IoT Networks,Boubakr Nour;Soumaya Cherkaoui,"With the advent of the Internet of Things (IoT) and 5G networks, edge computing is offering new opportunities for business model and use cases innovations. Service providers can now virtualize the cloud beyond the data center to meet the latency, data sovereignty, reliability, and interoperability requirements. Yet, many new applications (e.g., augmented reality, virtual reality, artificial intelligence) are computation-intensive and delay-sensitivity. These applications are invoked heavily with similar inputs that could lead to the same output. Compute-less networks aim to implement a network with a minimum amount of computation and communication. This can be realized by offloading prevalent services to the edge and thus minimizing communication in the core network and eliminating redundant computations using the computation reuse concept. In this paper, we present matching-based services offloading schemes for compute-less IoT networks. We adopt the matching theory to match service offloading to the appropriate edge server(s). Specifically, we design, WHISTLE, a vertical many-to-many offloading scheme that aims to offload the most invoked and highly reusable services to the appropriate edge servers. We further extend WHISTLE to provide horizontal one-to-many computation reuse sharing among edge servers which leads to bouncing less computation back to the cloud. We evaluate the efficiency and effectiveness of WHISTLE with a real-world dataset. The obtained findings show that WHISTLE is able to accelerate the tasks completion time by 20%, reduce the computation up to 77%, and decrease the communication up to 71%. Theoretical analyses also prove the stability of the designed schemes. △ Less","11 January, 2022",https://arxiv.org/pdf/2201.04195
A Survey on Applications of Digital Human Avatars toward Virtual Co-presence,Matthew Korban;Xin Li,"This paper investigates different approaches to build and use digital human avatars toward interactive Virtual Co-presence (VCP) environments. We evaluate the evolution of technologies for creating VCP environments and how the advancement in Artificial Intelligence (AI) and Computer Graphics affect the quality of VCP environments. We categorize different methods in the literature based on their applications and methodology and compare various groups and strategies based on their applications, contributions, and limitations. We also have a brief discussion about the approaches that other forms of human representation, rather than digital human avatars, have been utilized in VCP environments. Our goal is to fill the gap in the research domain where there is a lack of literature review investigating different approaches for creating avatar-based VCP environments. We hope this study will be useful for future research involving human representation in VCP or Virtual Reality (VR) environments. To the best of our knowledge, it is the first survey research that investigates avatar-based VCP environments. Specifically, the categorization methodology suggested in this paper for avatar-based methods is new. △ Less","11 January, 2022",https://arxiv.org/pdf/2201.04168
Where Is My Mind (looking at)? Predicting Visual Attention from Brain Activity,Victor Delvigne;Noé Tits;Luca La Fisca;Nathan Hubens;Antoine Maiorca;Hazem Wannous;Thierry Dutoit;Jean-Philippe Vandeborre,"Visual attention estimation is an active field of research at the crossroads of different disciplines: computer vision, artificial intelligence and medicine. One of the most common approaches to estimate a saliency map representing attention is based on the observed images. In this paper, we show that visual attention can be retrieved from EEG acquisition. The results are comparable to traditional predictions from observed images, which is of great interest. For this purpose, a set of signals has been recorded and different models have been developed to study the relationship between visual attention and brain activity. The results are encouraging and comparable with other approaches estimating attention with other modalities. The codes and dataset considered in this paper have been made available at \url{https://figshare.com/s/3e353bd1c621962888ad} to promote research in the field. △ Less","11 January, 2022",https://arxiv.org/pdf/2201.03902
An Accelerator for Rule Induction in Fuzzy Rough Theory,Suyun Zhao;Zhigang Dai;Xizhao Wang;Peng Ni;Hengheng Luo;Hong Chen;Cuiping Li,"Rule-based classifier, that extract a subset of induced rules to efficiently learn/mine while preserving the discernibility information, plays a crucial role in human-explainable artificial intelligence. However, in this era of big data, rule induction on the whole datasets is computationally intensive. So far, to the best of our knowledge, no known method focusing on accelerating rule induction has been reported. This is first study to consider the acceleration technique to reduce the scale of computation in rule induction. We propose an accelerator for rule induction based on fuzzy rough theory; the accelerator can avoid redundant computation and accelerate the building of a rule classifier. First, a rule induction method based on consistence degree, called Consistence-based Value Reduction (CVR), is proposed and used as basis to accelerate. Second, we introduce a compacted search space termed Key Set, which only contains the key instances required to update the induced rule, to conduct value reduction. The monotonicity of Key Set ensures the feasibility of our accelerator. Third, a rule-induction accelerator is designed based on Key Set, and it is theoretically guaranteed to display the same results as the unaccelerated version. Specifically, the rank preservation property of Key Set ensures consistency between the rule induction achieved by the accelerator and the unaccelerated method. Finally, extensive experiments demonstrate that the proposed accelerator can perform remarkably faster than the unaccelerated rule-based classifier methods, especially on datasets with numerous instances. △ Less","7 January, 2022",https://arxiv.org/pdf/2201.03649
Fusing Blockchain and AI with Metaverse: A Survey,Qinglin Yang;Yetong Zhao;Huawei Huang;Zehui Xiong;Jiawen Kang;Zibin Zheng,"Metaverse as the latest buzzword has attracted great attention from both industry and academia. Metaverse seamlessly integrates the real world with the virtual world and allows avatars to carry out rich activities including creation, display, entertainment, social networking, and trading. Thus, it is promising to build an exciting digital world and to transform a better physical world through the exploration of the metaverse. In this survey, we dive into the metaverse by discussing how Blockchain and Artificial Intelligence (AI) fuse with it through investigating the state-of-the-art studies across the metaverse components, digital currencies, AI applications in the virtual world, and blockchain-empowered technologies. Further exploitation and interdisciplinary research on the fusion of AI and Blockchain towards metaverse will definitely require collaboration from both academia and industries. We wish that our survey can help researchers, engineers, and educators build an open, fair, and rational future metaverse. △ Less","14 June, 2022",https://arxiv.org/pdf/2201.03201
Development of a hybrid machine-learning and optimization tool for performance-based solar shading design,Maryam Daneshi;Reza Taghavi Fard;Zahra Sadat Zomorodian;Mohammad Tahsildoost,"Solar shading design should be done for the desired Indoor Environmental Quality (IEQ) in the early design stages. This field can be very challenging and time-consuming also requires experts, sophisticated software, and a large amount of money. The primary purpose of this research is to design a simple tool to study various models of solar shadings and make decisions easier and faster in the early stages. Database generation methods, artificial intelligence, and optimization have been used to achieve this goal. This tool includes two main parts of 1. predicting the performance of the user-selected model along with proposing effective parameters and 2. proposing optimal pre-prepared models to the user. In this regard, initially, a side-lit shoebox model with variable parameters was modeled parametrically, and five common solar shading models with their variables were applied to the space. For each solar shadings and the state without shading, metrics related to daylight and glare, view, and initial costs were simulated. The database generated in this research includes 87912 alternatives and six calculated metrics introduced to optimized machine learning models, including neural network, random Forrest, support vector regression, and k nearest neighbor. According to the results, the most accurate and fastest estimation model was Random Forrest, with an r2_score of 0.967 to 1. Then, sensitivity analysis was performed to identify the most influential parameters for each shading model and the state without it. This analysis distinguished the most effective parameters, including window orientation, WWR, room width, length, and shading depth. Finally, by optimizing the estimation function of machine learning models with the NSGA II algorithm, about 7300 optimal models were identified. The developed tool can evaluate various design alternatives in less than a few seconds for each. △ Less","9 January, 2022",https://arxiv.org/pdf/2201.03028
Arguments about Highly Reliable Agent Designs as a Useful Path to Artificial Intelligence Safety,Issa Rice;David Manheim,"Several different approaches exist for ensuring the safety of future Transformative Artificial Intelligence (TAI) or Artificial Superintelligence (ASI) systems, and proponents of different approaches have made different and debated claims about the importance or usefulness of their work in the near term, and for future systems. Highly Reliable Agent Designs (HRAD) is one of the most controversial and ambitious approaches, championed by the Machine Intelligence Research Institute, among others, and various arguments have been made about whether and how it reduces risks from future AI systems. In order to reduce confusion in the debate about AI safety, here we build on a previous discussion by Rice which collects and presents four central arguments which are used to justify HRAD as a path towards safety of AI systems. We have titled the arguments (1) incidental utility,(2) deconfusion, (3) precise specification, and (4) prediction. Each of these makes different, partly conflicting claims about how future AI systems can be risky. We have explained the assumptions and claims based on a review of published and informal literature, along with consultation with experts who have stated positions on the topic. Finally, we have briefly outlined arguments against each approach and against the agenda overall. △ Less","9 January, 2022",https://arxiv.org/pdf/2201.02950
Building Human-like Communicative Intelligence: A Grounded Perspective,Marina Dubova,"Modern Artificial Intelligence (AI) systems excel at diverse tasks, from image classification to strategy games, even outperforming humans in many of these domains. After making astounding progress in language learning in the recent decade, AI systems, however, seem to approach the ceiling that does not reflect important aspects of human communicative capacities. Unlike human learners, communicative AI systems often fail to systematically generalize to new data, suffer from sample inefficiency, fail to capture common-sense semantic knowledge, and do not translate to real-world communicative situations. Cognitive Science offers several insights on how AI could move forward from this point. This paper aims to: (1) suggest that the dominant cognitively-inspired AI directions, based on nativist and symbolic paradigms, lack necessary substantiation and concreteness to guide progress in modern AI, and (2) articulate an alternative, ""grounded"", perspective on AI advancement, inspired by Embodied, Embedded, Extended, and Enactive Cognition (4E) research. I review results on 4E research lines in Cognitive Science to distinguish the main aspects of naturalistic learning conditions that play causal roles for human language development. I then use this analysis to propose a list of concrete, implementable components for building ""grounded"" linguistic intelligence. These components include embodying machines in a perception-action cycle, equipping agents with active exploration mechanisms so they can build their own curriculum, allowing agents to gradually develop motor abilities to promote piecemeal language development, and endowing the agents with adaptive feedback from their physical and social environment. I hope that these ideas can direct AI research towards building machines that develop human-like language abilities through their experiences with the world. △ Less","1 January, 2022",https://arxiv.org/pdf/2201.02734
AI for Beyond 5G Networks: A Cyber-Security Defense or Offense Enabler?,C. Benzaid;T. Taleb,"Artificial Intelligence (AI) is envisioned to play a pivotal role in empowering intelligent, adaptive and autonomous security management in 5G and beyond networks, thanks to its potential to uncover hidden patterns from a large set of time-varying multi-dimensional data, and deliver faster and accurate decisions. Unfortunately, AI's capabilities and vulnerabilities make it a double-edged sword that may jeopardize the security of future networks. This paper sheds light on how AI may impact the security of 5G and its successive from its posture of defender, offender or victim, and recommends potential defenses to safeguard from malevolent AI while pointing out their limitations and adoption challenges. △ Less","5 January, 2022",https://arxiv.org/pdf/2201.02730
Automatic Speech Recognition Datasets in Cantonese: A Survey and New Dataset,Tiezheng Yu;Rita Frieske;Peng Xu;Samuel Cahyawijaya;Cheuk Tung Shadow Yiu;Holy Lovenia;Wenliang Dai;Elham J. Barezi;Qifeng Chen;Xiaojuan Ma;Bertram E. Shi;Pascale Fung,"Automatic speech recognition (ASR) on low resource languages improves the access of linguistic minorities to technological advantages provided by artificial intelligence (AI). In this paper, we address the problem of data scarcity for the Hong Kong Cantonese language by creating a new Cantonese dataset. Our dataset, Multi-Domain Cantonese Corpus (MDCC), consists of 73.6 hours of clean read speech paired with transcripts, collected from Cantonese audiobooks from Hong Kong. It comprises philosophy, politics, education, culture, lifestyle and family domains, covering a wide range of topics. We also review all existing Cantonese datasets and analyze them according to their speech type, data source, total size and availability. We further conduct experiments with Fairseq S2T Transformer, a state-of-the-art ASR model, on the biggest existing dataset, Common Voice zh-HK, and our proposed MDCC, and the results show the effectiveness of our dataset. In addition, we create a powerful and robust Cantonese ASR model by applying multi-dataset learning on MDCC and Common Voice zh-HK. △ Less","17 January, 2022",https://arxiv.org/pdf/2201.02419
"From Textual Experiments to Experimental Texts: Expressive Repetition in ""Artificial Intelligence Literature""",Tianhua Zhu,"Since the birth of artificial intelligence 70 years ago, attempts at literary ""creation"" with computers are present in the course of technological development, creating what one might call ""artificial intelligence literature"" (AI literature). Evolving from ""textual experiments"" conducted by technologists to ""experimental texts"" that explore the possibilities of conceptions of literature, AI literature integrates primitive problems including machine thinking, text generation, and machine creativity, which exhibits the two-way interaction between social ideas and technology. In the early stage, the mutual support between technological path and artistic ideas turned out to be a failure, while AI-driven expressive repetitions are made probable in the contemporary technological context, paving the way for the transformation of AI literature from proof for technical possibilities to self-verification of literary value. △ Less","6 January, 2022",https://arxiv.org/pdf/2201.02303
A unified software/hardware scalable architecture for brain-inspired computing based on self-organizing neural models,Artem R. Muliukov;Laurent Rodriguez;Benoit Miramond;Lyes Khacef;Joachim Schmidt;Quentin Berthet;Andres Upegui,"The field of artificial intelligence has significantly advanced over the past decades, inspired by discoveries from the fields of biology and neuroscience. The idea of this work is inspired by the process of self-organization of cortical areas in the human brain from both afferent and lateral/internal connections. In this work, we develop an original brain-inspired neural model associating Self-Organizing Maps (SOM) and Hebbian learning in the Reentrant SOM (ReSOM) model. The framework is applied to multimodal classification problems. Compared to existing methods based on unsupervised learning with post-labeling, the model enhances the state-of-the-art results. This work also demonstrates the distributed and scalable nature of the model through both simulation results and hardware execution on a dedicated FPGA-based platform named SCALP (Self-configurable 3D Cellular Adaptive Platform). SCALP boards can be interconnected in a modular way to support the structure of the neural model. Such a unified software and hardware approach enables the processing to be scaled and allows information from several modalities to be merged dynamically. The deployment on hardware boards provides performance results of parallel execution on several devices, with the communication between each board through dedicated serial links. The proposed unified architecture, composed of the ReSOM model and the SCALP hardware platform, demonstrates a significant increase in accuracy thanks to multimodal association, and a good trade-off between latency and power consumption compared to a centralized GPU implementation. △ Less","6 January, 2022",https://arxiv.org/pdf/2201.02262
"Machine Learning: Algorithms, Models, and Applications",Jaydip Sen;Sidra Mehtab;Rajdeep Sen;Abhishek Dutta;Pooja Kherwa;Saheel Ahmed;Pranay Berry;Sahil Khurana;Sonali Singh;David W. W Cadotte;David W. Anderson;Kalum J. Ost;Racheal S. Akinbo;Oladunni A. Daramola;Bongs Lainjo,"Recent times are witnessing rapid development in machine learning algorithm systems, especially in reinforcement learning, natural language processing, computer and robot vision, image processing, speech, and emotional processing and understanding. In tune with the increasing importance and relevance of machine learning models, algorithms, and their applications, and with the emergence of more innovative uses cases of deep learning and artificial intelligence, the current volume presents a few innovative research works and their applications in real world, such as stock trading, medical and healthcare systems, and software automation. The chapters in the book illustrate how machine learning and deep learning algorithms and models are designed, optimized, and deployed. The volume will be useful for advanced graduate and doctoral students, researchers, faculty members of universities, practicing data scientists and data engineers, professionals, and consultants working on the broad areas of machine learning, deep learning, and artificial intelligence. △ Less","6 January, 2022",https://arxiv.org/pdf/2201.01943
Combining Reinforcement Learning and Inverse Reinforcement Learning for Asset Allocation Recommendations,Igor Halperin;Jiayu Liu;Xiao Zhang,"We suggest a simple practical method to combine the human and artificial intelligence to both learn best investment practices of fund managers, and provide recommendations to improve them. Our approach is based on a combination of Inverse Reinforcement Learning (IRL) and RL. First, the IRL component learns the intent of fund managers as suggested by their trading history, and recovers their implied reward function. At the second step, this reward function is used by a direct RL algorithm to optimize asset allocation decisions. We show that our method is able to improve over the performance of individual fund managers. △ Less","5 January, 2022",https://arxiv.org/pdf/2201.01874
Quantum Capsule Networks,Zidu Liu;Pei-Xin Shen;Weikang Li;L. -M. Duan;Dong-Ling Deng,"Capsule networks, which incorporate the paradigms of connectionism and symbolism, have brought fresh insights into artificial intelligence. The capsule, as the building block of capsule networks, is a group of neurons represented by a vector to encode different features of an entity. The information is extracted hierarchically through capsule layers via routing algorithms. Here, we introduce a quantum capsule network (dubbed QCapsNet) together with an efficient quantum dynamic routing algorithm. To benchmark the performance of the QCapsNet, we carry out extensive numerical simulations on the classification of handwritten digits and symmetry-protected topological phases, and show that the QCapsNet can achieve an enhanced accuracy and outperform conventional quantum classifiers evidently. We further unpack the output capsule state and find that a particular subspace may correspond to a human-understandable feature of the input data, which indicates the potential explainability of such networks. Our work reveals an intriguing prospect of quantum capsule networks in quantum machine learning, which may provide a valuable guide towards explainable quantum artificial intelligence. △ Less","5 December, 2022",https://arxiv.org/pdf/2201.01778
Using Deep Learning with Large Aggregated Datasets for COVID-19 Classification from Cough,Esin Darici Haritaoglu;Nicholas Rasmussen;Daniel C. H. Tan;Jennifer Ranjani J.;Jaclyn Xiao;Gunvant Chaudhari;Akanksha Rajput;Praveen Govindan;Christian Canham;Wei Chen;Minami Yamaura;Laura Gomezjurado;Aaron Broukhim;Amil Khanzada;Mert Pilanci,"The Covid-19 pandemic has been one of the most devastating events in recent history, claiming the lives of more than 5 million people worldwide. Even with the worldwide distribution of vaccines, there is an apparent need for affordable, reliable, and accessible screening techniques to serve parts of the World that do not have access to Western medicine. Artificial Intelligence can provide a solution utilizing cough sounds as a primary screening mode for COVID-19 diagnosis. This paper presents multiple models that have achieved relatively respectable performance on the largest evaluation dataset currently presented in academic literature. Through investigation of a self-supervised learning model (Area under the ROC curve, AUC = 0.807) and a convolutional nerual network (CNN) model (AUC = 0.802), we observe the possibility of model bias with limited datasets. Moreover, we observe that performance increases with training data size, showing the need for the worldwide collection of data to help combat the Covid-19 pandemic with non-traditional means. △ Less","29 March, 2022",https://arxiv.org/pdf/2201.01669
Challenges of Artificial Intelligence -- From Machine Learning and Computer Vision to Emotional Intelligence,Matti Pietikäinen;Olli Silven,"Artificial intelligence (AI) has become a part of everyday conversation and our lives. It is considered as the new electricity that is revolutionizing the world. AI is heavily invested in both industry and academy. However, there is also a lot of hype in the current AI debate. AI based on so-called deep learning has achieved impressive results in many problems, but its limits are already visible. AI has been under research since the 1940s, and the industry has seen many ups and downs due to over-expectations and related disappointments that have followed. The purpose of this book is to give a realistic picture of AI, its history, its potential and limitations. We believe that AI is a helper, not a ruler of humans. We begin by describing what AI is and how it has evolved over the decades. After fundamentals, we explain the importance of massive data for the current mainstream of artificial intelligence. The most common representations for AI, methods, and machine learning are covered. In addition, the main application areas are introduced. Computer vision has been central to the development of AI. The book provides a general introduction to computer vision, and includes an exposure to the results and applications of our own research. Emotions are central to human intelligence, but little use has been made in AI. We present the basics of emotional intelligence and our own research on the topic. We discuss super-intelligence that transcends human understanding, explaining why such achievement seems impossible on the basis of present knowledge,and how AI could be improved. Finally, a summary is made of the current state of AI and what to do in the future. In the appendix, we look at the development of AI education, especially from the perspective of contents at our own university. △ Less","5 January, 2022",https://arxiv.org/pdf/2201.01466
An Overview of 5G Advanced Evolution in 3GPP Release 18,Xingqin Lin,"The 3rd generation partnership project (3GPP) radio access network (RAN) plenary recently approved a work package for its Release 18, representing a major evolution and branded as the first release of 5G Advanced. The work package includes diverse study or work items that will significantly boost 5G performance and address a wide variety of new use cases. In particular, 3GPP Release 18 will embrace artificial intelligence and machine learning technologies to provide data-driven, intelligent network solutions. This article provides an overview of the 5G Advanced evolution in 3GPP Release 18, which is anticipated to trigger a paradigm shift and have a profound impact on future wireless networks. △ Less","4 January, 2022",https://arxiv.org/pdf/2201.01358
Self-directed Machine Learning,Wenwu Zhu;Xin Wang;Pengtao Xie,"Conventional machine learning (ML) relies heavily on manual design from machine learning experts to decide learning tasks, data, models, optimization algorithms, and evaluation metrics, which is labor-intensive, time-consuming, and cannot learn autonomously like humans. In education science, self-directed learning, where human learners select learning tasks and materials on their own without requiring hands-on guidance, has been shown to be more effective than passive teacher-guided learning. Inspired by the concept of self-directed human learning, we introduce the principal concept of Self-directed Machine Learning (SDML) and propose a framework for SDML. Specifically, we design SDML as a self-directed learning process guided by self-awareness, including internal awareness and external awareness. Our proposed SDML process benefits from self task selection, self data selection, self model selection, self optimization strategy selection and self evaluation metric selection through self-awareness without human guidance. Meanwhile, the learning performance of the SDML process serves as feedback to further improve self-awareness. We propose a mathematical formulation for SDML based on multi-level optimization. Furthermore, we present case studies together with potential applications of SDML, followed by discussing future research directions. We expect that SDML could enable machines to conduct human-like self-directed learning and provide a new perspective towards artificial general intelligence. △ Less","7 January, 2022",https://arxiv.org/pdf/2201.01289
Self-supervised Learning from 100 Million Medical Images,Florin C. Ghesu;Bogdan Georgescu;Awais Mansoor;Youngjin Yoo;Dominik Neumann;Pragneshkumar Patel;R. S. Vishwanath;James M. Balter;Yue Cao;Sasa Grbic;Dorin Comaniciu,"Building accurate and robust artificial intelligence systems for medical image assessment requires not only the research and design of advanced deep learning models but also the creation of large and curated sets of annotated training examples. Constructing such datasets, however, is often very costly -- due to the complex nature of annotation tasks and the high level of expertise required for the interpretation of medical images (e.g., expert radiologists). To counter this limitation, we propose a method for self-supervised learning of rich image features based on contrastive learning and online feature clustering. For this purpose we leverage large training datasets of over 100,000,000 medical images of various modalities, including radiography, computed tomography (CT), magnetic resonance (MR) imaging and ultrasonography. We propose to use these features to guide model training in supervised and hybrid self-supervised/supervised regime on various downstream tasks. We highlight a number of advantages of this strategy on challenging image assessment problems in radiography, CT and MR: 1) Significant increase in accuracy compared to the state-of-the-art (e.g., AUC boost of 3-7% for detection of abnormalities from chest radiography scans and hemorrhage detection on brain CT); 2) Acceleration of model convergence during training by up to 85% compared to using no pretraining (e.g., 83% when training a model for detection of brain metastases in MR scans); 3) Increase in robustness to various image augmentations, such as intensity variations, rotations or scaling reflective of data variation seen in the field. △ Less","4 January, 2022",https://arxiv.org/pdf/2201.01283
McXai: Local model-agnostic explanation as two games,Yiran Huang;Nicole Schaal;Michael Hefenbrock;Yexu Zhou;Till Riedel;Likun Fang;Michael Beigl,"To this day, a variety of approaches for providing local interpretability of black-box machine learning models have been introduced. Unfortunately, all of these methods suffer from one or more of the following deficiencies: They are either difficult to understand themselves, they work on a per-feature basis and ignore the dependencies between features and/or they only focus on those features asserting the decision made by the model. To address these points, this work introduces a reinforcement learning-based approach called Monte Carlo tree search for eXplainable Artificial Intelligent (McXai) to explain the decisions of any black-box classification model (classifier). Our method leverages Monte Carlo tree search and models the process of generating explanations as two games. In one game, the reward is maximized by finding feature sets that support the decision of the classifier, while in the second game, finding feature sets leading to alternative decisions maximizes the reward. The result is a human friendly representation as a tree structure, in which each node represents a set of features to be studied with smaller explanations at the top of the tree. Our experiments show, that the features found by our method are more informative with respect to classifications than those found by classical approaches like LIME and SHAP. Furthermore, by also identifying misleading features, our approach is able to guide towards improved robustness of the black-box model in many situations. △ Less","7 March, 2022",https://arxiv.org/pdf/2201.01044
AI visualization in Nanoscale Microscopy,Rajagopal A;Nirmala V;Andrew J;Arun Muthuraj Vedamanickam.,"Artificial Intelligence & Nanotechnology are promising areas for the future of humanity. While Deep Learning based Computer Vision has found applications in many fields from medicine to automotive, its application in nanotechnology can open doors for new scientific discoveries. Can we apply AI to explore objects that our eyes can't see such as nano scale sized objects? An AI platform to visualize nanoscale patterns learnt by a Deep Learning neural network can open new frontiers for nanotechnology. The objective of this paper is to develop a Deep Learning based visualization system on images of nanomaterials obtained by scanning electron microscope. This paper contributes an AI platform to enable any nanoscience researcher to use AI in visual exploration of nanoscale morphologies of nanomaterials. This AI is developed by a technique of visualizing intermediate activations of a Convolutional AutoEncoder. In this method, a nano scale specimen image is transformed into its feature representations by a Convolution Neural Network. The Convolutional AutoEncoder is trained on 100% SEM dataset, and then CNN visualization is applied. This AI generates various conceptual feature representations of the nanomaterial. While Deep Learning based image classification of SEM images are widely published in literature, there are not much publications that have visualized Deep neural networks of nanomaterials. There is a significant opportunity to gain insights from the learnings extracted by machine learning. This paper unlocks the potential of applying Deep Learning based Visualization on electron microscopy to offer AI extracted features and architectural patterns of various nanomaterials. This is a contribution in Explainable AI in nano scale objects. This paper contributes an open source AI with reproducible results at URL (https://sites.google.com/view/aifornanotechnology) △ Less","3 January, 2022",https://arxiv.org/pdf/2201.00966
An Adversarial Benchmark for Fake News Detection Models,Lorenzo Jaime Yu Flores;Yiding Hao,"With the proliferation of online misinformation, fake news detection has gained importance in the artificial intelligence community. In this paper, we propose an adversarial benchmark that tests the ability of fake news detectors to reason about real-world facts. We formulate adversarial attacks that target three aspects of ""understanding"": compositional semantics, lexical relations, and sensitivity to modifiers. We test our benchmark using BERT classifiers fine-tuned on the LIAR arXiv:arch-ive/1705648 and Kaggle Fake-News datasets, and show that both models fail to respond to changes in compositional and lexical meaning. Our results strengthen the need for such models to be used in conjunction with other fact checking methods. △ Less","3 January, 2022",https://arxiv.org/pdf/2201.00912
Rice Diseases Detection and Classification Using Attention Based Neural Network and Bayesian Optimization,Yibin Wang;Haifeng Wang;Zhaohua Peng,"In this research, an attention-based depthwise separable neural network with Bayesian optimization (ADSNN-BO) is proposed to detect and classify rice disease from rice leaf images. Rice diseases frequently result in 20 to 40 \% corp production loss in yield and is highly related to the global economy. Rapid disease identification is critical to plan treatment promptly and reduce the corp losses. Rice disease diagnosis is still mainly performed manually. To achieve AI assisted rapid and accurate disease detection, we proposed the ADSNN-BO model based on MobileNet structure and augmented attention mechanism. Moreover, Bayesian optimization method is applied to tune hyper-parameters of the model. Cross-validated classification experiments are conducted based on a public rice disease dataset with four categories in total. The experimental results demonstrate that our mobile compatible ADSNN-BO model achieves a test accuracy of 94.65\%, which outperforms all of the state-of-the-art models tested. To check the interpretability of our proposed model, feature analysis including activation map and filters visualization approach are also conducted. Results show that our proposed attention-based mechanism can more effectively guide the ADSNN-BO model to learn informative features. The outcome of this research will promote the implementation of artificial intelligence for fast plant disease diagnosis and control in the agricultural field. △ Less","3 January, 2022",https://arxiv.org/pdf/2201.00893
"AI & Racial Equity: Understanding Sentiment Analysis Artificial Intelligence, Data Security, and Systemic Theory in Criminal Justice Systems",Alia Abbas,"Various forms of implications of artificial intelligence that either exacerbate or decrease racial systemic injustice have been explored in this applied research endeavor. Taking each thematic area of identifying, analyzing, and debating an systemic issue have been leveraged in investigating merits and drawbacks of using algorithms to automate human decision making in racially sensitive environments. It has been asserted through the analysis of historical systemic patterns, implicit biases, existing algorithmic risks, and legal implications that natural language processing based AI, such as risk assessment tools, have racially disparate outcomes. It is concluded that more litigative policies are needed to regulate and restrict how internal government institutions and corporations utilize algorithms, privacy and security risks, and auditing requirements in order to diverge from racially injustice outcomes and practices of the past. △ Less","3 January, 2022",https://arxiv.org/pdf/2201.00855
Enabling Verification of Deep Neural Networks in Perception Tasks Using Fuzzy Logic and Concept Embeddings,Gesina Schwalbe;Christian Wirth;Ute Schmid,"One major drawback of deep convolutional neural networks (CNNs) for use in safety critical applications is their black-box nature. This makes it hard to verify or monitor complex, symbolic requirements on already trained computer vision CNNs. In this work, we present a simple, yet effective, approach to verify that a CNN complies with symbolic predicate logic rules which relate visual concepts. It is the first that (1) does not modify the CNN, (2) may use visual concepts that are no CNN in- or output feature, and (3) can leverage continuous CNN confidence outputs. To achieve this, we newly combine methods from explainable artificial intelligence and logic: First, using supervised concept embedding analysis, the output of a CNN is post-hoc enriched by concept outputs. Second, rules from prior knowledge are modelled as truth functions that accept the CNN outputs, and can be evaluated with little computational overhead. We here investigate the use of fuzzy logic, i.e., continuous truth values, and of proper output calibration, which both theoretically and practically show slight benefits. Applicability is demonstrated on state-of-the-art object detectors for three verification use-cases, where monitoring of rule breaches can reveal detection errors. △ Less","13 March, 2022",https://arxiv.org/pdf/2201.00572
Integrating Artificial Intelligence and Augmented Reality in Robotic Surgery: An Initial dVRK Study Using a Surgical Education Scenario,Yonghao Long;Jianfeng Cao;Anton Deguet;Russell H. Taylor;Qi Dou,"Robot-assisted surgery has become progressively more and more popular due to its clinical advantages. In the meanwhile, the artificial intelligence and augmented reality in robotic surgery are developing rapidly and receive lots of attention. However, current methods have not discussed the coherent integration of AI and AR in robotic surgery. In this paper, we develop a novel system by seamlessly merging artificial intelligence module and augmented reality visualization to automatically generate the surgical guidance for robotic surgery education. Specifically, we first leverage reinforcement leaning to learn from expert demonstration and then generate 3D guidance trajectory, providing prior context information of the surgical procedure. Along with other information such as text hint, the 3D trajectory is then overlaid in the stereo view of dVRK, where the user can perceive the 3D guidance and learn the procedure. The proposed system is evaluated through a preliminary experiment on surgical education task peg-transfer, which proves its feasibility and potential as the next generation of robot-assisted surgery education solution. △ Less","3 March, 2022",https://arxiv.org/pdf/2201.00383
Applications of Gaussian Mutation for Self Adaptation in Evolutionary Genetic Algorithms,Okezue Bell,"In recent years, optimization problems have become increasingly more prevalent due to the need for more powerful computational methods. With the more recent advent of technology such as artificial intelligence, new metaheuristics are needed that enhance the capabilities of classical algorithms. More recently, researchers have been looking at Charles Darwin's theory of natural selection and evolution as a means of enhancing current approaches using machine learning. In 1960, the first genetic algorithm was developed by John H. Holland and his student. We explore the mathematical intuition of the genetic algorithm in developing systems capable of evolving using Gaussian mutation, as well as its implications in solving optimization problems. △ Less","5 January, 2022",https://arxiv.org/pdf/2201.00285
IoT-based Route Recommendation for an Intelligent Waste Management System,Mohammadhossein Ghahramani;Mengchu Zhou;Anna Molter;Francesco Pilla,"The Internet of Things (IoT) is a paradigm characterized by a network of embedded sensors and services. These sensors are incorporated to collect various information, track physical conditions, e.g., waste bins' status, and exchange data with different centralized platforms. The need for such sensors is increasing; however, proliferation of technologies comes with various challenges. For example, how can IoT and its associated data be used to enhance waste management? In smart cities, an efficient waste management system is crucial. Artificial Intelligence (AI) and IoT-enabled approaches can empower cities to manage the waste collection. This work proposes an intelligent approach to route recommendation in an IoT-enabled waste management system given spatial constraints. It performs a thorough analysis based on AI-based methods and compares their corresponding results. Our solution is based on a multiple-level decision-making process in which bins' status and coordinates are taken into account to address the routing problem. Such AI-based models can help engineers design a sustainable infrastructure system. △ Less","1 January, 2022",https://arxiv.org/pdf/2201.00180
TransLog: A Unified Transformer-based Framework for Log Anomaly Detection,Hongcheng Guo;Xingyu Lin;Jian Yang;Yi Zhuang;Jiaqi Bai;Tieqiao Zheng;Bo Zhang;Zhoujun Li,"Log anomaly detection is a key component in the field of artificial intelligence for IT operations (AIOps). Considering log data of variant domains, retraining the whole network for unknown domains is inefficient in real industrial scenarios especially for low-resource domains. However, previous deep models merely focused on extracting the semantics of log sequence in the same domain, leading to poor generalization on multi-domain logs. Therefore, we propose a unified Transformer-based framework for log anomaly detection (\ourmethod{}), which is comprised of the pretraining and adapter-based tuning stage. Our model is first pretrained on the source domain to obtain shared semantic knowledge of log data. Then, we transfer the pretrained model to the target domain via the adapter-based tuning. The proposed method is evaluated on three public datasets including one source domain and two target domains. The experimental results demonstrate that our simple yet efficient approach, with fewer trainable parameters and lower training costs in the target domain, achieves state-of-the-art performance on three benchmarks. △ Less","16 January, 2022",https://arxiv.org/pdf/2201.00016
A Research Agenda for AI Planning in the Field of Flexible Production Systems,Aljosha Köcher;Rene Heesch;Niklas Widulle;Anna Nordhausen;Julian Putzke;Alexander Windmann;Oliver Niggemann,"Manufacturing companies face challenges when it comes to quickly adapting their production control to fluctuating demands or changing requirements. Control approaches that encapsulate production functions as services have shown to be promising in order to increase the flexibility of Cyber-Physical Production Systems. But an existing challenge of such approaches is finding a production plan based on provided functionalities for a demanded product, especially when there is no direct (i.e., syntactic) match between demanded and provided functions. While there is a variety of approaches to production planning, flexible production poses specific requirements that are not covered by existing research. In this contribution, we first capture these requirements for flexible production environments. Afterwards, an overview of current Artificial Intelligence approaches that can be utilized in order to overcome the aforementioned challenges is given. For this purpose, we focus on planning algorithms, but also consider models of production systems that can act as inputs to these algorithms. Approaches from both symbolic AI planning as well as approaches based on Machine Learning are discussed and eventually compared against the requirements. Based on this comparison, a research agenda is derived. △ Less","24 July, 2022",https://arxiv.org/pdf/2112.15484
Advanced Drone Swarm Security by Using Blockchain Governance Game,Song-Kyoo Kim,"This research contributes to the security design of an advanced smart drone swarm network based on a variant of the Blockchain Governance Game (BGG), which is the theoretical game model to predict the moments of security actions before attacks, and the Strategic Alliance for Blockchain Governance Game (SABGG), which is one of the BGG variants which has been adapted to construct the best strategies to take preliminary actions based on strategic alliance for protecting smart drones in a blockchain-based swarm network. Smart drones are artificial intelligence (AI)-enabled drones which are capable of being operated autonomously without having any command center. Analytically tractable solutions from the SABGG allow us to estimate the moments of taking preliminary actions by delivering the optimal accountability of drones for preventing attacks. This advanced secured swarm network within AI-enabled drones is designed by adapting the SABGG model. This research helps users to develop a new network-architecture-level security of a smart drone swarm which is based on a decentralized network. △ Less","23 November, 2022",https://arxiv.org/pdf/2112.15454
Making AI 'Smart': Bridging AI and Cognitive Science,Madhav Agarwal;Siddhant Bansal,"The last two decades have seen tremendous advances in Artificial Intelligence. The exponential growth in terms of computation capabilities has given us hope of developing humans like robots. The question is: are we there yet? Maybe not. With the integration of cognitive science, the 'artificial' characteristic of Artificial Intelligence (AI) might soon be replaced with 'smart'. This will help develop more powerful AI systems and simultaneously gives us a better understanding of how the human brain works. We discuss the various possibilities and challenges of bridging these two fields and how they can benefit each other. We argue that the possibility of AI taking over human civilization is low as developing such an advanced system requires a better understanding of the human brain first. △ Less","1 February, 2022",https://arxiv.org/pdf/2112.15360
Development of a face mask detection pipeline for mask-wearing monitoring in the era of the COVID-19 pandemic: A modular approach,Benjaphan Sommana;Ukrit Watchareeruetai;Ankush Ganguly;Samuel W. F. Earp;Taya Kitiyakara;Suparee Boonmanunt;Ratchainant Thammasudjarit,"During the SARS-Cov-2 pandemic, mask-wearing became an effective tool to prevent spreading and contracting the virus. The ability to monitor the mask-wearing rate in the population would be useful for determining public health strategies against the virus. However, artificial intelligence technologies for detecting face masks have not been deployed at a large scale in real-life to measure the mask-wearing rate in public. In this paper, we present a two-step face mask detection approach consisting of two separate modules: 1) face detection and alignment and 2) face mask classification. This approach allowed us to experiment with different combinations of face detection and face mask classification modules. More specifically, we experimented with PyramidKey and RetinaFace as face detectors while maintaining a lightweight backbone for the face mask classification module. Moreover, we also provide a relabeled annotation of the test set of the AIZOO dataset, where we rectified the incorrect labels for some face images. The evaluation results on the AIZOO and Moxa 3K datasets showed that the proposed face mask detection pipeline surpassed the state-of-the-art methods. The proposed pipeline also yielded a higher mAP on the relabeled test set of the AIZOO dataset than the original test set. Since we trained the proposed model using in-the-wild face images, we can successfully deploy our model to monitor the mask-wearing rate using public CCTV images. △ Less","1 August, 2022",https://arxiv.org/pdf/2112.15031
Towards a Shapley Value Graph Framework for Medical peer-influence,Jamie Duell;Monika Seisenberger;Gert Aarts;Shangming Zhou;Xiuyi Fan,"eXplainable Artificial Intelligence (XAI) is a sub-field of Artificial Intelligence (AI) that is at the forefront of AI research. In XAI, feature attribution methods produce explanations in the form of feature importance. People often use feature importance as guidance for intervention. However, a limitation of existing feature attribution methods is that there is a lack of explanation towards the consequence of intervention. In other words, although contribution towards a certain prediction is highlighted by feature attribution methods, the relation between features and the consequence of intervention is not studied. The aim of this paper is to introduce a new framework, called a peer influence framework to look deeper into explanations using graph representation for feature-to-feature interactions to improve the interpretability of black-box Machine Learning models and inform intervention. △ Less","8 February, 2022",https://arxiv.org/pdf/2112.14624
Explainability Is in the Mind of the Beholder: Establishing the Foundations of Explainable Artificial Intelligence,Kacper Sokol;Peter Flach,"Explainable artificial intelligence and interpretable machine learning are research domains growing in importance. Yet, the underlying concepts remain somewhat elusive and lack generally agreed definitions. While recent inspiration from social sciences has refocused the work on needs and expectations of human recipients, the field still misses a concrete conceptualisation. We take steps towards addressing this challenge by reviewing the philosophical and social foundations of human explainability, which we then translate into the technological realm. In particular, we scrutinise the notion of algorithmic black boxes and the spectrum of understanding determined by explanatory processes and explainees' background knowledge. This approach allows us to define explainability as (logical) reasoning applied to transparent insights (into, possibly black-box, predictive systems) interpreted under background knowledge and placed within a specific context -- a process that engenders understanding in a selected group of explainees. We then employ this conceptualisation to revisit strategies for evaluating explainability as well as the much disputed trade-off between transparency and predictive power, including its implications for ante-hoc and post-hoc techniques along with fairness and accountability established by explainability. We furthermore discuss components of the machine learning workflow that may be in need of interpretability, building on a range of ideas from human-centred explainability, with a particular focus on explainees, contrastive statements and explanatory processes. Our discussion reconciles and complements current research to help better navigate open questions -- rather than attempting to address any individual issue -- thus laying a solid foundation for a grounded discussion and future progress of explainable artificial intelligence and interpretable machine learning. △ Less","8 September, 2022",https://arxiv.org/pdf/2112.14466
"N-Omniglot, a large-scale neuromorphic dataset for spatio-temporal sparse few-shot learning",Yang Li;Yiting Dong;Dongcheng Zhao;Yi Zeng,"Few-shot learning (learning with a few samples) is one of the most important cognitive abilities of the human brain. However, the current artificial intelligence systems meet difficulties in achieving this ability. Similar challenges also exist for biologically plausible spiking neural networks (SNNs). Datasets for traditional few-shot learning domains provide few amounts of temporal information. and the absence of neuromorphic datasets has hindered the development of few-shot learning for SNNs. Here, to the best of our knowledge, we provide the first neuromorphic dataset for few-shot learning using SNNs: N-Omniglot, based on the Dynamic Vision Sensor. It contains 1,623 categories of handwritten characters, with only 20 samples per class. N-Omniglot eliminates the need for a neuromorphic dataset for SNNs with high spareness and tremendous temporal coherence. Additionally, the dataset provides a powerful challenge and a suitable benchmark for developing SNNs algorithms in the few-shot learning domain due to the chronological information of strokes. We also provide the improved nearest neighbor, convolutional network, SiameseNet, and meta-learning algorithm in the spiking version for verification. △ Less","3 December, 2022",https://arxiv.org/pdf/2112.13230
Continual Learning for Unsupervised Anomaly Detection in Continuous Auditing of Financial Accounting Data,Hamed Hemati;Marco Schreyer;Damian Borth,"International audit standards require the direct assessment of a financial statement's underlying accounting journal entries. Driven by advances in artificial intelligence, deep-learning inspired audit techniques emerged to examine vast quantities of journal entry data. However, in regular audits, most of the proposed methods are applied to learn from a comparably stationary journal entry population, e.g., of a financial quarter or year. Ignoring situations where audit relevant distribution changes are not evident in the training data or become incrementally available over time. In contrast, in continuous auditing, deep-learning models are continually trained on a stream of recorded journal entries, e.g., of the last hour. Resulting in situations where previous knowledge interferes with new information and will be entirely overwritten. This work proposes a continual anomaly detection framework to overcome both challenges and designed to learn from a stream of journal entry data experiences. The framework is evaluated based on deliberately designed audit scenarios and two real-world datasets. Our experimental results provide initial evidence that such a learning scheme offers the ability to reduce false-positive alerts and false-negative decisions. △ Less","31 March, 2022",https://arxiv.org/pdf/2112.13215
Toward a New Science of Common Sense,Ronald J. Brachman;Hector J. Levesque,"Common sense has always been of interest in Artificial Intelligence, but has rarely taken center stage. Despite its mention in one of John McCarthy's earliest papers and years of work by dedicated researchers, arguably no AI system with a serious amount of general common sense has ever emerged. Why is that? What's missing? Examples of AI systems' failures of common sense abound, and they point to AI's frequent focus on expertise as the cause. Those attempting to break the resulting brittleness barrier, even in the context of modern deep learning, have tended to invest their energy in large numbers of small bits of commonsense knowledge. While important, all the commonsense knowledge fragments in the world don't add up to a system that actually demonstrates common sense in a human-like way. We advocate examining common sense from a broader perspective than in the past. Common sense should be considered in the context of a full cognitive system with history, goals, desires, and drives, not just in isolated circumscribed examples. A fresh look is needed: common sense is worthy of its own dedicated scientific exploration. △ Less","6 February, 2022",https://arxiv.org/pdf/2112.12754
Explainable Artificial Intelligence Methods in Combating Pandemics: A Systematic Review,Felipe Giuste;Wenqi Shi;Yuanda Zhu;Tarun Naren;Monica Isgut;Ying Sha;Li Tong;Mitali Gupte;May D. Wang,"Despite the myriad peer-reviewed papers demonstrating novel Artificial Intelligence (AI)-based solutions to COVID-19 challenges during the pandemic, few have made significant clinical impact. The impact of artificial intelligence during the COVID-19 pandemic was greatly limited by lack of model transparency. This systematic review examines the use of Explainable Artificial Intelligence (XAI) during the pandemic and how its use could overcome barriers to real-world success. We find that successful use of XAI can improve model performance, instill trust in the end-user, and provide the value needed to affect user decision-making. We introduce the reader to common XAI techniques, their utility, and specific examples of their application. Evaluation of XAI results is also discussed as an important step to maximize the value of AI-based clinical decision support systems. We illustrate the classical, modern, and potential future trends of XAI to elucidate the evolution of novel XAI techniques. Finally, we provide a checklist of suggestions during the experimental design process supported by recent publications. Common challenges during the implementation of AI solutions are also addressed with specific examples of potential solutions. We hope this review may serve as a guide to improve the clinical impact of future AI-based solutions. △ Less","22 June, 2022",https://arxiv.org/pdf/2112.12705
SOLIS -- The MLOps journey from data acquisition to actionable insights,Razvan Ciobanu;Alexandru Purdila;Laurentiu Piciu;Andrei Damian,"Machine Learning operations is unarguably a very important and also one of the hottest topics in Artificial Intelligence lately. Being able to define very clear hypotheses for actual real-life problems that can be addressed by machine learning models, collecting and curating large amounts of data for model training and validation followed by model architecture search and actual optimization and finally presenting the results fits very well the scenario of Data Science experiments. This approach however does not supply the needed procedures and pipelines for the actual deployment of machine learning capabilities in real production grade systems. Automating live configuration mechanisms, on the fly adapting to live or offline data capture and consumption, serving multiple models in parallel either on edge or cloud architectures, addressing specific limitations of GPU memory or compute power, post-processing inference or prediction results and serving those either as APIs or with IoT based communication stacks in the same end-to-end pipeline are the real challenges that we try to address in this particular paper. In this paper we present a unified deployment pipeline and freedom-to-operate approach that supports all above requirements while using basic cross-platform tensor framework and script language engines. △ Less","28 January, 2022",https://arxiv.org/pdf/2112.11925
A Survey of Natural Language Generation,Chenhe Dong;Yinghui Li;Haifan Gong;Miaoxin Chen;Junxin Li;Ying Shen;Min Yang,"This paper offers a comprehensive review of the research on Natural Language Generation (NLG) over the past two decades, especially in relation to data-to-text generation and text-to-text generation deep learning methods, as well as new applications of NLG technology. This survey aims to (a) give the latest synthesis of deep learning research on the NLG core tasks, as well as the architectures adopted in the field; (b) detail meticulously and comprehensively various NLG tasks and datasets, and draw attention to the challenges in NLG evaluation, focusing on different evaluation methods and their relationships; (c) highlight some future emphasis and relatively recent research issues that arise due to the increasing synergy between NLG and other artificial intelligence areas, such as computer vision, text and computational creativity. △ Less","2 August, 2022",https://arxiv.org/pdf/2112.11739
Information Field Theory and Artificial Intelligence,Torsten Enßlin,"Information field theory (IFT), the information theory for fields, is a mathematical framework for signal reconstruction and non-parametric inverse problems. Artificial intelligence (AI) and machine learning (ML) aim at generating intelligent systems including such for perception, cognition, and learning. This overlaps with IFT, which is designed to address perception, reasoning, and inference tasks. Here, the relation between concepts and tools in IFT and those in AI and ML research are discussed. In the context of IFT, fields denote physical quantities that change continuously as a function of space (and time) and information theory refers to Bayesian probabilistic logic equipped with the associated entropic information measures. Reconstructing a signal with IFT is a computational problem similar to training a generative neural network (GNN) in ML. In this paper, the process of inference in IFT is reformulated in terms of GNN training. In contrast to classical neural networks, IFT based GNNs can operate without pre-training thanks to incorporating expert knowledge into their architecture. Furthermore, the cross-fertilization of variational inference methods used in IFT and ML are discussed. These discussions suggests that IFT is well suited to address many problems in AI and ML research and application. △ Less","7 March, 2022",https://arxiv.org/pdf/2112.10133
Interpretable Data-Based Explanations for Fairness Debugging,Romila Pradhan;Jiongli Zhu;Boris Glavic;Babak Salimi,"A wide variety of fairness metrics and eXplainable Artificial Intelligence (XAI) approaches have been proposed in the literature to identify bias in machine learning models that are used in critical real-life contexts. However, merely reporting on a model's bias, or generating explanations using existing XAI techniques is insufficient to locate and eventually mitigate sources of bias. We introduce Gopher, a system that produces compact, interpretable and causal explanations for bias or unexpected model behavior by identifying coherent subsets of the training data that are root-causes for this behavior. Specifically, we introduce the concept of causal responsibility that quantifies the extent to which intervening on training data by removing or updating subsets of it can resolve the bias. Building on this concept, we develop an efficient approach for generating the top-k patterns that explain model bias that utilizes techniques from the machine learning (ML) community to approximate causal responsibility and uses pruning rules to manage the large search space for patterns. Our experimental evaluation demonstrates the effectiveness of Gopher in generating interpretable explanations for identifying and debugging sources of bias. △ Less","9 April, 2022",https://arxiv.org/pdf/2112.09745
Implementation of a Binary Neural Network on a Passive Array of Magnetic Tunnel Junctions,Jonathan M. Goodwill;Nitin Prasad;Brian D. Hoskins;Matthew W. Daniels;Advait Madhavan;Lei Wan;Tiffany S. Santos;Michael Tran;Jordan A. Katine;Patrick M. Braganca;Mark D. Stiles;Jabez J. McClelland,"The increasing scale of neural networks and their growing application space have produced demand for more energy- and memory-efficient artificial-intelligence-specific hardware. Avenues to mitigate the main issue, the von Neumann bottleneck, include in-memory and near-memory architectures, as well as algorithmic approaches. Here we leverage the low-power and the inherently binary operation of magnetic tunnel junctions (MTJs) to demonstrate neural network hardware inference based on passive arrays of MTJs. In general, transferring a trained network model to hardware for inference is confronted by degradation in performance due to device-to-device variations, write errors, parasitic resistance, and nonidealities in the substrate. To quantify the effect of these hardware realities, we benchmark 300 unique weight matrix solutions of a 2-layer perceptron to classify the Wine dataset for both classification accuracy and write fidelity. Despite device imperfections, we achieve software-equivalent accuracy of up to 95.3 % with proper tuning of network parameters in 15 x 15 MTJ arrays having a range of device sizes. The success of this tuning process shows that new metrics are needed to characterize the performance and quality of networks reproduced in mixed signal hardware. △ Less","6 May, 2022",https://arxiv.org/pdf/2112.09159
ADEPT: Automatic Differentiable DEsign of Photonic Tensor Cores,Jiaqi Gu;Hanqing Zhu;Chenghao Feng;Zixuan Jiang;Mingjie Liu;Shuhan Zhang;Ray T. Chen;David Z. Pan,"Photonic tensor cores (PTCs) are essential building blocks for optical artificial intelligence (AI) accelerators based on programmable photonic integrated circuits. PTCs can achieve ultra-fast and efficient tensor operations for neural network (NN) acceleration. Current PTC designs are either manually constructed or based on matrix decomposition theory, which lacks the adaptability to meet various hardware constraints and device specifications. To our best knowledge, automatic PTC design methodology is still unexplored. It will be promising to move beyond the manual design paradigm and ""nurture"" photonic neurocomputing with AI and design automation. Therefore, in this work, for the first time, we propose a fully differentiable framework, dubbed ADEPT, that can efficiently search PTC designs adaptive to various circuit footprint constraints and foundry PDKs. Extensive experiments show superior flexibility and effectiveness of the proposed ADEPT framework to explore a large PTC design space. On various NN models and benchmarks, our searched PTC topology outperforms prior manually-designed structures with competitive matrix representability, 2-30x higher footprint compactness, and better noise robustness, demonstrating a new paradigm in photonic neural chip design. The code of ADEPT is available at https://github.com/JeremieMelo/ADEPT using the https://github.com/JeremieMelo/pytorch-onn (TorchONN) library. △ Less","3 May, 2022",https://arxiv.org/pdf/2112.08703
3D Question Answering,Shuquan Ye;Dongdong Chen;Songfang Han;Jing Liao,"Visual Question Answering (VQA) has witnessed tremendous progress in recent years. However, most efforts only focus on the 2D image question answering tasks. In this paper, we present the first attempt at extending VQA to the 3D domain, which can facilitate artificial intelligence's perception of 3D real-world scenarios. Different from image based VQA, 3D Question Answering (3DQA) takes the color point cloud as input and requires both appearance and 3D geometry comprehension ability to answer the 3D-related questions. To this end, we propose a novel transformer-based 3DQA framework ""3DQA-TR"", which consists of two encoders for exploiting the appearance and geometry information, respectively. The multi-modal information of appearance, geometry, and the linguistic question can finally attend to each other via a 3D-Linguistic Bert to predict the target answers. To verify the effectiveness of our proposed 3DQA framework, we further develop the first 3DQA dataset ""ScanQA"", which builds on the ScanNet dataset and contains \sim6K questions, \sim30K answers for 806 scenes. Extensive experiments on this dataset demonstrate the obvious superiority of our proposed 3DQA framework over existing VQA frameworks, and the effectiveness of our major designs. Our code and dataset will be made publicly available to facilitate the research in this direction. △ Less","28 November, 2022",https://arxiv.org/pdf/2112.08359
Generative Adversarial Networks for Data Generation in Structural Health Monitoring,Furkan Luleci;F. Necati Catbas;Onur Avci,"Structural Health Monitoring (SHM) has been continuously benefiting from the advancements in the field of data science. Various types of Artificial Intelligence (AI) methods have been utilized for the assessment and evaluation of civil structures. In AI, Machine Learning (ML) and Deep Learning (DL) algorithms require plenty of datasets to train; particularly, the more data DL models are trained with, the better output it yields. Yet, in SHM applications, collecting data from civil structures through sensors is expensive and obtaining useful data (damage associated data) is challenging. In this paper, 1-D Wasserstein loss Deep Convolutional Generative Adversarial Networks using Gradient Penalty (1-D WDCGAN-GP) is utilized to generate damage associated vibration datasets that are similar to the input. For the purpose of vibration-based damage diagnostics, a 1-D Deep Convolutional Neural Network (1-D DCNN) is built, trained, and tested on both real and generated datasets. The classification results from the 1-D DCNN on both datasets resulted to be very similar to each other. The presented work in this paper shows that for the cases of insufficient data in DL or ML-based damage diagnostics, 1-D WDCGAN-GP can successfully generate data for the model to be trained on. Keywords: 1-D Generative Adversarial Networks (GAN), Deep Convolutional Generative Adversarial Networks (DCGAN), Wasserstein Generative Adversarial Networks with Gradient Penalty (WGAN-GP), 1-D Convolutional Neural Networks (CNN), Structural Health Monitoring (SHM), Structural Damage Diagnostics, Structural Damage Detection △ Less","3 March, 2022",https://arxiv.org/pdf/2112.08196
Does a Face Mask Protect my Privacy?: Deep Learning to Predict Protected Attributes from Masked Face Images,Sachith Seneviratne;Nuran Kasthuriarachchi;Sanka Rasnayaka;Danula Hettiachchi;Ridwan Shariffdeen,"Contactless and efficient systems are implemented rapidly to advocate preventive methods in the fight against the COVID-19 pandemic. Despite the positive benefits of such systems, there is potential for exploitation by invading user privacy. In this work, we analyse the privacy invasiveness of face biometric systems by predicting privacy-sensitive soft-biometrics using masked face images. We train and apply a CNN based on the ResNet-50 architecture with 20,003 synthetic masked images and measure the privacy invasiveness. Despite the popular belief of the privacy benefits of wearing a mask among people, we show that there is no significant difference to privacy invasiveness when a mask is worn. In our experiments we were able to accurately predict sex (94.7%),race (83.1%) and age (MAE 6.21 and RMSE 8.33) from masked face images. Our proposed approach can serve as a baseline utility to evaluate the privacy-invasiveness of artificial intelligence systems that make use of privacy-sensitive information. We open-source all contributions for re-producibility and broader use by the research community. △ Less","26 January, 2022",https://arxiv.org/pdf/2112.07879
Assessing Human Interaction in Virtual Reality With Continually Learning Prediction Agents Based on Reinforcement Learning Algorithms: A Pilot Study,Dylan J. A. Brenneis;Adam S. Parker;Michael Bradley Johanson;Andrew Butcher;Elnaz Davoodi;Leslie Acker;Matthew M. Botvinick;Joseph Modayil;Adam White;Patrick M. Pilarski,"Artificial intelligence systems increasingly involve continual learning to enable flexibility in general situations that are not encountered during system training. Human interaction with autonomous systems is broadly studied, but research has hitherto under-explored interactions that occur while the system is actively learning, and can noticeably change its behaviour in minutes. In this pilot study, we investigate how the interaction between a human and a continually learning prediction agent develops as the agent develops competency. Additionally, we compare two different agent architectures to assess how representational choices in agent design affect the human-agent interaction. We develop a virtual reality environment and a time-based prediction task wherein learned predictions from a reinforcement learning (RL) algorithm augment human predictions. We assess how a participant's performance and behaviour in this task differs across agent types, using both quantitative and qualitative analyses. Our findings suggest that human trust of the system may be influenced by early interactions with the agent, and that trust in turn affects strategic behaviour, but limitations of the pilot study rule out any conclusive statement. We identify trust as a key feature of interaction to focus on when considering RL-based technologies, and make several recommendations for modification to this study in preparation for a larger-scale investigation. A video summary of this paper can be found at https://youtu.be/oVYJdnBqTwQ . △ Less","22 April, 2022",https://arxiv.org/pdf/2112.07774
"AI and extreme scale computing to learn and infer the physics of higher order gravitational wave modes of quasi-circular, spinning, non-precessing binary black hole mergers",Asad Khan;E. A. Huerta;Prayush Kumar,"We use artificial intelligence (AI) to learn and infer the physics of higher order gravitational wave modes of quasi-circular, spinning, non precessing binary black hole mergers. We trained AI models using 14 million waveforms, produced with the surrogate model NRHybSur3dq8, that include modes up to \ell \leq 4 and (5,5), except for (4,0) and (4,1), that describe binaries with mass-ratios q\leq8, individual spins s^z_{\{1,2\}}\in[-0.8, 0.8], and inclination angle θ\in[0,π].Our probabilistic AI surrogates can accurately constrain the mass-ratio, individual spins, effective spin, and inclination angle of numerical relativity waveforms that describe such signal manifold. We compared the predictions of our AI models with Gaussian process regression, random forest, k-nearest neighbors, and linear regression, and with traditional Bayesian inference methods through the PyCBC Inference toolkit, finding that AI outperforms all these approaches in terms of accuracy, and are between three to four orders of magnitude faster than traditional Bayesian inference methods. Our AI surrogates were trained within 3.4 hours using distributed training on 1,536 NVIDIA V100 GPUs in the Summit supercomputer. △ Less","26 October, 2022",https://arxiv.org/pdf/2112.07669
Large Language Models are not Models of Natural Language: they are Corpus Models,Csaba Veres,"Natural Language Processing (NLP) has become one of the leading application areas in the current Artificial Intelligence boom. Transfer learning has enabled large deep learning neural networks trained on the language modeling task to vastly improve performance in almost all downstream language tasks. Interestingly, when the language models are trained with data that includes software code, they demonstrate remarkable abilities in generating functioning computer code from natural language specifications. We argue that this creates a conundrum for the claim that eliminative neural models are a radical restructuring in our understanding of cognition in that they eliminate the need for symbolic abstractions like generative phrase structure grammars. Because the syntax of programming languages is by design determined by phrase structure grammars, neural models that produce syntactic code are apparently uninformative about the theoretical foundations of programming languages. The demonstration that neural models perform well on tasks that involve clearly symbolic systems, proves that they cannot be used as an argument that language and other cognitive systems are not symbolic. Finally, we argue as a corollary that the term language model is misleading and propose the adoption of the working term corpus model instead, which better reflects the genesis and contents of the model. △ Less","14 June, 2022",https://arxiv.org/pdf/2112.07055
Frontiers in Collective Intelligence: A Workshop Report,Tyler Millhouse;Melanie Moses;Melanie Mitchell,"In August of 2021, the Santa Fe Institute hosted a workshop on collective intelligence as part of its Foundations of Intelligence project. This project seeks to advance the field of artificial intelligence by promoting interdisciplinary research on the nature of intelligence. The workshop brought together computer scientists, biologists, philosophers, social scientists, and others to share their insights about how intelligence can emerge from interactions among multiple agents--whether those agents be machines, animals, or human beings. In this report, we summarize each of the talks and the subsequent discussions. We also draw out a number of key themes and identify important frontiers for future research. △ Less","10 October, 2022",https://arxiv.org/pdf/2112.06864
"Surfer100: Generating Surveys From Web Resources, Wikipedia-style",Irene Li;Alexander Fabbri;Rina Kawamura;Yixin Liu;Xiangru Tang;Jaesung Tae;Chang Shen;Sally Ma;Tomoe Mizutani;Dragomir Radev,"Fast-developing fields such as Artificial Intelligence (AI) often outpace the efforts of encyclopedic sources such as Wikipedia, which either do not completely cover recently-introduced topics or lack such content entirely. As a result, methods for automatically producing content are valuable tools to address this information overload. We show that recent advances in pretrained language modeling can be combined for a two-stage extractive and abstractive approach for Wikipedia lead paragraph generation. We extend this approach to generate longer Wikipedia-style summaries with sections and examine how such methods struggle in this application through detailed studies with 100 reference human-collected surveys. This is the first study on utilizing web resources for long Wikipedia-style summaries to the best of our knowledge. △ Less","22 June, 2022",https://arxiv.org/pdf/2112.06377
Marvin: an Innovative Omni-Directional Robotic Assistant for Domestic Environments,Andrea Eirale;Mauro Martini;Luigi Tagliavini;Dario Gandini;Marcello Chiaberge;Giuseppe Quaglia,"Population ageing and pandemics recently demonstrate to cause isolation of elderly people in their houses, generating the need for a reliable assistive figure. Robotic assistants are the new frontier of innovation for domestic welfare, and elderly monitoring is one of the services a robot can handle for collective well-being. Despite these emerging needs, in the actual landscape of robotic assistants there are no platform which successfully combines a reliable mobility in cluttered domestic spaces, with lightweight and offline Artificial Intelligence (AI) solutions for perception and interaction. In this work, we present Marvin, a novel assistive robotic platform we developed with a modular layer-based architecture, merging a flexible mechanical design with cutting-edge AI for perception and vocal control. We focus the design of Marvin on three target service functions: monitoring of elderly and reduced-mobility subjects, remote presence and connectivity, and night assistance. Compared to previous works, we propose a tiny omnidirectional platform, which enables agile mobility and effective obstacle avoidance. Moreover, we design a controllable positioning device, which easily allows the user to access the interface for connectivity and extends the visual range of the camera sensor. Nonetheless, we delicately consider the privacy issues arising from private data collection on cloud services, a critical aspect of commercial AI-based assistants. To this end, we demonstrate how lightweight deep learning solutions for visual perception and vocal command can be adopted, completely running offline on the embedded hardware of the robot. △ Less","14 July, 2022",https://arxiv.org/pdf/2112.05597
KGE-CL: Contrastive Learning of Tensor Decomposition Based Knowledge Graph Embeddings,Zhiping Luo;Wentao Xu;Weiqing Liu;Jiang Bian;Jian Yin;Tie-Yan Liu,"Learning the embeddings of knowledge graphs (KG) is vital in artificial intelligence, and can benefit various downstream applications, such as recommendation and question answering. In recent years, many research efforts have been proposed for knowledge graph embedding (KGE). However, most previous KGE methods ignore the semantic similarity between the related entities and entity-relation couples in different triples since they separately optimize each triple with the scoring function. To address this problem, we propose a simple yet efficient contrastive learning framework for tensor decomposition based (TDB) KGE, which can shorten the semantic distance of the related entities and entity-relation couples in different triples and thus improve the performance of KGE. We evaluate our proposed method on three standard KGE datasets: WN18RR, FB15k-237 and YAGO3-10. Our method can yield some new state-of-the-art results, achieving 51.2% MRR, 46.8% Hits@1 on the WN18RR dataset, 37.8% MRR, 28.6% Hits@1 on FB15k-237 dataset, and 59.1% MRR, 51.8% Hits@1 on the YAGO3-10 dataset. △ Less","24 October, 2022",https://arxiv.org/pdf/2112.04871
Efficient Calibration of Multi-Agent Simulation Models from Output Series with Bayesian Optimization,Yuanlu Bai;Henry Lam;Svitlana Vyetrenko;Tucker Balch,"Multi-agent simulation is commonly used across multiple disciplines, specifically in artificial intelligence in recent years, which creates an environment for downstream machine learning or reinforcement learning tasks. In many practical scenarios, however, only the output series that result from the interactions of simulation agents are observable. Therefore, simulators need to be calibrated so that the simulated output series resemble historical -- which amounts to solving a complex simulation optimization problem. In this paper, we propose a simple and efficient framework for calibrating simulator parameters from historical output series observations. First, we consider a novel concept of eligibility set to bypass the potential non-identifiability issue. Second, we generalize the two-sample Kolmogorov-Smirnov (K-S) test with Bonferroni correction to test the similarity between two high-dimensional distributions, which gives a simple yet effective distance metric between the output series sample sets. Third, we suggest using Bayesian optimization (BO) and trust-region BO (TuRBO) to minimize the aforementioned distance metric. Finally, we demonstrate the efficiency of our framework using numerical experiments both on a multi-agent financial market simulator. △ Less","20 September, 2022",https://arxiv.org/pdf/2112.03874
Generative Adversarial Networks for Labeled Acceleration Data Augmentation for Structural Damage Detection,Furkan Luleci;F. Necati Catbas;Onur Avci,"There has been a major advance in the field of Data Science in the last few decades, and these have been utilized for different engineering disciplines and applications. Artificial Intelligence (AI), Machine Learning (ML) and Deep Learning (DL) algorithms have been utilized for civil Structural Health Monitoring (SHM) especially for damage detection applications using sensor data. Although ML and DL methods show superior learning skills for complex data structures, they require plenty of data for training. However, in SHM, data collection from civil structures can be expensive and time taking; particularly getting useful data (damage associated data) can be challenging. The objective of this study is to address the data scarcity problem for damage detection applications. This paper employs 1-D Wasserstein Deep Convolutional Generative Adversarial Networks using Gradient Penalty (1-D WDCGAN-GP) for synthetic labelled acceleration data generation. Then, the generated data is augmented with varying ratios for the training dataset of a 1-D Deep Convolutional Neural Network (1-D DCNN) for damage detection application. The damage detection results show that the 1-D WDCGAN-GP can be successfully utilized to tackle data scarcity in vibration-based damage detection applications of civil structures. Keywords: Structural Health Monitoring (SHM), Structural Damage Detection, 1-D Deep Convolutional Neural Networks (1-D DCNN), 1-D Generative Adversarial Networks (1-D GAN), Wasserstein Generative Adversarial Networks with Gradient Penalty (WGAN-GP) △ Less","22 July, 2022",https://arxiv.org/pdf/2112.03478
Simulation Intelligence: Towards a New Generation of Scientific Methods,Alexander Lavin;David Krakauer;Hector Zenil;Justin Gottschlich;Tim Mattson;Johann Brehmer;Anima Anandkumar;Sanjay Choudry;Kamil Rocki;Atılım Güneş Baydin;Carina Prunkl;Brooks Paige;Olexandr Isayev;Erik Peterson;Peter L. McMahon;Jakob Macke;Kyle Cranmer;Jiaxin Zhang;Haruko Wainwright;Adi Hanuka;Manuela Veloso;Samuel Assefa;Stephan Zheng;Avi Pfeffer,"The original ""Seven Motifs"" set forth a roadmap of essential methods for the field of scientific computing, where a motif is an algorithmic method that captures a pattern of computation and data movement. We present the ""Nine Motifs of Simulation Intelligence"", a roadmap for the development and integration of the essential algorithms necessary for a merger of scientific computing, scientific simulation, and artificial intelligence. We call this merger simulation intelligence (SI), for short. We argue the motifs of simulation intelligence are interconnected and interdependent, much like the components within the layers of an operating system. Using this metaphor, we explore the nature of each layer of the simulation intelligence operating system stack (SI-stack) and the motifs therein: (1) Multi-physics and multi-scale modeling; (2) Surrogate modeling and emulation; (3) Simulation-based inference; (4) Causal modeling and inference; (5) Agent-based modeling; (6) Probabilistic programming; (7) Differentiable programming; (8) Open-ended optimization; (9) Machine programming. We believe coordinated efforts between motifs offers immense opportunity to accelerate scientific discovery, from solving inverse problems in synthetic biology and climate science, to directing nuclear energy experiments and predicting emergent behavior in socioeconomic settings. We elaborate on each layer of the SI-stack, detailing the state-of-art methods, presenting examples to highlight challenges and opportunities, and advocating for specific ways to advance the motifs and the synergies from their combinations. Advancing and integrating these technologies can enable a robust and efficient hypothesis-simulation-analysis type of scientific method, which we introduce with several use-cases for human-machine teaming and automated science. △ Less","27 November, 2022",https://arxiv.org/pdf/2112.03235
Classification of COVID-19 on chest X-Ray images using Deep Learning model with Histogram Equalization and Lungs Segmentation,Aman Swaraj;Karan Verma,"Background and Objective: Artificial intelligence (AI) methods coupled with biomedical analysis has a critical role during pandemics as it helps to release the overwhelming pressure from healthcare systems and physicians. As the ongoing COVID-19 crisis worsens in countries having dense populations and inadequate testing kits like Brazil and India, radiological imaging can act as an important diagnostic tool to accurately classify covid-19 patients and prescribe the necessary treatment in due time. With this motivation, we present our study based on deep learning architecture for detecting covid-19 infected lungs using chest X-rays. Dataset: We collected a total of 2470 images for three different class labels, namely, healthy lungs, ordinary pneumonia, and covid-19 infected pneumonia, out of which 470 X-ray images belong to the covid-19 category. Methods: We first pre-process all the images using histogram equalization techniques and segment them using U-net architecture. VGG-16 network is then used for feature extraction from the pre-processed images which is further sampled by SMOTE oversampling technique to achieve a balanced dataset. Finally, the class-balanced features are classified using a support vector machine (SVM) classifier with 10-fold cross-validation and the accuracy is evaluated. Result and Conclusion: Our novel approach combining well-known pre-processing techniques, feature extraction methods, and dataset balancing method, lead us to an outstanding rate of recognition of 98% for COVID-19 images over a dataset of 2470 X-ray images. Our model is therefore fit to be utilized in healthcare facilities for screening purposes. △ Less","11 July, 2022",https://arxiv.org/pdf/2112.02478
Meaningful human control: actionable properties for AI system development,Luciano Cavalcante Siebert;Maria Luce Lupetti;Evgeni Aizenberg;Niek Beckers;Arkady Zgonnikov;Herman Veluwenkamp;David Abbink;Elisa Giaccardi;Geert-Jan Houben;Catholijn M. Jonker;Jeroen van den Hoven;Deborah Forster;Reginald L. Lagendijk,"How can humans remain in control of artificial intelligence (AI)-based systems designed to perform tasks autonomously? Such systems are increasingly ubiquitous, creating benefits - but also undesirable situations where moral responsibility for their actions cannot be properly attributed to any particular person or group. The concept of meaningful human control has been proposed to address responsibility gaps and mitigate them by establishing conditions that enable a proper attribution of responsibility for humans; however, clear requirements for researchers, designers, and engineers are yet inexistent, making the development of AI-based systems that remain under meaningful human control challenging. In this paper, we address the gap between philosophical theory and engineering practice by identifying, through an iterative process of abductive thinking, four actionable properties for AI-based systems under meaningful human control, which we discuss making use of two applications scenarios: automated vehicles and AI-based hiring. First, a system in which humans and AI algorithms interact should have an explicitly defined domain of morally loaded situations within which the system ought to operate. Second, humans and AI agents within the system should have appropriate and mutually compatible representations. Third, responsibility attributed to a human should be commensurate with that human's ability and authority to control the system. Fourth, there should be explicit links between the actions of the AI agents and actions of humans who are aware of their moral responsibility. We argue that these four properties will support practically-minded professionals to take concrete steps toward designing and engineering for AI systems that facilitate meaningful human control. △ Less","19 May, 2022",https://arxiv.org/pdf/2112.01298
HRNET: AI on Edge for mask detection and social distancing,Kinshuk Sengupta;Praveen Ranjan Srivastava,"The purpose of the paper is to provide innovative emerging technology framework for community to combat epidemic situations. The paper proposes a unique outbreak response system framework based on artificial intelligence and edge computing for citizen centric services to help track and trace people eluding safety policies like mask detection and social distancing measure in public or workplace setup. The framework further provides implementation guideline in industrial setup as well for governance and contact tracing tasks. The adoption will thus lead in smart city planning and development focusing on citizen health systems contributing to improved quality of life. The conceptual framework presented is validated through quantitative data analysis via secondary data collection from researcher's public websites, GitHub repositories and renowned journals and further benchmarking were conducted for experimental results in Microsoft Azure cloud environment. The study includes selective AI-models for benchmark analysis and were assessed on performance and accuracy in edge computing environment for large scale societal setup. Overall YOLO model Outperforms in object detection task and is faster enough for mask detection and HRNetV2 outperform semantic segmentation problem applied to solve social distancing task in AI-Edge inferencing environmental setup. The paper proposes new Edge-AI algorithm for building technology-oriented solutions for detecting mask in human movement and social distance. The paper enriches the technological advancement in artificial intelligence and edge-computing applied to problems in society and healthcare systems. The framework further equips government agency, system providers to design and constructs technology-oriented models in community setup to Increase the quality of life using emerging technologies into smart urban environments. △ Less","3 February, 2022",https://arxiv.org/pdf/2111.15208
Collective Intelligence for Deep Learning: A Survey of Recent Developments,David Ha;Yujin Tang,"In the past decade, we have witnessed the rise of deep learning to dominate the field of artificial intelligence. Advances in artificial neural networks alongside corresponding advances in hardware accelerators with large memory capacity, together with the availability of large datasets enabled practitioners to train and deploy sophisticated neural network models that achieve state-of-the-art performance on tasks across several fields spanning computer vision, natural language processing, and reinforcement learning. However, as these neural networks become bigger, more complex, and more widely used, fundamental problems with current deep learning models become more apparent. State-of-the-art deep learning models are known to suffer from issues that range from poor robustness, inability to adapt to novel task settings, to requiring rigid and inflexible configuration assumptions. Collective behavior, commonly observed in nature, tends to produce systems that are robust, adaptable, and have less rigid assumptions about the environment configuration. Collective intelligence, as a field, studies the group intelligence that emerges from the interactions of many individuals. Within this field, ideas such as self-organization, emergent behavior, swarm optimization, and cellular automata were developed to model and explain complex systems. It is therefore natural to see these ideas incorporated into newer deep learning methods. In this review, we will provide a historical context of neural network research's involvement with complex systems, and highlight several active areas in modern deep learning research that incorporate the principles of collective intelligence to advance its current capabilities. We hope this review can serve as a bridge between the complex systems and deep learning communities. △ Less","10 March, 2022",https://arxiv.org/pdf/2111.14377
Efficient Federated Learning for AIoT Applications Using Knowledge Distillation,Tian Liu;Zhiwei Ling;Jun Xia;Xin Fu;Shui Yu;Mingsong Chen,"As a promising distributed machine learning paradigm, Federated Learning (FL) trains a central model with decentralized data without compromising user privacy, which has made it widely used by Artificial Intelligence Internet of Things (AIoT) applications. However, the traditional FL suffers from model inaccuracy since it trains local models using hard labels of data and ignores useful information of incorrect predictions with small probabilities. Although various solutions try to tackle the bottleneck of the traditional FL, most of them introduce significant communication and memory overhead, making the deployment of large-scale AIoT devices a great challenge. To address the above problem, this paper presents a novel Distillation-based Federated Learning (DFL) architecture that enables efficient and accurate FL for AIoT applications. Inspired by Knowledge Distillation (KD) that can increase the model accuracy, our approach adds the soft targets used by KD to the FL model training, which occupies negligible network resources. The soft targets are generated by local sample predictions of each AIoT device after each round of local training and used for the next round of model training. During the local training of DFL, both soft targets and hard labels are used as approximation objectives of model predictions to improve model accuracy by supplementing the knowledge of soft targets. To further improve the performance of our DFL model, we design a dynamic adjustment strategy for tuning the ratio of two loss functions used in KD, which can maximize the use of both soft targets and hard labels. Comprehensive experimental results on well-known benchmarks show that our approach can significantly improve the model accuracy of FL with both Independent and Identically Distributed (IID) and non-IID data. △ Less","12 May, 2022",https://arxiv.org/pdf/2111.14347
A Practical guide on Explainable AI Techniques applied on Biomedical use case applications,Adrien Bennetot;Ivan Donadello;Ayoub El Qadi;Mauro Dragoni;Thomas Frossard;Benedikt Wagner;Anna Saranti;Silvia Tulli;Maria Trocan;Raja Chatila;Andreas Holzinger;Artur d'Avila Garcez;Natalia Díaz-Rodríguez,"Last years have been characterized by an upsurge of opaque automatic decision support systems, such as Deep Neural Networks (DNNs). Although they have great generalization and prediction skills, their functioning does not allow obtaining detailed explanations of their behaviour. As opaque machine learning models are increasingly being employed to make important predictions in critical environments, the danger is to create and use decisions that are not justifiable or legitimate. Therefore, there is a general agreement on the importance of endowing machine learning models with explainability. EXplainable Artificial Intelligence (XAI) techniques can serve to verify and certify model outputs and enhance them with desirable notions such as trustworthiness, accountability, transparency and fairness. This guide is meant to be the go-to handbook for any audience with a computer science background aiming at getting intuitive insights on machine learning models, accompanied with straight, fast, and intuitive explanations out of the box. This article aims to fill the lack of compelling XAI guide by applying XAI techniques in their particular day-to-day models, datasets and use-cases. Figure 1 acts as a flowchart/map for the reader and should help him to find the ideal method to use according to his type of data. In each chapter, the reader will find a description of the proposed method as well as an example of use on a Biomedical application and a Python notebook. It can be easily modified in order to be applied to specific applications. △ Less","5 September, 2022",https://arxiv.org/pdf/2111.14260
Toward Next Generation Open Radio Access Network--What O-RAN Can and Cannot Do!,Aly S. Abdalla;Pratheek S. Upadhyaya;Vijay K. Shah;Vuk Marojevic,"The open radio access network (O-RAN) describes an industry-driven open architecture and interfaces for building next generation RANs with artificial intelligence (AI) controllers. We circulated a survey among researchers, developers, and practitioners to gather their perspectives on O-RAN as a framework for 6G wireless research and development (R&D). The majority responded in favor of O-RAN and identified R&D of interest to them. Motivated by these responses, this paper identifies the limitations of the current O-RAN specifications and the technologies for overcoming them. We recognize end-to-end security, deterministic latency, physical layer real-time control, and testing of AI-based RAN control applications as the critical features to enable and discuss R&D opportunities for extending the architectural capabilities of O-RAN as a platform for 6G wireless. △ Less","25 March, 2022",https://arxiv.org/pdf/2111.13754
Animal behavior classification via deep learning on embedded systems,Reza Arablouei;Liang Wang;Lachlan Currie;Jordan Yates;Flavio A. P. Alvarenga;Greg J. Bishop-Hurley,"We develop an end-to-end deep-neural-network-based algorithm for classifying animal behavior using accelerometry data on the embedded system of an artificial intelligence of things (AIoT) device installed in a wearable collar tag. The proposed algorithm jointly performs feature extraction and classification utilizing a set of infinite-impulse-response (IIR) and finite-impulse-response (FIR) filters together with a multilayer perceptron. The utilized IIR and FIR filters can be viewed as specific types of recurrent and convolutional neural network layers, respectively. We evaluate the performance of the proposed algorithm via two real-world datasets collected from total eighteen grazing beef cattle using collar tags. The results show that the proposed algorithm offers good intra- and inter-dataset classification accuracy and outperforms its closest contenders including two state-of-the-art convolutional-neural-network-based time-series classification algorithms, which are significantly more complex. We implement the proposed algorithm on the embedded system of the utilized collar tags' AIoT device to perform in-situ classification of animal behavior. We achieve real-time in-situ behavior inference from accelerometry data without imposing any strain on the available computational, memory, or energy resources of the embedded system. △ Less","26 October, 2022",https://arxiv.org/pdf/2111.12295
Aerial Intelligent Reflecting Surface Enabled Terahertz Covert Communications in Beyond-5G Internet of Things,Milad Tatar Mamaghani;Yi Hong,"Unmanned aerial vehicles (UAVs) are envisioned to be extensively employed for assisting wireless communications in Internet of Things (IoT) applications. On the other hand, terahertz (THz) enabled intelligent reflecting surface (IRS) is expected to be one of the core enabling technologies for forthcoming beyond-5G wireless communications that promise a broad range of data-demand applications. In this paper, we propose a UAV-mounted IRS (UIRS) communication system over THz bands for confidential data dissemination from an access point (AP) towards multiple ground user equipments (UEs) in IoT networks. Specifically, the AP intends to send data to the scheduled UE, while unscheduled UEs may pose potential adversaries. To protect information messages and the privacy of the scheduled UE, we aim to devise an energy-efficient multi-UAV covert communication scheme, where the UIRS is for reliable data transmissions, and an extra UAV is utilized as a cooperative jammer generating artificial noise (AN) to degrade unscheduled UEs detection. We then formulate a novel minimum average energy efficiency (mAEE) optimization problem, targetting to improve the covert throughput and reduce UAVs' propulsion energy consumption subject to the covertness requirement, which is determined analytically. Since the optimization problem is non-convex, we tackle it via the block successive convex approximation (BSCA) approach to iteratively solve a sequence of approximated convex sub-problems, designing the binary user scheduling, AP's power allocation, maximum AN jamming power, IRS beamforming, and both UAVs' trajectory planning. Finally, we present a low-complex overall algorithm for system performance enhancement with complexity and convergence analysis. Numerical results are provided to verify our analysis and demonstrate significant outperformance of our design over other existing benchmark schemes. △ Less","28 January, 2022",https://arxiv.org/pdf/2111.11650
Explainable Biomedical Recommendations via Reinforcement Learning Reasoning on Knowledge Graphs,Gavin Edwards;Sebastian Nilsson;Benedek Rozemberczki;Eliseo Papa,"For Artificial Intelligence to have a greater impact in biology and medicine, it is crucial that recommendations are both accurate and transparent. In other domains, a neurosymbolic approach of multi-hop reasoning on knowledge graphs has been shown to produce transparent explanations. However, there is a lack of research applying it to complex biomedical datasets and problems. In this paper, the approach is explored for drug discovery to draw solid conclusions on its applicability. For the first time, we systematically apply it to multiple biomedical datasets and recommendation tasks with fair benchmark comparisons. The approach is found to outperform the best baselines by 21.7% on average whilst producing novel, biologically relevant explanations. △ Less","7 October, 2022",https://arxiv.org/pdf/2111.10625
Patent Data for Engineering Design: A Critical Review and Future Directions,Shuo Jiang;Serhad Sarica;Binyang Song;Jie Hu;Jianxi Luo,"Patent data have long been used for engineering design research because of its large and expanding size, and widely varying massive amount of design information contained in patents. Recent advances in artificial intelligence and data science present unprecedented opportunities to develop data-driven design methods and tools, as well as advance design science, using the patent database. Herein, we survey and categorize the patent-for-design literature based on its contributions to design theories, methods, tools, and strategies, as well as the types of patent data and data-driven methods used in respective studies. Our review highlights promising future research directions in patent data-driven design research and practice. △ Less","14 June, 2022",https://arxiv.org/pdf/2111.08500
Free Will Belief as a consequence of Model-based Reinforcement Learning,Erik M. Rehn,"The debate on whether or not humans have free will has been raging for centuries. Although there are good arguments based on our current understanding of the laws of nature for the view that it is not possible for humans to have free will, most people believe they do. This discrepancy begs for an explanation. If we accept that we do not have free will, we are faced with two problems: (1) while freedom is a very commonly used concept that everyone intuitively understands, what are we actually referring to when we say that an action or choice is ""free"" or not? And, (2) why is the belief in free will so common? Where does this belief come from, and what is its purpose, if any? In this paper, we examine these questions from the perspective of reinforcement learning (RL). RL is a framework originally developed for training artificial intelligence agents. However, it can also be used as a computational model of human decision making and learning, and by doing so, we propose that the first problem can be answered by observing that people's common sense understanding of freedom is closely related to the information entropy of an RL agent's normalized action values, while the second can be explained by the necessity for agents to model themselves as if they could have taken decisions other than those they actually took, when dealing with the temporal credit assignment problem. Put simply, we suggest that by applying the RL framework as a model for human learning it becomes evident that in order for us to learn efficiently and be intelligent we need to view ourselves as if we have free will. △ Less","21 April, 2022",https://arxiv.org/pdf/2111.08435
"AI in Human-computer Gaming: Techniques, Challenges and Opportunities",Qiyue Yin;Jun Yang;Kaiqi Huang;Meijing Zhao;Wancheng Ni;Bin Liang;Yan Huang;Shu Wu;Liang Wang,"With breakthrough of the AlphaGo, human-computer gaming AI has ushered in a big explosion, attracting more and more researchers all around the world. As a recognized standard for testing artificial intelligence, various human-computer gaming AI systems (AIs) have been developed such as the Libratus, OpenAI Five and AlphaStar, beating professional human players. The rapid development of human-computer gaming AIs indicate a big step of decision making intelligence, and it seems that current techniques can handle very complex human-computer games. So, one natural question raises: what are the possible challenges of current techniques in human-computer gaming, and what are the future trends? To answer the above question, in this paper, we survey recent successful game AIs, covering board game AIs, card game AIs, first-person shooting game AIs and real time strategy game AIs. Through this survey, we 1) compare the main difficulties among different kinds of games and the corresponding techniques utilized for achieving professional human level AIs; 2) summarize the mainstream frameworks and techniques that can be properly relied on for developing AIs for complex human-computer gaming; 3) raise the challenges or drawbacks of current techniques in the successful AIs; and 4) try to point out future trends in human-computer gaming AIs. Finally, we hope this brief review can provide an introduction for beginners, and inspire insights for researchers in the field of AI in human-computer gaming. △ Less","17 August, 2022",https://arxiv.org/pdf/2111.07631
Physics in the Machine: Integrating Physical Knowledge in Autonomous Phase-Mapping,A. Gilad Kusne;Austin McDannald;Brian DeCost;Corey Oses;Cormac Toher;Stefano Curtarolo;Apurva Mehta;Ichiro Takeuchi,"Application of artificial intelligence (AI), and more specifically machine learning, to the physical sciences has expanded significantly over the past decades. In particular, science-informed AI, also known as scientific AI or inductive bias AI, has grown from a focus on data analysis to now controlling experiment design, simulation, execution and analysis in closed-loop autonomous systems. The CAMEO (closed-loop autonomous materials exploration and optimization) algorithm employs scientific AI to address two tasks: learning a material system's composition-structure relationship and identifying materials compositions with optimal functional properties. By integrating these, accelerated materials screening across compositional phase diagrams was demonstrated, resulting in the discovery of a best-in-class phase change memory material. Key to this success is the ability to guide subsequent measurements to maximize knowledge of the composition-structure relationship, or phase map. In this work we investigate the benefits of incorporating varying levels of prior physical knowledge into CAMEO's autonomous phase-mapping. This includes the use of ab-initio phase boundary data from the AFLOW repositories, which has been shown to optimize CAMEO's search when used as a prior. △ Less","16 February, 2022",https://arxiv.org/pdf/2111.07478
Introducing Variational Autoencoders to High School Students,Zhuoyue Lyu;Safinah Ali;Cynthia Breazeal,"Generative Artificial Intelligence (AI) models are a compelling way to introduce K-12 students to AI education using an artistic medium, and hence have drawn attention from K-12 AI educators. Previous Creative AI curricula mainly focus on Generative Adversarial Networks (GANs) while paying less attention to Autoregressive Models, Variational Autoencoders (VAEs), or other generative models, which have since become common in the field of generative AI. VAEs' latent-space structure and interpolation ability could effectively ground the interdisciplinary learning of AI, creative arts, and philosophy. Thus, we designed a lesson to teach high school students about VAEs. We developed a web-based game and used Plato's cave, a philosophical metaphor, to introduce how VAEs work. We used a Google Colab notebook for students to re-train VAEs with their hand-written digits to consolidate their understandings. Finally, we guided the exploration of creative VAE tools such as SketchRNN and MusicVAE to draw the connection between what they learned and real-world applications. This paper describes the lesson design and shares insights from the pilot studies with 22 students. We found that our approach was effective in teaching students about a novel AI concept. △ Less","6 January, 2022",https://arxiv.org/pdf/2111.07036
Optimization Framework for Splitting DNN Inference Jobs over Computing Networks,Sehun Jung;Hyang-Won Lee,"Ubiquitous artificial intelligence (AI) is considered one of the key services in 6G systems. AI services typically rely on deep neural network (DNN) requiring heavy computation. Hence, in order to support ubiquitous AI, it is crucial to provide a solution for offloading or distributing computational burden due to DNN, especially at end devices with limited resources. We develop an optimization framework for assigning the computation tasks of DNN inference jobs to computing resources in the network, so as to reduce the inference latency. To this end, we propose a layered graph model with which simple conventional routing jointly solves the problem of selecting nodes for computation and paths for data transfer between nodes. We show that using our model, the existing approaches to splitting DNN inference jobs can be equivalently reformulated as a routing problem that possesses better numerical properties. We also apply the proposed framework to derive algorithms for minimizing the end-to-end inference latency. We show through numerical evaluations that our new formulation can find a solution for DNN inference job distribution much faster than the existing formulation, and that our algorithms can select computing nodes and data paths adaptively to the computational attributes of given DNN inference jobs, so as to reduce the end-to-end latency. △ Less","26 July, 2022",https://arxiv.org/pdf/2111.07006
AlphaDDA: Strategies for Adjusting the Playing Strength of a Fully Trained AlphaZero System to a Suitable Human Training Partner,Kazuhisa Fujita,"Artificial intelligence (AI) has achieved superhuman performance in board games such as Go, chess, and Othello (Reversi). In other words, the AI system surpasses the level of a strong human expert player in such games. In this context, it is difficult for a human player to enjoy playing the games with the AI. To keep human players entertained and immersed in a game, the AI is required to dynamically balance its skill with that of the human player. To address this issue, we propose AlphaDDA, an AlphaZero-based AI with dynamic difficulty adjustment (DDA). AlphaDDA consists of a deep neural network (DNN) and a Monte Carlo tree search, as in AlphaZero. AlphaDDA learns and plays a game the same way as AlphaZero, but can change its skills. AlphaDDA estimates the value of the game state from only the board state using the DNN. AlphaDDA changes a parameter dominantly controlling its skills according to the estimated value. Consequently, AlphaDDA adjusts its skills according to a game state. AlphaDDA can adjust its skill using only the state of a game without any prior knowledge regarding an opponent. In this study, AlphaDDA plays Connect4, Othello, and 6x6 Othello with other AI agents. Other AI agents are AlphaZero, Monte Carlo tree search, the minimax algorithm, and a random player. This study shows that AlphaDDA can balance its skill with that of the other AI agents, except for a random player. The DDA ability of AlphaDDA is based on an accurate estimation of the value from the state of a game. We believe that the AlphaDDA approach for DDA can be used for any game AI system if the DNN can accurately estimate the value of the game state and we know a parameter controlling the skills of the AI system. △ Less","20 September, 2022",https://arxiv.org/pdf/2111.06266
Edge-Cloud Polarization and Collaboration: A Comprehensive Survey for AI,Jiangchao Yao;Shengyu Zhang;Yang Yao;Feng Wang;Jianxin Ma;Jianwei Zhang;Yunfei Chu;Luo Ji;Kunyang Jia;Tao Shen;Anpeng Wu;Fengda Zhang;Ziqi Tan;Kun Kuang;Chao Wu;Fei Wu;Jingren Zhou;Hongxia Yang,"Influenced by the great success of deep learning via cloud computing and the rapid development of edge chips, research in artificial intelligence (AI) has shifted to both of the computing paradigms, i.e., cloud computing and edge computing. In recent years, we have witnessed significant progress in developing more advanced AI models on cloud servers that surpass traditional deep learning models owing to model innovations (e.g., Transformers, Pretrained families), explosion of training data and soaring computing capabilities. However, edge computing, especially edge and cloud collaborative computing, are still in its infancy to announce their success due to the resource-constrained IoT scenarios with very limited algorithms deployed. In this survey, we conduct a systematic review for both cloud and edge AI. Specifically, we are the first to set up the collaborative learning mechanism for cloud and edge modeling with a thorough review of the architectures that enable such mechanism. We also discuss potentials and practical experiences of some on-going advanced edge AI topics including pretraining models, graph neural networks and reinforcement learning. Finally, we discuss the promising directions and challenges in this field. △ Less","23 May, 2022",https://arxiv.org/pdf/2111.06061
"Machine Learning Models Disclosure from Trusted Research Environments (TRE), Challenges and Opportunities",Esma Mansouri-Benssassi;Simon Rogers;Jim Smith;Felix Ritchie;Emily Jefferson,"Artificial intelligence (AI) applications in healthcare and medicine have increased in recent years. To enable access to personal data, Trusted Research environments (TREs) provide safe and secure environments in which researchers can access sensitive personal data and develop Artificial Intelligence (AI) and Machine Learning models. However currently few TREs support the use of automated AI-based modelling using Machine Learning. Early attempts have been made in the literature to present and introduce privacy preserving machine learning from the design point of view [1]. However, there exists a gap in the practical decision-making guidance for TREs in handling models disclosure. Specifically, the use of machine learning creates a need to disclose new types of outputs from TREs, such as trained machine learning models. Although TREs have clear policies for the disclosure of statistical outputs, the extent to which trained models can leak personal training data once released is not well understood and guidelines do not exist within TREs for the safe disclosure of these models. In this paper we introduce the challenge of disclosing trained machine learning models from TREs. We first give an overview of machine learning models in general and describe some of their applications in healthcare and medicine. We define the main vulnerabilities of trained machine learning models in general. We also describe the main factors affecting the vulnerabilities of disclosing machine learning models. This paper also provides insights and analyses methods that could be introduced within TREs to mitigate the risk of privacy breaches when disclosing trained models. △ Less","20 August, 2022",https://arxiv.org/pdf/2111.05628
Machine Learning Guided 3D Image Recognition for Carbonate Pore and Mineral Volumes Determination,Omar Alfarisi;Aikifa Raza;Hongtao Zhang;Djamel Ozzane;Mohamed Sassi;Tiejun Zhang,"Automated image processing algorithms can improve the quality, efficiency, and consistency of classifying the morphology of heterogeneous carbonate rock and can deal with a massive amount of data and images seamlessly. Geoscientists face difficulties in setting the direction of the optimum method for determining petrophysical properties from rock images, Micro-Computed Tomography (uCT), or Magnetic Resonance Imaging (MRI). Most of the successful work is from the homogeneous rocks focusing on 2D images with less focus on 3D and requiring numerical simulation. Currently, image analysis methods converge to three approaches: image processing, artificial intelligence, and combined image processing with artificial intelligence. In this work, we propose two methods to determine the porosity from 3D uCT and MRI images: an image processing method with Image Resolution Optimized Gaussian Algorithm (IROGA); advanced image recognition method enabled by Machine Learning Difference of Gaussian Random Forest (MLDGRF). We have built reference 3D micro models and collected images for calibration of IROGA and MLDGRF methods. To evaluate the predictive capability of these calibrated approaches, we ran them on 3D uCT and MRI images of natural heterogeneous carbonate rock. We measured the porosity and lithology of the carbonate rock using three and two industry-standard ways, respectively, as reference values. Notably, IROGA and MLDGRF have produced porosity results with an accuracy of 96.2% and 97.1% on the training set and 91.7% and 94.4% on blind test validation, respectively, in comparison with the three experimental measurements. We measured limestone and pyrite reference values using two methods, X-ray powder diffraction, and grain density measurements. MLDGRF has produced lithology (limestone and Pyrite) volumes with 97.7% accuracy. △ Less","5 January, 2022",https://arxiv.org/pdf/2111.04612
"Twitter Big Data as a Resource for Exoskeleton Research: A Large-Scale Dataset of about 140,000 Tweets and 100 Research Questions",Nirmalya Thakur,"The exoskeleton technology has been rapidly advancing in the recent past due to its multitude of applications and diverse use-cases in assisted living, military, healthcare, firefighting, and industry 4.0. The exoskeleton market is projected to increase by multiple times of its current value within the next two years. Therefore, it is crucial to study the degree and trends of user interest, views, opinions, perspectives, attitudes, acceptance, feedback, engagement, buying behavior, and satisfaction, towards exoskeletons, for which the availability of Big Data of conversations about exoskeletons is necessary. The Internet of Everything style of today's living, characterized by people spending more time on the internet than ever before, with a specific focus on social media platforms, holds the potential for the development of such a dataset by the mining of relevant social media conversations. Twitter, one such social media platform, is highly popular amongst all age groups, where the topics found in the conversation paradigms include emerging technologies such as exoskeletons. To address this research challenge, this work makes two scientific contributions to this field. First, it presents an open-access dataset of about 140,000 tweets about exoskeletons that were posted in a 5-year period from May 21, 2017, to May 21, 2022. Second, based on a comprehensive review of the recent works in the fields of Big Data, Natural Language Processing, Information Retrieval, Data Mining, Pattern Recognition, and Artificial Intelligence that may be applied to relevant Twitter data for advancing research, innovation, and discovery in the field of exoskeleton research, a total of 100 Research Questions are presented for researchers to study, analyze, evaluate, ideate, and investigate based on this dataset. △ Less","20 July, 2022",https://arxiv.org/pdf/2111.04476
"When Cyber-Physical Systems Meet AI: A Benchmark, an Evaluation, and a Way Forward",Jiayang Song;Deyun Lyu;Zhenya Zhang;Zhijie Wang;Tianyi Zhang;Lei Ma,"Cyber-physical systems (CPS) have been broadly deployed in safety-critical domains, such as automotive systems, avionics, medical devices, etc. In recent years, Artificial Intelligence (AI) has been increasingly adopted to control CPS. Despite the popularity of AI-enabled CPS, few benchmarks are publicly available. There is also a lack of deep understanding on the performance and reliability of AI-enabled CPS across different industrial domains. To bridge this gap, we initiate to create a public benchmark of industry-level CPS in seven domains and build AI controllers for them via state-of-the-art deep reinforcement learning (DRL) methods. Based on that, we further perform a systematic evaluation of these AI-enabled systems with their traditional counterparts to identify the current challenges and explore future opportunities. Our key findings include (1) AI controllers do not always outperform traditional controllers, (2) existing CPS testing techniques (falsification, specifically) fall short of analyzing AI-enabled CPS, and (3) building a hybrid system that strategically combines and switches between AI controllers and traditional controllers can achieve better performance across different domains. Our results highlight the need for new testing techniques for AI-enabled CPS and the need for more investigations into hybrid CPS systems to achieve optimal performance and reliability. △ Less","19 April, 2022",https://arxiv.org/pdf/2111.04324
A Review of Location Encoding for GeoAI: Methods and Applications,Gengchen Mai;Krzysztof Janowicz;Yingjie Hu;Song Gao;Bo Yan;Rui Zhu;Ling Cai;Ni Lao,"A common need for artificial intelligence models in the broader geoscience is to represent and encode various types of spatial data, such as points (e.g., points of interest), polylines (e.g., trajectories), polygons (e.g., administrative regions), graphs (e.g., transportation networks), or rasters (e.g., remote sensing images), in a hidden embedding space so that they can be readily incorporated into deep learning models. One fundamental step is to encode a single point location into an embedding space, such that this embedding is learning-friendly for downstream machine learning models such as support vector machines and neural networks. We call this process location encoding. However, there lacks a systematic review on the concept of location encoding, its potential applications, and key challenges that need to be addressed. This paper aims to fill this gap. We first provide a formal definition of location encoding, and discuss the necessity of location encoding for GeoAI research from a machine learning perspective. Next, we provide a comprehensive survey and discussion about the current landscape of location encoding research. We classify location encoding models into different categories based on their inputs and encoding methods, and compare them based on whether they are parametric, multi-scale, distance preserving, and direction aware. We demonstrate that existing location encoding models can be unified under a shared formulation framework. We also discuss the application of location encoding for different types of spatial data. Finally, we point out several challenges in location encoding research that need to be solved in the future. △ Less","10 March, 2022",https://arxiv.org/pdf/2111.04006
Roadmap on Signal Processing for Next Generation Measurement Systems,D. K. Iakovidis;M. Ooi;Y. C. Kuang;S. Demidenko;A. Shestakov;V. Sinitsin;M. Henry;A. Sciacchitano;A. Discetti;S. Donati;M. Norgia;A. Menychtas;I. Maglogiannis;S. C. Wriessnegger;L. A. Barradas Chacon;G. Dimas;D. Filos;A. H. Aletras;J. Töger;F. Dong;S. Ren;A. Uhl;J. Paziewski;J. Geng;F. Fioranelli,"Signal processing is a fundamental component of almost any sensor-enabled system, with a wide range of applications across different scientific disciplines. Time series data, images, and video sequences comprise representative forms of signals that can be enhanced and analysed for information extraction and quantification. The recent advances in artificial intelligence and machine learning are shifting the research attention towards intelligent, data-driven, signal processing. This roadmap presents a critical overview of the state-of-the-art methods and applications aiming to highlight future challenges and research opportunities towards next generation measurement systems. It covers a broad spectrum of topics ranging from basic to industrial research, organized in concise thematic sections that reflect the trends and the impacts of current and future developments per research field. Furthermore, it offers guidance to researchers and funding agencies in identifying new prospects. △ Less","28 January, 2022",https://arxiv.org/pdf/2111.02493
Natural Language Processing for Smart Healthcare,Binggui Zhou;Guanghua Yang;Zheng Shi;Shaodan Ma,"Smart healthcare has achieved significant progress in recent years. Emerging artificial intelligence (AI) technologies enable various smart applications across various healthcare scenarios. As an essential technology powered by AI, natural language processing (NLP) plays a key role in smart healthcare due to its capability of analysing and understanding human language. In this work, we review existing studies that concern NLP for smart healthcare from the perspectives of technique and application. We first elaborate on different NLP approaches and the NLP pipeline for smart healthcare from the technical point of view. Then, in the context of smart healthcare applications employing NLP techniques, we introduce representative smart healthcare scenarios, including clinical practice, hospital management, personal care, public health, and drug development. We further discuss two specific medical issues, i.e., the coronavirus disease 2019 (COVID-19) pandemic and mental health, in which NLP-driven smart healthcare plays an important role. Finally, we discuss the limitations of current works and identify the directions for future works. △ Less","25 September, 2022",https://arxiv.org/pdf/2110.15803
Paperswithtopic: Topic Identification from Paper Title Only,Daehyun Cho;Christian Wallraven,"The deep learning field is growing rapidly as witnessed by the exponential growth of papers submitted to journals, conferences, and pre-print servers. To cope with the sheer number of papers, several text mining tools from natural language processing (NLP) have been proposed that enable researchers to keep track of recent findings. In this context, our paper makes two main contributions: first, we collected and annotated a dataset of papers paired by title and sub-field from the field of artificial intelligence (AI), and, second, we present results on how to predict a paper's AI sub-field from a given paper title only. Importantly, for the latter, short-text classification task we compare several algorithms from conventional machine learning all the way up to recent, larger transformer architectures. Finally, for the transformer models, we also present gradient-based, attention visualizations to further explain the model's classification process. All code can be found at \url{https://github.com/1pha/paperswithtopic} △ Less","31 March, 2022",https://arxiv.org/pdf/2110.15721
Distill: Domain-Specific Compilation for Cognitive Models,Jan Vesely;Raghavendra Pradyumna Pothukuchi;Ketaki Joshi;Samyak Gupta;Jonathan D. Cohen;Abhishek Bhattacharjee,"This paper discusses our proposal and implementation of Distill, a domain-specific compilation tool based on LLVM to accelerate cognitive models. Cognitive models explain the process of cognitive function and offer a path to human-like artificial intelligence. However, cognitive modeling is laborious, requiring composition of many types of computational tasks, and suffers from poor performance as it relies on high-level languages like Python. In order to continue enjoying the flexibility of Python while achieving high performance, Distill uses domain-specific knowledge to compile Python-based cognitive models into LLVM IR, carefully stripping away features like dynamic typing and memory management that add overheads to the actual model. As we show, this permits significantly faster model execution. We also show that the code so generated enables using classical compiler data flow analysis passes to reveal properties about data flow in cognitive models that are useful to cognitive scientists. Distill is publicly available, is being used by researchers in cognitive science, and has led to patches that are currently being evaluated for integration into mainline LLVM. △ Less","14 January, 2022",https://arxiv.org/pdf/2110.15425
"NIDA-CLIFGAN: Natural Infrastructure Damage Assessment through Efficient Classification Combining Contrastive Learning, Information Fusion and Generative Adversarial Networks",Jie Wei;Zhigang Zhu;Erik Blasch;Bilal Abdulrahman;Billy Davila;Shuoxin Liu;Jed Magracia;Ling Fang,"During natural disasters, aircraft and satellites are used to survey the impacted regions. Usually human experts are needed to manually label the degrees of the building damage so that proper humanitarian assistance and disaster response (HADR) can be achieved, which is labor-intensive and time-consuming. Expecting human labeling of major disasters over a wide area gravely slows down the HADR efforts. It is thus of crucial interest to take advantage of the cutting-edge Artificial Intelligence and Machine Learning techniques to speed up the natural infrastructure damage assessment process to achieve effective HADR. Accordingly, the paper demonstrates a systematic effort to achieve efficient building damage classification. First, two novel generative adversarial nets (GANs) are designed to augment data used to train the deep-learning-based classifier. Second, a contrastive learning based method using novel data structures is developed to achieve great performance. Third, by using information fusion, the classifier is effectively trained with very few training data samples for transfer learning. All the classifiers are small enough to be loaded in a smart phone or simple laptop for first responders. Based on the available overhead imagery dataset, results demonstrate data and computational efficiency with 10% of the collected data combined with a GAN reducing the time of computation from roughly half a day to about 1 hour with roughly similar classification performances. △ Less","29 October, 2022",https://arxiv.org/pdf/2110.14518
Toward a Theory of Justice for Artificial Intelligence,Iason Gabriel,"This paper explores the relationship between artificial intelligence and principles of distributive justice. Drawing upon the political philosophy of John Rawls, it holds that the basic structure of society should be understood as a composite of socio-technical systems, and that the operation of these systems is increasingly shaped and influenced by AI. As a consequence, egalitarian norms of justice apply to the technology when it is deployed in these contexts. These norms entail that the relevant AI systems must meet a certain standard of public justification, support citizens rights, and promote substantively fair outcomes -- something that requires specific attention be paid to the impact they have on the worst-off members of society. △ Less","21 June, 2022",https://arxiv.org/pdf/2110.14419
Towards artificial general intelligence via a multimodal foundation model,Nanyi Fei;Zhiwu Lu;Yizhao Gao;Guoxing Yang;Yuqi Huo;Jingyuan Wen;Haoyu Lu;Ruihua Song;Xin Gao;Tao Xiang;Hao Sun;Ji-Rong Wen,"The fundamental goal of artificial intelligence (AI) is to mimic the core cognitive activities of human. Despite tremendous success in the AI research, most of existing methods have only single-cognitive ability. To overcome this limitation and take a solid step towards artificial general intelligence (AGI), we develop a foundation model pre-trained with huge multimodal data, which can be quickly adapted for various downstream cognitive tasks. To achieve this goal, we propose to pre-train our foundation model by self-supervised learning with weak semantic correlation data crawled from the Internet and show that promising results can be obtained on a wide range of downstream tasks. Particularly, with the developed model-interpretability tools, we demonstrate that strong imagination ability is now possessed by our foundation model. We believe that our work makes a transformative stride towards AGI, from our common practice of ""weak or narrow AI"" to that of ""strong or generalized AI"". △ Less","8 June, 2022",https://arxiv.org/pdf/2110.14378
Intelligent Meta-Imagers: From Compressed to Learned Sensing,Chloé Saigre-Tardif;Rashid Faqiri;Hanting Zhao;Lianlin Li;Philipp del Hougne,"Computational meta-imagers synergize metamaterial hardware with advanced signal processing approaches such as compressed sensing. Recent advances in artificial intelligence (AI) are gradually reshaping the landscape of meta-imaging. Most recent works use AI for data analysis, but some also use it to program the physical meta-hardware. The role of ""intelligence"" in the measurement process and its implications for critical metrics like latency are often not immediately clear. Here, we comprehensively review the evolution of computational meta-imaging from the earliest frequency-diverse compressive systems to modern programmable intelligent meta-imagers. We introduce a clear taxonomy in terms of the flow of task-relevant information that has direct links to information theory: compressive meta-imagers indiscriminately acquire all scene information in a task-agnostic measurement process that aims at a near-isometric embedding; intelligent meta-imagers highlight task-relevant information in a task-aware measurement process that is purposefully non-isometric. The measurement process of intelligent meta-imagers is thus simultaneously an analog wave processor that implements a first task-specific inference step ""over-the-air"". We provide explicit design tutorials for the integration of programmable meta-atoms as trainable physical weights into an intelligent end-to-end sensing pipeline. This merging of the physical world of metamaterial engineering and the digital world of AI enables the remarkable latency gains of intelligent meta-imagers. We further outline emerging opportunities for cognitive meta-imagers with reverberation-enhanced resolution and we point out how the meta-imaging community can reap recent advances in the vibrant field of metamaterial wave processors to reach the holy grail of low-energy ultra-fast all-analog intelligent meta-sensors. △ Less","28 January, 2022",https://arxiv.org/pdf/2110.14022
RGB Camera-based Physiological Sensing: Challenges and Future Directions,Xin Liu;Shwetak Patel;Daniel McDuff,"Numerous real-world applications have been driven by the recent algorithmic advancement of artificial intelligence (AI). Healthcare is no exception and AI technologies have great potential to revolutionize the industry. Non-contact camera-based physiological sensing, including remote photoplethysmography (rPPG), is a set of imaging methods that leverages ordinary RGB cameras (e.g., webcam or smartphone camera) to capture subtle changes in electromagnetic radiation (e.g., light) reflected by the body caused by physiological processes. RGB camera-based systems not only have the ability to measure the signals without contact with the body but also have the opportunity to capture multimodal information (e.g., facial expressions, activities and other context) from the same sensor. However, developing accessible, equitable and useful camera-based physiological sensing systems comes with various challenges. In this article, we identify four research challenges for the field of RGB camera-based physiological sensing and broader AI driven healthcare communities and suggest future directions to tackle these. We believe solving these challenges will help deliver accurate, equitable and generalizable AI systems for healthcare that are practical in real-world and clinical contexts. △ Less","21 February, 2022",https://arxiv.org/pdf/2110.13362
CoProtector: Protect Open-Source Code against Unauthorized Training Usage with Data Poisoning,Zhensu Sun;Xiaoning Du;Fu Song;Mingze Ni;Li Li,"Github Copilot, trained on billions of lines of public code, has recently become the buzzword in the computer science research and practice community. Although it is designed to help developers implement safe and effective code with powerful intelligence, practitioners and researchers raise concerns about its ethical and security problems, e.g., should the copyleft licensed code be freely leveraged or insecure code be considered for training in the first place? These problems pose a significant impact on Copilot and other similar products that aim to learn knowledge from large-scale open-source code through deep learning models, which are inevitably on the rise with the fast development of artificial intelligence. To mitigate such impacts, we argue that there is a need to invent effective mechanisms for protecting open-source code from being exploited by deep learning models. Here, we design and implement a prototype, CoProtector, which utilizes data poisoning techniques to arm source code repositories for defending against such exploits. Our large-scale experiments empirically show that CoProtector is effective in achieving its purpose, significantly reducing the performance of Copilot-like deep learning models while being able to stably reveal the secretly embedded watermark backdoors. △ Less","14 February, 2022",https://arxiv.org/pdf/2110.12925
Automated Scoring System of HER2 in Pathological Images under the Microscope,Zichen Zhang;Lang Wang;Shuhao Wang,"Breast cancer is the most common cancer among women worldwide. The human epidermal growth factor receptor 2 (HER2) with immunohistochemical (IHC) is widely used for pathological evaluation to provide the appropriate therapy for patients with breast cancer. However, the deficiency of pathologists and subjective and susceptible to inter-observer variation of visual diagnosis are the main challenges. Recently, with the rapid development of artificial intelligence (AI) in disease diagnosis, several automated HER2 scoring methods using traditional computer vision or machine learning methods indicate the improvement of the HER2 diagnostic accuracy, but the unreasonable interpretation in pathology, as well as the expensive and ethical issues for annotation, make these methods still have a long way to deploy in hospitals to ease pathologists' burden in real. In this paper, we propose a HER2 automated scoring system that strictly follows the HER2 scoring guidelines simulating the real workflow of HER2 scores diagnosis by pathologists. Unlike the previous work, our method considers the positive control of HER2 to make sure the assay performance for each slide, eliminating work for repeated comparison between the current field of view (FOV) and positive control FOV, especially for the borderline cases. Besides, for each selected FOV under the microscope, our system provides real-time HER2 scores analysis and visualizations of the membrane staining intensity and completeness corresponding with the cell classifications. Our rigorous workflow along with the flexible interactive adjustion in demand substantially assists pathologists to finish the HER2 diagnosis faster and improves the robustness and accuracy. The proposed system will be embedded in our Thorough Eye platform for deployment in hospitals. △ Less","6 March, 2022",https://arxiv.org/pdf/2110.12900
Unraveling the Hidden Environmental Impacts of AI Solutions for Environment,Anne-Laure Ligozat;Julien Lefèvre;Aurélie Bugeau;Jacques Combaz,"In the past ten years, artificial intelligence has encountered such dramatic progress that it is now seen as a tool of choice to solve environmental issues and in the first place greenhouse gas emissions (GHG). At the same time the deep learning community began to realize that training models with more and more parameters requires a lot of energy and as a consequence GHG emissions. To our knowledge, questioning the complete net environmental impacts of AI solutions for the environment (AI for Green), and not only GHG, has never been addressed directly. In this article, we propose to study the possible negative impacts of AI for Green. First, we review the different types of AI impacts, then we present the different methodologies used to assess those impacts, and show how to apply life cycle assessment to AI services. Finally, we discuss how to assess the environmental usefulness of a general AI service, and point out the limitations of existing work in AI for Green. △ Less","21 April, 2022",https://arxiv.org/pdf/2110.11822
ProtoShotXAI: Using Prototypical Few-Shot Architecture for Explainable AI,Samuel Hess;Gregory Ditzler,"Unexplainable black-box models create scenarios where anomalies cause deleterious responses, thus creating unacceptable risks. These risks have motivated the field of eXplainable Artificial Intelligence (XAI) to improve trust by evaluating local interpretability in black-box neural networks. Unfortunately, the ground truth is unavailable for the model's decision, so evaluation is limited to qualitative assessment. Further, interpretability may lead to inaccurate conclusions about the model or a false sense of trust. We propose to improve XAI from the vantage point of the user's trust by exploring a black-box model's latent feature space. We present an approach, ProtoShotXAI, that uses a Prototypical few-shot network to explore the contrastive manifold between nonlinear features of different classes. A user explores the manifold by perturbing the input features of a query sample and recording the response for a subset of exemplars from any class. Our approach is the first locally interpretable XAI model that can be extended to, and demonstrated on, few-shot networks. We compare ProtoShotXAI to the state-of-the-art XAI approaches on MNIST, Omniglot, and ImageNet to demonstrate, both quantitatively and qualitatively, that ProtoShotXAI provides more flexibility for model exploration. Finally, ProtoShotXAI also demonstrates novel explainabilty and detectabilty on adversarial samples. △ Less","26 September, 2022",https://arxiv.org/pdf/2110.11597
Privacy in Open Search: A Review of Challenges and Solutions,Samuel Sousa;Christian Guetl;Roman Kern,"Privacy is of worldwide concern regarding activities and processes that include sensitive data. For this reason, many countries and territories have been recently approving regulations controlling the extent to which organizations may exploit data provided by people. Artificial intelligence areas, such as machine learning and natural language processing, have already successfully employed privacy-preserving mechanisms in order to safeguard data privacy in a vast number of applications. Information retrieval (IR) is likewise prone to privacy threats, such as attacks and unintended disclosures of documents and search history, which may cripple the security of users and be penalized by data protection laws. This work aims at highlighting and discussing open challenges for privacy in the recent literature of IR, focusing on tasks featuring user-generated text data. Our contribution is threefold: firstly, we present an overview of privacy threats to IR tasks; secondly, we discuss applicable privacy-preserving mechanisms which may be employed in solutions to restrain privacy hazards; finally, we bring insights on the tradeoffs between privacy preservation and utility performance for IR tasks. △ Less","4 April, 2022",https://arxiv.org/pdf/2110.10720
Discontinuous Grammar as a Foreign Language,Daniel Fernández-González;Carlos Gómez-Rodríguez,"In order to achieve deep natural language understanding, syntactic constituent parsing is a vital step, highly demanded by many artificial intelligence systems to process both text and speech. One of the most recent proposals is the use of standard sequence-to-sequence models to perform constituent parsing as a machine translation task, instead of applying task-specific parsers. While they show a competitive performance, these text-to-parse transducers are still lagging behind classic techniques in terms of accuracy, coverage and speed. To close the gap, we here extend the framework of sequence-to-sequence models for constituent parsing, not only by providing a more powerful neural architecture for improving their performance, but also by enlarging their coverage to handle the most complex syntactic phenomena: discontinuous structures. To that end, we design several novel linearizations that can fully produce discontinuities and, for the first time, we test a sequence-to-sequence model on the main discontinuous benchmarks, obtaining competitive results on par with task-specific discontinuous constituent parsers and achieving state-of-the-art scores on the (discontinuous) English Penn Treebank. △ Less","22 December, 2022",https://arxiv.org/pdf/2110.10431
"AI-Based Detection, Classification and Prediction/Prognosis in Medical Imaging: Towards Radiophenomics",Fereshteh Yousefirizi;Pierre Decazes;Amine Amyar;Su Ruan;Babak Saboury;Arman Rahmim,"Artificial intelligence (AI) techniques have significant potential to enable effective, robust and automated image phenotyping including identification of subtle patterns. AI-based detection searches the image space to find the regions of interest based on patterns and features. There is a spectrum of tumor histologies from benign to malignant that can be identified by AI-based classification approaches using image features. The extraction of minable information from images gives way to the field of radiomics and can be explored via explicit (handcrafted/engineered) and deep radiomics frameworks. Radiomics analysis has the potential to be utilized as a noninvasive technique for the accurate characterization of tumors to improve diagnosis and treatment monitoring. This work reviews AI-based techniques, with a special focus on oncological PET and PET/CT imaging, for different detection, classification, and prediction/prognosis tasks. We also discuss needed efforts to enable the translation of AI techniques to routine clinical workflows, and potential improvements and complementary techniques such as the use of natural language processing on electronic health records and neuro-symbolic AI techniques. △ Less","13 January, 2022",https://arxiv.org/pdf/2110.10332
Result Diversification by Multi-objective Evolutionary Algorithms with Theoretical Guarantees,Chao Qian;Dan-Xuan Liu;Zhi-Hua Zhou,"Given a ground set of items, the result diversification problem aims to select a subset with high ""quality"" and ""diversity"" while satisfying some constraints. It arises in various real-world artificial intelligence applications, such as web-based search, document summarization and feature selection, and also has applications in other areas, e.g., computational geometry, databases, finance and operations research. Previous algorithms are mainly based on greedy or local search. In this paper, we propose to reformulate the result diversification problem as a bi-objective maximization problem, and solve it by a multi-objective evolutionary algorithm (EA), i.e., the GSEMO. We theoretically prove that the GSEMO can achieve the (asymptotically) optimal theoretical guarantees under both static and dynamic environments. For cardinality constraints, the GSEMO can achieve the optimal polynomial-time approximation ratio, 1/2. For more general matroid constraints, the GSEMO can achieve an asymptotically optimal polynomial-time approximation ratio, 1/2-ε/(4n), where ε>0 and n is the size of the ground set of items. Furthermore, when the objective function (i.e., a linear combination of quality and diversity) changes dynamically, the GSEMO can maintain this approximation ratio in polynomial running time, addressing the open question proposed by Borodin. This also theoretically shows the superiority of EAs over local search for solving dynamic optimization problems for the first time, and discloses the robustness of the mutation operator of EAs against dynamic changes. Experiments on the applications of web-based search, multi-label feature selection and document summarization show the superior performance of the GSEMO over the state-of-the-art algorithms (i.e., the greedy algorithm and local search) under both static and dynamic environments. △ Less","7 May, 2022",https://arxiv.org/pdf/2110.09332
Towards a Systematic Survey for Carbon Neutral Data Centers,Zhiwei Cao;Xin Zhou;Han Hu;Zhi Wang;Yonggang Wen,"Data centers are carbon-intensive enterprises due to their massive energy consumption, and it is estimated that data center industry will account for 8\% of global carbon emissions by 2030. However, both technological and policy instruments for reducing or even neutralizing data center carbon emissions have not been thoroughly investigated. To bridge this gap, this survey paper proposes a roadmap towards carbon-neutral data centers that takes into account both policy instruments and technological methodologies. We begin by presenting the carbon footprint of data centers, as well as some insights into the major sources of carbon emissions. Following that, carbon neutrality plans for major global cloud providers are discussed to summarize current industrial efforts in this direction. In what follows, we introduce the carbon market as a policy instrument to explain how to offset data center carbon emissions in a cost-efficient manner. On the technological front, we propose achieving carbon-neutral data centers by increasing renewable energy penetration, improving energy efficiency, and boosting energy circulation simultaneously. A comprehensive review of existing technologies on these three topics is elaborated subsequently. Based on this, a multi-pronged approach towards carbon neutrality is envisioned and a digital twin-powered industrial artificial intelligence (AI) framework is proposed to make this solution a reality. Furthermore, three key scientific challenges for putting such a framework in place are discussed. Finally, several applications for this framework are presented to demonstrate its enormous potential. △ Less","27 January, 2022",https://arxiv.org/pdf/2110.09284
Conformer-Based Self-Supervised Learning for Non-Speech Audio Tasks,Sangeeta Srivastava;Yun Wang;Andros Tjandra;Anurag Kumar;Chunxi Liu;Kritika Singh;Yatharth Saraf,"Representation learning from unlabeled data has been of major interest in artificial intelligence research. While self-supervised speech representation learning has been popular in the speech research community, very few works have comprehensively analyzed audio representation learning for non-speech audio tasks. In this paper, we propose a self-supervised audio representation learning method and apply it to a variety of downstream non-speech audio tasks. We combine the well-known wav2vec 2.0 framework, which has shown success in self-supervised learning for speech tasks, with parameter-efficient conformer architectures. Our self-supervised pre-training can reduce the need for labeled data by two-thirds. On the AudioSet benchmark, we achieve a mean average precision (mAP) score of 0.415, which is a new state-of-the-art on this dataset through audio-only self-supervised learning. Our fine-tuned conformers also surpass or match the performance of previous systems pre-trained in a supervised way on several downstream tasks. We further discuss the important design considerations for both pre-training and fine-tuning. △ Less","6 January, 2022",https://arxiv.org/pdf/2110.07313
"Interpretable AI forecasting for numerical relativity waveforms of quasi-circular, spinning, non-precessing binary black hole mergers",Asad Khan;E. A. Huerta;Huihuo Zheng,"We present a deep-learning artificial intelligence model that is capable of learning and forecasting the late-inspiral, merger and ringdown of numerical relativity waveforms that describe quasi-circular, spinning, non-precessing binary black hole mergers. We used the NRHybSur3dq8 surrogate model to produce train, validation and test sets of \ell=|m|=2 waveforms that cover the parameter space of binary black hole mergers with mass-ratios q\leq8 and individual spins |s^z_{\{1,2\}}| \leq 0.8. These waveforms cover the time range t\in[-5000\textrm{M}, 130\textrm{M}], where t=0M marks the merger event, defined as the maximum value of the waveform amplitude. We harnessed the ThetaGPU supercomputer at the Argonne Leadership Computing Facility to train our AI model using a training set of 1.5 million waveforms. We used 16 NVIDIA DGX A100 nodes, each consisting of 8 NVIDIA A100 Tensor Core GPUs and 2 AMD Rome CPUs, to fully train our model within 3.5 hours. Our findings show that artificial intelligence can accurately forecast the dynamical evolution of numerical relativity waveforms in the time range t\in[-100\textrm{M}, 130\textrm{M}]. Sampling a test set of 190,000 waveforms, we find that the average overlap between target and predicted waveforms is \gtrsim99\% over the entire parameter space under consideration. We also combined scientific visualization and accelerated computing to identify what components of our model take in knowledge from the early and late-time waveform evolution to accurately forecast the latter part of numerical relativity waveforms. This work aims to accelerate the creation of scalable, computationally efficient and interpretable artificial intelligence models for gravitational wave astrophysics. △ Less","17 January, 2022",https://arxiv.org/pdf/2110.06968
Compositional Generalization in Dependency Parsing,Emily Goodwin;Siva Reddy;Timothy J. O'Donnell;Dzmitry Bahdanau,"Compositionality -- the ability to combine familiar units like words into novel phrases and sentences -- has been the focus of intense interest in artificial intelligence in recent years. To test compositional generalization in semantic parsing, Keysers et al. (2020) introduced Compositional Freebase Queries (CFQ). This dataset maximizes the similarity between the test and train distributions over primitive units, like words, while maximizing the compound divergence: the dissimilarity between test and train distributions over larger structures, like phrases. Dependency parsing, however, lacks a compositional generalization benchmark. In this work, we introduce a gold-standard set of dependency parses for CFQ, and use this to analyze the behavior of a state-of-the art dependency parser (Qi et al., 2020) on the CFQ dataset. We find that increasing compound divergence degrades dependency parsing performance, although not as dramatically as semantic parsing performance. Additionally, we find the performance of the dependency parser does not uniformly degrade relative to compound divergence, and the parser performs differently on different splits with the same compound divergence. We explore a number of hypotheses for what causes the non-uniform degradation in dependency parsing performance, and identify a number of syntactic structures that drive the dependency parser's lower performance on the most challenging splits. △ Less","15 March, 2022",https://arxiv.org/pdf/2110.06843
Planning from Pixels in Environments with Combinatorially Hard Search Spaces,Marco Bagatella;Mirek Olšák;Michal Rolínek;Georg Martius,"The ability to form complex plans based on raw visual input is a litmus test for current capabilities of artificial intelligence, as it requires a seamless combination of visual processing and abstract algorithmic execution, two traditionally separate areas of computer science. A recent surge of interest in this field brought advances that yield good performance in tasks ranging from arcade games to continuous control; these methods however do not come without significant issues, such as limited generalization capabilities and difficulties when dealing with combinatorially hard planning instances. Our contribution is two-fold: (i) we present a method that learns to represent its environment as a latent graph and leverages state reidentification to reduce the complexity of finding a good policy from exponential to linear (ii) we introduce a set of lightweight environments with an underlying discrete combinatorial structure in which planning is challenging even for humans. Moreover, we show that our methods achieves strong empirical generalization to variations in the environment, even across highly disadvantaged regimes, such as ""one-shot"" planning, or in an offline RL paradigm which only provides low-quality trajectories. △ Less","18 March, 2022",https://arxiv.org/pdf/2110.06149
Humans' Assessment of Robots as Moral Regulators: Importance of Perceived Fairness and Legitimacy,Boyoung Kim;Elizabeth Phillips,"Previous research has shown that the fairness and the legitimacy of a moral decision-maker are important for people's acceptance of and compliance with the decision-maker. As technology rapidly advances, there have been increasing hopes and concerns about building artificially intelligent entities that are designed to intervene against norm violations. However, it is unclear how people would perceive artificial moral regulators that impose punishment on human wrongdoers. Grounded in theories of psychology and law, we predict that the perceived fairness of punishment imposed by a robot would increase the legitimacy of the robot functioning as a moral regulator, which would in turn, increase people's willingness to accept and comply with the robot's decisions. We close with a conceptual framework for building a robot moral regulator that successfully can regulate norm violations. △ Less","7 October, 2022",https://arxiv.org/pdf/2110.04729
Can AI detect pain and express pain empathy? A review from emotion recognition and a human-centered AI perspective,Siqi Cao;Di Fu;Xu Yang;Stefan Wermter;Xun Liu;Haiyan Wu,"Sensory and emotional experiences such as pain and empathy are essential for mental and physical health. Cognitive neuroscience has been working on revealing mechanisms underlying pain and empathy. Furthermore, as trending research areas, computational pain recognition and empathic artificial intelligence (AI) show progress and promise for healthcare or human-computer interaction. Although AI research has recently made it increasingly possible to create artificial systems with affective processing, most cognitive neuroscience and AI research do not jointly address the issues of empathy in AI and cognitive neuroscience. The main aim of this paper is to introduce key advances, cognitive challenges and technical barriers in computational pain recognition and the implementation of artificial empathy. Our discussion covers the following topics: How can AI recognize pain from unimodal and multimodal information? Is it crucial for AI to be empathic? What are the benefits and challenges of empathic AI? Despite some consensus on the importance of AI, including empathic recognition and responses, we also highlight future challenges for artificial empathy and possible paths from interdisciplinary perspectives. Furthermore, we discuss challenges for responsible evaluation of cognitive methods and computational techniques and show approaches to future work to contribute to affective assistants capable of empathy. △ Less","1 December, 2022",https://arxiv.org/pdf/2110.04249
Empowering Local Communities Using Artificial Intelligence,Yen-Chia Hsu;Ting-Hao 'Kenneth' Huang;Himanshu Verma;Andrea Mauri;Illah Nourbakhsh;Alessandro Bozzon,"Artificial Intelligence (AI) is increasingly used to analyze large amounts of data in various practices, such as object recognition. We are specifically interested in using AI-powered systems to engage local communities in developing plans or solutions for pressing societal and environmental concerns. Such local contexts often involve multiple stakeholders with different and even contradictory agendas, resulting in mismatched expectations of these systems' behaviors and desired outcomes. There is a need to investigate if AI models and pipelines can work as expected in different contexts through co-creation and field deployment. Based on case studies in co-creating AI-powered systems with local people, we explain challenges that require more attention and provide viable paths to bridge AI research with citizen needs. We advocate for developing new collaboration approaches and mindsets that are needed to co-create AI-powered systems in multi-stakeholder contexts to address local concerns. △ Less","26 April, 2022",https://arxiv.org/pdf/2110.02007
Trustworthy AI: From Principles to Practices,Bo Li;Peng Qi;Bo Liu;Shuai Di;Jingen Liu;Jiquan Pei;Jinfeng Yi;Bowen Zhou,"The rapid development of Artificial Intelligence (AI) technology has enabled the deployment of various systems based on it. However, many current AI systems are found vulnerable to imperceptible attacks, biased against underrepresented groups, lacking in user privacy protection. These shortcomings degrade user experience and erode people's trust in all AI systems. In this review, we provide AI practitioners with a comprehensive guide for building trustworthy AI systems. We first introduce the theoretical framework of important aspects of AI trustworthiness, including robustness, generalization, explainability, transparency, reproducibility, fairness, privacy preservation, and accountability. To unify currently available but fragmented approaches toward trustworthy AI, we organize them in a systematic approach that considers the entire lifecycle of AI systems, ranging from data acquisition to model development, to system development and deployment, finally to continuous monitoring and governance. In this framework, we offer concrete action items for practitioners and societal stakeholders (e.g., researchers, engineers, and regulators) to improve AI trustworthiness. Finally, we identify key opportunities and challenges for the future development of trustworthy AI systems, where we identify the need for a paradigm shift toward comprehensively trustworthy AI systems. △ Less","26 May, 2022",https://arxiv.org/pdf/2110.01167
Exploration of Artificial Intelligence-oriented Power System Dynamic Simulators,Tannan Xiao;Ying Chen;Jianquan Wang;Shaowei Huang;Weilin Tong;Tirui He,"With the rapid development of artificial intelligence (AI), it is foreseeable that the accuracy and efficiency of dynamic analysis for future power system will be greatly improved by the integration of dynamic simulators and AI. To explore the interaction mechanism of power system dynamic simulations and AI, a general design of an AI-oriented power system dynamic simulator is proposed, which consists of a high-performance simulator with neural network supportability and flexible external and internal application programming interfaces (APIs). With the support of APIs, simulation-assisted AI and AI-assisted simulation form a comprehensive interaction mechanism between power system dynamic simulations and AI. A prototype of this design is implemented and made public based on a highly efficient electromechanical simulator. Tests of this prototype are carried out under four scenarios including sample generation, AI-based stability prediction, data-driven dynamic component modeling, and AI-aided stability control, which prove the validity, flexibility, and efficiency of the design and implementation of the AI-oriented power system dynamic simulator. △ Less","6 July, 2022",https://arxiv.org/pdf/2110.00931
Algorithm Fairness in AI for Medicine and Healthcare,Richard J. Chen;Tiffany Y. Chen;Jana Lipkova;Judy J. Wang;Drew F. K. Williamson;Ming Y. Lu;Sharifa Sahai;Faisal Mahmood,"In the current development and deployment of many artificial intelligence (AI) systems in healthcare, algorithm fairness is a challenging problem in delivering equitable care. Recent evaluation of AI models stratified across race sub-populations have revealed inequalities in how patients are diagnosed, given treatments, and billed for healthcare costs. In this perspective article, we summarize the intersectional field of fairness in machine learning through the context of current issues in healthcare, outline how algorithmic biases (e.g. - image acquisition, genetic variation, intra-observer labeling variability) arise in current clinical workflows and their resulting healthcare disparities. Lastly, we also review emerging technology for mitigating bias via federated learning, disentanglement, and model explainability, and their role in AI-SaMD development. △ Less","23 March, 2022",https://arxiv.org/pdf/2110.00603
A survey on datasets for fairness-aware machine learning,Tai Le Quy;Arjun Roy;Vasileios Iosifidis;Wenbin Zhang;Eirini Ntoutsi,"As decision-making increasingly relies on Machine Learning (ML) and (big) data, the issue of fairness in data-driven Artificial Intelligence (AI) systems is receiving increasing attention from both research and industry. A large variety of fairness-aware machine learning solutions have been proposed which involve fairness-related interventions in the data, learning algorithms and/or model outputs. However, a vital part of proposing new approaches is evaluating them empirically on benchmark datasets that represent realistic and diverse settings. Therefore, in this paper, we overview real-world datasets used for fairness-aware machine learning. We focus on tabular data as the most common data representation for fairness-aware machine learning. We start our analysis by identifying relationships between the different attributes, particularly w.r.t. protected attributes and class attribute, using a Bayesian network. For a deeper understanding of bias in the datasets, we investigate the interesting relationships using exploratory analysis. △ Less","21 January, 2022",https://arxiv.org/pdf/2110.00530
Predicting Flat-Fading Channels via Meta-Learned Closed-Form Linear Filters and Equilibrium Propagation,Sangwoo Park;Osvaldo Simeone,"Predicting fading channels is a classical problem with a vast array of applications, including as an enabler of artificial intelligence (AI)-based proactive resource allocation for cellular networks. Under the assumption that the fading channel follows a stationary complex Gaussian process, as for Rayleigh and Rician fading models, the optimal predictor is linear, and it can be directly computed from the Doppler spectrum via standard linear minimum mean squared error (LMMSE) estimation. However, in practice, the Doppler spectrum is unknown, and the predictor has only access to a limited time series of estimated channels. This paper proposes to leverage meta-learning in order to mitigate the requirements in terms of training data for channel fading prediction. Specifically, it first develops an offline low-complexity solution based on linear filtering via a meta-trained quadratic regularization. Then, an online method is proposed based on gradient descent and equilibrium propagation (EP). Numerical results demonstrate the advantages of the proposed approach, showing its capacity to approach the genie-aided LMMSE solution with a small number of training data points. △ Less","6 March, 2022",https://arxiv.org/pdf/2110.00414
Automatic Discovery and Description of Human Planning Strategies,Julian Skirzynski;Yash Raj Jain;Falk Lieder,"Scientific discovery concerns finding patterns in data and creating insightful hypotheses that explain these patterns. Traditionally, this process required human ingenuity, but with the galloping advances in artificial intelligence (AI) it becomes feasible to automate some parts of scientific discovery. In this work we leverage AI for strategy discovery for understanding human planning. In the state-of-the-art methods data about the process of human planning is often used to group similar behaviors together and formulate verbal descriptions of the strategies which might underlie those groups. Here, we automate these two steps. Our method utilizes a new algorithm, called Human-Interpret, that performs imitation learning to describe sequences of planning operations in terms of a procedural formula and then translates that formula to natural language. We test our method on a benchmark data set that researchers have previously scrutinized manually. We find that the descriptions of human planning strategies obtained automatically are about as understandable as human-generated descriptions. They also cover a substantial proportion of of relevant types of human planning strategies that had been discovered manually. Our method saves scientists' time and effort as all the reasoning about human planning is done automatically. This might make it feasible to more rapidly scale up the search for yet undiscovered cognitive strategies to many new decision environments, populations, tasks, and domains. Given these results, we believe that the presented work may accelerate scientific discovery in psychology, and due to its generality, extend to problems from other fields. △ Less","28 October, 2022",https://arxiv.org/pdf/2109.14493
KITTI-360: A Novel Dataset and Benchmarks for Urban Scene Understanding in 2D and 3D,Yiyi Liao;Jun Xie;Andreas Geiger,"For the last few decades, several major subfields of artificial intelligence including computer vision, graphics, and robotics have progressed largely independently from each other. Recently, however, the community has realized that progress towards robust intelligent systems such as self-driving cars requires a concerted effort across the different fields. This motivated us to develop KITTI-360, successor of the popular KITTI dataset. KITTI-360 is a suburban driving dataset which comprises richer input modalities, comprehensive semantic instance annotations and accurate localization to facilitate research at the intersection of vision, graphics and robotics. For efficient annotation, we created a tool to label 3D scenes with bounding primitives and developed a model that transfers this information into the 2D image domain, resulting in over 150k images and 1B 3D points with coherent semantic instance annotations across 2D and 3D. Moreover, we established benchmarks and baselines for several tasks relevant to mobile perception, encompassing problems from computer vision, graphics, and robotics on the same dataset, e.g., semantic scene understanding, novel view synthesis and semantic SLAM. KITTI-360 will enable progress at the intersection of these research areas and thus contribute towards solving one of today's grand challenges: the development of fully autonomous self-driving systems. △ Less","3 June, 2022",https://arxiv.org/pdf/2109.13410
Design of quantum optical experiments with logic artificial intelligence,Alba Cervera-Lierta;Mario Krenn;Alán Aspuru-Guzik,"Logic Artificial Intelligence (AI) is a subfield of AI where variables can take two defined arguments, True or False, and are arranged in clauses that follow the rules of formal logic. Several problems that span from physical systems to mathematical conjectures can be encoded into these clauses and solved by checking their satisfiability (SAT). In contrast to machine learning approaches where the results can be approximations or local minima, Logic AI delivers formal and mathematically exact solutions to those problems. In this work, we propose the use of logic AI for the design of optical quantum experiments. We show how to map into a SAT problem the experimental preparation of an arbitrary quantum state and propose a logic-based algorithm, called Klaus, to find an interpretable representation of the photonic setup that generates it. We compare the performance of Klaus with the state-of-the-art algorithm for this purpose based on continuous optimization. We also combine both logic and numeric strategies to find that the use of logic AI significantly improves the resolution of this problem, paving the path to developing more formal-based approaches in the context of quantum physics experiments. △ Less","7 October, 2022",https://arxiv.org/pdf/2109.13273
Towards A Measure Of General Machine Intelligence,Gautham Venkatasubramanian;Sibesh Kar;Abhimanyu Singh;Shubham Mishra;Dushyant Yadav;Shreyansh Chandak,"To build general-purpose artificial intelligence systems that can deal with unknown variables across unknown domains, we need benchmarks that measure how well these systems perform on tasks they have never seen before. A prerequisite for this is a measure of a task's generalization difficulty, or how dissimilar it is from the system's prior knowledge and experience. If the skill of an intelligence system in a particular domain is defined as it's ability to consistently generate a set of instructions (or programs) to solve tasks in that domain, current benchmarks do not quantitatively measure the efficiency of acquiring new skills, making it possible to brute-force skill acquisition by training with unlimited amounts of data and compute power. With this in mind, we first propose a common language of instruction, a programming language that allows the expression of programs in the form of directed acyclic graphs across a wide variety of real-world domains and computing platforms. Using programs generated in this language, we demonstrate a match-based method to both score performance and calculate the generalization difficulty of any given set of tasks. We use these to define a numeric benchmark called the generalization index, or the g-index, to measure and compare the skill-acquisition efficiency of any intelligence system on a set of real-world tasks. Finally, we evaluate the suitability of some well-known models as general intelligence systems by calculating their g-index scores. △ Less","24 May, 2022",https://arxiv.org/pdf/2109.12075
Meta-brain Models: biologically-inspired cognitive agents,Bradly Alicea;Jesse Parent,"Artificial Intelligence (AI) systems based solely on neural networks or symbolic computation present a representational complexity challenge. While minimal representations can produce behavioral outputs like locomotion or simple decision-making, more elaborate internal representations might offer a richer variety of behaviors. We propose that these issues can be addressed with a computational approach we call meta-brain models. Meta-brain models are embodied hybrid models that include layered components featuring varying degrees of representational complexity. We will propose combinations of layers composed using specialized types of models. Rather than using a generic black box approach to unify each component, this relationship mimics systems like the neocortical-thalamic system relationship of the mammalian brain, which utilizes both feedforward and feedback connectivity to facilitate functional communication. Importantly, the relationship between layers can be made anatomically explicit. This allows for structural specificity that can be incorporated into the model's function in interesting ways. We will propose several types of layers that might be functionally integrated into agents that perform unique types of tasks, from agents that simultaneously perform morphogenesis and perception, to agents that undergo morphogenesis and the acquisition of conceptual representations simultaneously. Our approach to meta-brain models involves creating models with different degrees of representational complexity, creating a layered meta-architecture that mimics the structural and functional heterogeneity of biological brains, and an input/output methodology flexible enough to accommodate cognitive functions, social interactions, and adaptive behaviors more generally. We will conclude by proposing next steps in the development of this flexible and open-source approach. △ Less","16 June, 2022",https://arxiv.org/pdf/2109.11938
Paving the Way for Distributed Artificial Intelligence over the Air,Guoqing Ma;Shuping Dang;Chuanting Zhang;Basem Shihada,"Distributed Artificial Intelligence (DAI) is regarded as one of the most promising techniques to provide intelligent services under strict privacy protection regulations for multiple clients. By applying DAI, training on raw data is carried out locally, while the trained outputs, e.g., model parameters, from multiple local clients, are sent back to a central server for aggregation. Recently, for achieving better practicality, DAI is studied in conjunction with wireless communication networks, incorporating various random effects brought by wireless channels. However, because of the complex and case-dependent nature of wireless channels, a generic simulator for applying DAI in wireless communication networks is still lacking. To accelerate the development of DAI applied in wireless communication networks, we propose a generic system design in this paper as well as an associated simulator that can be set according to wireless channels and system-level configurations. Details of the system design and analysis of the impacts of wireless environments are provided to facilitate further implementations and updates. We employ a series of experiments to verify the effectiveness and efficiency of the proposed system design and reveal its superior scalability. △ Less","21 January, 2022",https://arxiv.org/pdf/2109.11774
Internet of Behavior (IoB) and Explainable AI Systems for Influencing IoT Behavior,Haya Elayan;Moayad Aloqaily;Fakhri Karray;Mohsen Guizani,"Pandemics and natural disasters over the years have changed the behavior of people, which has had a tremendous impact on all life aspects. With the technologies available in each era, governments, organizations, and companies have used these technologies to track, control, and influence the behavior of individuals for a benefit. Nowadays, the use of the Internet of Things (IoT), cloud computing, and artificial intelligence (AI) have made it easier to track and change the behavior of users through changing IoT behavior. This article introduces and discusses the concept of the Internet of Behavior (IoB) and its integration with Explainable AI (XAI) techniques to provide trusted and evident experience in the process of changing IoT behavior to ultimately improving users' behavior. Therefore, a system based on IoB and XAI has been proposed in a use case scenario of electrical power consumption that aims to influence user consuming behavior to reduce power consumption and cost. The scenario results showed a decrease of 522.2 kW of active power when compared to original consumption over a 200-hours period. It also showed a total power cost saving of 95.04 Euro for the same period. Moreover, decreasing the global active power will reduce the power intensity through the positive correlation. △ Less","10 May, 2022",https://arxiv.org/pdf/2109.07239
Multihop: Leveraging Complex Models to Learn Accurate Simple Models,Amit Dhurandhar;Tejaswini Pedapati,"Knowledge transfer from a complex high performing model to a simpler and potentially low performing one in order to enhance its performance has been of great interest over the last few years as it finds applications in important problems such as explainable artificial intelligence, model compression, robust model building and learning from small data. Known approaches to this problem (viz. Knowledge Distillation, Model compression, ProfWeight, etc.) typically transfer information directly (i.e. in a single/one hop) from the complex model to the chosen simple model through schemes that modify the target or reweight training examples on which the simple model is trained. In this paper, we propose a meta-approach where we transfer information from the complex model to the simple model by dynamically selecting and/or constructing a sequence of intermediate models of decreasing complexity that are less intricate than the original complex model. Our approach can transfer information between consecutive models in the sequence using any of the previously mentioned approaches as well as work in 1-hop fashion, thus generalizing these approaches. In the experiments on real data, we observe that we get consistent gains for different choices of models over 1-hop, which on average is more than 2\% and reaches up to 8\% in a particular case. We also empirically analyze conditions under which the multi-hop approach is likely to be beneficial over the traditional 1-hop approach, and report other interesting insights. To the best of our knowledge, this is the first work that proposes such a multi-hop approach to perform knowledge transfer given a single high performing complex model, making it in our opinion, an important methodological contribution. △ Less","8 September, 2022",https://arxiv.org/pdf/2109.06961
An Apparatus for the Simulation of Breathing Disorders: Physically Meaningful Generation of Surrogate Data,Harry J. Davies;Ghena Hammour;Hongjian Xiao;Danilo P. Mandic,"The rapidly increasing prevalence of debilitating breathing disorders, such as chronic obstructive pulmonary disease (COPD), calls for a meaningful integration of artificial intelligence (AI) into healthcare. While this promises improved detection and monitoring of breathing disorders, AI techniques are almost invariably ""data hungry"" which highlights the importance of generating physically meaningful surrogate data. Indeed, domain aware surrogates would enable both an improved understanding of respiratory waveform changes with different breathing disorders, and enhance the training of machine learning algorithms. To this end, we introduce an apparatus comprising of PVC tubes and 3D printed parts as a simple yet effective method of simulating both obstructive and restrictive respiratory waveforms in healthy subjects. Independent control over both inspiratory and expiratory resistances allows for the simulation of obstructive breathing disorders through the whole spectrum of FEV1/FVC spirometry ratios (used to classify COPD), ranging from healthy values to values seen in severe chronic obstructive pulmonary disease. Moreover, waveform characteristics of breathing disorders, such as a change in inspiratory duty cycle or peak flow are also observed in the waveforms resulting from use of the artificial breathing disorder simulation apparatus. Overall, the proposed apparatus provides us with a simple, effective and physically meaningful way to generate faithful surrogate breathing disorder waveforms, a prerequisite for the use of artificial intelligence in respiratory health. △ Less","6 October, 2022",https://arxiv.org/pdf/2109.06699
Improving the Robustness of Adversarial Attacks Using an Affine-Invariant Gradient Estimator,Wenzhao Xiang;Hang Su;Chang Liu;Yandong Guo;Shibao Zheng,"As designers of artificial intelligence try to outwit hackers, both sides continue to hone in on AI's inherent vulnerabilities. Designed and trained from certain statistical distributions of data, AI's deep neural networks (DNNs) remain vulnerable to deceptive inputs that violate a DNN's statistical, predictive assumptions. Before being fed into a neural network, however, most existing adversarial examples cannot maintain malicious functionality when applied to an affine transformation. For practical purposes, maintaining that malicious functionality serves as an important measure of the robustness of adversarial attacks. To help DNNs learn to defend themselves more thoroughly against attacks, we propose an affine-invariant adversarial attack, which can consistently produce more robust adversarial examples over affine transformations. For efficiency, we propose to disentangle current affine-transformation strategies from the Euclidean geometry coordinate plane with its geometric translations, rotations and dilations; we reformulate the latter two in polar coordinates. Afterwards, we construct an affine-invariant gradient estimator by convolving the gradient at the original image with derived kernels, which can be integrated with any gradient-based attack methods. Extensive experiments on ImageNet, including some experiments under physical condition, demonstrate that our method can significantly improve the affine invariance of adversarial examples and, as a byproduct, improve the transferability of adversarial examples, compared with alternative state-of-the-art methods. △ Less","22 April, 2022",https://arxiv.org/pdf/2109.05820
Saliency Guided Experience Packing for Replay in Continual Learning,Gobinda Saha;Kaushik Roy,"Artificial learning systems aspire to mimic human intelligence by continually learning from a stream of tasks without forgetting past knowledge. One way to enable such learning is to store past experiences in the form of input examples in episodic memory and replay them when learning new tasks. However, performance of such method suffers as the size of the memory becomes smaller. In this paper, we propose a new approach for experience replay, where we select the past experiences by looking at the saliency maps which provide visual explanations for the model's decision. Guided by these saliency maps, we pack the memory with only the parts or patches of the input images important for the model's prediction. While learning a new task, we replay these memory patches with appropriate zero-padding to remind the model about its past decisions. We evaluate our algorithm on CIFAR-100, miniImageNet and CUB datasets and report better performance than the state-of-the-art approaches. With qualitative and quantitative analyses we show that our method captures richer summaries of past experiences without any memory increase, and hence performs well with small episodic memory. △ Less","12 October, 2022",https://arxiv.org/pdf/2109.04954
EmoWOZ: A Large-Scale Corpus and Labelling Scheme for Emotion Recognition in Task-Oriented Dialogue Systems,Shutong Feng;Nurul Lubis;Christian Geishauser;Hsien-chin Lin;Michael Heck;Carel van Niekerk;Milica Gašić,"The ability to recognise emotions lends a conversational artificial intelligence a human touch. While emotions in chit-chat dialogues have received substantial attention, emotions in task-oriented dialogues remain largely unaddressed. This is despite emotions and dialogue success having equally important roles in a natural system. Existing emotion-annotated task-oriented corpora are limited in size, label richness, and public availability, creating a bottleneck for downstream tasks. To lay a foundation for studies on emotions in task-oriented dialogues, we introduce EmoWOZ, a large-scale manually emotion-annotated corpus of task-oriented dialogues. EmoWOZ is based on MultiWOZ, a multi-domain task-oriented dialogue dataset. It contains more than 11K dialogues with more than 83K emotion annotations of user utterances. In addition to Wizard-of-Oz dialogues from MultiWOZ, we collect human-machine dialogues within the same set of domains to sufficiently cover the space of various emotions that can happen during the lifetime of a data-driven dialogue system. To the best of our knowledge, this is the first large-scale open-source corpus of its kind. We propose a novel emotion labelling scheme, which is tailored to task-oriented dialogues. We report a set of experimental results to show the usability of this corpus for emotion recognition and state tracking in task-oriented dialogues. △ Less","2 May, 2022",https://arxiv.org/pdf/2109.04919
Bootstrapped Meta-Learning,Sebastian Flennerhag;Yannick Schroecker;Tom Zahavy;Hado van Hasselt;David Silver;Satinder Singh,"Meta-learning empowers artificial intelligence to increase its efficiency by learning how to learn. Unlocking this potential involves overcoming a challenging meta-optimisation problem. We propose an algorithm that tackles this problem by letting the meta-learner teach itself. The algorithm first bootstraps a target from the meta-learner, then optimises the meta-learner by minimising the distance to that target under a chosen (pseudo-)metric. Focusing on meta-learning with gradients, we establish conditions that guarantee performance improvements and show that the metric can control meta-optimisation. Meanwhile, the bootstrapping mechanism can extend the effective meta-learning horizon without requiring backpropagation through all updates. We achieve a new state-of-the art for model-free agents on the Atari ALE benchmark and demonstrate that it yields both performance and efficiency gains in multi-task meta-learning. Finally, we explore how bootstrapping opens up new possibilities and find that it can meta-learn efficient exploration in an epsilon-greedy Q-learning agent, without backpropagating through the update rule. △ Less","16 March, 2022",https://arxiv.org/pdf/2109.04504
EvilModel 2.0: Bringing Neural Network Models into Malware Attacks,Zhi Wang;Chaoge Liu;Xiang Cui;Jie Yin;Xutong Wang,"Security issues have gradually emerged with the continuous development of artificial intelligence (AI). Earlier work verified the possibility of converting neural network models into stegomalware, embedding malware into a model with limited impact on the model's performance. However, existing methods are not applicable in real-world attack scenarios and do not attract enough attention from the security community due to performance degradation and additional workload. Therefore, we propose an improved stegomalware EvilModel. By analyzing the composition of the neural network model, three new methods for embedding malware into the model are proposed: MSB reservation, fast substitution, and half substitution, which can embed malware that accounts for half of the model's volume without affecting the model's performance. We built 550 EvilModels using ten mainstream neural network models and 19 malware samples. The experiment shows that EvilModel achieved an embedding rate of 48.52\%. A quantitative algorithm is proposed to evaluate the existing embedding methods. We also design a trigger and propose a threat scenario for the targeted attack. The practicality and effectiveness of the proposed methods were demonstrated by experiments and analyses of the embedding capacity, performance impact, and detection evasion. △ Less","28 June, 2022",https://arxiv.org/pdf/2109.04344
First Responders Got Wings: UAVs to the Rescue of Localization Operations in Beyond 5G Systems,Antonio Albanese;Vincenzo Sciancalepore;Xavier Costa-Pérez,"Natural and human-made disasters have dramatically increased during the last decades. Given the strong relationship between first responders localization time and the final number of deaths, the modernization of search-and-rescue operations has become imperative. In this context, Unmanned Aerial Vehicles (UAVs)-based solutions are the most promising candidates to take up on the localization challenge by leveraging on emerging technologies such as: Artificial Intelligence (AI), Reconfigurable Intelligent Surfaces (RIS) and Orthogonal Time Frequency Space (OTFS) modulations. In this paper, we capitalize on such recently available techniques by shedding light on the main challenges and future opportunities to boost the localization performance of state-of-the-art techniques to give birth to unprecedentedly effective missing victims localization solutions. △ Less","17 February, 2022",https://arxiv.org/pdf/2109.03180
A brief history of AI: how to prevent another winter (a critical review),Amirhosein Toosi;Andrea Bottino;Babak Saboury;Eliot Siegel;Arman Rahmim,"The field of artificial intelligence (AI), regarded as one of the most enigmatic areas of science, has witnessed exponential growth in the past decade including a remarkably wide array of applications, having already impacted our everyday lives. Advances in computing power and the design of sophisticated AI algorithms have enabled computers to outperform humans in a variety of tasks, especially in the areas of computer vision and speech recognition. Yet, AI's path has never been smooth, having essentially fallen apart twice in its lifetime ('winters' of AI), both after periods of popular success ('summers' of AI). We provide a brief rundown of AI's evolution over the course of decades, highlighting its crucial moments and major turning points from inception to the present. In doing so, we attempt to learn, anticipate the future, and discuss what steps may be taken to prevent another 'winter'. △ Less","8 December, 2022",https://arxiv.org/pdf/2109.01517
Assessing domain adaptation techniques for mitosis detection in multi-scanner breast cancer histopathology images,Jack Breen;Kieran Zucker;Nicolas M. Orsi;Nishant Ravikumar,"Breast cancer is the most commonly diagnosed cancer worldwide, with over two million new cases each year. During diagnostic tumour grading, pathologists manually count the number of dividing cells (mitotic figures) in biopsy or tumour resection specimens. Since the process is subjective and time-consuming, data-driven artificial intelligence (AI) methods have been developed to automatically detect mitotic figures. However, these methods often generalise poorly, with performance reduced by variations in tissue types, staining protocols, or the scanners used to digitise whole-slide images. Domain adaptation approaches have been adopted in various applications to mitigate this issue of domain shift. We evaluate two unsupervised domain adaptation methods, CycleGAN and Neural Style Transfer, using the MIDOG 2021 Challenge dataset. This challenge focuses on detecting mitotic figures in whole-slide images digitised using different scanners. Two baseline mitosis detection models based on U-Net and RetinaNet were investigated in combination with the aforementioned domain adaptation methods. Both baseline models achieved human expert level performance, but had reduced performance when evaluated on images which had been digitised using a different scanner. The domain adaptation techniques were each found to be beneficial for detection with data from some scanners but not for others, with the only average increase across all scanners being achieved by CycleGAN on the RetinaNet detector. These techniques require further refinement to ensure consistency in mitosis detection. △ Less","18 January, 2022",https://arxiv.org/pdf/2109.00869
Impossibility Results in AI: A Survey,Mario Brcic;Roman V. Yampolskiy,"An impossibility theorem demonstrates that a particular problem or set of problems cannot be solved as described in the claim. Such theorems put limits on what is possible to do concerning artificial intelligence, especially the super-intelligent one. As such, these results serve as guidelines, reminders, and warnings to AI safety, AI policy, and governance researchers. These might enable solutions to some long-standing questions in the form of formalizing theories in the framework of constraint satisfaction without committing to one option. We strongly believe this to be the most prudent approach to long-term AI safety initiatives. In this paper, we have categorized impossibility theorems applicable to AI into five mechanism-based categories: deduction, indistinguishability, induction, tradeoffs, and intractability. We found that certain theorems are too specific or have implicit assumptions that limit application. Also, we added new results (theorems) such as the unfairness of explainability, the first explainability-related result in the induction category. The remaining results deal with misalignment between the clones and put a limit to the self-awareness of agents. We concluded that deductive impossibilities deny 100%-guarantees for security. In the end, we give some ideas that hold potential in explainability, controllability, value alignment, ethics, and group decision-making. They can be deepened by further investigation. △ Less","19 February, 2022",https://arxiv.org/pdf/2109.00484
MEDIC: A Multi-Task Learning Dataset for Disaster Image Classification,Firoj Alam;Tanvirul Alam;Md. Arid Hasan;Abul Hasnat;Muhammad Imran;Ferda Ofli,"Recent research in disaster informatics demonstrates a practical and important use case of artificial intelligence to save human lives and suffering during natural disasters based on social media contents (text and images). While notable progress has been made using texts, research on exploiting the images remains relatively under-explored. To advance image-based approaches, we propose MEDIC (Available at: https://crisisnlp.qcri.org/medic/index.html), which is the largest social media image classification dataset for humanitarian response consisting of 71,198 images to address four different tasks in a multi-task learning setup. This is the first dataset of its kind: social media images, disaster response, and multi-task learning research. An important property of this dataset is its high potential to facilitate research on multi-task learning, which recently receives much interest from the machine learning community and has shown remarkable results in terms of memory, inference speed, performance, and generalization capability. Therefore, the proposed dataset is an important resource for advancing image-based disaster management and multi-task machine learning research. We experiment with different deep learning architectures and report promising results, which are above the majority baselines for all tasks. Along with the dataset, we also release all relevant scripts (https://github.com/firojalam/medic). △ Less","8 June, 2022",https://arxiv.org/pdf/2108.12828
Cascading Neural Network Methodology for Artificial Intelligence-Assisted Radiographic Detection and Classification of Lead-Less Implanted Electronic Devices within the Chest,Mutlu Demirer;Richard D. White;Vikash Gupta;Ronnie A. Sebro;Barbaros S. Erdal,"Background & Purpose: Chest X-Ray (CXR) use in pre-MRI safety screening for Lead-Less Implanted Electronic Devices (LLIEDs), easily overlooked or misidentified on a frontal view (often only acquired), is common. Although most LLIED types are ""MRI conditional"": 1. Some are stringently conditional; 2. Different conditional types have specific patient- or device- management requirements; and 3. Particular types are ""MRI unsafe"". This work focused on developing CXR interpretation-assisting Artificial Intelligence (AI) methodology with: 1. 100% detection for LLIED presence/location; and 2. High classification in LLIED typing. Materials & Methods: Data-mining (03/1993-02/2021) produced an AI Model Development Population (1,100 patients/4,871 images) creating 4,924 LLIED Region-Of-Interests (ROIs) (with image-quality grading) used in Training, Validation, and Testing. For developing the cascading neural network (detection via Faster R-CNN and classification via Inception V3), ""ground-truth"" CXR annotation (ROI labeling per LLIED), as well as inference display (as Generated Bounding Boxes (GBBs)), relied on a GPU-based graphical user interface. Results: To achieve 100% LLIED detection, probability threshold reduction to 0.00002 was required by Model 1, resulting in increasing GBBs per LLIED-related ROI. Targeting LLIED-type classification following detection of all LLIEDs, Model 2 multi-classified to reach high-performance while decreasing falsely positive GBBs. Despite 24% suboptimal ROI image quality, classification was correct in 98.9% and AUCs for the 9 LLIED-types were 1.00 for 8 and 0.92 for 1. For all misclassification cases: 1. None involved stringently conditional or unsafe LLIEDs; and 2. Most were attributable to suboptimal images. Conclusion: This project successfully developed a LLIED-related AI methodology supporting: 1. 100% detection; and 2. Typically 100% type classification. △ Less","26 April, 2022",https://arxiv.org/pdf/2108.11954
"""Look! It's a Computer Program! It's an Algorithm! It's AI!"": Does Terminology Affect Human Perceptions and Evaluations of Algorithmic Decision-Making Systems?",Markus Langer;Tim Hunsicker;Tina Feldkamp;Cornelius J. König;Nina Grgić-Hlača,"In the media, in policy-making, but also in research articles, algorithmic decision-making (ADM) systems are referred to as algorithms, artificial intelligence, and computer programs, amongst other terms. We hypothesize that such terminological differences can affect people's perceptions of properties of ADM systems, people's evaluations of systems in application contexts, and the replicability of research as findings may be influenced by terminological differences. In two studies (N = 397, N = 622), we show that terminology does indeed affect laypeople's perceptions of system properties (e.g., perceived complexity) and evaluations of systems (e.g., trust). Our findings highlight the need to be mindful when choosing terms to describe ADM systems, because terminology can have unintended consequences, and may impact the robustness and replicability of HCI research. Additionally, our findings indicate that terminology can be used strategically (e.g., in communication about ADM systems) to influence people's perceptions and evaluations of these systems. △ Less","26 May, 2022",https://arxiv.org/pdf/2108.11486
Adaptive Explainable Continual Learning Framework for Regression Problems with Focus on Power Forecasts,Yujiang He,"Compared with traditional deep learning techniques, continual learning enables deep neural networks to learn continually and adaptively. Deep neural networks have to learn new tasks and overcome forgetting the knowledge obtained from the old tasks as the amount of data keeps increasing in applications. In this article, two continual learning scenarios will be proposed to describe the potential challenges in this context. Besides, based on our previous work regarding the CLeaR framework, which is short for continual learning for regression tasks, the work will be further developed to enable models to extend themselves and learn data successively. Research topics are related but not limited to developing continual deep learning algorithms, strategies for non-stationarity detection in data streams, explainable and visualizable artificial intelligence, etc. Moreover, the framework- and algorithm-related hyperparameters should be dynamically updated in applications. Forecasting experiments will be conducted based on power generation and consumption data collected from real-world applications. A series of comprehensive evaluation metrics and visualization tools can help analyze the experimental results. The proposed framework is expected to be generally applied to other constantly changing scenarios. △ Less","7 February, 2022",https://arxiv.org/pdf/2108.10781
Select Wisely and Explain: Active Learning and Probabilistic Local Post-hoc Explainability,Aditya Saini;Ranjitha Prasad,"Albeit the tremendous performance improvements in designing complex artificial intelligence (AI) systems in data-intensive domains, the black-box nature of these systems leads to the lack of trustworthiness. Post-hoc interpretability methods explain the prediction of a black-box ML model for a single instance, and such explanations are being leveraged by domain experts to diagnose the underlying biases of these models. Despite their efficacy in providing valuable insights, existing approaches fail to deliver consistent and reliable explanations. In this paper, we propose an active learning-based technique called UnRAvEL (Uncertainty driven Robust Active Learning Based Locally Faithful Explanations), which consists of a novel acquisition function that is locally faithful and uses uncertainty-driven sampling based on the posterior distribution on the probabilistic locality using Gaussian process regression(GPR). We present a theoretical analysis of UnRAvEL by treating it as a local optimizer and analyzing its regret in terms of instantaneous regrets over a global optimizer. We demonstrate the efficacy of the local samples generated by UnRAvEL by incorporating different kernels such as the Matern and linear kernels in GPR. Through a series of experiments, we show that UnRAvEL outperforms the baselines with respect to stability and local fidelity on several real-world models and datasets. We show that UnRAvEL is an efficient surrogate dataset generator by deriving importance scores on this surrogate dataset using sparse linear models. We also showcase the sample efficiency and flexibility of the developed framework on the Imagenet dataset using a pre-trained ResNet model. △ Less","22 April, 2022",https://arxiv.org/pdf/2108.06907
PatrickStar: Parallel Training of Pre-trained Models via Chunk-based Memory Management,Jiarui Fang;Zilin Zhu;Shenggui Li;Hui Su;Yang Yu;Jie Zhou;Yang You,"The pre-trained model (PTM) is revolutionizing Artificial Intelligence (AI) technology. However, the hardware requirement of PTM training is prohibitively high, making it a game for a small proportion of people. Therefore, we proposed PatrickStar system to lower the hardware requirements of PTMs and make them accessible to everyone. PatrickStar uses the CPU-GPU heterogeneous memory space to store the model data. Different from existing works, we organize the model data in memory chunks and dynamically distribute them in the heterogeneous memory. Guided by the runtime memory statistics collected in a warm-up iteration, chunks are orchestrated efficiently in heterogeneous memory and generate lower CPU-GPU data transmission volume and higher bandwidth utilization. Symbiosis with the Zero Redundancy Optimizer, PatrickStar scales to multiple GPUs on multiple nodes. % using data parallelism. The system can train tasks on bigger models and larger batch sizes, which cannot be accomplished by existing works. Experimental results show that PatrickStar extends model scales 2.27 and 2.5 times of DeepSpeed, and consistently exhibits significantly higher execution speed. PatricStar also successfully runs the 175B GPT3 training task on a 32 GPU cluster. Our code is publicly available at https://github.com/Tencent/PatrickStar. △ Less","1 August, 2022",https://arxiv.org/pdf/2108.05818
Meta-repository of screening mammography classifiers,Benjamin Stadnick;Jan Witowski;Vishwaesh Rajiv;Jakub Chłędowski;Farah E. Shamout;Kyunghyun Cho;Krzysztof J. Geras,"Artificial intelligence (AI) is showing promise in improving clinical diagnosis. In breast cancer screening, recent studies show that AI has the potential to improve early cancer diagnosis and reduce unnecessary workup. As the number of proposed models and their complexity grows, it is becoming increasingly difficult to re-implement them. To enable reproducibility of research and to enable comparison between different methods, we release a meta-repository containing models for classification of screening mammograms. This meta-repository creates a framework that enables the evaluation of AI models on any screening mammography data set. At its inception, our meta-repository contains five state-of-the-art models with open-source implementations and cross-platform compatibility. We compare their performance on seven international data sets. Our framework has a flexible design that can be generalized to other medical image analysis tasks. The meta-repository is available at https://www.github.com/nyukat/mammography_metarepository. △ Less","18 January, 2022",https://arxiv.org/pdf/2108.04800
Rapid Automated Analysis of Skull Base Tumor Specimens Using Intraoperative Optical Imaging and Artificial Intelligence,Cheng Jiang;Abhishek Bhattacharya;Joseph Linzey;Rushikesh S. Joshi;Sung Jik Cha;Sudharsan Srinivasan;Daniel Alber;Akhil Kondepudi;Esteban Urias;Balaji Pandian;Wajd Al-Holou;Steve Sullivan;B. Gregory Thompson;Jason Heth;Chris Freudiger;Siri Khalsa;Donato Pacione;John G. Golfinos;Sandra Camelo-Piragua;Daniel A. Orringer;Honglak Lee;Todd Hollon,"Background: Accurate diagnosis of skull base tumors is essential for providing personalized surgical treatment strategies. Intraoperative diagnosis can be challenging due to tumor diversity and lack of intraoperative pathology resources. Objective: To develop an independent and parallel intraoperative pathology workflow that can provide rapid and accurate skull base tumor diagnoses using label-free optical imaging and artificial intelligence. Method: We used a fiber laser-based, label-free, non-consumptive, high-resolution microscopy method (< 60 sec per 1 \times 1 mm^\text{2}), called stimulated Raman histology (SRH), to image a consecutive, multicenter cohort of skull base tumor patients. SRH images were then used to train a convolutional neural network (CNN) model using three representation learning strategies: cross-entropy, self-supervised contrastive learning, and supervised contrastive learning. Our trained CNN models were tested on a held-out, multicenter SRH dataset. Results: SRH was able to image the diagnostic features of both benign and malignant skull base tumors. Of the three representation learning strategies, supervised contrastive learning most effectively learned the distinctive and diagnostic SRH image features for each of the skull base tumor types. In our multicenter testing set, cross-entropy achieved an overall diagnostic accuracy of 91.5%, self-supervised contrastive learning 83.9%, and supervised contrastive learning 96.6%. Our trained model was able to identify tumor-normal margins and detect regions of microscopic tumor infiltration in whole-slide SRH images. Conclusion: SRH with trained artificial intelligence models can provide rapid and accurate intraoperative analysis of skull base tumor specimens to inform surgical decision-making. △ Less","19 June, 2022",https://arxiv.org/pdf/2108.03555
A review on vision-based analysis for automatic dietary assessment,Wei Wang;Weiqing Min;Tianhao Li;Xiaoxiao Dong;Haisheng Li;Shuqiang Jiang,"Background: Maintaining a healthy diet is vital to avoid health-related issues, e.g., undernutrition, obesity and many non-communicable diseases. An indispensable part of the health diet is dietary assessment. Traditional manual recording methods are not only burdensome but time-consuming, and contain substantial biases and errors. Recent advances in Artificial Intelligence (AI), especially computer vision technologies, have made it possible to develop automatic dietary assessment solutions, which are more convenient, less time-consuming and even more accurate to monitor daily food intake. Scope and approach: This review presents Vision-Based Dietary Assessment (VBDA) architectures, including multi-stage architecture and end-to-end one. The multi-stage dietary assessment generally consists of three stages: food image analysis, volume estimation and nutrient derivation. The prosperity of deep learning makes VBDA gradually move to an end-to-end implementation, which applies food images to a single network to directly estimate the nutrition. The recently proposed end-to-end methods are also discussed. We further analyze existing dietary assessment datasets, indicating that one large-scale benchmark is urgently needed, and finally highlight critical challenges and future trends for VBDA. Key findings and conclusions: After thorough exploration, we find that multi-task end-to-end deep learning approaches are one important trend of VBDA. Despite considerable research progress, many challenges remain for VBDA due to the meal complexity. We also provide the latest ideas for future development of VBDA, e.g., fine-grained food analysis and accurate volume estimation. This review aims to encourage researchers to propose more practical solutions for VBDA. △ Less","6 March, 2022",https://arxiv.org/pdf/2108.02947
A FAIR and AI-ready Higgs boson decay dataset,Yifan Chen;E. A. Huerta;Javier Duarte;Philip Harris;Daniel S. Katz;Mark S. Neubauer;Daniel Diaz;Farouk Mokhtar;Raghav Kansal;Sang Eon Park;Volodymyr V. Kindratenko;Zhizhen Zhao;Roger Rusack,"To enable the reusability of massive scientific datasets by humans and machines, researchers aim to adhere to the principles of findability, accessibility, interoperability, and reusability (FAIR) for data and artificial intelligence (AI) models. This article provides a domain-agnostic, step-by-step assessment guide to evaluate whether or not a given dataset meets these principles. We demonstrate how to use this guide to evaluate the FAIRness of an open simulated dataset produced by the CMS Collaboration at the CERN Large Hadron Collider. This dataset consists of Higgs boson decays and quark and gluon background, and is available through the CERN Open Data Portal. We use additional available tools to assess the FAIRness of this dataset, and incorporate feedback from members of the FAIR community to validate our results. This article is accompanied by a Jupyter notebook to visualize and explore this dataset. This study marks the first in a planned series of articles that will guide scientists in the creation of FAIR AI models and datasets in high energy particle physics. △ Less","16 February, 2022",https://arxiv.org/pdf/2108.02214
A Gaze Data-based Comparative Study to Build a Trustworthy Human-AI Collaboration in Crash Anticipation,Yu Li;Muhammad Monjurul Karim;Ruwen Qin,"Vehicles with a safety function for anticipating crashes in advance can enhance drivers' ability to avoid crashes. As dashboard cameras have become a low-cost sensor device accessible to almost every vehicle, deep neural networks for crash anticipation from a dashboard camera are receiving growing interest. However, drivers' trust in the Artificial Intelligence (AI)-enabled safety function is built on the validation of its safety enhancement toward zero deaths. This paper is motivated to establish a method that uses gaze data and corresponding measures to evaluate human drivers' ability to anticipate crashes. A laboratory experiment is designed and performed, wherein a screen-based eye tracker collects the gaze data of six volunteers while watching 100 driving videos that include both normal and crash scenarios. Statistical analyses of the experimental data show that, on average, drivers can anticipate a crash up to 2.61 seconds before it occurs in this pilot study. The chance that drivers have successfully anticipated crashes before they occur is 92.8%. A state-of-the-art AI model can anticipate crashes 1.02 seconds earlier than drivers on average. The study finds that crash-involving traffic agents in the driving videos can vary drivers' instant attention level, average attention level, and spatial attention distribution. This finding supports the development of a spatial-temporal attention mechanism for AI models to strengthen their ability to anticipate crashes. Results from the comparison also suggest the development of collaborative intelligence that keeps human-in-the-loop of AI models to further enhance the reliability of AI-enabled safety functions. △ Less","9 November, 2022",https://arxiv.org/pdf/2108.01599
Bayesian Active Meta-Learning for Few Pilot Demodulation and Equalization,Kfir M. Cohen;Sangwoo Park;Osvaldo Simeone;Shlomo Shamai,"Two of the main principles underlying the life cycle of an artificial intelligence (AI) module in communication networks are adaptation and monitoring. Adaptation refers to the need to adjust the operation of an AI module depending on the current conditions; while monitoring requires measures of the reliability of an AI module's decisions. Classical frequentist learning methods for the design of AI modules fall short on both counts of adaptation and monitoring, catering to one-off training and providing overconfident decisions. This paper proposes a solution to address both challenges by integrating meta-learning with Bayesian learning. As a specific use case, the problems of demodulation and equalization over a fading channel based on the availability of few pilots are studied. Meta-learning processes pilot information from multiple frames in order to extract useful shared properties of effective demodulators across frames. The resulting trained demodulators are demonstrated, via experiments, to offer better calibrated soft decisions, at the computational cost of running an ensemble of networks at run time. The capacity to quantify uncertainty in the model parameter space is further leveraged by extending Bayesian meta-learning to an active setting. In it, the designer can select in a sequential fashion channel conditions under which to generate data for meta-learning from a channel simulator. Bayesian active meta-learning is seen in experiments to significantly reduce the number of frames required to obtain efficient adaptation procedure for new frames. △ Less","5 December, 2022",https://arxiv.org/pdf/2108.00785
Towards explainable artificial intelligence (XAI) for early anticipation of traffic accidents,Muhammad Monjurul Karim;Yu Li;Ruwen Qin,"Traffic accident anticipation is a vital function of Automated Driving Systems (ADSs) for providing a safety-guaranteed driving experience. An accident anticipation model aims to predict accidents promptly and accurately before they occur. Existing Artificial Intelligence (AI) models of accident anticipation lack a human-interpretable explanation of their decision-making. Although these models perform well, they remain a black-box to the ADS users, thus difficult to get their trust. To this end, this paper presents a Gated Recurrent Unit (GRU) network that learns spatio-temporal relational features for the early anticipation of traffic accidents from dashcam video data. A post-hoc attention mechanism named Grad-CAM is integrated into the network to generate saliency maps as the visual explanation of the accident anticipation decision. An eye tracker captures human eye fixation points for generating human attention maps. The explainability of network-generated saliency maps is evaluated in comparison to human attention maps. Qualitative and quantitative results on a public crash dataset confirm that the proposed explainable network can anticipate an accident on average 4.57 seconds before it occurs, with 94.02% average precision. In further, various post-hoc attention-based XAI methods are evaluated and compared. It confirms that the Grad-CAM chosen by this study can generate high-quality, human-interpretable saliency maps (with 1.23 Normalized Scanpath Saliency) for explaining the crash anticipation decision. Importantly, results confirm that the proposed AI model, with a human-inspired design, can outperform humans in the accident anticipation. △ Less","7 January, 2022",https://arxiv.org/pdf/2108.00273
Unstructured Handwashing Recognition using Smartwatch to Reduce Contact Transmission of Pathogens,Emanuele Lattanzi;Lorenzo Calisti;Valerio Freschi,"Current guidelines from the World Health Organization indicate that the SARS-CoV-2 coronavirus, which results in the novel coronavirus disease (COVID-19), is transmitted through respiratory droplets or by contact. Contact transmission occurs when contaminated hands touch the mucous membrane of the mouth, nose, or eyes so hands hygiene is extremely important to prevent the spread of the SARSCoV-2 as well as of other pathogens. The vast proliferation of wearable devices, such as smartwatches, containing acceleration, rotation, magnetic field sensors, etc., together with the modern technologies of artificial intelligence, such as machine learning and more recently deep-learning, allow the development of accurate applications for recognition and classification of human activities such as: walking, climbing stairs, running, clapping, sitting, sleeping, etc. In this work, we evaluate the feasibility of a machine learning based system which, starting from inertial signals collected from wearable devices such as current smartwatches, recognizes when a subject is washing or rubbing its hands. Preliminary results, obtained over two different datasets, show a classification accuracy of about 95% and of about 94% for respectively deep and standard learning techniques. △ Less","6 June, 2022",https://arxiv.org/pdf/2107.13405
Towards Industrial Private AI: A two-tier framework for data and model security,Sunder Ali Khowaja;Kapal Dev;Nawab Muhammad Faseeh Qureshi;Parus Khuwaja;Luca Foschini,"With the advances in 5G and IoT devices, the industries are vastly adopting artificial intelligence (AI) techniques for improving classification and prediction-based services. However, the use of AI also raises concerns regarding privacy and security that can be misused or leaked. Private AI was recently coined to address the data security issue by combining AI with encryption techniques, but existing studies have shown that model inversion attacks can be used to reverse engineer the images from model parameters. In this regard, we propose a Federated Learning and Encryption-based Private (FLEP) AI framework that provides two-tier security for data and model parameters in an IIoT environment. We proposed a three-layer encryption method for data security and provide a hypothetical method to secure the model parameters. Experimental results show that the proposed method achieves better encryption quality at the expense of slightly increased execution time. We also highlight several open issues and challenges regarding the FLEP AI framework's realization. △ Less","18 January, 2022",https://arxiv.org/pdf/2107.12806
Pointer Value Retrieval: A new benchmark for understanding the limits of neural network generalization,Chiyuan Zhang;Maithra Raghu;Jon Kleinberg;Samy Bengio,"Central to the success of artificial neural networks is their ability to generalize. But does neural network generalization primarily rely on seeing highly similar training examples (memorization)? Or are neural networks capable of human-intelligence styled reasoning, and if so, to what extent? These remain fundamental open questions on artificial neural networks. In this paper, as steps towards answering these questions, we introduce a new benchmark, Pointer Value Retrieval (PVR) to study the limits of neural network reasoning. The PVR suite of tasks is based on reasoning about indirection, a hallmark of human intelligence, where a first stage (task) contains instructions for solving a second stage (task). In PVR, this is done by having one part of the task input act as a pointer, giving instructions on a different input location, which forms the output. We show this simple rule can be applied to create a diverse set of tasks across different input modalities and configurations. Importantly, this use of indirection enables systematically varying task difficulty through distribution shifts and increasing functional complexity. We conduct a detailed empirical study of different PVR tasks, discovering large variations in performance across dataset sizes, neural network architectures and task complexity. Further, by incorporating distribution shift and increased functional complexity, we develop nuanced tests for reasoning, revealing subtle failures and surprising successes, suggesting many promising directions of exploration on this benchmark. △ Less","18 February, 2022",https://arxiv.org/pdf/2107.12580
Playtesting: What is Beyond Personas,Sinan Ariyurek;Elif Surer;Aysu Betin-Can,"Playtesting is an essential step in the game design process. Game designers use the feedback from playtests to refine their designs. Game designers may employ procedural personas to automate the playtesting process. In this paper, we present two approaches to improve automated playtesting. First, we propose developing persona, which allows a persona to progress to different goals. In contrast, the procedural persona is fixed to a single goal. Second, a human playtester knows which paths she has tested before, and during the consequent tests, she may test different paths. However, Reinforcement Learning (RL) agents disregard these previous paths. We propose a novel methodology that we refer to as Alternative Path Finder (APF). We train APF with previous paths and employ APF during the training of an RL agent. APF modulates the reward structure of the environment while preserving the agent's goal. When evaluated, the agent generates a different trajectory that achieves the same goal. We use the General Video Game Artificial Intelligence (GVG-AI) and VizDoom frameworks to test our proposed methodologies. We use Proximal Policy Optimization (PPO) RL agent during experiments. First, we compare the playtest data generated by developing and procedural persona. Our experiments show that developing persona provides better insight into the game and how different players would play. Second, we present the alternative paths found using APF and argue why traditional RL agents cannot learn those paths. △ Less","6 April, 2022",https://arxiv.org/pdf/2107.11965
Preliminary Steps Towards Federated Sentiment Classification,Xin-Chun Li;Lan Li;De-Chuan Zhan;Yunfeng Shao;Bingshuai Li;Shaoming Song,"Automatically mining sentiment tendency contained in natural language is a fundamental research to some artificial intelligent applications, where solutions alternate with challenges. Transfer learning and multi-task learning techniques have been leveraged to mitigate the supervision sparsity and collaborate multiple heterogeneous domains correspondingly. Recent years, the sensitive nature of users' private data raises another challenge for sentiment classification, i.e., data privacy protection. In this paper, we resort to federated learning for multiple domain sentiment classification under the constraint that the corpora must be stored on decentralized devices. In view of the heterogeneous semantics across multiple parties and the peculiarities of word embedding, we pertinently provide corresponding solutions. First, we propose a Knowledge Transfer Enhanced Private-Shared (KTEPS) framework for better model aggregation and personalization in federated sentiment classification. Second, we propose KTEPS^\star with the consideration of the rich semantic and huge embedding size properties of word vectors, utilizing Projection-based Dimension Reduction (PDR) methods for privacy protection and efficient transmission simultaneously. We propose two federated sentiment classification scenes based on public benchmarks, and verify the superiorities of our proposed methods with abundant experimental investigations. △ Less","31 March, 2022",https://arxiv.org/pdf/2107.11956
Machine Learning Characterization of Cancer Patients-Derived Extracellular Vesicles using Vibrational Spectroscopies,Abicumaran Uthamacumaran;Samir Elouatik;Mohamed Abdouh;Michael Berteau-Rainville;Zu-hua Gao;Goffredo Arena,"The early detection of cancer is a challenging problem in medicine. The blood sera of cancer patients are enriched with heterogeneous secretory lipid bound extracellular vesicles (EVs), which present a complex repertoire of information and biomarkers, representing their cell of origin, that are being currently studied in the field of liquid biopsy and cancer screening. Vibrational spectroscopies provide non-invasive approaches for the assessment of structural and biophysical properties in complex biological samples. In this pilot study, multiple Raman spectroscopy measurements were performed on the EVs extracted from the blood sera of 9 patients consisting of four different cancer subtypes (colorectal cancer, hepatocellular carcinoma, breast cancer and pancreatic cancer) and five healthy patients (controls). FTIR (Fourier Transform Infrared) spectroscopy measurements were performed as a complementary approach to Raman analysis, on two of the four cancer subtypes. The AdaBoost Random Forest Classifier, Decision Trees, and Support Vector Machines (SVM) distinguished the baseline corrected Raman spectra of cancer EVs from those of healthy controls (18 spectra) with a classification accuracy of above 90 percent when reduced to a spectral frequency range of 1800 to 1940 inverse cm and subjected to a 50:50 training: testing split. FTIR classification accuracy on 14 spectra showed an 80 percent classification accuracy. Our findings demonstrate that basic machine learning algorithms are powerful applied intelligence tools to distinguish the complex vibrational spectra of cancer patient EVs from those of healthy patients. These experimental methods hold promise as valid and efficient liquid biopsy for artificial intelligence-assisted early cancer screening. △ Less","13 February, 2022",https://arxiv.org/pdf/2107.10332
Data synthesis and adversarial networks: A review and meta-analysis in cancer imaging,Richard Osuala;Kaisar Kushibar;Lidia Garrucho;Akis Linardos;Zuzanna Szafranowska;Stefan Klein;Ben Glocker;Oliver Diaz;Karim Lekadir,"Despite technological and medical advances, the detection, interpretation, and treatment of cancer based on imaging data continue to pose significant challenges. These include inter-observer variability, class imbalance, dataset shifts, inter- and intra-tumour heterogeneity, malignancy determination, and treatment effect uncertainty. Given the recent advancements in Generative Adversarial Networks (GANs), data synthesis, and adversarial training, we assess the potential of these technologies to address a number of key challenges of cancer imaging. We categorise these challenges into (a) data scarcity and imbalance, (b) data access and privacy, (c) data annotation and segmentation, (d) cancer detection and diagnosis, and (e) tumour profiling, treatment planning and monitoring. Based on our analysis of 164 publications that apply adversarial training techniques in the context of cancer imaging, we highlight multiple underexplored solutions with research potential. We further contribute the Synthesis Study Trustworthiness Test (SynTRUST), a meta-analysis framework for assessing the validation rigour of medical image synthesis studies. SynTRUST is based on 26 concrete measures of thoroughness, reproducibility, usefulness, scalability, and tenability. Based on SynTRUST, we analyse 16 of the most promising cancer imaging challenge solutions and observe a high validation rigour in general, but also several desirable improvements. With this work, we strive to bridge the gap between the needs of the clinical cancer imaging community and the current and prospective research on data synthesis and adversarial networks in the artificial intelligence community. △ Less","27 November, 2022",https://arxiv.org/pdf/2107.09543
Relay-Assisted Cooperative Federated Learning,Zehong Lin;Hang Liu;Ying-Jun Angela Zhang,"Federated learning (FL) has recently emerged as a promising technology to enable artificial intelligence (AI) at the network edge, where distributed mobile devices collaboratively train a shared AI model under the coordination of an edge server. To significantly improve the communication efficiency of FL, over-the-air computation allows a large number of mobile devices to concurrently upload their local models by exploiting the superposition property of wireless multi-access channels. Due to wireless channel fading, the model aggregation error at the edge server is dominated by the weakest channel among all devices, causing severe straggler issues. In this paper, we propose a relay-assisted cooperative FL scheme to effectively address the straggler issue. In particular, we deploy multiple half-duplex relays to cooperatively assist the devices in uploading the local model updates to the edge server. The nature of the over-the-air computation poses system objectives and constraints that are distinct from those in traditional relay communication systems. Moreover, the strong coupling between the design variables renders the optimization of such a system challenging. To tackle the issue, we propose an alternating-optimization-based algorithm to optimize the transceiver and relay operation with low complexity. Then, we analyze the model aggregation error in a single-relay case and show that our relay-assisted scheme achieves a smaller error than the one without relays provided that the relay transmit power and the relay channel gains are sufficiently large. The analysis provides critical insights on relay deployment in the implementation of cooperative FL. Extensive numerical results show that our design achieves faster convergence compared with state-of-the-art schemes. △ Less","3 March, 2022",https://arxiv.org/pdf/2107.09518
MDE4QAI: Towards Model-Driven Engineering for Quantum Artificial Intelligence,Armin Moin;Moharram Challenger;Atta Badii;Stephan Günnemann,"Over the past decade, Artificial Intelligence (AI) has provided enormous new possibilities and opportunities, but also new demands and requirements for software systems. In particular, Machine Learning (ML) has proven useful in almost every vertical application domain. In the decade ahead, an unprecedented paradigm shift from classical computing towards Quantum Computing (QC), with perhaps a quantum-classical hybrid model, is expected. We argue that the Model-Driven Engineering (MDE) paradigm can be an enabler and a facilitator, when it comes to the quantum and the quantum-classical hybrid applications. This includes not only automated code generation, but also automated model checking and verification, as well as model analysis in the early design phases, and model-to-model transformations both at the design-time and at the runtime. In this paper, the vision is focused on MDE for Quantum AI, particularly Quantum ML for the Internet of Things (IoT) and smart Cyber-Physical Systems (CPS) applications. △ Less","11 July, 2022",https://arxiv.org/pdf/2107.06708
Application of artificial intelligence techniques for automated detection of myocardial infarction: A review,Javad Hassannataj Joloudari;Sanaz Mojrian;Issa Nodehi;Amir Mashmool;Zeynab Kiani Zadegan;Sahar Khanjani Shirkharkolaie;Roohallah Alizadehsani;Tahereh Tamadon;Samiyeh Khosravi;Mitra Akbari Kohnehshari;Edris Hassannatajjeloudari;Danial Sharifrazi;Amir Mosavi;Hui Wen Loh;Ru-San Tan;U Rajendra Acharya,"Myocardial infarction (MI) results in heart muscle injury due to receiving insufficient blood flow. MI is the most common cause of mortality in middle-aged and elderly individuals around the world. To diagnose MI, clinicians need to interpret electrocardiography (ECG) signals, which requires expertise and is subject to observer bias. Artificial intelligence-based methods can be utilized to screen for or diagnose MI automatically using ECG signals. In this work, we conducted a comprehensive assessment of artificial intelligence-based approaches for MI detection based on ECG as well as other biophysical signals, including machine learning (ML) and deep learning (DL) models. The performance of traditional ML methods relies on handcrafted features and manual selection of ECG signals, whereas DL models can automate these tasks. The review observed that deep convolutional neural networks (DCNNs) yielded excellent classification performance for MI diagnosis, which explains why they have become prevalent in recent years. To our knowledge, this is the first comprehensive survey of artificial intelligence techniques employed for MI diagnosis using ECG and other biophysical signals. △ Less","21 February, 2022",https://arxiv.org/pdf/2107.06179
IGrow: A Smart Agriculture Solution to Autonomous Greenhouse Control,Xiaoyan Cao;Yao Yao;Lanqing Li;Wanpeng Zhang;Zhicheng An;Zhong Zhang;Li Xiao;Shihui Guo;Xiaoyu Cao;Meihong Wu;Dijun Luo,"Agriculture is the foundation of human civilization. However, the rapid increase of the global population poses a challenge on this cornerstone by demanding more food. Modern autonomous greenhouses, equipped with sensors and actuators, provide a promising solution to the problem by empowering precise control for high-efficient food production. However, the optimal control of autonomous greenhouses is challenging, requiring decision-making based on high-dimensional sensory data, and the scaling of production is limited by the scarcity of labor capable of handling this task. With the advances of artificial intelligence (AI), the internet of things (IoT), and cloud computing technologies, we are hopeful to provide a solution to automate and smarten greenhouse control to address the above challenges. In this paper, we propose a smart agriculture solution named iGrow, for autonomous greenhouse control (AGC): (1) for the first time, we formulate the AGC problem as a Markov decision process (MDP) optimization problem; (2) we design a neural network-based simulator incorporated with the incremental mechanism to simulate the complete planting process of an autonomous greenhouse, which provides a testbed for the optimization of control strategies; (3) we propose a closed-loop bi-level optimization algorithm, which can dynamically re-optimize the greenhouse control strategy with newly observed data during real-world production. We not only conduct simulation experiments but also deploy iGrow in real scenarios, and experimental results demonstrate the effectiveness and superiority of iGrow in autonomous greenhouse simulation and optimal control. Particularly, compelling results from the tomato pilot project in real autonomous greenhouses show that our solution significantly increases crop yield (+10.15\%) and net profit (+92.70\%) with statistical significance compared to planting experts. △ Less","14 March, 2022",https://arxiv.org/pdf/2107.05464
Immune Moral Models? Pro-Social Rule Breaking as a Moral Enhancement Approach for Ethical AI,Rajitha Ramanayake;Philipp Wicke;Vivek Nallur,"We are moving towards a future where Artificial Intelligence (AI) based agents make many decisions on behalf of humans. From healthcare decision making to social media censoring, these agents face problems, and make decisions with ethical and societal implications. Ethical behaviour is a critical characteristic that we would like in a human-centric AI. A common observation in human-centric industries, like the service industry and healthcare, is that their professionals tend to break rules, if necessary, for pro-social reasons. This behaviour among humans is defined as pro-social rule breaking. To make AI agents more human centric, we argue that there is a need for a mechanism that helps AI agents identify when to break rules set by their designers. To understand when AI agents need to break rules, we examine the conditions under which humans break rules for pro-social reasons. In this paper, we present a study that introduces a 'vaccination strategy dilemma' to human participants and analyses their responses. In this dilemma, one needs to decide whether they would distribute Covid-19 vaccines only to members of a high-risk group (follow the enforced rule) or, in selected cases, administer the vaccine to a few social influencers (break the rule), which might yield an overall greater benefit to society. The results of the empirical study suggest a relationship between stakeholder utilities and pro-social rule breaking (PSRB), which neither deontological nor utilitarian ethics completely explain. Finally, the paper discusses the design characteristics of an ethical agent capable of PSRB and the future research directions on PSRB in the AI realm. We hope that this will inform the design of future AI agents, and their decision-making behaviour. △ Less","9 May, 2022",https://arxiv.org/pdf/2107.04022
BF-QC: Belief Functions on Quantum Circuits,Qianli Zhou;Guojing Tian;Yong Deng,"Dempster-Shafer Theory (DST) of belief function is a basic theory of artificial intelligence, which can represent the underlying knowledge more reasonably than Probability Theory (ProbT). Because of the computation complexity exploding exponentially with the increasing number of elements, the practical application scenarios of DST are limited. In this paper, we encode Basic Belief Assignments (BBA) into quantum superposition states and propose the implementation and operation methods of BBA on quantum circuits. We decrease the computation complexity of the matrix evolution on BBA (MEoB) on quantum circuits. Based on the MEoB, we realize the quantum belief functions' implementation, the similarity measurements of BBAs, evidence Combination Rules (CR), and probability transformation on quantum circuits. △ Less","12 October, 2022",https://arxiv.org/pdf/2107.03930
Demystifying the Draft EU Artificial Intelligence Act,Michael Veale;Frederik Zuiderveen Borgesius,"In April 2021, the European Commission proposed a Regulation on Artificial Intelligence, known as the AI Act. We present an overview of the Act and analyse its implications, drawing on scholarship ranging from the study of contemporary AI practices to the structure of EU product safety regimes over the last four decades. Aspects of the AI Act, such as different rules for different risk-levels of AI, make sense. But we also find that some provisions of the Draft AI Act have surprising legal implications, whilst others may be largely ineffective at achieving their stated goals. Several overarching aspects, including the enforcement regime and the risks of maximum harmonisation pre-empting legitimate national AI policy, engender significant concern. These issues should be addressed as a priority in the legislative process. △ Less","13 June, 2022",https://arxiv.org/pdf/2107.03721
Remote sensing and AI for building climate adaptation applications,Beril Sirmacek;Ricardo Vinuesa,"Urban areas are not only one of the biggest contributors to climate change, but also they are one of the most vulnerable areas with high populations who would together experience the negative impacts. In this paper, we address some of the opportunities brought by satellite remote sensing imaging and artificial intelligence (AI) in order to measure climate adaptation of cities automatically. We propose a framework combining AI and simulation which may be useful for extracting indicators from remote-sensing images and may help with predictive estimation of future states of these climate-adaptation-related indicators. When such models become more robust and used in real life applications, they may help decision makers and early responders to choose the best actions to sustain the well-being of society, natural resources and biodiversity. We underline that this is an open field and an on-going area of research for many scientists, therefore we offer an in-depth discussion on the challenges and limitations of data-driven methods and the predictive estimation models in general. △ Less","12 July, 2022",https://arxiv.org/pdf/2107.02693
ML-Quadrat & DriotData: A Model-Driven Engineering Tool and a Low-Code Platform for Smart IoT Services,Armin Moin;Andrei Mituca;Moharram Challenger;Atta Badii;Stephan Günnemann,"In this paper, we present ML-Quadrat, an open-source research prototype that is based on the Eclipse Modeling Framework (EMF) and the state of the art in the literature of Model-Driven Software Engineering (MDSE) for smart Cyber-Physical Systems (CPS) and the Internet of Things (IoT). Its envisioned users are mostly software developers who might not have deep knowledge and skills in the heterogeneous IoT platforms and the diverse Artificial Intelligence (AI) technologies, specifically regarding Machine Learning (ML). ML-Quadrat is released under the terms of the Apache 2.0 license on Github. Additionally, we demonstrate an early tool prototype of DriotData, a web-based Low-Code platform targeting citizen data scientists and citizen/end-user software developers. DriotData exploits and adopts ML-Quadrat in the industry by offering an extended version of it as a subscription-based service to companies, mainly Small- and Medium-Sized Enterprises (SME). The current preliminary version of DriotData has three web-based model editors: text-based, tree-/form-based and diagram-based. The latter is designed for domain experts in the problem or use case domains (namely the IoT vertical domains) who might not have knowledge and skills in the field of IT. Finally, a short video demonstrating the tools is available on YouTube: https://youtu.be/VAuz25w0a5k △ Less","16 February, 2022",https://arxiv.org/pdf/2107.02692
Supporting AI Engineering on the IoT Edge through Model-Driven TinyML,Armin Moin;Moharram Challenger;Atta Badii;Stephan Günnemann,"Software engineering of network-centric Artificial Intelligence (AI) and Internet of Things (IoT) enabled Cyber-Physical Systems (CPS) and services, involves complex design and validation challenges. In this paper, we propose a novel approach, based on the model-driven software engineering paradigm, in particular the domain-specific modeling methodology. We focus on a sub-discipline of AI, namely Machine Learning (ML) and propose the delegation of data analytics and ML to the IoT edge. This way, we may increase the service quality of ML, for example, its availability and performance, regardless of the network conditions, as well as maintaining the privacy, security and sustainability. We let practitioners assign ML tasks to heterogeneous edge devices, including highly resource-constrained embedded microcontrollers with main memories in the order of Kilobytes, and energy consumption in the order of milliwatts. This is known as TinyML. Furthermore, we show how software models with different levels of abstraction, namely platform-independent and platform-specific models can be used in the software development process. Finally, we validate the proposed approach using a case study addressing the predictive maintenance of a hydraulics system with various networked sensors and actuators. △ Less","3 April, 2022",https://arxiv.org/pdf/2107.02690
A precise bare simulation approach to the minimization of some distances. Foundations,Michel Broniatowski;Wolfgang Stummer,"In information theory -- as well as in the adjacent fields of statistics, machine learning, artificial intelligence, signal processing and pattern recognition -- many flexibilizations of the omnipresent Kullback-Leibler information distance (relative entropy) and of the closely related Shannon entropy have become frequently used tools. To tackle corresponding constrained minimization (respectively maximization) problems by a newly developed dimension-free bare (pure) simulation method, is the main goal of this paper. Almost no assumptions (like convexity) on the set of constraints are needed, within our discrete setup of arbitrary dimension, and our method is precise (i.e., converges in the limit). As a side effect, we also derive an innovative way of constructing new useful distances/divergences. To illustrate the core of our approach, we present numerous solved cases. The potential for widespread applicability is indicated, too; in particular, we deliver many recent references for uses of the involved distances/divergences and entropies in various different research fields (which may also serve as an interdisciplinary interface). △ Less","15 November, 2022",https://arxiv.org/pdf/2107.01693
SinGAN-Seg: Synthetic training data generation for medical image segmentation,Vajira Thambawita;Pegah Salehi;Sajad Amouei Sheshkal;Steven A. Hicks;Hugo L. Hammer;Sravanthi Parasa;Thomas de Lange;Pål Halvorsen;Michael A. Riegler,"Analyzing medical data to find abnormalities is a time-consuming and costly task, particularly for rare abnormalities, requiring tremendous efforts from medical experts. Artificial intelligence has become a popular tool for the automatic processing of medical data, acting as a supportive tool for doctors. However, the machine learning models used to build these tools are highly dependent on the data used to train them. Large amounts of data can be difficult to obtain in medicine due to privacy, expensive and time-consuming annotations, and a general lack of data samples for infrequent lesions. Here, we present a novel synthetic data generation pipeline, called SinGAN-Seg, to produce synthetic medical images with corresponding masks using a single training image. Our method is different from the traditional GANs because our model needs only a single image and the corresponding ground truth to train. Our method produces alternative artificial segmentation datasets with ground truth masks when real datasets are not allowed to share. The pipeline is evaluated using qualitative and quantitative comparisons between real and synthetic data to show that the style transfer technique used in our pipeline significantly improves the quality of the generated data and our method is better than other state-of-the-art GANs to prepare synthetic images when the size of training datasets are limited. By training UNet++ using both real and the synthetic data generated from the SinGAN-Seg pipeline, we show that models trained with synthetic data have very close performances to those trained on real data when the datasets have a considerable amount of data. In contrast, Synthetic data generated from the SinGAN-Seg pipeline can improve the performance of segmentation models when training datasets do not have a considerable amount of data. The code is available on GitHub. △ Less","25 April, 2022",https://arxiv.org/pdf/2107.00471
"What Is Consciousness? Artificial Intelligence, Real Intelligence, Quantum Mind, And Qualia",Stuart A. Kauffman;Andrea Roli,"We approach the question ""What is Consciousness?"" in a new way, not as Descartes' ""systematic doubt"", but as how organisms find their way in their world. Finding one's way involves finding possible uses of features of the world that might be beneficial or avoiding those that might be harmful. ""Possible uses of X to accomplish Y"" are ""Affordances"". The number of uses of X is indefinite (or unknown), the different uses are unordered, are not listable, and are not deducible from one another. All biological adaptations are either affordances seized by heritable variation and selection or, far faster, by the organism acting in its world finding uses of X to accomplish Y. Based on this, we reach rather astonishing conclusions: (1) Artificial general intelligence based on universal Turing machines (UTMs) is not possible, since UTMs cannot ""find"" novel affordances. (2) Brain-mind is not purely classical physics for no classical physics system can be an analogue computer whose dynamical behaviour can be isomorphic to ""possible uses"". (3) Brain mind must be partly quantum-supported by increasing evidence at 6.0 sigma to 7.3 sigma. (4) Based on Heisenberg's interpretation of the quantum state as ""potentia"" converted to ""actuals"" by measurement, where this interpretation is not a substance dualism, a natural hypothesis is that mind actualizes potentia. This is supported at 5.2 sigma. Then mind's actualizations of entangled brain-mind-world states are experienced as qualia and allow ""seeing"" or ""perceiving"" of uses of X to accomplish Y. We can and do jury-rig. Computers cannot. (5) Beyond familiar quantum computers, we discuss the potentialities of trans-Turing-systems. △ Less","29 June, 2022",https://arxiv.org/pdf/2106.15515
Caching and Computation Offloading in High Altitude Platform Station (HAPS) Assisted Intelligent Transportation Systems,Qiqi Ren;Omid Abbasi;Gunes Karabulut Kurt;Halim Yanikomeroglu;Jian Chen,"Edge intelligence, a new paradigm to accelerate artificial intelligence (AI) applications by leveraging computing resources on the network edge, can be used to improve intelligent transportation systems (ITS). However, due to physical limitations and energy-supply constraints, the computing powers of edge equipment are usually limited. High altitude platform station (HAPS) computing can be considered as a promising extension of edge computing. HAPS is deployed in the stratosphere to provide wide coverage and strong computational capabilities. It is suitable to coordinate terrestrial resources and store the fundamental data associated with ITS-based applications. In this work, three computing layers,i.e., vehicles, terrestrial network edges, and HAPS, are integrated to build a computation framework for ITS, where the HAPS data library stores the fundamental data needed for the applications. In addition, the caching technique is introduced for network edges to store some of the fundamental data from the HAPS so that large propagation delays can be reduced. We aim to minimize the delay of the system by optimizing computation offloading and caching decisions as well as bandwidth and computing resource allocations. The simulation results highlight the benefits of HAPS computing for mitigating delays and the significance of caching at network edges. △ Less","13 January, 2022",https://arxiv.org/pdf/2106.14928
Understanding Dynamics of Nonlinear Representation Learning and Its Application,Kenji Kawaguchi;Linjun Zhang;Zhun Deng,"Representations of the world environment play a crucial role in artificial intelligence. It is often inefficient to conduct reasoning and inference directly in the space of raw sensory representations, such as pixel values of images. Representation learning allows us to automatically discover suitable representations from raw sensory data. For example, given raw sensory data, a deep neural network learns nonlinear representations at its hidden layers, which are subsequently used for classification (or regression) at its output layer. This happens implicitly during training through minimizing a supervised or unsupervised loss in common practical regimes of deep learning, unlike the neural tangent kernel (NTK) regime. In this paper, we study the dynamics of such implicit nonlinear representation learning, which is beyond the NTK regime. We identify a pair of a new assumption and a novel condition, called the common model structure assumption and the data-architecture alignment condition. Under the common model structure assumption, the data-architecture alignment condition is shown to be sufficient for the global convergence and necessary for the global optimality. Moreover, our theory explains how and when increasing the network size does and does not improve the training behaviors in the practical regime. Our results provide practical guidance for designing a model structure: e.g., the common model structure assumption can be used as a justification for using a particular model structure instead of others. We also derive a new training framework based on the theory. The proposed framework is empirically shown to maintain competitive (practical) test performances while providing global convergence guarantees for deep residual neural networks with convolutions, skip connections, and batch normalization with standard benchmark datasets, including CIFAR-10, CIFAR-100, and SVHN. △ Less","9 April, 2022",https://arxiv.org/pdf/2106.14836
Self-Evolving Integrated Vertical Heterogeneous Networks,Amin Farajzadeh;Mohammad G. Khoshkholgh;Halim Yanikomeroglu;Ozgur Ercetin,"6G and beyond networks tend towards fully intelligent and adaptive design in order to provide better operational agility in maintaining universal wireless access and supporting a wide range of services and use cases while dealing with network complexity efficiently. Such enhanced network agility will require developing a self-evolving capability in designing both the network architecture and resource management to intelligently utilize resources, reduce operational costs, and achieve the coveted quality of service (QoS). To enable this capability, the necessity of considering an integrated vertical heterogeneous network (VHetNet) architecture appears to be inevitable due to its high inherent agility. Moreover, employing an intelligent framework is another crucial requirement for self-evolving networks to deal with real-time network optimization problems. Hence, in this work, to provide a better insight on network architecture design in support of self-evolving networks, we highlight the merits of integrated VHetNet architecture while proposing an intelligent framework for self-evolving integrated vertical heterogeneous networks (SEI-VHetNets). The impact of the challenges associated with SEI-VHetNet architecture, on network management is also studied considering a generalized network model. Furthermore, the current literature on network management of integrated VHetNets along with the recent advancements in artificial intelligence (AI)/machine learning (ML) solutions are discussed. Accordingly, the core challenges of integrating AI/ML in SEI-VHetNets are identified. Finally, the potential future research directions for advancing the autonomous and self-evolving capabilities of SEI-VHetNets are discussed. △ Less","29 December, 2022",https://arxiv.org/pdf/2106.13950
Core Challenges in Embodied Vision-Language Planning,Jonathan Francis;Nariaki Kitamura;Felix Labelle;Xiaopeng Lu;Ingrid Navarro;Jean Oh,"Recent advances in the areas of multimodal machine learning and artificial intelligence (AI) have led to the development of challenging tasks at the intersection of Computer Vision, Natural Language Processing, and Embodied AI. Whereas many approaches and previous survey pursuits have characterised one or two of these dimensions, there has not been a holistic analysis at the center of all three. Moreover, even when combinations of these topics are considered, more focus is placed on describing, e.g., current architectural methods, as opposed to also illustrating high-level challenges and opportunities for the field. In this survey paper, we discuss Embodied Vision-Language Planning (EVLP) tasks, a family of prominent embodied navigation and manipulation problems that jointly use computer vision and natural language. We propose a taxonomy to unify these tasks and provide an in-depth analysis and comparison of the new and current algorithmic approaches, metrics, simulated environments, as well as the datasets used for EVLP tasks. Finally, we present the core challenges that we believe new EVLP works should seek to address, and we advocate for task construction that enables model generalizability and furthers real-world deployment. △ Less","24 May, 2022",https://arxiv.org/pdf/2106.13948
A multi-stage machine learning model on diagnosis of esophageal manometry,Wenjun Kou;Dustin A. Carlson;Alexandra J. Baumann;Erica N. Donnan;Jacob M. Schauer;Mozziyar Etemadi;John E. Pandolfino,"High-resolution manometry (HRM) is the primary procedure used to diagnose esophageal motility disorders. Its interpretation and classification includes an initial evaluation of swallow-level outcomes and then derivation of a study-level diagnosis based on Chicago Classification (CC), using a tree-like algorithm. This diagnostic approach on motility disordered using HRM was mirrored using a multi-stage modeling framework developed using a combination of various machine learning approaches. Specifically, the framework includes deep-learning models at the swallow-level stage and feature-based machine learning models at the study-level stage. In the swallow-level stage, three models based on convolutional neural networks (CNNs) were developed to predict swallow type, swallow pressurization, and integrated relaxation pressure (IRP). At the study-level stage, model selection from families of the expert-knowledge-based rule models, xgboost models and artificial neural network(ANN) models were conducted, with the latter two model designed and augmented with motivation from the export knowledge. A simple model-agnostic strategy of model balancing motivated by Bayesian principles was utilized, which gave rise to model averaging weighted by precision scores. The averaged (blended) models and individual models were compared and evaluated, of which the best performance on test dataset is 0.81 in top-1 prediction, 0.92 in top-2 predictions. This is the first artificial-intelligence-style model to automatically predict CC diagnosis of HRM study from raw multi-swallow data. Moreover, the proposed modeling framework could be easily extended to multi-modal tasks, such as diagnosis of esophageal patients based on clinical data from both HRM and functional luminal imaging probe panometry (FLIP). △ Less","24 May, 2022",https://arxiv.org/pdf/2106.13869
SeaNet -- Towards A Knowledge Graph Based Autonomic Management of Software Defined Networks,Qianru Zhou;Alasdair J. G. Gray;Stephen McLaughlin,"Automatic network management driven by Artificial Intelligent technologies has been heatedly discussed over decades. However, current reports mainly focus on theoretic proposals and architecture designs, works on practical implementations on real-life networks are yet to appear. This paper proposes our effort toward the implementation of knowledge graph driven approach for autonomic network management in software defined networks (SDNs), termed as SeaNet. Driven by the ToCo ontology, SeaNet is reprogrammed based on Mininet (a SDN emulator). It consists three core components, a knowledge graph generator, a SPARQL engine, and a network management API. The knowledge graph generator represents the knowledge in the telecommunication network management tasks into formally represented ontology driven model. Expert experience and network management rules can be formalized into knowledge graph and by automatically inferenced by SPARQL engine, Network management API is able to packet technology-specific details and expose technology-independent interfaces to users. The Experiments are carried out to evaluate proposed work by comparing with a commercial SDN controller Ryu implemented by the same language Python. The evaluation results show that SeaNet is considerably faster in most circumstances than Ryu and the SeaNet code is significantly more compact. Benefit from RDF reasoning, SeaNet is able to achieve O(1) time complexity on different scales of the knowledge graph while the traditional database can achieve O(nlogn) at its best. With the developed network management API, SeaNet enables researchers to develop semantic-intelligent applications on their own SDNs. △ Less","27 May, 2022",https://arxiv.org/pdf/2106.13367
Three-body problem -- from Newton to supercomputer plus machine learning,Shijun Liao;Xiaoming Li;Yu Yang,"The famous three-body problem can be traced back to Newton in 1687, but quite few families of periodic orbits were found in 300 years thereafter. In this paper, we propose an effective approach and roadmap to numerically gain planar periodic orbits of three-body systems with arbitrary masses by means of machine learning based on an artificial neural network (ANN) model. Given any a known periodic orbit as a starting point, this approach can provide more and more periodic orbits (of the same family name) with variable masses, while the mass domain having periodic orbits becomes larger and larger, and the ANN model becomes wiser and wiser. Finally we have an ANN model trained by means of all obtained periodic orbits of the same family, which provides a convenient way to give accurate enough predictions of periodic orbits with arbitrary masses for physicists and astronomers. It suggests that the high-performance computer and artificial intelligence (including machine learning) should be the key to gain periodic orbits of the famous three-body problem. △ Less","5 May, 2022",https://arxiv.org/pdf/2106.11010
Rational Shapley Values,David S. Watson,"Explaining the predictions of opaque machine learning algorithms is an important and challenging task, especially as complex models are increasingly used to assist in high-stakes decisions such as those arising in healthcare and finance. Most popular tools for post-hoc explainable artificial intelligence (XAI) are either insensitive to context (e.g., feature attributions) or difficult to summarize (e.g., counterfactuals). In this paper, I introduce \textit{rational Shapley values}, a novel XAI method that synthesizes and extends these seemingly incompatible approaches in a rigorous, flexible manner. I leverage tools from decision theory and causal modeling to formalize and implement a pragmatic approach that resolves a number of known challenges in XAI. By pairing the distribution of random variables with the appropriate reference class for a given explanation task, I illustrate through theory and experiments how user goals and knowledge can inform and constrain the solution set in an iterative fashion. The method compares favorably to state of the art XAI tools in a range of quantitative and qualitative comparisons. △ Less","16 May, 2022",https://arxiv.org/pdf/2106.10191
A Direct Slip Ratio Estimation Method based on an Intelligent Tire and Machine Learning,Nan Xu;Zepeng Tang;Hassan Askari;Jianfeng Zhou;Amir Khajepour,"Accurate estimation of the tire slip ratio is critical for vehicle safety, as it is necessary for vehicle control purposes. In this paper, an intelligent tire system is presented to develop a novel slip ratio estimation model using machine learning algorithms. The accelerations, generated by a triaxial accelerometer installed onto the inner liner of the tire, are varied when the tire rotates to update the contact patch. Meanwhile, the slip ratio reference value can be measured by the MTS Flat-Trac tire test platform. Then, by analyzing the variation between the accelerations and slip ratio, highly useful features are discovered, which are especially promising for assessing vertical acceleration. For these features, machine learning (ML) algorithms are trained to build the slip ratio estimation model, in which the ML algorithms include artificial neural networks (ANNs), gradient boosting machines (GBMs), random forests (RFs), and support vector machines (SVMs). Finally, the estimated NRMS errors are evaluated using 10-fold cross-validation (CV). The proposed estimation model is able to estimate the slip ratio continuously and stably using only the acceleration from the intelligent tire system, and the estimated slip ratio range can reach 30%. The estimation results have high robustness to vehicle velocity and load, where the best NRMS errors can reach 4.88%. In summary, the present study with the fusion of an intelligent tire system and machine learning paves the way for the accurate estimation of the tire slip ratio under different driving conditions, which create new opportunities for autonomous vehicles, intelligent tires, and tire slip ratio estimation. △ Less","22 January, 2022",https://arxiv.org/pdf/2106.08961
CBLUE: A Chinese Biomedical Language Understanding Evaluation Benchmark,Ningyu Zhang;Mosha Chen;Zhen Bi;Xiaozhuan Liang;Lei Li;Xin Shang;Kangping Yin;Chuanqi Tan;Jian Xu;Fei Huang;Luo Si;Yuan Ni;Guotong Xie;Zhifang Sui;Baobao Chang;Hui Zong;Zheng Yuan;Linfeng Li;Jun Yan;Hongying Zan;Kunli Zhang;Buzhou Tang;Qingcai Chen,"Artificial Intelligence (AI), along with the recent progress in biomedical language understanding, is gradually changing medical practice. With the development of biomedical language understanding benchmarks, AI applications are widely used in the medical field. However, most benchmarks are limited to English, which makes it challenging to replicate many of the successes in English for other languages. To facilitate research in this direction, we collect real-world biomedical data and present the first Chinese Biomedical Language Understanding Evaluation (CBLUE) benchmark: a collection of natural language understanding tasks including named entity recognition, information extraction, clinical diagnosis normalization, single-sentence/sentence-pair classification, and an associated online platform for model evaluation, comparison, and analysis. To establish evaluation on these tasks, we report empirical results with the current 11 pre-trained Chinese models, and experimental results show that state-of-the-art neural models perform by far worse than the human ceiling. Our benchmark is released at \url{https://tianchi.aliyun.com/dataset/dataDetail?dataId=95414&lang=en-us}. △ Less","7 March, 2022",https://arxiv.org/pdf/2106.08087
Non-Transferable Learning: A New Approach for Model Ownership Verification and Applicability Authorization,Lixu Wang;Shichao Xu;Ruiqi Xu;Xiao Wang;Qi Zhu,"As Artificial Intelligence as a Service gains popularity, protecting well-trained models as intellectual property is becoming increasingly important. There are two common types of protection methods: ownership verification and usage authorization. In this paper, we propose Non-Transferable Learning (NTL), a novel approach that captures the exclusive data representation in the learned model and restricts the model generalization ability to certain domains. This approach provides effective solutions to both model verification and authorization. Specifically: 1) For ownership verification, watermarking techniques are commonly used but are often vulnerable to sophisticated watermark removal methods. By comparison, our NTL-based ownership verification provides robust resistance to state-of-the-art watermark removal methods, as shown in extensive experiments with 6 removal approaches over the digits, CIFAR10 & STL10, and VisDA datasets. 2) For usage authorization, prior solutions focus on authorizing specific users to access the model, but authorized users can still apply the model to any data without restriction. Our NTL-based authorization approach instead provides data-centric protection, which we call applicability authorization, by significantly degrading the performance of the model on unauthorized data. Its effectiveness is also shown through experiments on the aforementioned datasets. △ Less","27 February, 2022",https://arxiv.org/pdf/2106.06916
Entropy-based Logic Explanations of Neural Networks,Pietro Barbiero;Gabriele Ciravegna;Francesco Giannini;Pietro Lió;Marco Gori;Stefano Melacci,"Explainable artificial intelligence has rapidly emerged since lawmakers have started requiring interpretable models for safety-critical domains. Concept-based neural networks have arisen as explainable-by-design methods as they leverage human-understandable symbols (i.e. concepts) to predict class memberships. However, most of these approaches focus on the identification of the most relevant concepts but do not provide concise, formal explanations of how such concepts are leveraged by the classifier to make predictions. In this paper, we propose a novel end-to-end differentiable approach enabling the extraction of logic explanations from neural networks using the formalism of First-Order Logic. The method relies on an entropy-based criterion which automatically identifies the most relevant concepts. We consider four different case studies to demonstrate that: (i) this entropy-based criterion enables the distillation of concise logic explanations in safety-critical domains from clinical data to computer vision; (ii) the proposed approach outperforms state-of-the-art white-box models in terms of classification accuracy and matches black box performances. △ Less","31 January, 2022",https://arxiv.org/pdf/2106.06804
Exploiting auto-encoders and segmentation methods for middle-level explanations of image classification systems,Andrea Apicella;Salvatore Giugliano;Francesco Isgrò;Roberto Prevete,"A central issue addressed by the rapidly growing research area of eXplainable Artificial Intelligence (XAI) is to provide methods to give explanations for the behaviours of Machine Learning (ML) non-interpretable models after the training. Recently, it is becoming more and more evident that new directions to create better explanations should take into account what a good explanation is to a human user. This paper suggests taking advantage of developing an XAI framework that allows producing multiple explanations for the response of image a classification system in terms of potentially different middle-level input features. To this end, we propose an XAI framework able to construct explanations in terms of input features extracted by auto-encoders. We start from the hypothesis that some autoencoders, relying on standard data representation approaches, could extract more salient and understandable input properties, which we call here \textit{Middle-Level input Features} (MLFs), for a user with respect to raw low-level features. Furthermore, extracting different types of MLFs through different type of autoencoders, different types of explanations for the same ML system behaviour can be returned. We experimentally tested our method on two different image datasets and using three different types of MLFs. The results are encouraging. Although our novel approach was tested in the context of image classification, it can potentially be used on other data types to the extent that auto-encoders to extract humanly understandable representations can be applied. △ Less","23 August, 2022",https://arxiv.org/pdf/2106.05037
Light Field Networks: Neural Scene Representations with Single-Evaluation Rendering,Vincent Sitzmann;Semon Rezchikov;William T. Freeman;Joshua B. Tenenbaum;Fredo Durand,"Inferring representations of 3D scenes from 2D observations is a fundamental problem of computer graphics, computer vision, and artificial intelligence. Emerging 3D-structured neural scene representations are a promising approach to 3D scene understanding. In this work, we propose a novel neural scene representation, Light Field Networks or LFNs, which represent both geometry and appearance of the underlying 3D scene in a 360-degree, four-dimensional light field parameterized via a neural implicit representation. Rendering a ray from an LFN requires only a single network evaluation, as opposed to hundreds of evaluations per ray for ray-marching or volumetric based renderers in 3D-structured neural scene representations. In the setting of simple scenes, we leverage meta-learning to learn a prior over LFNs that enables multi-view consistent light field reconstruction from as little as a single image observation. This results in dramatic reductions in time and memory complexity, and enables real-time rendering. The cost of storing a 360-degree light field via an LFN is two orders of magnitude lower than conventional methods such as the Lumigraph. Utilizing the analytical differentiability of neural implicit representations and a novel parameterization of light space, we further demonstrate the extraction of sparse depth maps from LFNs. △ Less","18 January, 2022",https://arxiv.org/pdf/2106.02634
Network and Physical Layer Attacks and countermeasures to AI-Enabled 6G O-RAN,Talha F. Rahman;Aly Sabri Abdalla;Keith Powell;Walaa AlQwider;Vuk Marojevic,"Artificial intelligence (AI) will play an increasing role in cellular network deployment, configuration and management. This paper examines the security implications of AI-driven 6G radio access networks (RANs). While the expected timeline for 6G standardization is still several years out, pre-standardization efforts related to 6G security are already ongoing and will benefit from fundamental and experimental research. The Open RAN (O-RAN) describes an industry-driven open architecture and interfaces for building next generation RANs with AI control. Considering this architecture, we identify the critical threats to data driven network and physical layer elements, the corresponding countermeasures, and the research directions. △ Less","2 September, 2022",https://arxiv.org/pdf/2106.02494
A Clarification of the Nuances in the Fairness Metrics Landscape,Alessandro Castelnovo;Riccardo Crupi;Greta Greco;Daniele Regoli;Ilaria Giuseppina Penco;Andrea Claudio Cosentini,"In recent years, the problem of addressing fairness in Machine Learning (ML) and automatic decision-making has attracted a lot of attention in the scientific communities dealing with Artificial Intelligence. A plethora of different definitions of fairness in ML have been proposed, that consider different notions of what is a ""fair decision"" in situations impacting individuals in the population. The precise differences, implications and ""orthogonality"" between these notions have not yet been fully analyzed in the literature. In this work, we try to make some order out of this zoo of definitions. △ Less","11 March, 2022",https://arxiv.org/pdf/2106.00467
The effectiveness of feature attribution methods and its correlation with automatic evaluation scores,Giang Nguyen;Daeyoung Kim;Anh Nguyen,"Explaining the decisions of an Artificial Intelligence (AI) model is increasingly critical in many real-world, high-stake applications. Hundreds of papers have either proposed new feature attribution methods, discussed or harnessed these tools in their work. However, despite humans being the target end-users, most attribution methods were only evaluated on proxy automatic-evaluation metrics (Zhang et al. 2018; Zhou et al. 2016; Petsiuk et al. 2018). In this paper, we conduct the first user study to measure attribution map effectiveness in assisting humans in ImageNet classification and Stanford Dogs fine-grained classification, and when an image is natural or adversarial (i.e., contains adversarial perturbations). Overall, feature attribution is surprisingly not more effective than showing humans nearest training-set examples. On a harder task of fine-grained dog categorization, presenting attribution maps to humans does not help, but instead hurts the performance of human-AI teams compared to AI alone. Importantly, we found automatic attribution-map evaluation measures to correlate poorly with the actual human-AI team performance. Our findings encourage the community to rigorously test their methods on the downstream human-in-the-loop applications and to rethink the existing evaluation metrics. △ Less","22 July, 2022",https://arxiv.org/pdf/2105.14944
Designing ECG Monitoring Healthcare System with Federated Transfer Learning and Explainable AI,Ali Raza;Kim Phuc Tran;Ludovic Koehl;Shujun Li,"Deep learning play a vital role in classifying different arrhythmias using the electrocardiography (ECG) data. Nevertheless, training deep learning models normally requires a large amount of data and it can lead to privacy concerns. Unfortunately, a large amount of healthcare data cannot be easily collected from a single silo. Additionally, deep learning models are like black-box, with no explainability of the predicted results, which is often required in clinical healthcare. This limits the application of deep learning in real-world health systems. In this paper, we design a new explainable artificial intelligence (XAI) based deep learning framework in a federated setting for ECG-based healthcare applications. The federated setting is used to solve issues such as data availability and privacy concerns. Furthermore, the proposed framework setting effectively classifies arrhythmia's using an autoencoder and a classifier, both based on a convolutional neural network (CNN). Additionally, we propose an XAI-based module on top of the proposed classifier to explain the classification results, which help clinical practitioners make quick and reliable decisions. The proposed framework was trained and tested using the MIT-BIH Arrhythmia database. The classifier achieved accuracy up to 94% and 98% for arrhythmia detection using noisy and clean data, respectively, with five-fold cross-validation. △ Less","10 January, 2022",https://arxiv.org/pdf/2105.12497
Towards Trusted and Intelligent Cyber-Physical Systems: A Security-by-Design Approach,Sabah Suhail;Raja Jurdak,"The complexity of cyberattacks in Cyber-Physical Systems (CPSs) calls for a mechanism that can evaluate the operational behaviour and security without negatively affecting the operation of live systems. In this regard, Digital Twins (DTs) are revolutionizing the CPSs. DTs strengthen the security of CPSs throughout the product lifecycle, while assuming that the DT data is trusted, providing agility to predict and respond to real-time changes. However, existing DTs solutions in CPS are constrained with untrustworthy data dissemination among multiple stakeholders and timely course correction. Such limitations reinforce the significance of designing trustworthy distributed solutions with the ability to create actionable insights in real-time. To do so, we propose a framework that focuses on trusted and intelligent DT by integrating blockchain and Artificial Intelligence (AI). Following a hybrid approach, the proposed framework not only acquires process knowledge from the specifications of the CPS, but also relies on AI to learn security threats based on sensor data. Furthermore, we integrate blockchain to safeguard product lifecycle data. We discuss the applicability of the proposed framework for the automotive industry as a CPS use case. Finally, we identify the open challenges that impede the implementation of intelligence-driven architectures in CPSs. △ Less","2 May, 2022",https://arxiv.org/pdf/2105.08886
Conscious AI,Hadi Esmaeilzadeh;Reza Vaezi,"Recent advances in artificial intelligence (AI) have achieved human-scale speed and accuracy for classification tasks. In turn, these capabilities have made AI a viable replacement for many human activities that at their core involve classification, such as basic mechanical and analytical tasks in low-level service jobs. Current systems do not need to be conscious to recognize patterns and classify them. However, for AI to progress to more complicated tasks requiring intuition and empathy, it must develop capabilities such as metathinking, creativity, and empathy akin to human self-awareness or consciousness. We contend that such a paradigm shift is possible only through a fundamental shift in the state of artificial intelligence toward consciousness, a shift similar to what took place for humans through the process of natural selection and evolution. As such, this paper aims to theoretically explore the requirements for the emergence of consciousness in AI. It also provides a principled understanding of how conscious AI can be detected and how it might be manifested in contrast to the dominant paradigm that seeks to ultimately create machines that are linguistically indistinguishable from humans. △ Less","20 May, 2022",https://arxiv.org/pdf/2105.07879
People construct simplified mental representations to plan,Mark K. Ho;David Abel;Carlos G. Correa;Michael L. Littman;Jonathan D. Cohen;Thomas L. Griffiths,"One of the most striking features of human cognition is the capacity to plan. Two aspects of human planning stand out: its efficiency and flexibility. Efficiency is especially impressive because plans must often be made in complex environments, and yet people successfully plan solutions to myriad everyday problems despite having limited cognitive resources. Standard accounts in psychology, economics, and artificial intelligence have suggested human planning succeeds because people have a complete representation of a task and then use heuristics to plan future actions in that representation. However, this approach generally assumes that task representations are fixed. Here, we propose that task representations can be controlled and that such control provides opportunities to quickly simplify problems and more easily reason about them. We propose a computational account of this simplification process and, in a series of pre-registered behavioral experiments, show that it is subject to online cognitive control and that people optimally balance the complexity of a task representation and its utility for planning and acting. These results demonstrate how strategically perceiving and conceiving problems facilitates the effective use of limited cognitive resources. △ Less","26 November, 2022",https://arxiv.org/pdf/2105.06948
Dynamic Multi-Branch Layers for On-Device Neural Machine Translation,Zhixing Tan;Zeyuan Yang;Meng Zhang;Qun Liu;Maosong Sun;Yang Liu,"With the rapid development of artificial intelligence (AI), there is a trend in moving AI applications, such as neural machine translation (NMT), from cloud to mobile devices. Constrained by limited hardware resources and battery, the performance of on-device NMT systems is far from satisfactory. Inspired by conditional computation, we propose to improve the performance of on-device NMT systems with dynamic multi-branch layers. Specifically, we design a layer-wise dynamic multi-branch network with only one branch activated during training and inference. As not all branches are activated during training, we propose shared-private reparameterization to ensure sufficient training for each branch. At almost the same computational cost, our method achieves improvements of up to 1.7 BLEU points on the WMT14 English-German translation task and 1.8 BLEU points on the WMT20 Chinese-English translation task over the Transformer model, respectively. Compared with a strong baseline that also uses multiple branches, the proposed method is up to 1.5 times faster with the same number of parameters. △ Less","17 March, 2022",https://arxiv.org/pdf/2105.06679
Conversational AI Systems for Social Good: Opportunities and Challenges,Peng Qi;Jing Huang;Youzheng Wu;Xiaodong He;Bowen Zhou,"Conversational artificial intelligence (ConvAI) systems have attracted much academic and commercial attention recently, making significant progress on both fronts. However, little existing work discusses how these systems can be developed and deployed for social good in real-world applications, with comprehensive case studies and analyses of pros and cons. In this paper, we briefly review the progress the community has made towards better ConvAI systems and reflect on how existing technologies can help advance social good initiatives from various angles that are unique for ConvAI, or not yet become common knowledge in the community. We further discuss about the challenges ahead for ConvAI systems to better help us achieve these goals and highlight the risks involved in their development and deployment in the real world. △ Less","7 January, 2022",https://arxiv.org/pdf/2105.06457
SUPR-GAN: SUrgical PRediction GAN for Event Anticipation in Laparoscopic and Robotic Surgery,Yutong Ban;Guy Rosman;Jennifer A. Eckhoff;Thomas M. Ward;Daniel A. Hashimoto;Taisei Kondo;Hidekazu Iwaki;Ozanan R. Meireles;Daniela Rus,"Comprehension of surgical workflow is the foundation upon which artificial intelligence (AI) and machine learning (ML) holds the potential to assist intraoperative decision-making and risk mitigation. In this work, we move beyond mere identification of past surgical phases, into the prediction of future surgical steps and specification of the transitions between them. We use a novel Generative Adversarial Network (GAN) formulation to sample future surgical phases trajectories conditioned on past video frames from laparoscopic cholecystectomy (LC) videos and compare it to state-of-the-art approaches for surgical video analysis and alternative prediction methods. We demonstrate the GAN formulation's effectiveness through inferring and predicting the progress of LC videos. We quantify the horizon-accuracy trade-off and explored average performance, as well as the performance on the more challenging, and clinically relevant transitions between phases. Furthermore, we conduct a survey, asking 16 surgeons of different specialties and educational levels to qualitatively evaluate predicted surgery phases. △ Less","9 March, 2022",https://arxiv.org/pdf/2105.04642
Pervasive AI for IoT applications: A Survey on Resource-efficient Distributed Artificial Intelligence,Emna Baccour;Naram Mhaisen;Alaa Awad Abdellatif;Aiman Erbad;Amr Mohamed;Mounir Hamdi;Mohsen Guizani,"Artificial intelligence (AI) has witnessed a substantial breakthrough in a variety of Internet of Things (IoT) applications and services, spanning from recommendation systems to robotics control and military surveillance. This is driven by the easier access to sensory data and the enormous scale of pervasive/ubiquitous devices that generate zettabytes (ZB) of real-time data streams. Designing accurate models using such data streams, to predict future insights and revolutionize the decision-taking process, inaugurates pervasive systems as a worthy paradigm for a better quality-of-life. The confluence of pervasive computing and artificial intelligence, Pervasive AI, expanded the role of ubiquitous IoT systems from mainly data collection to executing distributed computations with a promising alternative to centralized learning, presenting various challenges. In this context, a wise cooperation and resource scheduling should be envisaged among IoT devices (e.g., smartphones, smart vehicles) and infrastructure (e.g. edge nodes, and base stations) to avoid communication and computation overheads and ensure maximum performance. In this paper, we conduct a comprehensive survey of the recent techniques developed to overcome these resource challenges in pervasive AI systems. Specifically, we first present an overview of the pervasive computing, its architecture, and its intersection with artificial intelligence. We then review the background, applications and performance metrics of AI, particularly Deep Learning (DL) and online learning, running in a ubiquitous system. Next, we provide a deep literature review of communication-efficient techniques, from both algorithmic and system perspectives, of distributed inference, training and online learning tasks across the combination of IoT devices, edge devices and cloud servers. Finally, we discuss our future vision and research challenges. △ Less","27 August, 2022",https://arxiv.org/pdf/2105.01798
"Intelligent Zero Trust Architecture for 5G/6G Networks: Principles, Challenges, and the Role of Machine Learning in the context of O-RAN",Keyvan Ramezanpour;Jithin Jagannath,"In this position paper, we discuss the critical need for integrating zero trust (ZT) principles into next-generation communication networks (5G/6G). We highlight the challenges and introduce the concept of an intelligent zero trust architecture (i-ZTA) as a security framework in 5G/6G networks with untrusted components. While network virtualization, software-defined networking (SDN), and service-based architectures (SBA) are key enablers of 5G networks, operating in an untrusted environment has also become a key feature of the networks. Further, seamless connectivity to a high volume of devices has broadened the attack surface on information infrastructure. Network assurance in a dynamic untrusted environment calls for revolutionary architectures beyond existing static security frameworks. To the best of our knowledge, this is the first position paper that presents the architectural concept design of an i-ZTA upon which modern artificial intelligence (AI) algorithms can be developed to provide information security in untrusted networks. We introduce key ZT principles as real-time Monitoring of the security state of network assets, Evaluating the risk of individual access requests, and Deciding on access authorization using a dynamic trust algorithm, called MED components. To ensure ease of integration, the envisioned architecture adopts an SBA-based design, similar to the 3GPP specification of 5G networks, by leveraging the open radio access network (O-RAN) architecture with appropriate real-time engines and network interfaces for collecting necessary machine learning data. Therefore, this work provides novel research directions to design machine learning based components that contribute towards i-ZTA for the future 5G/6G networks. △ Less","27 July, 2022",https://arxiv.org/pdf/2105.01478
Bird-Area Water-Bodies Dataset (BAWD) and Predictive AI Model for Avian Botulism Outbreak (AVI-BoT),Narayani Bhatia;Devang Mahesh;Jashandeep Singh;Manan Suri,"Avian botulism is a paralytic bacterial disease in birds often leading to high fatality. In-vitro diagnostic techniques such as Mouse Bioassay, ELISA, PCR are usually non-preventive, post-mortem in nature, and require invasive sample collection from affected sites or dead birds. In this study, we build a first-ever multi-spectral, remote-sensing imagery based global Bird-Area Water-bodies Dataset (BAWD) (i.e. fused satellite images of warm-water lakes/marshy-lands or similar water-body sites that are important for avian fauna) backed by on-ground reporting evidence of outbreaks. BAWD consists of 16 topographically diverse global sites monitored over a time-span of 4 years (2016-2021). We propose a first-ever Artificial Intelligence based (AI) model to predict potential outbreak of Avian botulism called AVI-BoT (Aerosol Visible, Infra-red (NIR/SWIR) and Bands of Thermal). We also train and investigate a simpler (5-band) Causative-Factor model (based on prominent physiological factors reported in literature) to predict Avian botulism. AVI-BoT demonstrates a training accuracy of 0.96 and validation accuracy of 0.989 on BAWD, far superior in comparison to our model based on causative factors. We also perform an ablation study and perform a detailed feature-space analysis. We further analyze three test case study locations - Lower Klamath National Wildlife Refuge and Langvlei and Rondevlei lakes where an outbreak had occurred, and Pong Dam where an outbreak had not occurred and confirm predictions with on-ground reportings. The proposed technique presents a scale-able, low-cost, non-invasive methodology for continuous monitoring of bird-habitats against botulism outbreaks with the potential of saving valuable fauna lives. △ Less","17 November, 2022",https://arxiv.org/pdf/2105.00924
End-to-End Video-To-Speech Synthesis using Generative Adversarial Networks,Rodrigo Mira;Konstantinos Vougioukas;Pingchuan Ma;Stavros Petridis;Björn W. Schuller;Maja Pantic,"Video-to-speech is the process of reconstructing the audio speech from a video of a spoken utterance. Previous approaches to this task have relied on a two-step process where an intermediate representation is inferred from the video, and is then decoded into waveform audio using a vocoder or a waveform reconstruction algorithm. In this work, we propose a new end-to-end video-to-speech model based on Generative Adversarial Networks (GANs) which translates spoken video to waveform end-to-end without using any intermediate representation or separate waveform synthesis algorithm. Our model consists of an encoder-decoder architecture that receives raw video as input and generates speech, which is then fed to a waveform critic and a power critic. The use of an adversarial loss based on these two critics enables the direct synthesis of raw audio waveform and ensures its realism. In addition, the use of our three comparative losses helps establish direct correspondence between the generated audio and the input video. We show that this model is able to reconstruct speech with remarkable realism for constrained datasets such as GRID, and that it is the first end-to-end model to produce intelligible speech for LRW (Lip Reading in the Wild), featuring hundreds of speakers recorded entirely `in the wild'. We evaluate the generated samples in two different scenarios -- seen and unseen speakers -- using four objective metrics which measure the quality and intelligibility of artificial speech. We demonstrate that the proposed approach outperforms all previous works in most metrics on GRID and LRW. △ Less","15 August, 2022",https://arxiv.org/pdf/2104.13332
TrustyAI Explainability Toolkit,Rob Geada;Tommaso Teofili;Rui Vieira;Rebecca Whitworth;Daniele Zonca,"Artificial intelligence (AI) is becoming increasingly more popular and can be found in workplaces and homes around the world. The decisions made by such ""black box"" systems are often opaque; that is, so complex as to be functionally impossible to understand. How do we ensure that these systems are behaving as desired? TrustyAI is an initiative which looks into explainable artificial intelligence (XAI) solutions to address this issue of explainability in the context of both AI models and decision services. This paper presents the TrustyAI Explainability Toolkit, a Java and Python library that provides XAI explanations of decision services and predictive models for both enterprise and data science use-cases. We describe the TrustyAI implementations and extensions to techniques such as LIME, SHAP and counterfactuals, which are benchmarked against existing implementations in a variety of experiments. △ Less","26 May, 2022",https://arxiv.org/pdf/2104.12717
Exploiting Explanations for Model Inversion Attacks,Xuejun Zhao;Wencan Zhang;Xiaokui Xiao;Brian Y. Lim,"The successful deployment of artificial intelligence (AI) in many domains from healthcare to hiring requires their responsible use, particularly in model explanations and privacy. Explainable artificial intelligence (XAI) provides more information to help users to understand model decisions, yet this additional knowledge exposes additional risks for privacy attacks. Hence, providing explanation harms privacy. We study this risk for image-based model inversion attacks and identified several attack architectures with increasing performance to reconstruct private image data from model explanations. We have developed several multi-modal transposed CNN architectures that achieve significantly higher inversion performance than using the target model prediction only. These XAI-aware inversion models were designed to exploit the spatial knowledge in image explanations. To understand which explanations have higher privacy risk, we analyzed how various explanation types and factors influence inversion performance. In spite of some models not providing explanations, we further demonstrate increased inversion performance even for non-explainable target models by exploiting explanations of surrogate models through attention transfer. This method first inverts an explanation from the target prediction, then reconstructs the target image. These threats highlight the urgent and significant privacy risks of explanations and calls attention for new privacy preservation techniques that balance the dual-requirement for AI explainability and privacy. △ Less","14 March, 2022",https://arxiv.org/pdf/2104.12669
Causal Learning for Socially Responsible AI,Lu Cheng;Ahmadreza Mosallanezhad;Paras Sheth;Huan Liu,"There have been increasing concerns about Artificial Intelligence (AI) due to its unfathomable potential power. To make AI address ethical challenges and shun undesirable outcomes, researchers proposed to develop socially responsible AI (SRAI). One of these approaches is causal learning (CL). We survey state-of-the-art methods of CL for SRAI. We begin by examining the seven CL tools to enhance the social responsibility of AI, then review how existing works have succeeded using these tools to tackle issues in developing SRAI such as fairness. The goal of this survey is to bring forefront the potentials and promises of CL for SRAI. △ Less","2 May, 2022",https://arxiv.org/pdf/2104.12278
An End-to-End Computer Vision Methodology for Quantitative Metallography,Matan Rusanovsky;Ofer Beeri;Gal Oren,"Metallography is crucial for a proper assessment of material's properties. It involves mainly the investigation of spatial distribution of grains and the occurrence and characteristics of inclusions or precipitates. This work presents an holistic artificial intelligence model for Anomaly Detection that automatically quantifies the degree of anomaly of impurities in alloys. We suggest the following examination process: (1) Deep semantic segmentation is performed on the inclusions (based on a suitable metallographic database of alloys and corresponding tags of inclusions), producing inclusions masks that are saved into a separated database. (2) Deep image inpainting is performed to fill the removed inclusions parts, resulting in 'clean' metallographic images, which contain the background of grains. (3) Grains' boundaries are marked using deep semantic segmentation (based on another metallographic database of alloys), producing boundaries that are ready for further inspection on the distribution of grains' size. (4) Deep anomaly detection and pattern recognition is performed on the inclusions masks to determine spatial, shape and area anomaly detection of the inclusions. Finally, the system recommends to an expert on areas of interests for further examination. The performance of the model is presented and analyzed based on few representative cases. Although the models presented here were developed for metallography analysis, most of them can be generalized to a wider set of problems in which anomaly detection of geometrical objects is desired. All models as well as the data-sets that were created for this work, are publicly available at https://github.com/Scientific-Computing-Lab-NRCN/MLography. △ Less","1 March, 2022",https://arxiv.org/pdf/2104.11159
Randomized Algorithms for Scientific Computing (RASC),Aydin Buluc;Tamara G. Kolda;Stefan M. Wild;Mihai Anitescu;Anthony DeGennaro;John Jakeman;Chandrika Kamath;Ramakrishnan Kannan;Miles E. Lopes;Per-Gunnar Martinsson;Kary Myers;Jelani Nelson;Juan M. Restrepo;C. Seshadhri;Draguna Vrabie;Brendt Wohlberg;Stephen J. Wright;Chao Yang;Peter Zwart,"Randomized algorithms have propelled advances in artificial intelligence and represent a foundational research area in advancing AI for Science. Future advancements in DOE Office of Science priority areas such as climate science, astrophysics, fusion, advanced materials, combustion, and quantum computing all require randomized algorithms for surmounting challenges of complexity, robustness, and scalability. This report summarizes the outcomes of that workshop, ""Randomized Algorithms for Scientific Computing (RASC),"" held virtually across four days in December 2020 and January 2021. △ Less","21 March, 2022",https://arxiv.org/pdf/2104.11079
"On Generating and Labeling Network Traffic with Realistic, Self-Propagating Malware",Molly Buchanan;Jeffrey W. Collyer;Jack W. Davidson;Saikat Dey;Mark Gardner;Jason D. Hiser;Jeffry Lang;Alastair Nottingham;Alina Oprea,"Research and development of techniques which detect or remediate malicious network activity require access to diverse, realistic, contemporary data sets containing labeled malicious connections. In the absence of such data, said techniques cannot be meaningfully trained, tested, and evaluated. Synthetically produced data containing fabricated or merged network traffic is of limited value as it is easily distinguishable from real traffic by even simple machine learning (ML) algorithms. Real network data is preferable, but while ubiquitous is broadly both sensitive and lacking in ground truth labels, limiting its utility for ML research. This paper presents a multi-faceted approach to generating a data set of labeled malicious connections embedded within anonymized network traffic collected from large production networks. Real-world malware is defanged and introduced to simulated, secured nodes within those networks to generate realistic traffic while maintaining sufficient isolation to protect real data and infrastructure. Network sensor data, including this embedded malware traffic, is collected at a network edge and anonymized for research use. Network traffic was collected and produced in accordance with the aforementioned methods at two major educational institutions. The result is a highly realistic, long term, multi-institution data set with embedded data labels spanning over 1.5 trillion connections and over a petabyte of sensor log data. The usability of this data set is demonstrated by its utility to our artificial intelligence and machine learning (AI/ML) research program. △ Less","27 May, 2022",https://arxiv.org/pdf/2104.10034
Toward the Prevention of Privacy Threats: How Can We Persuade Our Social Network Platform Users?,Ramon Ruiz-Dolz;Jose Alemany;Stella Heras;Ana García-Fornes,"Complex decision-making problems such as the privacy policy selection when sharing content in online social networks can significantly benefit from artificial intelligence systems. With the use of Computational Argumentation, it is possible to persuade human users to modify their initial decisions to avoid potential privacy threats and violations. In this paper, we present a study performed over 186 teenage users aimed at analysing their behaviour when we try to persuade them to modify the publication of sensitive content in Online Social Networks (OSN) with different arguments. The results of the study revealed that the personality traits and the social interaction data (e.g., number of comments, friends, and likes) of our participants were significantly correlated with the persuasive power of the arguments. Therefore, these sets of features can be used to model OSN users, and to estimate the persuasive power of different arguments when used in human-computer interactions. The findings presented in this paper are helpful for personalising decision support systems aimed at educating and preventing privacy violations in OSNs using arguments. △ Less","9 November, 2022",https://arxiv.org/pdf/2104.10004
Graph Representation Learning in Biomedicine,Michelle M. Li;Kexin Huang;Marinka Zitnik,"Biomedical networks (or graphs) are universal descriptors for systems of interacting elements, from molecular interactions and disease co-morbidity to healthcare systems and scientific knowledge. Advances in artificial intelligence, specifically deep learning, have enabled us to model, analyze, and learn with such networked data. In this review, we put forward an observation that long-standing principles of systems biology and medicine -- while often unspoken in machine learning research -- provide the conceptual grounding for representation learning on graphs, explain its current successes and limitations, and even inform future advancements. We synthesize a spectrum of algorithmic approaches that, at their core, leverage graph topology to embed networks into compact vector spaces. We also capture the breadth of ways in which representation learning has dramatically improved the state-of-the-art in biomedical machine learning. Exemplary domains covered include identifying variants underlying complex traits, disentangling behaviors of single cells and their effects on health, assisting in diagnosis and treatment of patients, and developing safe and effective medicines. △ Less","10 June, 2022",https://arxiv.org/pdf/2104.04883
Smart and Secure CAV Networks Empowered by AI-Enabled Blockchain: The Next Frontier for Intelligent Safe Driving Assessment,Le Xia;Yao Sun;Rafiq Swash;Lina Mohjazi;Lei Zhang;Muhammad Ali Imran,"Securing safe driving for connected and autonomous vehicles (CAVs) continues to be a widespread concern, despite various sophisticated functions delivered by artificial intelligence for in-vehicle devices. Diverse malicious network attacks are ubiquitous, along with the worldwide implementation of the Internet of Vehicles, which exposes a range of reliability and privacy threats for managing data in CAV networks. Combined with the fact that the capability of existing CAVs in handling intensive computation tasks is limited, this implies a need for designing an efficient assessment system to guarantee autonomous driving safety without compromising data security. In this article we propose a novel framework, namely Blockchain-enabled intElligent Safe-driving assessmenT (BEST), which offers a smart and reliable approach for conducting safe driving supervision while protecting vehicular information. Specifically, a promising solution that exploits a long short-term memory model is introduced to assess the safety level of the moving CAVs. Then we investigate how a distributed blockchain obtains adequate trustworthiness and robustness for CAV data by adopting a byzantine fault tolerance-based delegated proof-of-stake consensus mechanism. Simulation results demonstrate that our presented BEST gains better data credibility with a higher prediction accuracy for vehicular safety assessment when compared with existing schemes. Finally, we discuss several open challenges that need to be addressed in future CAV networks. △ Less","11 February, 2022",https://arxiv.org/pdf/2104.04572
Detecting of a Patient's Condition From Clinical Narratives Using Natural Language Representation,Thanh-Dung Le;Rita Noumeir;Jerome Rambaud;Guillaume Sans;Philippe Jouvet,"The rapid progress in clinical data management systems and artificial intelligence approaches enable the era of personalized medicine. Intensive care units (ICUs) are the ideal clinical research environment for such development because they collect many clinical data and are highly computerized environments. We designed a retrospective clinical study on a prospective ICU database using clinical natural language to help in the early diagnosis of heart failure in critically ill children. The methodology consisted of empirical experiments of a learning algorithm to learn the hidden interpretation and presentation of the French clinical note data. This study included 1386 patients' clinical notes with 5444 single lines of notes. There were 1941 positive cases (36 % of total) and 3503 negative cases classified by two independent physicians using a standardized approach. The multilayer perceptron neural network outperforms other discriminative and generative classifiers. Consequently, the proposed framework yields an overall classification performance with 89 % accuracy, 88 % recall, and 89 % precision. This study successfully applied learning representation and machine learning algorithms to detect heart failure from clinical natural language in a single French institution. Further work is needed to use the same methodology in other institutions and other languages. △ Less","23 September, 2022",https://arxiv.org/pdf/2104.03969
ASER: Towards Large-scale Commonsense Knowledge Acquisition via Higher-order Selectional Preference over Eventualities,Hongming Zhang;Xin Liu;Haojie Pan;Haowen Ke;Jiefu Ou;Tianqing Fang;Yangqiu Song,"Commonsense knowledge acquisition and reasoning have long been a core artificial intelligence problem. However, in the past, there has been a lack of scalable methods to collect commonsense knowledge. In this paper, we propose to develop principles for collecting commonsense knowledge based on selectional preference. We generalize the definition of selectional preference from one-hop linguistic syntactic relations to higher-order relations over linguistic graphs. Unlike previous commonsense knowledge definition (e.g., ConceptNet), selectional preference (SP) knowledge only relies on statistical distribution over linguistic graphs, which can be efficiently and accurately acquired from the unlabeled corpus with modern tools. Following this principle, we develop a large-scale eventuality (a linguistic term covering activity, state, and event)-based knowledge graph ASER, where each eventuality is represented as a dependency graph, and the relation between them is a discourse relation defined in shallow discourse parsing. The higher-order selectional preference over collected linguistic graphs reflects various kinds of commonsense knowledge. Moreover, motivated by the observation that humans understand events by abstracting the observed events to a higher level and can thus transfer their knowledge to new events, we propose a conceptualization module to significantly boost the coverage of ASER. In total, ASER contains 648 million edges between 438 million eventualities. After conceptualization with Probase, a selectional preference based concept-instance relational knowledge base, our concept graph contains 15 million conceptualized eventualities and 224 million edges between them. Detailed analysis is provided to demonstrate its quality. All the collected data, APIs, and tools are available at https://github.com/HKUST-KnowComp/ASER. △ Less","16 January, 2022",https://arxiv.org/pdf/2104.02137
quantum Case-Based Reasoning (qCBR),Parfait Atchade-Adelomou;Daniel Casado-Fauli;Elisabet Golobardes-Ribe;Xavier Vilasis-Cardona,"Case-Based Reasoning (CBR) is an artificial intelligence approach to problem-solving with a good record of success. This article proposes using Quantum Computing to improve some of the key processes of CBR, such that a Quantum Case-Based Reasoning (qCBR) paradigm can be defined. The focus is set on designing and implementing a qCBR based on the variational principle that improves its classical counterpart in terms of average accuracy, scalability and tolerance to overlapping. A comparative study of the proposed qCBR with a classic CBR is performed for the case of the Social Workers' Problem as a sample of a combinatorial optimization problem with overlapping. The algorithm's quantum feasibility is modelled with docplex and tested on IBMQ computers, and experimented on the Qibo framework. △ Less","11 January, 2022",https://arxiv.org/pdf/2104.00409
Projection: A Mechanism for Human-like Reasoning in Artificial Intelligence,Frank Guerin,"Artificial Intelligence systems cannot yet match human abilities to apply knowledge to situations that vary from what they have been programmed for, or trained for. In visual object recognition methods of inference exploiting top-down information (from a model) have been shown to be effective for recognising entities in difficult conditions. Here this type of inference, called `projection', is shown to be a key mechanism to solve the problem of applying knowledge to varied or challenging situations, across a range of AI domains, such as vision, robotics, or language. Finally the relevance of projection to tackling the commonsense knowledge problem is discussed. △ Less","17 May, 2022",https://arxiv.org/pdf/2103.13512
"Neuromorphic Computing with Ferroelectric FinFETs in the Presence of Temperature, Process Variation, Device Aging and Flicker Noise",Sourav De;Bo-Han Qiu;Wei-Xuan Bu;Md. Aftab Baig;Chung-Jun Su;Yao-Jen Lee;Darsen Lu,"This paper reports a comprehensive study on the impacts of temperature-change, process variation, flicker noise and device aging on the inference accuracy of pre-trained all-ferroelectric (FE) FinFET deep neural networks. Multiple-level-cell (MLC) operation with a novel adaptive-program-and-read algorithm with 100ns write pulse has been experimentally demonstrated in 5 nm thick hafnium zirconium oxide (HZO)-based FE-FinFET. With pre-trained neural network (NN) with 97.5% inference accuracy on MNIST dataset as baseline, device to device variation is shown to have negligible impact. Flicker noise characterization at various bias conditions depicts that drain current fluctuation is less than 0.7% with virtually no inference accuracy degradation. The conductance drift of a programmed cell, as an aftermath of temperature change, was captured by a compact model over a wide range of gate biases. Despite significant inference accuracy degradation at 233K for a NN trained at 300K, gate bias optimization for recovering the accuracy is demonstrated. Endurance above 10^8 cycles and extrapolated retention above 10 years are shown, which paves the way for edge device artificial intelligence with FE-FinFETs. △ Less","2 July, 2022",https://arxiv.org/pdf/2103.13302
"Interpretable Deep Learning: Interpretation, Interpretability, Trustworthiness, and Beyond",Xuhong Li;Haoyi Xiong;Xingjian Li;Xuanyu Wu;Xiao Zhang;Ji Liu;Jiang Bian;Dejing Dou,"Deep neural networks have been well-known for their superb handling of various machine learning and artificial intelligence tasks. However, due to their over-parameterized black-box nature, it is often difficult to understand the prediction results of deep models. In recent years, many interpretation tools have been proposed to explain or reveal how deep models make decisions. In this paper, we review this line of research and try to make a comprehensive survey. Specifically, we first introduce and clarify two basic concepts -- interpretations and interpretability -- that people usually get confused about. To address the research efforts in interpretations, we elaborate the designs of a number of interpretation algorithms, from different perspectives, by proposing a new taxonomy. Then, to understand the interpretation results, we also survey the performance metrics for evaluating interpretation algorithms. Further, we summarize the current works in evaluating models' interpretability using ""trustworthy"" interpretation algorithms. Finally, we review and discuss the connections between deep models' interpretations and other factors, such as adversarial robustness and learning from interpretations, and we introduce several open-source libraries for interpretation algorithms and evaluation approaches. △ Less","15 July, 2022",https://arxiv.org/pdf/2103.10689
Neural Network Attribution Methods for Problems in Geoscience: A Novel Synthetic Benchmark Dataset,Antonios Mamalakis;Imme Ebert-Uphoff;Elizabeth A. Barnes,"Despite the increasingly successful application of neural networks to many problems in the geosciences, their complex and nonlinear structure makes the interpretation of their predictions difficult, which limits model trust and does not allow scientists to gain physical insights about the problem at hand. Many different methods have been introduced in the emerging field of eXplainable Artificial Intelligence (XAI), which aim at attributing the network s prediction to specific features in the input domain. XAI methods are usually assessed by using benchmark datasets (like MNIST or ImageNet for image classification). However, an objective, theoretically derived ground truth for the attribution is lacking for most of these datasets, making the assessment of XAI in many cases subjective. Also, benchmark datasets specifically designed for problems in geosciences are rare. Here, we provide a framework, based on the use of additively separable functions, to generate attribution benchmark datasets for regression problems for which the ground truth of the attribution is known a priori. We generate a large benchmark dataset and train a fully connected network to learn the underlying function that was used for simulation. We then compare estimated heatmaps from different XAI methods to the ground truth in order to identify examples where specific XAI methods perform well or poorly. We believe that attribution benchmarks as the ones introduced herein are of great importance for further application of neural networks in the geosciences, and for more objective assessment and accurate implementation of XAI methods, which will increase model trust and assist in discovering new science. △ Less","10 June, 2022",https://arxiv.org/pdf/2103.10005
Hierarchical Reinforcement Learning Framework for Stochastic Spaceflight Campaign Design,Yuji Takubo;Hao Chen;Koki Ho,"This paper develops a hierarchical reinforcement learning architecture for multimission spaceflight campaign design under uncertainty, including vehicle design, infrastructure deployment planning, and space transportation scheduling. This problem involves a high-dimensional design space and is challenging especially with uncertainty present. To tackle this challenge, the developed framework has a hierarchical structure with reinforcement learning and network-based mixed-integer linear programming (MILP), where the former optimizes campaign-level decisions (e.g., design of the vehicle used throughout the campaign, destination demand assigned to each mission in the campaign), whereas the latter optimizes the detailed mission-level decisions (e.g., when to launch what from where to where). The framework is applied to a set of human lunar exploration campaign scenarios with uncertain in situ resource utilization performance as a case study. The main value of this work is its integration of the rapidly growing reinforcement learning research and the existing MILP-based space logistics methods through a hierarchical framework to handle the otherwise intractable complexity of space mission design under uncertainty. This unique framework is expected to be a critical steppingstone for the emerging research direction of artificial intelligence for space mission design. △ Less","17 February, 2022",https://arxiv.org/pdf/2103.08981
A Whole Brain Probabilistic Generative Model: Toward Realizing Cognitive Architectures for Developmental Robots,Tadahiro Taniguchi;Hiroshi Yamakawa;Takayuki Nagai;Kenji Doya;Masamichi Sakagami;Masahiro Suzuki;Tomoaki Nakamura;Akira Taniguchi,"Building a humanlike integrative artificial cognitive system, that is, an artificial general intelligence (AGI), is the holy grail of the artificial intelligence (AI) field. Furthermore, a computational model that enables an artificial system to achieve cognitive development will be an excellent reference for brain and cognitive science. This paper describes an approach to develop a cognitive architecture by integrating elemental cognitive modules to enable the training of the modules as a whole. This approach is based on two ideas: (1) brain-inspired AI, learning human brain architecture to build human-level intelligence, and (2) a probabilistic generative model(PGM)-based cognitive system to develop a cognitive system for developmental robots by integrating PGMs. The development framework is called a whole brain PGM (WB-PGM), which differs fundamentally from existing cognitive architectures in that it can learn continuously through a system based on sensory-motor information. In this study, we describe the rationale of WB-PGM, the current status of PGM-based elemental cognitive modules, their relationship with the human brain, the approach to the integration of the cognitive modules, and future challenges. Our findings can serve as a reference for brain studies. As PGMs describe explicit informational relationships between variables, this description provides interpretable guidance from computational sciences to brain science. By providing such information, researchers in neuroscience can provide feedback to researchers in AI and robotics on what the current models lack with reference to the brain. Further, it can facilitate collaboration among researchers in neuro-cognitive sciences as well as AI and robotics. △ Less","9 January, 2022",https://arxiv.org/pdf/2103.08183
Hippocampal formation-inspired probabilistic generative model,Akira Taniguchi;Ayako Fukawa;Hiroshi Yamakawa,"In building artificial intelligence (AI) agents, referring to how brains function in real environments can accelerate development by reducing the design space. In this study, we propose a probabilistic generative model (PGM) for navigation in uncertain environments by integrating the neuroscientific knowledge of hippocampal formation (HF) and the engineering knowledge in robotics and AI, namely, simultaneous localization and mapping (SLAM). We follow the approach of brain reference architecture (BRA) (Yamakawa, 2021) to compose the PGM and outline how to verify the model. To this end, we survey and discuss the relationship between the HF findings and SLAM models. The proposed hippocampal formation-inspired probabilistic generative model (HF-PGM) is designed to be highly consistent with the anatomical structure and functions of the HF. By referencing the brain, we elaborate on the importance of integration of egocentric/allocentric information from the entorhinal cortex to the hippocampus and the use of discrete-event queues. △ Less","21 March, 2022",https://arxiv.org/pdf/2103.07356
Deep and Statistical Learning in Biomedical Imaging: State of the Art in 3D MRI Brain Tumor Segmentation,K. Ruwani M. Fernando;Chris P. Tsokos,"Clinical diagnostic and treatment decisions rely upon the integration of patient-specific data with clinical reasoning. Cancer presents a unique context that influence treatment decisions, given its diverse forms of disease evolution. Biomedical imaging allows noninvasive assessment of disease based on visual evaluations leading to better clinical outcome prediction and therapeutic planning. Early methods of brain cancer characterization predominantly relied upon statistical modeling of neuroimaging data. Driven by the breakthroughs in computer vision, deep learning became the de facto standard in the domain of medical imaging. Integrated statistical and deep learning methods have recently emerged as a new direction in the automation of the medical practice unifying multi-disciplinary knowledge in medicine, statistics, and artificial intelligence. In this study, we critically review major statistical and deep learning models and their applications in brain imaging research with a focus on MRI-based brain tumor segmentation. The results do highlight that model-driven classical statistics and data-driven deep learning is a potent combination for developing automated systems in clinical oncology. △ Less","16 December, 2022",https://arxiv.org/pdf/2103.05529
"Believe The HiPe: Hierarchical Perturbation for Fast, Robust, and Model-Agnostic Saliency Mapping",Jessica Cooper;Ognjen Arandjelović;David J Harrison,"Understanding the predictions made by Artificial Intelligence (AI) systems is becoming more and more important as deep learning models are used for increasingly complex and high-stakes tasks. Saliency mapping -- a popular visual attribution method -- is one important tool for this, but existing formulations are limited by either computational cost or architectural constraints. We therefore propose Hierarchical Perturbation, a very fast and completely model-agnostic method for interpreting model predictions with robust saliency maps. Using standard benchmarks and datasets, we show that our saliency maps are of competitive or superior quality to those generated by existing model-agnostic methods -- and are over 20 times faster to compute. △ Less","11 April, 2022",https://arxiv.org/pdf/2103.05108
A Survey of Embodied AI: From Simulators to Research Tasks,Jiafei Duan;Samson Yu;Hui Li Tan;Hongyuan Zhu;Cheston Tan,"There has been an emerging paradigm shift from the era of ""internet AI"" to ""embodied AI"", where AI algorithms and agents no longer learn from datasets of images, videos or text curated primarily from the internet. Instead, they learn through interactions with their environments from an egocentric perception similar to humans. Consequently, there has been substantial growth in the demand for embodied AI simulators to support various embodied AI research tasks. This growing interest in embodied AI is beneficial to the greater pursuit of Artificial General Intelligence (AGI), but there has not been a contemporary and comprehensive survey of this field. This paper aims to provide an encyclopedic survey for the field of embodied AI, from its simulators to its research. By evaluating nine current embodied AI simulators with our proposed seven features, this paper aims to understand the simulators in their provision for use in embodied AI research and their limitations. Lastly, this paper surveys the three main research tasks in embodied AI -- visual exploration, visual navigation and embodied question answering (QA), covering the state-of-the-art approaches, evaluation metrics and datasets. Finally, with the new insights revealed through surveying the field, the paper will provide suggestions for simulator-for-task selections and recommendations for the future directions of the field. △ Less","5 January, 2022",https://arxiv.org/pdf/2103.04918
"An overview of artificial intelligence techniques for diagnosis of Schizophrenia based on magnetic resonance imaging modalities: Methods, challenges, and future works",Delaram Sadeghi;Afshin Shoeibi;Navid Ghassemi;Parisa Moridian;Ali Khadem;Roohallah Alizadehsani;Mohammad Teshnehlab;Juan M. Gorriz;Fahime Khozeimeh;Yu-Dong Zhang;Saeid Nahavandi;U Rajendra Acharya,"Schizophrenia (SZ) is a mental disorder that typically emerges in late adolescence or early adulthood. It reduces the life expectancy of patients by 15 years. Abnormal behavior, perception of emotions, social relationships, and reality perception are among its most significant symptoms. Past studies have revealed that SZ affects the temporal and anterior lobes of hippocampus regions of the brain. Also, increased volume of cerebrospinal fluid (CSF) and decreased volume of white and gray matter can be observed due to this disease. Magnetic resonance imaging (MRI) is the popular neuroimaging technique used to explore structural/functional brain abnormalities in SZ disorder, owing to its high spatial resolution. Various artificial intelligence (AI) techniques have been employed with advanced image/signal processing methods to accurately diagnose SZ. This paper presents a comprehensive overview of studies conducted on the automated diagnosis of SZ using MRI modalities. First, an AI-based computer aided-diagnosis system (CADS) for SZ diagnosis and its relevant sections are presented. Then, this section introduces the most important conventional machine learning (ML) and deep learning (DL) techniques in the diagnosis of diagnosing SZ. A comprehensive comparison is also made between ML and DL studies in the discussion section. In the following, the most important challenges in diagnosing SZ are addressed. Future works in diagnosing SZ using AI techniques and MRI modalities are recommended in another section. Results, conclusion, and research findings are also presented at the end. △ Less","10 May, 2022",https://arxiv.org/pdf/2103.03081
"Human-AI Interactions in Public Sector Decision-Making: ""Automation Bias"" and ""Selective Adherence"" to Algorithmic Advice",Saar Alon-Barkat;Madalina Busuioc,"Artificial intelligence algorithms are increasingly adopted as decisional aides by public bodies, with the promise of overcoming biases of human decision-makers. At the same time, they may introduce new biases in the human-algorithm interaction. Drawing on psychology and public administration literatures, we investigate two key biases: overreliance on algorithmic advice even in the face of warning signals from other sources (automation bias), and selective adoption of algorithmic advice when this corresponds to stereotypes (selective adherence). We assess these via three experimental studies conducted in the NetherlandsWe discuss the implications of our findings for public sector decision making in the age of automation. Overall, our study speaks to potential negative effects of automation of the administrative state for already vulnerable and disadvantaged citizens. △ Less","8 June, 2022",https://arxiv.org/pdf/2103.02381
Towards Personalized Federated Learning,Alysa Ziying Tan;Han Yu;Lizhen Cui;Qiang Yang,"In parallel with the rapid adoption of Artificial Intelligence (AI) empowered by advances in AI research, there have been growing awareness and concerns of data privacy. Recent significant developments in the data regulation landscape have prompted a seismic shift in interest towards privacy-preserving AI. This has contributed to the popularity of Federated Learning (FL), the leading paradigm for the training of machine learning models on data silos in a privacy-preserving manner. In this survey, we explore the domain of Personalized FL (PFL) to address the fundamental challenges of FL on heterogeneous data, a universal characteristic inherent in all real-world datasets. We analyze the key motivations for PFL and present a unique taxonomy of PFL techniques categorized according to the key challenges and personalization strategies in PFL. We highlight their key ideas, challenges and opportunities and envision promising future trajectories of research towards new PFL architectural design, realistic PFL benchmarking, and trustworthy PFL approaches. △ Less","17 March, 2022",https://arxiv.org/pdf/2103.00710
"Towards Continual, Online, Self-Supervised Depth",Muhammad Umar Karim Khan,"Although depth extraction with passive sensors has seen remarkable improvement with deep learning, these approaches may fail to obtain correct depth if they are exposed to environments not observed during training. Online adaptation, where the neural network trains while deployed, with self-supervised learning provides a convenient solution as the network can learn from the scene where it is deployed without external supervision. However, online adaptation causes a neural network to forget the past. Thus, past training is wasted and the network is not able to provide good results if it observes past scenes. This work deals with practical online-adaptation where the input is online and temporally-correlated, and training is completely self-supervised. Regularization and replay-based methods without task boundaries are proposed to avoid catastrophic forgetting while adapting to online data. Effort has been made to make the proposed approach suitable for practical use. We apply our method to both structure-from-motion and stereo depth estimation. We evaluate our method on diverse public datasets that include outdoor, indoor and synthetic scenes. Qualitative and quantitative results with both structure-from-motion and stereo show superior forgetting as well as adaptation performance compared to recent methods. Furthermore, the proposed method incurs negligible overhead compared to fine-tuning for online adaptation, proving to be an adequate choice in terms of plasticity, stability and applicability. The proposed approach is more inline with the artificial general intelligence paradigm as the neural network learns continually with no supervision. Source code is available at https://github.com/umarKarim/cou_sfm and https://github.com/umarKarim/cou_stereo. △ Less","19 June, 2022",https://arxiv.org/pdf/2103.00369
A Local Method for Identifying Causal Relations under Markov Equivalence,Zhuangyan Fang;Yue Liu;Zhi Geng;Shengyu Zhu;Yangbo He,"Causality is important for designing interpretable and robust methods in artificial intelligence research. We propose a local approach to identify whether a variable is a cause of a given target under the framework of causal graphical models of directed acyclic graphs (DAGs). In general, the causal relation between two variables may not be identifiable from observational data as many causal DAGs encoding different causal relations are Markov equivalent. In this paper, we first introduce a sufficient and necessary graphical condition to check the existence of a causal path from a variable to a target in every Markov equivalent DAG. Next, we provide local criteria for identifying whether a variable is a cause/non-cause of a target based only on the local structure instead of the entire graph. Finally, we propose a local learning algorithm for this causal query via learning the local structure of the variable and some additional statistical independence tests related to the target. Simulation studies show that our local algorithm is efficient and effective, compared with other state-of-art methods. △ Less","5 March, 2022",https://arxiv.org/pdf/2102.12685
A Combination of Multi-Objective Genetic Algorithm and Deep Learning for Music Harmony Generation,Maryam Majidi;Rahil Mahdian Toroghi,"Automatic Music Generation (AMG) has become an interesting research topic for many scientists in artificial intelligence, who are also interested in the music industry. One of the main challenges in AMG is that there is no clear objective evaluation criterion that can measure the music grammar, structural rules, and audience satisfaction. Also, original music contains different elements that should work together, such as melody, harmony, and rhythm; but in the most of previous works, AMG works only for one element (e.g., melody). Therefore, in this paper, we propose a Multi-Objective Genetic Algorithm (MO-GA) to generate polyphonic music pieces, considering grammar and listener satisfaction. In this method, we use three objective functions. The first objective function is the accuracy of the generated music piece, based on music theory; and the other two objective functions are modeled scores provided by music experts and ordinary listeners. The scoring of experts and listeners separately are modeled using Bi-directional Long Short-Term Memory (Bi-LSTM) neural networks. The proposed music generation system tries to maximize mentioned objective functions to generate a new piece of music, including melody and harmony. The results show that the proposed method can generate pleasant pieces with desired styles and lengths, along with harmonic sounds that follow the grammar. △ Less","3 June, 2022",https://arxiv.org/pdf/2102.07960
"Hardware-aware in \ situ
Boltzmann machine learning using stochastic magnetic tunnel junctions",Jan Kaiser;William A. Borders;Kerem Y. Camsari;Shunsuke Fukami;Hideo Ohno;Supriyo Datta,"One of the big challenges of current electronics is the design and implementation of hardware neural networks that perform fast and energy-efficient machine learning. Spintronics is a promising catalyst for this field with the capabilities of nanosecond operation and compatibility with existing microelectronics. Considering large-scale, viable neuromorphic systems however, variability of device properties is a serious concern. In this paper, we show an autonomously operating circuit that performs hardware-aware machine learning utilizing probabilistic neurons built with stochastic magnetic tunnel junctions. We show that in \ situ learning of weights and biases in a Boltzmann machine can counter device-to-device variations and learn the probability distribution of meaningful operations such as a full adder. This scalable autonomously operating learning circuit using spintronics-based neurons could be especially of interest for standalone artificial-intelligence devices capable of fast and efficient learning at the edge. △ Less","13 January, 2022",https://arxiv.org/pdf/2102.05137
Symbolic Behaviour in Artificial Intelligence,Adam Santoro;Andrew Lampinen;Kory Mathewson;Timothy Lillicrap;David Raposo,"The ability to use symbols is the pinnacle of human intelligence, but has yet to be fully replicated in machines. Here we argue that the path towards symbolically fluent artificial intelligence (AI) begins with a reinterpretation of what symbols are, how they come to exist, and how a system behaves when it uses them. We begin by offering an interpretation of symbols as entities whose meaning is established by convention. But crucially, something is a symbol only for those who demonstrably and actively participate in this convention. We then outline how this interpretation thematically unifies the behavioural traits humans exhibit when they use symbols. This motivates our proposal that the field place a greater emphasis on symbolic behaviour rather than particular computational mechanisms inspired by more restrictive interpretations of symbols. Finally, we suggest that AI research explore social and cultural engagement as a tool to develop the cognitive machinery necessary for symbolic behaviour to emerge. This approach will allow for AI to interpret something as symbolic on its own rather than simply manipulate things that are only symbols to human onlookers, and thus will ultimately lead to AI with more human-like symbolic fluency. △ Less","21 January, 2022",https://arxiv.org/pdf/2102.03406
Truly Sparse Neural Networks at Scale,Selima Curci;Decebal Constantin Mocanu;Mykola Pechenizkiyi,"Recently, sparse training methods have started to be established as a de facto approach for training and inference efficiency in artificial neural networks. Yet, this efficiency is just in theory. In practice, everyone uses a binary mask to simulate sparsity since the typical deep learning software and hardware are optimized for dense matrix operations. In this paper, we take an orthogonal approach, and we show that we can train truly sparse neural networks to harvest their full potential. To achieve this goal, we introduce three novel contributions, specially designed for sparse neural networks: (1) a parallel training algorithm and its corresponding sparse implementation from scratch, (2) an activation function with non-trainable parameters to favour the gradient flow, and (3) a hidden neurons importance metric to eliminate redundancies. All in one, we are able to break the record and to train the largest neural network ever trained in terms of representational power -- reaching the bat brain size. The results show that our approach has state-of-the-art performance while opening the path for an environmentally friendly artificial intelligence era. △ Less","12 July, 2022",https://arxiv.org/pdf/2102.01732
The Work of Art in an Age of Mechanical Generation,Steven J. Frank,"Can we define what it means to be ""creative,"" and if so, can our definition drive artificial intelligence (AI) systems to feats of creativity indistinguishable from human efforts? This mixed question is considered from technological and social perspectives. Beginning with an exploration of the value we attach to authenticity in works of art, the article considers the ability of AI to detect forgeries of renowned paintings and, in so doing, somehow reveal the quiddity of a work of art. We conclude by considering whether evolving technical capability can revise traditional relationships among art, artist, and the market. △ Less","10 August, 2022",https://arxiv.org/pdf/2101.11587
Deep Learning for Instance Retrieval: A Survey,Wei Chen;Yu Liu;Weiping Wang;Erwin Bakker;Theodoros Georgiou;Paul Fieguth;Li Liu;Michael S. Lew,"In recent years a vast amount of visual content has been generated and shared from many fields, such as social media platforms, medical imaging, and robotics. This abundance of content creation and sharing has introduced new challenges, particularly that of searching databases for similar content-Content Based Image Retrieval (CBIR)-a long-established research area in which improved efficiency and accuracy are needed for real-time retrieval. Artificial intelligence has made progress in CBIR and has significantly facilitated the process of instance search. In this survey we review recent instance retrieval works that are developed based on deep learning algorithms and techniques, with the survey organized by deep network architecture types, deep features, feature embedding and aggregation methods, and network fine-tuning strategies. Our survey considers a wide variety of recent methods, whereby we identify milestone work, reveal connections among various methods and present the commonly used benchmarks, evaluation results, common challenges, and propose promising future directions. △ Less","30 October, 2022",https://arxiv.org/pdf/2101.11282
Visual explanation of black-box model: Similarity Difference and Uniqueness (SIDU) method,Satya M. Muddamsetty;Mohammad N. S. Jahromi;Andreea E. Ciontos;Laura M. Fenoy;Thomas B. Moeslund,"Explainable Artificial Intelligence (XAI) has in recent years become a well-suited framework to generate human understandable explanations of ""black-box"" models. In this paper, a novel XAI visual explanation algorithm known as the Similarity Difference and Uniqueness (SIDU) method that can effectively localize entire object regions responsible for prediction is presented in full detail. The SIDU algorithm robustness and effectiveness is analyzed through various computational and human subject experiments. In particular, the SIDU algorithm is assessed using three different types of evaluations (Application, Human and Functionally-Grounded) to demonstrate its superior performance. The robustness of SIDU is further studied in the presence of adversarial attack on ""black-box"" models to better understand its performance. Our code is available at: https://github.com/satyamahesh84/SIDU_XAI_CODE. △ Less","10 July, 2022",https://arxiv.org/pdf/2101.10710
A Survey on Ensemble Learning under the Era of Deep Learning,Yongquan Yang;Haijun Lv;Ning Chen,"Due to the dominant position of deep learning (mostly deep neural networks) in various artificial intelligence applications, recently, ensemble learning based on deep neural networks (ensemble deep learning) has shown significant performances in improving the generalization of learning system. However, since modern deep neural networks usually have millions to billions of parameters, the time and space overheads for training multiple base deep learners and testing with the ensemble deep learner are far greater than that of traditional ensemble learning. Though several algorithms of fast ensemble deep learning have been proposed to promote the deployment of ensemble deep learning in some applications, further advances still need to be made for many applications in specific fields, where the developing time and computing resources are usually restricted or the data to be processed is of large dimensionality. An urgent problem needs to be solved is how to take the significant advantages of ensemble deep learning while reduce the required expenses so that many more applications in specific fields can benefit from it. For the alleviation of this problem, it is essential to know about how ensemble learning has developed under the era of deep learning. Thus, in this article, we present fundamental discussions focusing on data analyses of published works, methodologies, recent advances and unattainability of traditional ensemble learning and ensemble deep learning. We hope this article will be helpful to realize the intrinsic problems and technical challenges faced by future developments of ensemble learning under the era of deep learning. △ Less","27 September, 2022",https://arxiv.org/pdf/2101.08387
mt5se: An Open Source Framework for Building Autonomous Trading Robots,Paulo André Lima de Castro,"Autonomous trading robots have been studied in artificial intelligence area for quite some time. Many AI techniques have been tested for building autonomous agents able to trade financial assets. These initiatives include traditional neural networks, fuzzy logic, reinforcement learning but also more recent approaches like deep neural networks and deep reinforcement learning. Many developers claim to be successful in creating robots with great performance when simulating execution with historical price series, so called backtesting. However, when these robots are used in real markets frequently they present poor performance in terms of risks and return. In this paper, we propose an open source framework (mt5se) that helps the development, backtesting, live testing and real operation of autonomous traders. We built and tested several traders using mt5se. The results indicate that it may help the development of better traders. Furthermore, we discuss the simple architecture that is used in many studies and propose an alternative multiagent architecture. Such architecture separates two main concerns for portfolio manager (PM) : price prediction and capital allocation. More than achieve a high accuracy, a PM should increase profits when it is right and reduce loss when it is wrong. Furthermore, price prediction is highly dependent of asset's nature and history, while capital allocation is dependent only on analyst's prediction performance and assets' correlation. Finally, we discuss some promising technologies in the area. △ Less","28 June, 2022",https://arxiv.org/pdf/2101.08169
A Note on Rough Set Algebra and Core Regular Double Stone Algebras,Daniel J. Clouse,"Rough Set Theory (RST), first introduced by Pawlak in 1982, is an approach for dealing with information systems where knowledge is uncertain or incomplete.\cite{Pawlak} It is of fundamental importance in many subfields of artificial intelligence and cognitive science.\cite{RSTppf} Given a universe U with an equivalence relation θ, the pair \langle U,θ\rangle is referred to as an information system and we denote its collection of rough sets R_θ. In our main Theorem we show R_θ with |θ_u| > 1\ \forall\ u \in U to be isomorphic to core regular double Stone algebras, CRDSA, that are complete and atomic, and that the crisp, or definable, sets form a complete atomistic Boolean algebra. These guarantees of infimum/supremeum for arbitrary subsets and formulations in terms of fundamental elements are likely useful if dealing with equivalence relations with an infinite number of partitions, such as projective Hilbert spaces. We further derive that every CRDSA is isomorphic to a subalgebra of a principal rough set algebra, R_θ, for some approximation space \langle U,θ\rangle. In our main Corollary we show explicitly how to embed R_θ into the CRDSA and first demonstrate by extending the culminating finite example of \cite{RCRDSA}. As our capstone, we consider the projective Hilbert space of complex numbers, \mathbb{C} and show, among other things, the power set of the set of pure states is a complete, atomistic Boolean algebra. In closing, we suggest other Quantum relevant applications that may be useful, such as Hilbert spaces of operators △ Less","1 February, 2022",https://arxiv.org/pdf/2101.02313
Characterizing Intersectional Group Fairness with Worst-Case Comparisons,Avijit Ghosh;Lea Genuit;Mary Reagan,"Machine Learning or Artificial Intelligence algorithms have gained considerable scrutiny in recent times owing to their propensity towards imitating and amplifying existing prejudices in society. This has led to a niche but growing body of work that identifies and attempts to fix these biases. A first step towards making these algorithms more fair is designing metrics that measure unfairness. Most existing work in this field deals with either a binary view of fairness (protected vs. unprotected groups) or politically defined categories (race or gender). Such categorization misses the important nuance of intersectionality - biases can often be amplified in subgroups that combine membership from different categories, especially if such a subgroup is particularly underrepresented in historical platforms of opportunity. In this paper, we discuss why fairness metrics need to be looked at under the lens of intersectionality, identify existing work in intersectional fairness, suggest a simple worst case comparison method to expand the definitions of existing group fairness metrics to incorporate intersectionality, and finally conclude with the social, legal and political framework to handle intersectional fairness in the modern context. △ Less","4 May, 2022",https://arxiv.org/pdf/2101.01673
Explainability in Graph Neural Networks: A Taxonomic Survey,Hao Yuan;Haiyang Yu;Shurui Gui;Shuiwang Ji,"Deep learning methods are achieving ever-increasing performance on many artificial intelligence tasks. A major limitation of deep models is that they are not amenable to interpretability. This limitation can be circumvented by developing post hoc techniques to explain the predictions, giving rise to the area of explainability. Recently, explainability of deep models on images and texts has achieved significant progress. In the area of graph data, graph neural networks (GNNs) and their explainability are experiencing rapid developments. However, there is neither a unified treatment of GNN explainability methods, nor a standard benchmark and testbed for evaluations. In this survey, we provide a unified and taxonomic view of current GNN explainability methods. Our unified and taxonomic treatments of this subject shed lights on the commonalities and differences of existing methods and set the stage for further methodological developments. To facilitate evaluations, we generate a set of benchmark graph datasets specifically for GNN explainability. We summarize current datasets and metrics for evaluating GNN explainability. Altogether, this work provides a unified methodological treatment of GNN explainability and a standardized testbed for evaluations. △ Less","1 July, 2022",https://arxiv.org/pdf/2012.15445
Artificial Intelligence Development Races in Heterogeneous Settings,Theodor Cimpeanu;Francisco C. Santos;Luis Moniz Pereira;Tom Lenaerts;The Anh Han,"Regulation of advanced technologies such as Artificial Intelligence (AI) has become increasingly important, given the associated risks and apparent ethical issues. With the great benefits promised from being able to first supply such technologies, safety precautions and societal consequences might be ignored or shortchanged in exchange for speeding up the development, therefore engendering a racing narrative among the developers. Starting from a game-theoretical model describing an idealised technology race in a fully connected world of players, here we investigate how different interaction structures among race participants can alter collective choices and requirements for regulatory actions. Our findings indicate that, when participants portray a strong diversity in terms of connections and peer-influence (e.g., when scale-free networks shape interactions among parties), the conflicts that exist in homogeneous settings are significantly reduced, thereby lessening the need for regulatory actions. Furthermore, our results suggest that technology governance and regulation may profit from the world's patent heterogeneity and inequality among firms and nations, so as to enable the design and implementation of meticulous interventions on a minority of participants, which is capable of influencing an entire population towards an ethical and sustainable use of advanced technologies. △ Less","4 January, 2022",https://arxiv.org/pdf/2012.15234
Hierarchical principles of embodied reinforcement learning: A review,Manfred Eppe;Christian Gumbsch;Matthias Kerzel;Phuong D. H. Nguyen;Martin V. Butz;Stefan Wermter,"Cognitive Psychology and related disciplines have identified several critical mechanisms that enable intelligent biological agents to learn to solve complex problems. There exists pressing evidence that the cognitive mechanisms that enable problem-solving skills in these species build on hierarchical mental representations. Among the most promising computational approaches to provide comparable learning-based problem-solving abilities for artificial agents and robots is hierarchical reinforcement learning. However, so far the existing computational approaches have not been able to equip artificial agents with problem-solving abilities that are comparable to intelligent animals, including human and non-human primates, crows, or octopuses. Here, we first survey the literature in Cognitive Psychology, and related disciplines, and find that many important mental mechanisms involve compositional abstraction, curiosity, and forward models. We then relate these insights with contemporary hierarchical reinforcement learning methods, and identify the key machine intelligence approaches that realise these mechanisms. As our main result, we show that all important cognitive mechanisms have been implemented independently in isolated computational architectures, and there is simply a lack of approaches that integrate them appropriately. We expect our results to guide the development of more sophisticated cognitively inspired hierarchical methods, so that future artificial agents achieve a problem-solving performance on the level of intelligent animals. △ Less","18 August, 2022",https://arxiv.org/pdf/2012.10147
Predicting Decisions in Language Based Persuasion Games,Reut Apel;Ido Erev;Roi Reichart;Moshe Tennenholtz,"Sender-receiver interactions, and specifically persuasion games, are widely researched in economic modeling and artificial intelligence. However, in the classic persuasion games setting, the messages sent from the expert to the decision-maker (DM) are abstract or well-structured signals rather than natural language messages. This paper addresses the use of natural language in persuasion games. For this purpose, we conduct an online repeated interaction experiment. At each trial of the interaction, an informed expert aims to sell an uninformed decision-maker a vacation in a hotel, by sending her a review that describes the hotel. While the expert is exposed to several scored reviews, the decision-maker observes only the single review sent by the expert, and her payoff in case she chooses to take the hotel is a random draw from the review score distribution available to the expert only. We also compare the behavioral patterns in this experiment to the equivalent patterns in similar experiments where the communication is based on the numerical values of the reviews rather than the reviews' text, and observe substantial differences which can be explained through an equilibrium analysis of the game. We consider a number of modeling approaches for our verbal communication setup, differing from each other in the model type (deep neural network vs. linear classifier), the type of features used by the model (textual, behavioral or both) and the source of the textual features (DNN-based vs. hand-crafted). Our results demonstrate that given a prefix of the interaction sequence, our models can predict the future decisions of the decision-maker, particularly when a sequential modeling approach and hand-crafted textual features are applied. Further analysis of the hand-crafted textual features allows us to make initial observations about the aspects of text that drive decision making in our setup △ Less","31 March, 2022",https://arxiv.org/pdf/2012.09966
Autotelic Agents with Intrinsically Motivated Goal-Conditioned Reinforcement Learning: a Short Survey,Cédric Colas;Tristan Karch;Olivier Sigaud;Pierre-Yves Oudeyer,"Building autonomous machines that can explore open-ended environments, discover possible interactions and build repertoires of skills is a general objective of artificial intelligence. Developmental approaches argue that this can only be achieved by autotelic agents: intrinsically motivated learning agents that can learn to represent, generate, select and solve their own problems. In recent years, the convergence of developmental approaches with deep reinforcement learning (RL) methods has been leading to the emergence of a new field: developmental reinforcement learning. Developmental RL is concerned with the use of deep RL algorithms to tackle a developmental problem -- the intrinsically motivated acquisition of open-ended repertoires of skills. The self-generation of goals requires the learning of compact goal encodings as well as their associated goal-achievement functions. This raises new challenges compared to standard RL algorithms originally designed to tackle pre-defined sets of goals using external reward signals. The present paper introduces developmental RL and proposes a computational framework based on goal-conditioned RL to tackle the intrinsically motivated skills acquisition problem. It proceeds to present a typology of the various goal representations used in the literature, before reviewing existing methods to learn to represent and prioritize goals in autonomous systems. We finally close the paper by discussing some open challenges in the quest of intrinsically motivated skills acquisition. △ Less","12 July, 2022",https://arxiv.org/pdf/2012.09830
Privacy and Robustness in Federated Learning: Attacks and Defenses,Lingjuan Lyu;Han Yu;Xingjun Ma;Chen Chen;Lichao Sun;Jun Zhao;Qiang Yang;Philip S. Yu,"As data are increasingly being stored in different silos and societies becoming more aware of data privacy issues, the traditional centralized training of artificial intelligence (AI) models is facing efficiency and privacy challenges. Recently, federated learning (FL) has emerged as an alternative solution and continue to thrive in this new reality. Existing FL protocol design has been shown to be vulnerable to adversaries within or outside of the system, compromising data privacy and system robustness. Besides training powerful global models, it is of paramount importance to design FL systems that have privacy guarantees and are resistant to different types of adversaries. In this paper, we conduct the first comprehensive survey on this topic. Through a concise introduction to the concept of FL, and a unique taxonomy covering: 1) threat models; 2) poisoning attacks and defenses against robustness; 3) inference attacks and defenses against privacy, we provide an accessible review of this important topic. We highlight the intuitions, key techniques as well as fundamental assumptions adopted by various attacks and defenses. Finally, we discuss promising future research directions towards robust and privacy-preserving federated learning. △ Less","18 January, 2022",https://arxiv.org/pdf/2012.06337
Hindsight and Sequential Rationality of Correlated Play,Dustin Morrill;Ryan D'Orazio;Reca Sarfati;Marc Lanctot;James R. Wright;Amy Greenwald;Michael Bowling,"Driven by recent successes in two-player, zero-sum game solving and playing, artificial intelligence work on games has increasingly focused on algorithms that produce equilibrium-based strategies. However, this approach has been less effective at producing competent players in general-sum games or those with more than two players than in two-player, zero-sum games. An appealing alternative is to consider adaptive algorithms that ensure strong performance in hindsight relative to what could have been achieved with modified behavior. This approach also leads to a game-theoretic analysis, but in the correlated play that arises from joint learning dynamics rather than factored agent behavior at equilibrium. We develop and advocate for this hindsight rationality framing of learning in general sequential decision-making settings. To this end, we re-examine mediated equilibrium and deviation types in extensive-form games, thereby gaining a more complete understanding and resolving past misconceptions. We present a set of examples illustrating the distinct strengths and weaknesses of each type of equilibrium in the literature, and prove that no tractable concept subsumes all others. This line of inquiry culminates in the definition of the deviation and equilibrium classes that correspond to algorithms in the counterfactual regret minimization (CFR) family, relating them to all others in the literature. Examining CFR in greater detail further leads to a new recursive definition of rationality in correlated play that extends sequential rationality in a way that naturally applies to hindsight evaluation. △ Less","22 June, 2022",https://arxiv.org/pdf/2012.05874
CRAFT: A Benchmark for Causal Reasoning About Forces and inTeractions,Tayfun Ates;M. Samil Atesoglu;Cagatay Yigit;Ilker Kesen;Mert Kobas;Erkut Erdem;Aykut Erdem;Tilbe Goksun;Deniz Yuret,"Humans are able to perceive, understand and reason about causal events. Developing models with similar physical and causal understanding capabilities is a long-standing goal of artificial intelligence. As a step towards this direction, we introduce CRAFT, a new video question answering dataset that requires causal reasoning about physical forces and object interactions. It contains 58K video and question pairs that are generated from 10K videos from 20 different virtual environments, containing various objects in motion that interact with each other and the scene. Two question categories in CRAFT include previously studied descriptive and counterfactual questions. Additionally, inspired by the Force Dynamics Theory in cognitive linguistics, we introduce a new causal question category that involves understanding the causal interactions between objects through notions like cause, enable, and prevent. Our results show that even though the questions in CRAFT are easy for humans, the tested baseline models, including existing state-of-the-art methods, do not yet deal with the challenges posed in our benchmark. △ Less","1 March, 2022",https://arxiv.org/pdf/2012.04293
Towards an AI assistant for power grid operators,Antoine Marot;Alexandre Rozier;Matthieu Dussartre;Laure Crochepierre;Benjamin Donnot,"Power grids are becoming more complex to operate in the digital age given the current energy transition to cope with climate change. As a result, real-time decision-making is getting more challenging as the human operator has to deal with more information, more uncertainty, more applications, and more coordination. While supervision has been primarily used to help them make decisions over the last decades, it cannot reasonably scale up anymore. There is a great need for rethinking the human-machine interface under more unified and interactive frameworks. Taking advantage of the latest developments in Human-Machine Interface and Artificial Intelligence, we expose our vision of a new assistant framework relying on an hypervision interface and greater bidirectional interaction. We review the known principles of decision-making driving our assistant design alongside with its supporting assistance functions. We finally share some guidelines to make progress towards the development of such an assistant. △ Less","29 May, 2022",https://arxiv.org/pdf/2012.02026
Monadic Pavlovian associative learning in a backpropagation-free photonic network,James Y. S. Tan;Zengguang Cheng;Johannes Feldmann;Xuan Li;Nathan Youngblood;Utku E. Ali;C. David Wright;Wolfram H. P. Pernice;Harish Bhaskaran,"Over a century ago, Ivan P. Pavlov, in a classic experiment, demonstrated how dogs can learn to associate a ringing bell with food, thereby causing a ring to result in salivation. Today, it is rare to find the use of Pavlovian type associative learning for artificial intelligence (AI) applications even though other learning concepts, in particular backpropagation on artificial neural networks (ANNs) have flourished. However, training using the backpropagation method on 'conventional' ANNs, especially in the form of modern deep neural networks (DNNs), is computationally and energy intensive. Here we experimentally demonstrate a form of backpropagation-free learning using a single (or monadic) associative hardware element. We realize this on an integrated photonic platform using phase-change materials combined with on-chip cascaded directional couplers. We then develop a scaled-up circuit network using our monadic Pavlovian photonic hardware that delivers a distinct machine-learning framework based on single-element associations and, importantly, using backpropagation-free architectures to address general learning tasks. Our approach reduces the computational burden imposed by learning in conventional neural network approaches, thereby increasing speed, whilst also offering higher bandwidth inherent to our photonic implementation. △ Less","5 August, 2022",https://arxiv.org/pdf/2011.14709
A Definition and a Test for Human-Level Artificial Intelligence,Deokgun Park;Md Ashaduzzaman Rubel Mondol;Aishwarya Pothula;Mazharul Islam,"Despite recent advances of AI research in many application-specific domains, we do not know how to build a human-level artificial intelligence (HLAI). We conjecture that learning from others' experience with the language is the essential characteristic that distinguishes human intelligence from the rest. Humans can update the action-value function with the verbal description as if they experience states, actions, and corresponding rewards sequences firsthand. In this paper, we present a classification of intelligence according to how individual agents learn and propose a definition and a test for HLAI. The main idea is that language acquisition without explicit rewards can be a sufficient test for HLAI. △ Less","14 December, 2022",https://arxiv.org/pdf/2011.09410
"Detection of masses and architectural distortions in digital breast tomosynthesis: a publicly available dataset of 5,060 patients and a deep learning model",Mateusz Buda;Ashirbani Saha;Ruth Walsh;Sujata Ghate;Nianyi Li;Albert Święcicki;Joseph Y. Lo;Maciej A. Mazurowski,"Breast cancer screening is one of the most common radiological tasks with over 39 million exams performed each year. While breast cancer screening has been one of the most studied medical imaging applications of artificial intelligence, the development and evaluation of the algorithms are hindered due to the lack of well-annotated large-scale publicly available datasets. This is particularly an issue for digital breast tomosynthesis (DBT) which is a relatively new breast cancer screening modality. We have curated and made publicly available a large-scale dataset of digital breast tomosynthesis images. It contains 22,032 reconstructed DBT volumes belonging to 5,610 studies from 5,060 patients. This included four groups: (1) 5,129 normal studies, (2) 280 studies where additional imaging was needed but no biopsy was performed, (3) 112 benign biopsied studies, and (4) 89 studies with cancer. Our dataset included masses and architectural distortions which were annotated by two experienced radiologists. Additionally, we developed a single-phase deep learning detection model and tested it using our dataset to serve as a baseline for future research. Our model reached a sensitivity of 65% at 2 false positives per breast. Our large, diverse, and highly-curated dataset will facilitate development and evaluation of AI algorithms for breast cancer screening through providing data for training as well as common set of cases for model validation. The performance of the model developed in our study shows that the task remains challenging and will serve as a baseline for future model development. △ Less","20 November, 2022",https://arxiv.org/pdf/2011.07995
A Review of the Family of Artificial Fish Swarm Algorithms: Recent Advances and Applications,Farhad Pourpanah;Ran Wang;Chee Peng Lim;Xi-Zhao Wang;Danial Yazdani,"The Artificial Fish Swarm Algorithm (AFSA) is inspired by the ecological behaviors of fish schooling in nature, viz., the preying, swarming and following behaviors. Owing to a number of salient properties, which include flexibility, fast convergence, and insensitivity to the initial parameter settings, the family of AFSA has emerged as an effective Swarm Intelligence (SI) methodology that has been widely applied to solve real-world optimization problems. Since its introduction in 2002, many improved and hybrid AFSA models have been developed to tackle continuous, binary, and combinatorial optimization problems. This paper aims to present a concise review of the continuous AFSA, encompassing the original ASFA, its improvements and hybrid models, as well as their associated applications. We focus on articles published in high-quality journals since 2013. Our review provides insights into AFSA parameters modifications, procedures and sub-functions. The main reasons for these enhancements and the comparison results with other hybrid methods are discussed. In addition, hybrid, multi-objective and dynamic AFSA models that have been proposed to solve continuous optimization problems are elucidated. We also analyse possible AFSA enhancements and highlight future research directions for advancing AFSA-based models. △ Less","12 May, 2022",https://arxiv.org/pdf/2011.05700
Behavioral Use Licensing for Responsible AI,Danish Contractor;Daniel McDuff;Julia Haines;Jenny Lee;Christopher Hines;Brent Hecht;Nicholas Vincent;Hanlin Li,"With the growing reliance on artificial intelligence (AI) for many different applications, the sharing of code, data, and models is important to ensure the replicability and democratization of scientific knowledge. Many high-profile academic publishing venues expect code and models to be submitted and released with papers. Furthermore, developers often want to release these assets to encourage development of technology that leverages their frameworks and services. A number of organizations have expressed concerns about the inappropriate or irresponsible use of AI and have proposed ethical guidelines around the application of such systems. While such guidelines can help set norms and shape policy, they are not easily enforceable. In this paper, we advocate the use of licensing to enable legally enforceable behavioral use conditions on software and code and provide several case studies that demonstrate the feasibility of behavioral use licensing. We envision how licensing may be implemented in accordance with existing responsible AI guidelines. △ Less","20 October, 2022",https://arxiv.org/pdf/2011.03116
Generating Radiology Reports via Memory-driven Transformer,Zhihong Chen;Yan Song;Tsung-Hui Chang;Xiang Wan,"Medical imaging is frequently used in clinical practice and trials for diagnosis and treatment. Writing imaging reports is time-consuming and can be error-prone for inexperienced radiologists. Therefore, automatically generating radiology reports is highly desired to lighten the workload of radiologists and accordingly promote clinical automation, which is an essential task to apply artificial intelligence to the medical domain. In this paper, we propose to generate radiology reports with memory-driven Transformer, where a relational memory is designed to record key information of the generation process and a memory-driven conditional layer normalization is applied to incorporating the memory into the decoder of Transformer. Experimental results on two prevailing radiology report datasets, IU X-Ray and MIMIC-CXR, show that our proposed approach outperforms previous models with respect to both language generation metrics and clinical evaluations. Particularly, this is the first work reporting the generation results on MIMIC-CXR to the best of our knowledge. Further analyses also demonstrate that our approach is able to generate long reports with necessary medical terms as well as meaningful image-text attention mappings. △ Less","27 April, 2022",https://arxiv.org/pdf/2010.16056
Does anatomical contextual information improve 3D U-Net based brain tumor segmentation?,Iulian Emil Tampu;Neda Haj-Hosseini;Anders Eklund,"Effective, robust, and automatic tools for brain tumor segmentation are needed for the extraction of information useful in treatment planning from magnetic resonance (MR) images. Context-aware artificial intelligence is an emerging concept for the development of deep learning applications for computer-aided medical image analysis. In this work, it is investigated whether the addition of contextual information from the brain anatomy in the form of white matter, gray matter, and cerebrospinal fluid masks and probability maps improves U-Net-based brain tumor segmentation. The BraTS2020 dataset was used to train and test two standard 3D U-Net models that, in addition to the conventional MR image modalities, used the anatomical contextual information as extra channels in the form of binary masks (CIM) or probability maps (CIP). A baseline model (BLM) that only used the conventional MR image modalities was also trained. The impact of adding contextual information was investigated in terms of overall segmentation accuracy, model training time, domain generalization, and compensation for fewer MR modalities available for each subject. Results show that there is no statistically significant difference when comparing Dice scores between the baseline model and the contextual information models, even when comparing performances for high- and low-grade tumors independently. Only in the case of compensation for fewer MR modalities available for each subject did the addition of anatomical contextual information significantly improve the segmentation of the whole tumor. Overall, there is no overall significant improvement in segmentation performance when using anatomical contextual information in the form of either binary masks or probability maps as extra channels. △ Less","4 March, 2022",https://arxiv.org/pdf/2010.13460
A Survey on Deep Learning and Explainability for Automatic Report Generation from Medical Images,Pablo Messina;Pablo Pino;Denis Parra;Alvaro Soto;Cecilia Besa;Sergio Uribe;Marcelo andía;Cristian Tejos;Claudia Prieto;Daniel Capurro,"Every year physicians face an increasing demand of image-based diagnosis from patients, a problem that can be addressed with recent artificial intelligence methods. In this context, we survey works in the area of automatic report generation from medical images, with emphasis on methods using deep neural networks, with respect to: (1) Datasets, (2) Architecture Design, (3) Explainability and (4) Evaluation Metrics. Our survey identifies interesting developments, but also remaining challenges. Among them, the current evaluation of generated reports is especially weak, since it mostly relies on traditional Natural Language Processing (NLP) metrics, which do not accurately capture medical correctness. △ Less","8 January, 2022",https://arxiv.org/pdf/2010.10563
Deciding Fast and Slow: The Role of Cognitive Biases in AI-assisted Decision-making,Charvi Rastogi;Yunfeng Zhang;Dennis Wei;Kush R. Varshney;Amit Dhurandhar;Richard Tomsett,"Several strands of research have aimed to bridge the gap between artificial intelligence (AI) and human decision-makers in AI-assisted decision-making, where humans are the consumers of AI model predictions and the ultimate decision-makers in high-stakes applications. However, people's perception and understanding are often distorted by their cognitive biases, such as confirmation bias, anchoring bias, availability bias, to name a few. In this work, we use knowledge from the field of cognitive science to account for cognitive biases in the human-AI collaborative decision-making setting, and mitigate their negative effects on collaborative performance. To this end, we mathematically model cognitive biases and provide a general framework through which researchers and practitioners can understand the interplay between cognitive biases and human-AI accuracy. We then focus specifically on anchoring bias, a bias commonly encountered in human-AI collaboration. We implement a time-based de-anchoring strategy and conduct our first user experiment that validates its effectiveness in human-AI collaborative decision-making. With this result, we design a time allocation strategy for a resource-constrained setting that achieves optimal human-AI collaboration under some assumptions. We, then, conduct a second user experiment which shows that our time allocation strategy with explanation can effectively de-anchor the human and improve collaborative performance when the AI model has low confidence and is incorrect. △ Less","4 April, 2022",https://arxiv.org/pdf/2010.07938
Modeling emotion for human-like behavior in future intelligent robots,Marwen Belkaid;Luiz Pessoa,"Over the past decades, research in cognitive and affective neuroscience has emphasized that emotion is crucial for human intelligence and in fact inseparable from cognition. Concurrently, there has been growing interest in simulating and modeling emotion-related processes in robots and artificial agents. In this opinion paper, our goal is to provide a snapshot of the present landscape in emotion modeling and to show how neuroscience can help advance the current state of the art. We start with an overview of the existing literature on emotion modeling in three areas of research: affective computing, social robotics, and neurorobotics. Briefly summarizing the current state of knowledge on natural emotion, we then highlight how existing proposals in artificial emotion do not make sufficient contact with neuroscientific evidence. We conclude by providing a set of principles to help guide future research in artificial emotion and intelligent machines more generally. Overall, we argue that a stronger integration of emotion-related processes in robot models is critical for the design of human-like behavior in future intelligent machines. Such integration not only will contribute to the development of autonomous social machines capable of tackling real-world problems but would contribute to advancing understanding of human emotion. △ Less","19 July, 2022",https://arxiv.org/pdf/2009.14810
Signs for Ethical AI: A Route Towards Transparency,Dario Garcia-Gasulla;Atia Cortés;Sergio Alvarez-Napagao;Ulises Cortés,"Today, Artificial Intelligence (AI) has a direct impact on the daily life of billions of people. Being applied to sectors like finance, health, security and advertisement, AI fuels some of the biggest companies and research institutions in the world. Its impact in the near future seems difficult to predict or bound. In contrast to all this power, society remains mostly ignorant of the capabilities and standard practices of AI today. To address this imbalance, improving current interactions between people and AI systems, we propose a transparency scheme to be implemented on any AI system open to the public. The scheme is based on two pillars: Data Privacy and AI Transparency. The first recognizes the relevance of data for AI, and is supported by GDPR. The second considers aspects of AI transparency currently unregulated: AI capabilities, purpose and source. We design this pillar based on ethical principles. For each of the two pillars, we define a three-level display. The first level is based on visual signs, inspired by traffic signs managing the interaction between people and cars, and designed for quick and universal interpretability. The second level uses factsheets, providing limited details. The last level provides access to all available information. After detailing and exemplifying the proposed transparency scheme, we define a set of principles for creating transparent by design software, to be used during the integration of AI components on user-oriented services. △ Less","9 May, 2022",https://arxiv.org/pdf/2009.13871
Image-Based Sorghum Head Counting When You Only Look Once,Lawrence Mosley;Hieu Pham;Yogesh Bansal;Eric Hare,"Modern trends in digital agriculture have seen a shift towards artificial intelligence for crop quality assessment and yield estimation. In this work, we document how a parameter tuned single-shot object detection algorithm can be used to identify and count sorghum head from aerial drone images. Our approach involves a novel exploratory analysis that identified key structural elements of the sorghum images and motivated the selection of parameter-tuned anchor boxes that contributed significantly to performance. These insights led to the development of a deep learning model that outperformed the baseline model and achieved an out-of-sample mean average precision of 0.95. △ Less","13 June, 2022",https://arxiv.org/pdf/2009.11929
A narrowing of AI research?,Joel Klinger;Juan Mateos-Garcia;Konstantinos Stathoulopoulos,"The arrival of deep learning techniques able to infer patterns from large datasets has dramatically improved the performance of Artificial Intelligence (AI) systems. Deep learning's rapid development and adoption, in great part led by large technology companies, has however created concerns about a premature narrowing in the technological trajectory of AI research despite its weaknesses, which include lack of robustness, high environmental costs, and potentially unfair outcomes. We seek to improve the evidence base with a semantic analysis of AI research in arXiv, a popular pre-prints database. We study the evolution of the thematic diversity of AI research, compare the thematic diversity of AI research in academia and the private sector and measure the influence of private companies in AI research through the citations they receive and their collaborations with other institutions. Our results suggest that diversity in AI research has stagnated in recent years, and that AI research involving the private sector tends to be less diverse and more influential than research in academia. We also find that private sector AI researchers tend to specialise in data-hungry and computationally intensive deep learning methods at the expense of research involving other AI methods, research that considers the societal and ethical implications of AI, and applications in sectors like health. Our results provide a rationale for policy action to prevent a premature narrowing of AI research that could constrain its societal benefits, but we note the informational, incentive and scale hurdles standing in the way of such interventions. △ Less","11 January, 2022",https://arxiv.org/pdf/2009.10385
What is an intelligent system?,Martin Molina,"The concept of intelligent system has emerged in information technology as a type of system derived from successful applications of artificial intelligence. The goal of this paper is to give a general description of an intelligent system, which integrates previous approaches and takes into account recent advances in artificial intelligence. The paper describes an intelligent system in a generic way, identifying its main properties and functional components. The presented description follows a pragmatic approach to be used in an engineering context as a general framework to analyze and build intelligent systems. Its generality and its use is illustrated with real-world system examples and related with artificial intelligence methods. △ Less","18 December, 2022",https://arxiv.org/pdf/2009.09083
Quantifying Explainability of Saliency Methods in Deep Neural Networks with a Synthetic Dataset,Erico Tjoa;Cuntai Guan,"Post-hoc analysis is a popular category in eXplainable artificial intelligence (XAI) study. In particular, methods that generate heatmaps have been used to explain the deep neural network (DNN), a black-box model. Heatmaps can be appealing due to the intuitive and visual ways to understand them but assessing their qualities might not be straightforward. Different ways to assess heatmaps' quality have their own merits and shortcomings. This paper introduces a synthetic dataset that can be generated adhoc along with the ground-truth heatmaps for more objective quantitative assessment. Each sample data is an image of a cell with easily recognized features that are distinguished from localization ground-truth mask, hence facilitating a more transparent assessment of different XAI methods. Comparison and recommendations are made, shortcomings are clarified along with suggestions for future research directions to handle the finer details of select post-hoc analysis methods. Furthermore, mabCAM is introduced as the heatmap generation method compatible with our ground-truth heatmaps. The framework is easily generalizable and uses only standard deep learning components. △ Less","10 December, 2022",https://arxiv.org/pdf/2009.02899
Action and Perception as Divergence Minimization,Danijar Hafner;Pedro A. Ortega;Jimmy Ba;Thomas Parr;Karl Friston;Nicolas Heess,"To learn directed behaviors in complex environments, intelligent agents need to optimize objective functions. Various objectives are known for designing artificial agents, including task rewards and intrinsic motivation. However, it is unclear how the known objectives relate to each other, which objectives remain yet to be discovered, and which objectives better describe the behavior of humans. We introduce the Action Perception Divergence (APD), an approach for categorizing the space of possible objective functions for embodied agents. We show a spectrum that reaches from narrow to general objectives. While the narrow objectives correspond to domain-specific rewards as typical in reinforcement learning, the general objectives maximize information with the environment through latent variable models of input sequences. Intuitively, these agents use perception to align their beliefs with the world and use actions to align the world with their beliefs. They infer representations that are informative of past inputs, explore future inputs that are informative of their representations, and select actions or skills that maximally influence future inputs. This explains a wide range of unsupervised objectives from a single principle, including representation learning, information gain, empowerment, and skill discovery. Our findings suggest leveraging powerful world models for unsupervised exploration as a path toward highly adaptive agents that seek out large niches in their environments, rendering task rewards optional. △ Less","12 February, 2022",https://arxiv.org/pdf/2009.01791
Explainable Empirical Risk Minimization,L. Zhang;G. Karakasidis;A. Odnoblyudova;L. Dogruel;A. Jung,"The successful application of machine learning (ML) methods becomes increasingly dependent on their interpretability or explainability. Designing explainable ML systems is instrumental to ensuring transparency of automated decision-making that targets humans. The explainability of ML methods is also an essential ingredient for trustworthy artificial intelligence. A key challenge in ensuring explainability is its dependence on the specific human user (""explainee""). The users of machine learning methods might have vastly different background knowledge about machine learning principles. One user might have a university degree in machine learning or related fields, while another user might have never received formal training in high-school mathematics. This paper applies information-theoretic concepts to develop a novel measure for the subjective explainability of the predictions delivered by a ML method. We construct this measure via the conditional entropy of predictions, given a user feedback. The user feedback might be obtained from user surveys or biophysical measurements. Our main contribution is the explainable empirical risk minimization (EERM) principle of learning a hypothesis that optimally balances between the subjective explainability and risk. The EERM principle is flexible and can be combined with arbitrary machine learning models. We present several practical implementations of EERM for linear models and decision trees. Numerical experiments demonstrate the application of EERM to detecting the use of inappropriate language on social media. △ Less","1 July, 2022",https://arxiv.org/pdf/2009.01492
"On modularity in reactive control architectures, with an application to formal verification",Oliver Biggar;Mohammad Zamani;Iman Shames,"Modularity is a central principle throughout the design process for cyber-physical systems. Modularity reduces complexity and increases reuse of behavior. In this paper we pose and answer the following question: how can we identify independent `modules' within the structure of reactive control architectures? To this end, we propose a graph-structured control architecture we call a decision structure, and show how it generalises some reactive control architectures which are popular in Artificial Intelligence (AI) and robotics, specifically Teleo-Reactive programs (TRs), Decision Trees (DTs), Behavior Trees (BTs) and Generalised Behavior Trees (k-BTs). Inspired by the definition of a module in graph theory, we define modules in decision structures and show how each decision structure possesses a canonical decomposition into its modules. We can naturally characterise each of the BTs, k-BTs, DTs and TRs by properties of their module decomposition. This allows us to recognise which decision structures are equivalent to each of these architectures in quadratic time. Our proposed concept of modules extends to formal verification, under any verification scheme capable of verifying a decision structure. Namely, we prove that a modification to a module within a decision structure has no greater flow-on effects than a modification to an individual action within that structure. This enables verification on modules to be done locally and hierarchically, where structures can be verified and then repeatedly locally modified, with modules replaced by modules while preserving correctness. To illustrate the findings, we present an example of a solar-powered drone controlled by a decision structure. We use a Linear Temporal Logic-based verification scheme to verify the correctness of this structure, and then show how one can modify modules while preserving its correctness. △ Less","30 January, 2022",https://arxiv.org/pdf/2008.12515
A Survey of Knowledge-based Sequential Decision Making under Uncertainty,Shiqi Zhang;Mohan Sridharan,"Reasoning with declarative knowledge (RDK) and sequential decision-making (SDM) are two key research areas in artificial intelligence. RDK methods reason with declarative domain knowledge, including commonsense knowledge, that is either provided a priori or acquired over time, while SDM methods (probabilistic planning and reinforcement learning) seek to compute action policies that maximize the expected cumulative utility over a time horizon; both classes of methods reason in the presence of uncertainty. Despite the rich literature in these two areas, researchers have not fully explored their complementary strengths. In this paper, we survey algorithms that leverage RDK methods while making sequential decisions under uncertainty. We discuss significant developments, open problems, and directions for future work. △ Less","30 June, 2022",https://arxiv.org/pdf/2008.08548
Artificial Intelligence in the Battle against Coronavirus (COVID-19): A Survey and Future Research Directions,Thanh Thi Nguyen;Quoc Viet Hung Nguyen;Dung Tien Nguyen;Samuel Yang;Peter W. Eklund;Thien Huynh-The;Thanh Tam Nguyen;Quoc-Viet Pham;Imran Razzak;Edbert B. Hsu,"Artificial intelligence (AI) has been applied widely in our daily lives in a variety of ways with numerous success stories. AI has also contributed to dealing with the coronavirus disease (COVID-19) pandemic, which has been happening around the globe. This paper presents a survey of AI methods being used in various applications in the fight against the COVID-19 outbreak and outlines the crucial role of AI research in this unprecedented battle. We touch on areas where AI plays as an essential component, from medical image processing, data analytics, text mining and natural language processing, the Internet of Things, to computational biology and medicine. A summary of COVID-19 related data sources that are available for research purposes is also presented. Research directions on exploring the potential of AI and enhancing its capability and power in the pandemic battle are thoroughly discussed. We identify 13 groups of problems related to the COVID-19 pandemic and highlight promising AI methods and tools that can be used to address these problems. It is envisaged that this study will provide AI researchers and the wider community with an overview of the current status of AI applications, and motivate researchers to harness AI's potential in the fight against COVID-19. △ Less","16 March, 2022",https://arxiv.org/pdf/2008.07343
Self-organizing Democratized Learning: Towards Large-scale Distributed Learning Systems,Minh N. H. Nguyen;Shashi Raj Pandey;Tri Nguyen Dang;Eui-Nam Huh;Nguyen H. Tran;Walid Saad;Choong Seon Hong,"Emerging cross-device artificial intelligence (AI) applications require a transition from conventional centralized learning systems towards large-scale distributed AI systems that can collaboratively perform complex learning tasks. In this regard, democratized learning (Dem-AI) lays out a holistic philosophy with underlying principles for building large-scale distributed and democratized machine learning systems. The outlined principles are meant to study a generalization in distributed learning systems that goes beyond existing mechanisms such as federated learning. Moreover, such learning systems rely on hierarchical self-organization of well-connected distributed learning agents who have limited and highly personalized data and can evolve and regulate themselves based on the underlying duality of specialized and generalized processes. Inspired by Dem-AI philosophy, a novel distributed learning approach is proposed in this paper. The approach consists of a self-organizing hierarchical structuring mechanism based on agglomerative clustering, hierarchical generalization, and corresponding learning mechanism. Subsequently, hierarchical generalized learning problems in recursive forms are formulated and shown to be approximately solved using the solutions of distributed personalized learning problems and hierarchical update mechanisms. To that end, a distributed learning algorithm, namely DemLearn is proposed. Extensive experiments on benchmark MNIST, Fashion-MNIST, FE-MNIST, and CIFAR-10 datasets show that the proposed algorithms demonstrate better results in the generalization performance of learning models in agents compared to the conventional FL algorithms. The detailed analysis provides useful observations to further handle both the generalization and specialization performance of the learning models in Dem-AI systems. △ Less","27 April, 2022",https://arxiv.org/pdf/2007.03278
Multi-Winner Voting with Approval Preferences,Martin Lackner;Piotr Skowron,"Multi-winner voting is the process of selecting a fixed-size set of representative candidates based on voters' preferences. It occurs in applications ranging from politics (parliamentary elections) to the design of modern computer applications (collaborative filtering, dynamic Q&A platforms, diversifying search results). All these applications share the problem of identifying a representative subset of alternatives -- and the study of multi-winner voting is the principled analysis of this task. This book provides a thorough and in-depth look at multi-winner voting based on approval preferences. One speaks of approval preferences if voters express their preferences by providing a set of candidates they approve. Approval preferences thus separate candidates in approved and disapproved ones, a simple, binary classification. The corresponding multi-winner voting rules are called approval-based committee (ABC) rules. Due to the simplicity of approval preferences, ABC rules are widely suitable for practical use. Recent years have seen a rising interest in ABC voting. While multi-winner voting has been originally a topic studied by economists and political scientists, a significant share of recent progress has occurred in the field of computational social choice. This discipline is situated in the intersection of artificial intelligence, computer science, economics, and (to a lesser degree) political science, combining insights and methods from these distinct fields. The goal of this book is to present fundamental concepts and results for ABC voting and to discuss the recent advances in computational social choice. The main focus is on axiomatic analysis, algorithmic results, and relevant applications. △ Less","29 August, 2022",https://arxiv.org/pdf/2007.01795
"Hedging using reinforcement learning: Contextual k
-Armed Bandit versus Q
-learning",Loris Cannelli;Giuseppe Nuti;Marzio Sala;Oleg Szehr,"The construction of replication strategies for contingent claims in the presence of risk and market friction is a key problem of financial engineering. In real markets, continuous replication, such as in the model of Black, Scholes and Merton (BSM), is not only unrealistic but it is also undesirable due to high transaction costs. A variety of methods have been proposed to balance between effective replication and losses in the incomplete market setting. With the rise of Artificial Intelligence (AI), AI-based hedgers have attracted considerable interest, where particular attention was given to Recurrent Neural Network systems and variations of the Q-learning algorithm. From a practical point of view, sufficient samples for training such an AI can only be obtained from a simulator of the market environment. Yet if an agent was trained solely on simulated data, the run-time performance will primarily reflect the accuracy of the simulation, which leads to the classical problem of model choice and calibration. In this article, the hedging problem is viewed as an instance of a risk-averse contextual k-armed bandit problem, which is motivated by the simplicity and sample-efficiency of the architecture. This allows for realistic online model updates from real-world data. We find that the k-armed bandit model naturally fits to the Profit and Loss formulation of hedging, providing for a more accurate and sample efficient approach than Q-learning and reducing to the Black-Scholes model in the absence of transaction costs and risks. △ Less","6 February, 2022",https://arxiv.org/pdf/2007.01623
Model-based Reinforcement Learning: A Survey,Thomas M. Moerland;Joost Broekens;Aske Plaat;Catholijn M. Jonker,"Sequential decision making, commonly formalized as Markov Decision Process (MDP) optimization, is a important challenge in artificial intelligence. Two key approaches to this problem are reinforcement learning (RL) and planning. This paper presents a survey of the integration of both fields, better known as model-based reinforcement learning. Model-based RL has two main steps. First, we systematically cover approaches to dynamics model learning, including challenges like dealing with stochasticity, uncertainty, partial observability, and temporal abstraction. Second, we present a systematic categorization of planning-learning integration, including aspects like: where to start planning, what budgets to allocate to planning and real data collection, how to plan, and how to integrate planning in the learning and acting loop. After these two sections, we also discuss implicit model-based RL as an end-to-end alternative for model learning and planning, and we cover the potential benefits of model-based RL. Along the way, the survey also draws connections to several related RL fields, like hierarchical RL and transfer learning. Altogether, the survey presents a broad conceptual overview of the combination of planning and learning for MDP optimization. △ Less","31 March, 2022",https://arxiv.org/pdf/2006.16712
A Unifying Framework for Reinforcement Learning and Planning,Thomas M. Moerland;Joost Broekens;Aske Plaat;Catholijn M. Jonker,"Sequential decision making, commonly formalized as optimization of a Markov Decision Process, is a key challenge in artificial intelligence. Two successful approaches to MDP optimization are reinforcement learning and planning, which both largely have their own research communities. However, if both research fields solve the same problem, then we might be able to disentangle the common factors in their solution approaches. Therefore, this paper presents a unifying algorithmic framework for reinforcement learning and planning (FRAP), which identifies underlying dimensions on which MDP planning and learning algorithms have to decide. At the end of the paper, we compare a variety of well-known planning, model-free and model-based RL algorithms along these dimensions. Altogether, the framework may help provide deeper insight in the algorithmic design space of planning and reinforcement learning. △ Less","31 March, 2022",https://arxiv.org/pdf/2006.15009
Deep Learning Based Single Sample Per Person Face Recognition: A Survey,Fan Liu;Delong Chen;Fei Wang;Zewen Li;Feng Xu,"Face recognition has long been an active research area in the field of artificial intelligence, particularly since the rise of deep learning in recent years. In some practical situations, each identity has only a single sample available for training. Face recognition under this situation is referred to as single sample face recognition and poses significant challenges to the effective training of deep models. Therefore, in recent years, researchers have attempted to unleash more potential of deep learning and improve the model recognition performance in the single sample situation. While several comprehensive surveys have been conducted on traditional single sample face recognition approaches, emerging deep learning based methods are rarely involved in these reviews. Accordingly, we focus on the deep learning-based methods in this paper, classifying them into virtual sample methods and generic learning methods. In the former category, virtual images or virtual features are generated to benefit the training of the deep model. In the latter one, additional multi-sample generic sets are used. There are three types of generic learning methods: combining traditional methods and deep features, improving the loss function, and improving network structure, all of which are covered in our analysis. Moreover, we review face datasets that have been commonly used for evaluating single sample face recognition models and go on to compare the results of different types of models. Additionally, we discuss problems with existing single sample face recognition methods, including identity information preservation in virtual sample methods, domain adaption in generic learning methods. Furthermore, we regard developing unsupervised methods is a promising future direction, and point out that the semantic gap as an important issue that needs to be further considered. △ Less","10 August, 2022",https://arxiv.org/pdf/2006.11395
Online Learning in Iterated Prisoner's Dilemma to Mimic Human Behavior,Baihan Lin;Djallel Bouneffouf;Guillermo Cecchi,"As an important psychological and social experiment, the Iterated Prisoner's Dilemma (IPD) treats the choice to cooperate or defect as an atomic action. We propose to study the behaviors of online learning algorithms in the Iterated Prisoner's Dilemma (IPD) game, where we investigate the full spectrum of reinforcement learning agents: multi-armed bandits, contextual bandits and reinforcement learning. We evaluate them based on a tournament of iterated prisoner's dilemma where multiple agents can compete in a sequential fashion. This allows us to analyze the dynamics of policies learned by multiple self-interested independent reward-driven agents, and also allows us study the capacity of these algorithms to fit the human behaviors. Results suggest that considering the current situation to make decision is the worst in this kind of social dilemma game. Multiples discoveries on online learning behaviors and clinical validations are stated, as an effort to connect artificial intelligence algorithms with human behaviors and their abnormal states in neuropsychiatric conditions. △ Less","26 August, 2022",https://arxiv.org/pdf/2006.06580
Learning Monotone Dynamics by Neural Networks,Yu Wang;Qitong Gao;Miroslav Pajic,"Feed-forward neural networks (FNNs) work as standard building blocks in applying artificial intelligence (AI) to the physical world. They allow learning the dynamics of unknown physical systems (e.g., biological and chemical) {to predict their future behavior}. However, they are likely to violate the physical constraints of those systems without proper treatment. This work focuses on imposing two important physical constraints: monotonicity (i.e., a partial order of system states is preserved over time) and stability (i.e., the system states converge over time) when using FNNs to learn physical dynamics. For monotonicity constraints, we propose to use nonnegative neural networks and batch normalization. For both monotonicity and stability constraints, we propose to learn the system dynamics and corresponding Lyapunov function simultaneously. As demonstrated by case studies, our methods can preserve the stability and monotonicity of FNNs and significantly reduce their prediction errors. △ Less","21 June, 2022",https://arxiv.org/pdf/2006.06417
AI from concrete to abstract: demystifying artificial intelligence to the general public,Rubens Lacerda Queiroz;Fábio Ferrentini Sampaio;Cabral Lima;Priscila Machado Vieira Lima,"Artificial Intelligence (AI) has been adopted in a wide range of domains. This shows the imperative need to develop means to endow common people with a minimum understanding of what AI means. Combining visual programming and WiSARD weightless artificial neural networks, this article presents a new methodology, AI from concrete to abstract (AIcon2abs), to enable general people (including children) to achieve this goal. The main strategy adopted by is to promote a demystification of artificial intelligence via practical activities related to the development of learning machines, as well as through the observation of their learning process. Thus, it is possible to provide subjects with skills that contributes to making them insightful actors in debates and decisions involving the adoption of artificial intelligence mechanisms. Currently, existing approaches to the teaching of basic AI concepts through programming treat machine intelligence as an external element/module. After being trained, that external module is coupled to the main application being developed by the learners. In the methodology herein presented, both training and classification tasks are blocks that compose the main program, just as the other programming constructs. As a beneficial side effect of AIcon2abs, the difference between a program capable of learning from data and a conventional computer program becomes more evident. In addition, the simplicity of the WiSARD weightless artificial neural network model enables easy visualization and understanding of training and classification tasks internal realization. △ Less","12 June, 2022",https://arxiv.org/pdf/2006.04013
Explainable Goal-Driven Agents and Robots -- A Comprehensive Review,Fatai Sado;Chu Kiong Loo;Wei Shiung Liew;Matthias Kerzel;Stefan Wermter,"Recent applications of autonomous agents and robots, such as self-driving cars, scenario-based trainers, exploration robots, and service robots have brought attention to crucial trust-related challenges associated with the current generation of artificial intelligence (AI) systems. AI systems based on the connectionist deep learning neural network approach lack capabilities of explaining their decisions and actions to others, despite their great successes. Without symbolic interpretation capabilities, they are black boxes, which renders their decisions or actions opaque, making it difficult to trust them in safety-critical applications. The recent stance on the explainability of AI systems has witnessed several approaches on eXplainable Artificial Intelligence (XAI); however, most of the studies have focused on data-driven XAI systems applied in computational sciences. Studies addressing the increasingly pervasive goal-driven agents and robots are still missing. This paper reviews approaches on explainable goal-driven intelligent agents and robots, focusing on techniques for explaining and communicating agents perceptual functions (example, senses, and vision) and cognitive reasoning (example, beliefs, desires, intention, plans, and goals) with humans in the loop. The review highlights key strategies that emphasize transparency, understandability, and continual learning for explainability. Finally, the paper presents requirements for explainability and suggests a roadmap for the possible realization of effective goal-driven explainable agents and robots. △ Less","23 September, 2022",https://arxiv.org/pdf/2004.09705
Hcore-Init: Neural Network Initialization based on Graph Degeneracy,Stratis Limnios;George Dasoulas;Dimitrios M. Thilikos;Michalis Vazirgiannis,"Neural networks are the pinnacle of Artificial Intelligence, as in recent years we witnessed many novel architectures, learning and optimization techniques for deep learning. Capitalizing on the fact that neural networks inherently constitute multipartite graphs among neuron layers, we aim to analyze directly their structure to extract meaningful information that can improve the learning process. To our knowledge graph mining techniques for enhancing learning in neural networks have not been thoroughly investigated. In this paper we propose an adapted version of the k-core structure for the complete weighted multipartite graph extracted from a deep learning architecture. As a multipartite graph is a combination of bipartite graphs, that are in turn the incidence graphs of hypergraphs, we design k-hypercore decomposition, the hypergraph analogue of k-core degeneracy. We applied k-hypercore to several neural network architectures, more specifically to convolutional neural networks and multilayer perceptrons for image recognition tasks after a very short pretraining. Then we used the information provided by the hypercore numbers of the neurons to re-initialize the weights of the neural network, thus biasing the gradient optimization scheme. Extensive experiments proved that k-hypercore outperforms the state-of-the-art initialization methods. △ Less","9 September, 2022",https://arxiv.org/pdf/2004.07636
SenseCare: A Research Platform for Medical Image Informatics and Interactive 3D Visualization,Qi Duan;Guotai Wang;Rui Wang;Chao Fu;Xinjun Li;Na Wang;Yechong Huang;Xiaodi Huang;Tao Song;Liang Zhao;Xinglong Liu;Qing Xia;Zhiqiang Hu;Yinan Chen;Shaoting Zhang,"Clinical research on smart health has an increasing demand for intelligent and clinic-oriented medical image computing algorithms and platforms that support various applications. To this end, we have developed SenseCare research platform, which is designed to facilitate translational research on intelligent diagnosis and treatment planning in various clinical scenarios. To enable clinical research with Artificial Intelligence (AI), SenseCare provides a range of AI toolkits for different tasks, including image segmentation, registration, lesion and landmark detection from various image modalities ranging from radiology to pathology. In addition, SenseCare is clinic-oriented and supports a wide range of clinical applications such as diagnosis and surgical planning for lung cancer, pelvic tumor, coronary artery disease, etc. SenseCare provides several appealing functions and features such as advanced 3D visualization, concurrent and efficient web-based access, fast data synchronization and high data security, multi-center deployment, support for collaborative research, etc. In this report, we present an overview of SenseCare as an efficient platform providing comprehensive toolkits and high extensibility for intelligent image analysis and clinical research in different application scenarios. We also summarize the research outcome through the collaboration with multiple hospitals. △ Less","2 September, 2022",https://arxiv.org/pdf/2004.07031
Grasping and Manipulation with a Multi-Fingered Hand,Claudio Zito,"This thesis is concerned with deriving planning algorithms for robot manipulators. Manipulation has two effects, the robot has a physical effect on the object, and it also acquires information about the object. This thesis presents algorithms that treat both problems. First, I present an extension of the well-known piano mover's problem where a robot pushing an object must plan its movements as well as those of the object. This requires simultaneous planning in the joint space of the robot and the configuration space of the object, in contrast to the original problem which only requires planning in the latter space. The effects of a robot action on the object configuration are determined by the non-invertible rigid body mechanics. Second, I consider planning under uncertainty and in particular planning for information effects. I consider the case where a robot has to reach and grasp an object under pose uncertainty caused by shape incompleteness. The approach presented in this report is to study and possibly extend a new approach to artificial intelligence (A.I.) which has emerged in the last years in response to the necessity of building intelligent controllers for agents operating in unstructured stochastic environments. Such agents require the ability to learn by interaction with its environment an optimal action-selection behaviour. The main issue is that real-world problems are usually dynamic and unpredictable. Thus, the agent needs to update constantly its current image of the world using its sensors, which provide only a noisy description of the surrounding environment. Although there are different schools of thinking, with their own set of techniques, a brand new direction which unifies many A.I. researches is to formalise such agent/environment interactions as embedded systems with stochastic dynamics. △ Less","19 January, 2022",https://arxiv.org/pdf/2002.03306
The Risk to Population Health Equity Posed by Automated Decision Systems: A Narrative Review,Mitchell Burger,"Artificial intelligence is already ubiquitous, and is increasingly being used to autonomously make ever more consequential decisions. However, there has been relatively little research into the existing and possible consequences for population health equity. A narrative review was undertaken using a hermeneutic approach to explore current and future uses of narrow AI and automated decision systems (ADS) in medicine and public health, issues that have emerged, and implications for equity. Accounts reveal a tremendous expectation on AI to transform medical and public health practices. Prominent demonstrations of AI capability - particularly in diagnostic decision making, risk prediction, and surveillance - are stimulating rapid adoption, spurred by COVID-19. Automated decisions being made have significant consequences for individual and population health and wellbeing. Meanwhile, it is evident that hazards including bias, incontestability, and privacy erosion have emerged in sensitive domains such as criminal justice where narrow AI and ADS are in common use. Reports of issues arising from their use in health are already appearing. As the use of ADS in health expands, it is probable that these hazards will manifest more widely. Bias, incontestability, and privacy erosion give rise to mechanisms by which existing social, economic and health disparities are perpetuated and amplified. Consequently, there is a significant risk that use of ADS in health will exacerbate existing population health inequities. The industrial scale and rapidity with which ADS can be applied heightens the risk to population health equity. It is incumbent on health practitioners and policy makers therefore to explore the potential implications of using ADS, to ensure the use of artificial intelligence promotes population health and equity. △ Less","20 January, 2022",https://arxiv.org/pdf/2001.06615
Modeling Historical AIS Data For Vessel Path Prediction: A Comprehensive Treatment,Enmei Tu;Guanghao Zhang;Shangbo Mao;Lily Rachmawati;Guang-Bin Huang,"The prosperity of artificial intelligence has aroused intensive interests in intelligent/autonomous navigation, in which path prediction is a key functionality for decision supports, e.g. route planning, collision warning, and traffic regulation. For maritime intelligence, Automatic Identification System (AIS) plays an important role because it recently has been made compulsory for large international commercial vessels and is able to provide nearly real-time information of the vessel. Therefore AIS data based vessel path prediction is a promising way in future maritime intelligence. However, real-world AIS data collected online are just highly irregular trajectory segments (AIS message sequences) from different types of vessels and geographical regions, with possibly very low data quality. So even there are some works studying how to build a path prediction model using historical AIS data, but still, it is a very challenging problem. In this paper, we propose a comprehensive framework to model massive historical AIS trajectory segments for accurate vessel path prediction. Experimental comparisons with existing popular methods are made to validate the proposed approach and results show that our approach could outperform the baseline methods by a wide margin. △ Less","9 January, 2022",https://arxiv.org/pdf/2001.01592
FANN-on-MCU: An Open-Source Toolkit for Energy-Efficient Neural Network Inference at the Edge of the Internet of Things,Xiaying Wang;Michele Magno;Lukas Cavigelli;Luca Benini,"The growing number of low-power smart devices in the Internet of Things is coupled with the concept of ""Edge Computing"", that is moving some of the intelligence, especially machine learning, towards the edge of the network. Enabling machine learning algorithms to run on resource-constrained hardware, typically on low-power smart devices, is challenging in terms of hardware (optimized and energy-efficient integrated circuits), algorithmic and firmware implementations. This paper presents FANN-on-MCU, an open-source toolkit built upon the Fast Artificial Neural Network (FANN) library to run lightweight and energy-efficient neural networks on microcontrollers based on both the ARM Cortex-M series and the novel RISC-V-based Parallel Ultra-Low-Power (PULP) platform. The toolkit takes multi-layer perceptrons trained with FANN and generates code targeted at execution on low-power microcontrollers either with a floating-point unit (i.e., ARM Cortex-M4F and M7F) or without (i.e., ARM Cortex M0-M3 or PULP-based processors). This paper also provides an architectural performance evaluation of neural networks on the most popular ARM Cortex-M family and the parallel RISC-V processor called Mr. Wolf. The evaluation includes experimental results for three different applications using a self-sustainable wearable multi-sensor bracelet. Experimental results show a measured latency in the order of only a few microseconds and a power consumption of few milliwatts while keeping the memory requirements below the limitations of the targeted microcontrollers. In particular, the parallel implementation on the octa-core RISC-V platform reaches a speedup of 22x and a 69% reduction in energy consumption with respect to a single-core implementation on Cortex-M4 for continuous real-time classification. △ Less","17 February, 2022",https://arxiv.org/pdf/1911.03314
"FAT Forensics: A Python Toolbox for Algorithmic Fairness, Accountability and Transparency",Kacper Sokol;Raul Santos-Rodriguez;Peter Flach,"Today, artificial intelligence systems driven by machine learning algorithms can be in a position to take important, and sometimes legally binding, decisions about our everyday lives. In many cases, however, these systems and their actions are neither regulated nor certified. To help counter the potential harm that such algorithms can cause we developed an open source toolbox that can analyse selected fairness, accountability and transparency aspects of the machine learning process: data (and their features), models and predictions, allowing to automatically and objectively report them to relevant stakeholders. In this paper we describe the design, scope, usage and impact of this Python package, which is published under the 3-Clause BSD open source licence. △ Less","25 August, 2022",https://arxiv.org/pdf/1909.05167
Defining the scope of AI regulations,Jonas Schuett,"The paper argues that the material scope of AI regulations should not rely on the term ""artificial intelligence (AI)"". The argument is developed by proposing a number of requirements for legal definitions, surveying existing AI definitions, and then discussing the extent to which they meet the proposed requirements. It is shown that existing definitions of AI do not meet the most important requirements for legal definitions. Next, the paper argues that a risk-based approach would be preferable. Rather than using the term AI, policy makers should focus on the specific risks they want to reduce. It is shown that the requirements for legal definitions can be better met by defining the main sources of relevant risks: certain technical approaches (e.g. reinforcement learning), applications (e.g. facial recognition), and capabilities (e.g. the ability to physically interact with the environment). Finally, the paper discusses the extent to which this approach can also be applied to more advanced AI systems. △ Less","20 November, 2022",https://arxiv.org/pdf/1909.01095
MedGCN: Medication recommendation and lab test imputation via graph convolutional networks,Chengsheng Mao;Liang Yao;Yuan Luo,"Laboratory testing and medication prescription are two of the most important routines in daily clinical practice. Developing an artificial intelligence system that can automatically make lab test imputations and medication recommendations can save costs on potentially redundant lab tests and inform physicians of a more effective prescription. We present an intelligent medical system (named MedGCN) that can automatically recommend the patients' medications based on their incomplete lab tests, and can even accurately estimate the lab values that have not been taken. In our system, we integrate the complex relations between multiple types of medical entities with their inherent features in a heterogeneous graph. Then we model the graph to learn a distributed representation for each entity in the graph based on graph convolutional networks (GCN). By the propagation of graph convolutional networks, the entity representations can incorporate multiple types of medical information that can benefit multiple medical tasks. Moreover, we introduce a cross regularization strategy to reduce overfitting for multi-task training by the interaction between the multiple tasks. In this study, we construct a graph to associate 4 types of medical entities, i.e., patients, encounters, lab tests, and medications, and applied a graph neural network to learn node embeddings for medication recommendation and lab test imputation. we validate our MedGCN model on two real-world datasets: NMEDW and MIMIC-III. The experimental results on both datasets demonstrate that our model can outperform the state-of-the-art in both tasks. We believe that our innovative system can provide a promising and reliable way to assist physicians to make medication prescriptions and to save costs on potentially redundant lab tests. △ Less","3 February, 2022",https://arxiv.org/pdf/1904.00326
A General End-to-end Diagnosis Framework for Manufacturing Systems,Ye Yuan;Guijun Ma;Cheng Cheng;Beitong Zhou;Huan Zhao;Hai-Tao Zhang;Han Ding,"The manufacturing sector is envisioned to be heavily influenced by artificial intelligence-based technologies with the extraordinary increases in computational power and data volumes. A central challenge in manufacturing sector lies in the requirement of a general framework to ensure satisfied diagnosis and monitoring performances in different manufacturing applications. Here we propose a general data-driven, end-to-end framework for the monitoring of manufacturing systems. This framework, derived from deep learning techniques, evaluates fused sensory measurements to detect and even predict faults and wearing conditions. This work exploits the predictive power of deep learning to automatically extract hidden degradation features from noisy, time-course data. We have experimented the proposed framework on ten representative datasets drawn from a wide variety of manufacturing applications. Results reveal that the framework performs well in examined benchmark applications and can be applied in diverse contexts, indicating its potential use as a critical corner stone in smart manufacturing. △ Less","30 August, 2022",https://arxiv.org/pdf/1901.02057
Applying Data Augmentation to Handwritten Arabic Numeral Recognition Using Deep Learning Neural Networks,Akm Ashiquzzaman;Abdul Kawsar Tushar;Md Ashiqur Rahman,"Handwritten character recognition has been the center of research and a benchmark problem in the sector of pattern recognition and artificial intelligence, and it continues to be a challenging research topic. Due to its enormous application many works have been done in this field focusing on different languages. Arabic, being a diversified language has a huge scope of research with potential challenges. A convolutional neural network model for recognizing handwritten numerals in Arabic language is proposed in this paper, where the dataset is subject to various augmentation in order to add robustness needed for deep learning approach. The proposed method is empowered by the presence of dropout regularization to do away with the problem of data overfitting. Moreover, suitable change is introduced in activation function to overcome the problem of vanishing gradient. With these modifications, the proposed system achieves an accuracy of 99.4\% which performs better than every previous work on the dataset. △ Less","8 September, 2022",https://arxiv.org/pdf/1708.05969
Approximations of Algorithmic and Structural Complexity Validate Cognitive-behavioural Experimental Results,Hector Zenil;James A. R. Marshall;Jesper Tegnér,"Being able to objectively characterise the intrinsic complexity of behavioural patterns resulting from human or animal decisions is fundamental for deconvolving cognition and designing autonomous artificial intelligence systems. Yet complexity is difficult in practice, particularly when strings are short. By numerically approximating algorithmic (Kolmogorov) complexity (K), we establish an objective tool to characterise behavioural complexity. Next, we approximate structural (Bennett's Logical Depth) complexity (LD) to assess the amount of computation required for generating a behavioural string. We apply our toolbox to three landmark studies of animal behaviour of increasing sophistication and degree of environmental influence, including studies of foraging communication by ants, flight patterns of fruit flies, and tactical deception and competition (e.g., predator-prey) strategies. We find that ants harness the environmental condition in their internal decision process, modulating their behavioural complexity accordingly. Our analysis of flight (fruit flies) invalidated the common hypothesis that animals navigating in an environment devoid of stimuli adopt a random strategy. Fruit flies exposed to a featureless environment deviated the most from Levy flight, suggesting an algorithmic bias in their attempt to devise a useful (navigation) strategy. Similarly, a logical depth analysis of rats revealed that the structural complexity of the rat always ends up matching the structural complexity of the competitor, with the rats' behaviour simulating algorithmic randomness. Finally, we discuss how experiments on how humans perceive randomness suggest the existence of an algorithmic bias in our reasoning and decision processes, in line with our analysis of the animal experiments. △ Less","20 December, 2022",https://arxiv.org/pdf/1509.06338
